[
    {
        "id": 0,
        "seek": 0,
        "start": 0.0,
        "end": 5.519999980926514,
        "text": " so it took me a lot of time working with this myself and just visualizing things and trying",
        "tokens": [
            50364,
            370,
            309,
            1890,
            385,
            257,
            688,
            295,
            565,
            1364,
            365,
            341,
            2059,
            293,
            445,
            5056,
            3319,
            721,
            293,
            1382,
            50640
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19304411113262177,
        "compression_ratio": 1.6349693536758423,
        "no_speech_prob": 0.004829134326428175
    },
    {
        "id": 1,
        "seek": 0,
        "start": 5.519999980926514,
        "end": 10.0,
        "text": " to really understand what is happening here because the documentation unfortunately is,",
        "tokens": [
            50640,
            281,
            534,
            1223,
            437,
            307,
            2737,
            510,
            570,
            264,
            14333,
            7015,
            307,
            11,
            50864
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19304411113262177,
        "compression_ratio": 1.6349693536758423,
        "no_speech_prob": 0.004829134326428175
    },
    {
        "id": 2,
        "seek": 0,
        "start": 10.0,
        "end": 15.600000381469727,
        "text": " in my opinion, not super amazing. But it is a very nice repo that is available to you if you'd",
        "tokens": [
            50864,
            294,
            452,
            4800,
            11,
            406,
            1687,
            2243,
            13,
            583,
            309,
            307,
            257,
            588,
            1481,
            49040,
            300,
            307,
            2435,
            281,
            291,
            498,
            291,
            1116,
            51144
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19304411113262177,
        "compression_ratio": 1.6349693536758423,
        "no_speech_prob": 0.004829134326428175
    },
    {
        "id": 3,
        "seek": 0,
        "start": 15.600000381469727,
        "end": 19.360000610351562,
        "text": " like to train your own tokenizer right now. Okay, let me now switch gears again as we're",
        "tokens": [
            51144,
            411,
            281,
            3847,
            428,
            1065,
            14862,
            6545,
            558,
            586,
            13,
            1033,
            11,
            718,
            385,
            586,
            3679,
            20915,
            797,
            382,
            321,
            434,
            51332
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19304411113262177,
        "compression_ratio": 1.6349693536758423,
        "no_speech_prob": 0.004829134326428175
    },
    {
        "id": 4,
        "seek": 0,
        "start": 19.360000610351562,
        "end": 23.520000457763672,
        "text": " starting to slowly wrap up here. I want to revisit this issue in a bit more detail of",
        "tokens": [
            51332,
            2891,
            281,
            5692,
            7019,
            493,
            510,
            13,
            286,
            528,
            281,
            32676,
            341,
            2734,
            294,
            257,
            857,
            544,
            2607,
            295,
            51540
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19304411113262177,
        "compression_ratio": 1.6349693536758423,
        "no_speech_prob": 0.004829134326428175
    },
    {
        "id": 5,
        "seek": 0,
        "start": 23.520000457763672,
        "end": 26.8799991607666,
        "text": " how we should set the vocab size and what are some of the considerations around it.",
        "tokens": [
            51540,
            577,
            321,
            820,
            992,
            264,
            2329,
            455,
            2744,
            293,
            437,
            366,
            512,
            295,
            264,
            24070,
            926,
            309,
            13,
            51708
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19304411113262177,
        "compression_ratio": 1.6349693536758423,
        "no_speech_prob": 0.004829134326428175
    },
    {
        "id": 6,
        "seek": 2688,
        "start": 27.68000030517578,
        "end": 32.560001373291016,
        "text": " So for this, I'd like to go back to the model architecture that we developed in the last video",
        "tokens": [
            50404,
            407,
            337,
            341,
            11,
            286,
            1116,
            411,
            281,
            352,
            646,
            281,
            264,
            2316,
            9482,
            300,
            321,
            4743,
            294,
            264,
            1036,
            960,
            50648
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1901041716337204,
        "compression_ratio": 1.725165605545044,
        "no_speech_prob": 0.02096235565841198
    },
    {
        "id": 7,
        "seek": 2688,
        "start": 32.560001373291016,
        "end": 38.47999954223633,
        "text": " when we built the GPT from scratch. So this here was the file that we built in the previous video",
        "tokens": [
            50648,
            562,
            321,
            3094,
            264,
            26039,
            51,
            490,
            8459,
            13,
            407,
            341,
            510,
            390,
            264,
            3991,
            300,
            321,
            3094,
            294,
            264,
            3894,
            960,
            50944
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1901041716337204,
        "compression_ratio": 1.725165605545044,
        "no_speech_prob": 0.02096235565841198
    },
    {
        "id": 8,
        "seek": 2688,
        "start": 38.47999954223633,
        "end": 42.400001525878906,
        "text": " and we defined the transformer model. And let's specifically look at vocab size and",
        "tokens": [
            50944,
            293,
            321,
            7642,
            264,
            31782,
            2316,
            13,
            400,
            718,
            311,
            4682,
            574,
            412,
            2329,
            455,
            2744,
            293,
            51140
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1901041716337204,
        "compression_ratio": 1.725165605545044,
        "no_speech_prob": 0.02096235565841198
    },
    {
        "id": 9,
        "seek": 2688,
        "start": 42.400001525878906,
        "end": 47.599998474121094,
        "text": " where it appears in this file. So here we define the vocab size. At this time,",
        "tokens": [
            51140,
            689,
            309,
            7038,
            294,
            341,
            3991,
            13,
            407,
            510,
            321,
            6964,
            264,
            2329,
            455,
            2744,
            13,
            1711,
            341,
            565,
            11,
            51400
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1901041716337204,
        "compression_ratio": 1.725165605545044,
        "no_speech_prob": 0.02096235565841198
    },
    {
        "id": 10,
        "seek": 2688,
        "start": 47.599998474121094,
        "end": 51.7599983215332,
        "text": " it was 65 or something like that, extremely small number. So this will grow much larger.",
        "tokens": [
            51400,
            309,
            390,
            11624,
            420,
            746,
            411,
            300,
            11,
            4664,
            1359,
            1230,
            13,
            407,
            341,
            486,
            1852,
            709,
            4833,
            13,
            51608
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1901041716337204,
        "compression_ratio": 1.725165605545044,
        "no_speech_prob": 0.02096235565841198
    },
    {
        "id": 11,
        "seek": 2688,
        "start": 52.63999938964844,
        "end": 55.68000030517578,
        "text": " You'll see that vocab size doesn't come up too much in most of these layers.",
        "tokens": [
            51652,
            509,
            603,
            536,
            300,
            2329,
            455,
            2744,
            1177,
            380,
            808,
            493,
            886,
            709,
            294,
            881,
            295,
            613,
            7914,
            13,
            51804
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1901041716337204,
        "compression_ratio": 1.725165605545044,
        "no_speech_prob": 0.02096235565841198
    },
    {
        "id": 12,
        "seek": 5568,
        "start": 55.68000030517578,
        "end": 59.599998474121094,
        "text": " The only place that it comes up to is in exactly these two places here.",
        "tokens": [
            50364,
            440,
            787,
            1081,
            300,
            309,
            1487,
            493,
            281,
            307,
            294,
            2293,
            613,
            732,
            3190,
            510,
            13,
            50560
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17097108066082,
        "compression_ratio": 1.765734314918518,
        "no_speech_prob": 6.92217581672594e-05
    },
    {
        "id": 13,
        "seek": 5568,
        "start": 60.47999954223633,
        "end": 64.16000366210938,
        "text": " So when we define the language model, there's the token embedding table,",
        "tokens": [
            50604,
            407,
            562,
            321,
            6964,
            264,
            2856,
            2316,
            11,
            456,
            311,
            264,
            14862,
            12240,
            3584,
            3199,
            11,
            50788
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17097108066082,
        "compression_ratio": 1.765734314918518,
        "no_speech_prob": 6.92217581672594e-05
    },
    {
        "id": 14,
        "seek": 5568,
        "start": 64.80000305175781,
        "end": 69.19999694824219,
        "text": " which is this two-dimensional array where the vocab size is basically the number of rows.",
        "tokens": [
            50820,
            597,
            307,
            341,
            732,
            12,
            18759,
            10225,
            689,
            264,
            2329,
            455,
            2744,
            307,
            1936,
            264,
            1230,
            295,
            13241,
            13,
            51040
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17097108066082,
        "compression_ratio": 1.765734314918518,
        "no_speech_prob": 6.92217581672594e-05
    },
    {
        "id": 15,
        "seek": 5568,
        "start": 70.16000366210938,
        "end": 75.91999816894531,
        "text": " And each vocabulary element, each token has a vector that we're going to train using back",
        "tokens": [
            51088,
            400,
            1184,
            19864,
            4478,
            11,
            1184,
            14862,
            575,
            257,
            8062,
            300,
            321,
            434,
            516,
            281,
            3847,
            1228,
            646,
            51376
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17097108066082,
        "compression_ratio": 1.765734314918518,
        "no_speech_prob": 6.92217581672594e-05
    },
    {
        "id": 16,
        "seek": 5568,
        "start": 75.91999816894531,
        "end": 80.16000366210938,
        "text": " propagation. That vector is of size and embed, which is number of channels in the transformer.",
        "tokens": [
            51376,
            38377,
            13,
            663,
            8062,
            307,
            295,
            2744,
            293,
            12240,
            11,
            597,
            307,
            1230,
            295,
            9235,
            294,
            264,
            31782,
            13,
            51588
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17097108066082,
        "compression_ratio": 1.765734314918518,
        "no_speech_prob": 6.92217581672594e-05
    },
    {
        "id": 17,
        "seek": 5568,
        "start": 80.95999908447266,
        "end": 85.27999877929688,
        "text": " And basically, as vocab size increases, this embedding table, as I mentioned earlier,",
        "tokens": [
            51628,
            400,
            1936,
            11,
            382,
            2329,
            455,
            2744,
            8637,
            11,
            341,
            12240,
            3584,
            3199,
            11,
            382,
            286,
            2835,
            3071,
            11,
            51844
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17097108066082,
        "compression_ratio": 1.765734314918518,
        "no_speech_prob": 6.92217581672594e-05
    },
    {
        "id": 18,
        "seek": 8528,
        "start": 85.27999877929688,
        "end": 87.19999694824219,
        "text": " is going to also grow. We're going to be adding rows.",
        "tokens": [
            50364,
            307,
            516,
            281,
            611,
            1852,
            13,
            492,
            434,
            516,
            281,
            312,
            5127,
            13241,
            13,
            50460
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18627740442752838,
        "compression_ratio": 1.8075472116470337,
        "no_speech_prob": 9.610201959731057e-05
    },
    {
        "id": 19,
        "seek": 8528,
        "start": 88.16000366210938,
        "end": 92.0,
        "text": " In addition to that, at the end of the transformer, there's this LM head layer,",
        "tokens": [
            50508,
            682,
            4500,
            281,
            300,
            11,
            412,
            264,
            917,
            295,
            264,
            31782,
            11,
            456,
            311,
            341,
            46529,
            1378,
            4583,
            11,
            50700
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18627740442752838,
        "compression_ratio": 1.8075472116470337,
        "no_speech_prob": 9.610201959731057e-05
    },
    {
        "id": 20,
        "seek": 8528,
        "start": 92.0,
        "end": 96.63999938964844,
        "text": " which is a linear layer. And you'll notice that that layer is used at the very end to",
        "tokens": [
            50700,
            597,
            307,
            257,
            8213,
            4583,
            13,
            400,
            291,
            603,
            3449,
            300,
            300,
            4583,
            307,
            1143,
            412,
            264,
            588,
            917,
            281,
            50932
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18627740442752838,
        "compression_ratio": 1.8075472116470337,
        "no_speech_prob": 9.610201959731057e-05
    },
    {
        "id": 21,
        "seek": 8528,
        "start": 96.63999938964844,
        "end": 101.04000091552734,
        "text": " produce the logits, which become the probabilities for the next token in a sequence.",
        "tokens": [
            50932,
            5258,
            264,
            3565,
            1208,
            11,
            597,
            1813,
            264,
            33783,
            337,
            264,
            958,
            14862,
            294,
            257,
            8310,
            13,
            51152
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18627740442752838,
        "compression_ratio": 1.8075472116470337,
        "no_speech_prob": 9.610201959731057e-05
    },
    {
        "id": 22,
        "seek": 8528,
        "start": 101.04000091552734,
        "end": 106.08000183105469,
        "text": " And so intuitively, we're trying to produce a probability for every single token that might",
        "tokens": [
            51152,
            400,
            370,
            46506,
            11,
            321,
            434,
            1382,
            281,
            5258,
            257,
            8482,
            337,
            633,
            2167,
            14862,
            300,
            1062,
            51404
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18627740442752838,
        "compression_ratio": 1.8075472116470337,
        "no_speech_prob": 9.610201959731057e-05
    },
    {
        "id": 23,
        "seek": 8528,
        "start": 106.08000183105469,
        "end": 110.95999908447266,
        "text": " come next at every point in time of that transformer. And if we have more and more",
        "tokens": [
            51404,
            808,
            958,
            412,
            633,
            935,
            294,
            565,
            295,
            300,
            31782,
            13,
            400,
            498,
            321,
            362,
            544,
            293,
            544,
            51648
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18627740442752838,
        "compression_ratio": 1.8075472116470337,
        "no_speech_prob": 9.610201959731057e-05
    },
    {
        "id": 24,
        "seek": 11096,
        "start": 110.95999908447266,
        "end": 115.19999694824219,
        "text": " tokens, we need to produce more and more probabilities. So every single token is going",
        "tokens": [
            50364,
            22667,
            11,
            321,
            643,
            281,
            5258,
            544,
            293,
            544,
            33783,
            13,
            407,
            633,
            2167,
            14862,
            307,
            516,
            50576
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17115935683250427,
        "compression_ratio": 1.878787875175476,
        "no_speech_prob": 0.002396696014329791
    },
    {
        "id": 25,
        "seek": 11096,
        "start": 115.19999694824219,
        "end": 119.83999633789062,
        "text": " to introduce an additional dot product that we have to do here in this linear layer for this",
        "tokens": [
            50576,
            281,
            5366,
            364,
            4497,
            5893,
            1674,
            300,
            321,
            362,
            281,
            360,
            510,
            294,
            341,
            8213,
            4583,
            337,
            341,
            50808
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17115935683250427,
        "compression_ratio": 1.878787875175476,
        "no_speech_prob": 0.002396696014329791
    },
    {
        "id": 26,
        "seek": 11096,
        "start": 119.83999633789062,
        "end": 125.68000030517578,
        "text": " final layer in the transformer. So why can't vocab size be infinite? Why can't we grow to",
        "tokens": [
            50808,
            2572,
            4583,
            294,
            264,
            31782,
            13,
            407,
            983,
            393,
            380,
            2329,
            455,
            2744,
            312,
            13785,
            30,
            1545,
            393,
            380,
            321,
            1852,
            281,
            51100
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17115935683250427,
        "compression_ratio": 1.878787875175476,
        "no_speech_prob": 0.002396696014329791
    },
    {
        "id": 27,
        "seek": 11096,
        "start": 125.68000030517578,
        "end": 131.67999267578125,
        "text": " infinity? Well, number one, your token embedding table is going to grow. Your linear layer is going",
        "tokens": [
            51100,
            13202,
            30,
            1042,
            11,
            1230,
            472,
            11,
            428,
            14862,
            12240,
            3584,
            3199,
            307,
            516,
            281,
            1852,
            13,
            2260,
            8213,
            4583,
            307,
            516,
            51400
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17115935683250427,
        "compression_ratio": 1.878787875175476,
        "no_speech_prob": 0.002396696014329791
    },
    {
        "id": 28,
        "seek": 11096,
        "start": 131.67999267578125,
        "end": 135.9199981689453,
        "text": " to grow. So we're going to be doing a lot more computation here because this LM head layer will",
        "tokens": [
            51400,
            281,
            1852,
            13,
            407,
            321,
            434,
            516,
            281,
            312,
            884,
            257,
            688,
            544,
            24903,
            510,
            570,
            341,
            46529,
            1378,
            4583,
            486,
            51612
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17115935683250427,
        "compression_ratio": 1.878787875175476,
        "no_speech_prob": 0.002396696014329791
    },
    {
        "id": 29,
        "seek": 11096,
        "start": 135.9199981689453,
        "end": 140.39999389648438,
        "text": " become more computationally expensive. Number two, because we have more parameters, we could",
        "tokens": [
            51612,
            1813,
            544,
            24903,
            379,
            5124,
            13,
            5118,
            732,
            11,
            570,
            321,
            362,
            544,
            9834,
            11,
            321,
            727,
            51836
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17115935683250427,
        "compression_ratio": 1.878787875175476,
        "no_speech_prob": 0.002396696014329791
    },
    {
        "id": 30,
        "seek": 14040,
        "start": 140.39999389648438,
        "end": 146.8000030517578,
        "text": " be worried that we are going to be under-training some of these parameters. So intuitively, if you",
        "tokens": [
            50364,
            312,
            5804,
            300,
            321,
            366,
            516,
            281,
            312,
            833,
            12,
            17227,
            1760,
            512,
            295,
            613,
            9834,
            13,
            407,
            46506,
            11,
            498,
            291,
            50684
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1786380559206009,
        "compression_ratio": 1.8782894611358643,
        "no_speech_prob": 0.00010390993702458218
    },
    {
        "id": 31,
        "seek": 14040,
        "start": 146.8000030517578,
        "end": 151.52000427246094,
        "text": " have a very large vocabulary size, say we have a million tokens, then every one of these tokens is",
        "tokens": [
            50684,
            362,
            257,
            588,
            2416,
            19864,
            2744,
            11,
            584,
            321,
            362,
            257,
            2459,
            22667,
            11,
            550,
            633,
            472,
            295,
            613,
            22667,
            307,
            50920
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1786380559206009,
        "compression_ratio": 1.8782894611358643,
        "no_speech_prob": 0.00010390993702458218
    },
    {
        "id": 32,
        "seek": 14040,
        "start": 151.52000427246094,
        "end": 155.75999450683594,
        "text": " going to come up more and more rarely in the training data because there's a lot more other",
        "tokens": [
            50920,
            516,
            281,
            808,
            493,
            544,
            293,
            544,
            13752,
            294,
            264,
            3097,
            1412,
            570,
            456,
            311,
            257,
            688,
            544,
            661,
            51132
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1786380559206009,
        "compression_ratio": 1.8782894611358643,
        "no_speech_prob": 0.00010390993702458218
    },
    {
        "id": 33,
        "seek": 14040,
        "start": 155.75999450683594,
        "end": 161.0399932861328,
        "text": " tokens all over the place. And so we're going to be seeing fewer and fewer examples for each",
        "tokens": [
            51132,
            22667,
            439,
            670,
            264,
            1081,
            13,
            400,
            370,
            321,
            434,
            516,
            281,
            312,
            2577,
            13366,
            293,
            13366,
            5110,
            337,
            1184,
            51396
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1786380559206009,
        "compression_ratio": 1.8782894611358643,
        "no_speech_prob": 0.00010390993702458218
    },
    {
        "id": 34,
        "seek": 14040,
        "start": 161.0399932861328,
        "end": 165.60000610351562,
        "text": " individual token. And you might be worried that basically the vectors associated with every token",
        "tokens": [
            51396,
            2609,
            14862,
            13,
            400,
            291,
            1062,
            312,
            5804,
            300,
            1936,
            264,
            18875,
            6615,
            365,
            633,
            14862,
            51624
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1786380559206009,
        "compression_ratio": 1.8782894611358643,
        "no_speech_prob": 0.00010390993702458218
    },
    {
        "id": 35,
        "seek": 14040,
        "start": 165.60000610351562,
        "end": 169.60000610351562,
        "text": " will be under-trained as a result because they just don't come up too often and they don't",
        "tokens": [
            51624,
            486,
            312,
            833,
            12,
            17227,
            2001,
            382,
            257,
            1874,
            570,
            436,
            445,
            500,
            380,
            808,
            493,
            886,
            2049,
            293,
            436,
            500,
            380,
            51824
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1786380559206009,
        "compression_ratio": 1.8782894611358643,
        "no_speech_prob": 0.00010390993702458218
    },
    {
        "id": 36,
        "seek": 16960,
        "start": 169.60000610351562,
        "end": 174.47999572753906,
        "text": " participate in the forward-backward pass. In addition to that, as your vocab size grows, you're going to",
        "tokens": [
            50364,
            8197,
            294,
            264,
            2128,
            12,
            3207,
            1007,
            1320,
            13,
            682,
            4500,
            281,
            300,
            11,
            382,
            428,
            2329,
            455,
            2744,
            13156,
            11,
            291,
            434,
            516,
            281,
            50608
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2076115906238556,
        "compression_ratio": 1.6847457885742188,
        "no_speech_prob": 6.014102109475061e-05
    },
    {
        "id": 37,
        "seek": 16960,
        "start": 174.47999572753906,
        "end": 179.44000244140625,
        "text": " start shrinking your sequences a lot, right? And that's really nice because that means that we're",
        "tokens": [
            50608,
            722,
            41684,
            428,
            22978,
            257,
            688,
            11,
            558,
            30,
            400,
            300,
            311,
            534,
            1481,
            570,
            300,
            1355,
            300,
            321,
            434,
            50856
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2076115906238556,
        "compression_ratio": 1.6847457885742188,
        "no_speech_prob": 6.014102109475061e-05
    },
    {
        "id": 38,
        "seek": 16960,
        "start": 179.44000244140625,
        "end": 183.75999450683594,
        "text": " going to be attending to more and more text. So that's nice. But also you might be worrying that",
        "tokens": [
            50856,
            516,
            281,
            312,
            15862,
            281,
            544,
            293,
            544,
            2487,
            13,
            407,
            300,
            311,
            1481,
            13,
            583,
            611,
            291,
            1062,
            312,
            18788,
            300,
            51072
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2076115906238556,
        "compression_ratio": 1.6847457885742188,
        "no_speech_prob": 6.014102109475061e-05
    },
    {
        "id": 39,
        "seek": 16960,
        "start": 183.75999450683594,
        "end": 189.9199981689453,
        "text": " too large of chunks are being squished into single tokens. And so the model just doesn't have as much",
        "tokens": [
            51072,
            886,
            2416,
            295,
            24004,
            366,
            885,
            2339,
            4729,
            666,
            2167,
            22667,
            13,
            400,
            370,
            264,
            2316,
            445,
            1177,
            380,
            362,
            382,
            709,
            51380
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2076115906238556,
        "compression_ratio": 1.6847457885742188,
        "no_speech_prob": 6.014102109475061e-05
    },
    {
        "id": 40,
        "seek": 16960,
        "start": 189.9199981689453,
        "end": 196.55999755859375,
        "text": " sort of time to think per sort of some number of characters in the text. Or you can think about",
        "tokens": [
            51380,
            1333,
            295,
            565,
            281,
            519,
            680,
            1333,
            295,
            512,
            1230,
            295,
            4342,
            294,
            264,
            2487,
            13,
            1610,
            291,
            393,
            519,
            466,
            51712
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2076115906238556,
        "compression_ratio": 1.6847457885742188,
        "no_speech_prob": 6.014102109475061e-05
    },
    {
        "id": 41,
        "seek": 19656,
        "start": 196.55999755859375,
        "end": 201.1999969482422,
        "text": " it that way, right? So basically we're squishing too much information into a single token and then",
        "tokens": [
            50364,
            309,
            300,
            636,
            11,
            558,
            30,
            407,
            1936,
            321,
            434,
            2339,
            3807,
            886,
            709,
            1589,
            666,
            257,
            2167,
            14862,
            293,
            550,
            50596
        ],
        "temperature": 0.0,
        "avg_logprob": -0.188057541847229,
        "compression_ratio": 1.715116262435913,
        "no_speech_prob": 0.0025508764665573835
    },
    {
        "id": 42,
        "seek": 19656,
        "start": 201.1999969482422,
        "end": 204.9600067138672,
        "text": " the forward pass of the transformer is not enough to actually process that information appropriately.",
        "tokens": [
            50596,
            264,
            2128,
            1320,
            295,
            264,
            31782,
            307,
            406,
            1547,
            281,
            767,
            1399,
            300,
            1589,
            23505,
            13,
            50784
        ],
        "temperature": 0.0,
        "avg_logprob": -0.188057541847229,
        "compression_ratio": 1.715116262435913,
        "no_speech_prob": 0.0025508764665573835
    },
    {
        "id": 43,
        "seek": 19656,
        "start": 205.52000427246094,
        "end": 209.1999969482422,
        "text": " And so these are some of the considerations you're thinking about when you're designing the vocab size.",
        "tokens": [
            50812,
            400,
            370,
            613,
            366,
            512,
            295,
            264,
            24070,
            291,
            434,
            1953,
            466,
            562,
            291,
            434,
            14685,
            264,
            2329,
            455,
            2744,
            13,
            50996
        ],
        "temperature": 0.0,
        "avg_logprob": -0.188057541847229,
        "compression_ratio": 1.715116262435913,
        "no_speech_prob": 0.0025508764665573835
    },
    {
        "id": 44,
        "seek": 19656,
        "start": 209.1999969482422,
        "end": 213.27999877929688,
        "text": " As I mentioned, this is mostly an empirical hyperparameter and it seems like in state-of-the-art",
        "tokens": [
            50996,
            1018,
            286,
            2835,
            11,
            341,
            307,
            5240,
            364,
            31886,
            9848,
            2181,
            335,
            2398,
            293,
            309,
            2544,
            411,
            294,
            1785,
            12,
            2670,
            12,
            3322,
            12,
            446,
            51200
        ],
        "temperature": 0.0,
        "avg_logprob": -0.188057541847229,
        "compression_ratio": 1.715116262435913,
        "no_speech_prob": 0.0025508764665573835
    },
    {
        "id": 45,
        "seek": 19656,
        "start": 213.27999877929688,
        "end": 218.72000122070312,
        "text": " architectures today, this is usually in the high 10,000s or somewhere around 100,000 today.",
        "tokens": [
            51200,
            6331,
            1303,
            965,
            11,
            341,
            307,
            2673,
            294,
            264,
            1090,
            1266,
            11,
            1360,
            82,
            420,
            4079,
            926,
            2319,
            11,
            1360,
            965,
            13,
            51472
        ],
        "temperature": 0.0,
        "avg_logprob": -0.188057541847229,
        "compression_ratio": 1.715116262435913,
        "no_speech_prob": 0.0025508764665573835
    },
    {
        "id": 46,
        "seek": 19656,
        "start": 218.72000122070312,
        "end": 223.0399932861328,
        "text": " And the next consideration I want to briefly talk about is what if we want to take a pre-trained",
        "tokens": [
            51472,
            400,
            264,
            958,
            12381,
            286,
            528,
            281,
            10515,
            751,
            466,
            307,
            437,
            498,
            321,
            528,
            281,
            747,
            257,
            659,
            12,
            17227,
            2001,
            51688
        ],
        "temperature": 0.0,
        "avg_logprob": -0.188057541847229,
        "compression_ratio": 1.715116262435913,
        "no_speech_prob": 0.0025508764665573835
    },
    {
        "id": 47,
        "seek": 22304,
        "start": 223.0399932861328,
        "end": 228.0,
        "text": " model and we want to extend the vocab size. And this is done fairly commonly actually. So for",
        "tokens": [
            50364,
            2316,
            293,
            321,
            528,
            281,
            10101,
            264,
            2329,
            455,
            2744,
            13,
            400,
            341,
            307,
            1096,
            6457,
            12719,
            767,
            13,
            407,
            337,
            50612
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1896853744983673,
        "compression_ratio": 1.698581576347351,
        "no_speech_prob": 0.00014653078687842935
    },
    {
        "id": 48,
        "seek": 22304,
        "start": 228.0,
        "end": 233.75999450683594,
        "text": " example, when you're doing fine tuning for chat GPT, a lot more new special tokens get introduced",
        "tokens": [
            50612,
            1365,
            11,
            562,
            291,
            434,
            884,
            2489,
            15164,
            337,
            5081,
            26039,
            51,
            11,
            257,
            688,
            544,
            777,
            2121,
            22667,
            483,
            7268,
            50900
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1896853744983673,
        "compression_ratio": 1.698581576347351,
        "no_speech_prob": 0.00014653078687842935
    },
    {
        "id": 49,
        "seek": 22304,
        "start": 233.75999450683594,
        "end": 239.44000244140625,
        "text": " on top of the base model to maintain the metadata and all the structure of conversation objects",
        "tokens": [
            50900,
            322,
            1192,
            295,
            264,
            3096,
            2316,
            281,
            6909,
            264,
            26603,
            293,
            439,
            264,
            3877,
            295,
            3761,
            6565,
            51184
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1896853744983673,
        "compression_ratio": 1.698581576347351,
        "no_speech_prob": 0.00014653078687842935
    },
    {
        "id": 50,
        "seek": 22304,
        "start": 239.44000244140625,
        "end": 244.16000366210938,
        "text": " between the user and an assistant. So that takes a lot of special tokens. You might also try to",
        "tokens": [
            51184,
            1296,
            264,
            4195,
            293,
            364,
            10994,
            13,
            407,
            300,
            2516,
            257,
            688,
            295,
            2121,
            22667,
            13,
            509,
            1062,
            611,
            853,
            281,
            51420
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1896853744983673,
        "compression_ratio": 1.698581576347351,
        "no_speech_prob": 0.00014653078687842935
    },
    {
        "id": 51,
        "seek": 22304,
        "start": 244.16000366210938,
        "end": 249.1199951171875,
        "text": " throw in more special tokens, for example, for using the browser or any other tool. And so it's",
        "tokens": [
            51420,
            3507,
            294,
            544,
            2121,
            22667,
            11,
            337,
            1365,
            11,
            337,
            1228,
            264,
            11185,
            420,
            604,
            661,
            2290,
            13,
            400,
            370,
            309,
            311,
            51668
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1896853744983673,
        "compression_ratio": 1.698581576347351,
        "no_speech_prob": 0.00014653078687842935
    },
    {
        "id": 52,
        "seek": 24912,
        "start": 249.1199951171875,
        "end": 254.16000366210938,
        "text": " very tempting to add a lot of tokens for all kinds of special functionality. So if you want to be",
        "tokens": [
            50364,
            588,
            37900,
            281,
            909,
            257,
            688,
            295,
            22667,
            337,
            439,
            3685,
            295,
            2121,
            14980,
            13,
            407,
            498,
            291,
            528,
            281,
            312,
            50616
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1875920295715332,
        "compression_ratio": 1.75,
        "no_speech_prob": 3.7636400520568714e-05
    },
    {
        "id": 53,
        "seek": 24912,
        "start": 254.16000366210938,
        "end": 259.0400085449219,
        "text": " adding a token, that's totally possible, right? All we have to do is we have to resize this embedding.",
        "tokens": [
            50616,
            5127,
            257,
            14862,
            11,
            300,
            311,
            3879,
            1944,
            11,
            558,
            30,
            1057,
            321,
            362,
            281,
            360,
            307,
            321,
            362,
            281,
            50069,
            341,
            12240,
            3584,
            13,
            50860
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1875920295715332,
        "compression_ratio": 1.75,
        "no_speech_prob": 3.7636400520568714e-05
    },
    {
        "id": 54,
        "seek": 24912,
        "start": 259.0400085449219,
        "end": 263.9200134277344,
        "text": " So we have to add rows. We would initialize these parameters from scratch, which would be small",
        "tokens": [
            50860,
            407,
            321,
            362,
            281,
            909,
            13241,
            13,
            492,
            576,
            5883,
            1125,
            613,
            9834,
            490,
            8459,
            11,
            597,
            576,
            312,
            1359,
            51104
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1875920295715332,
        "compression_ratio": 1.75,
        "no_speech_prob": 3.7636400520568714e-05
    },
    {
        "id": 55,
        "seek": 24912,
        "start": 263.9200134277344,
        "end": 269.5199890136719,
        "text": " random numbers. And then we have to extend the weight inside this linear. So we have to start",
        "tokens": [
            51104,
            4974,
            3547,
            13,
            400,
            550,
            321,
            362,
            281,
            10101,
            264,
            3364,
            1854,
            341,
            8213,
            13,
            407,
            321,
            362,
            281,
            722,
            51384
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1875920295715332,
        "compression_ratio": 1.75,
        "no_speech_prob": 3.7636400520568714e-05
    },
    {
        "id": 56,
        "seek": 24912,
        "start": 269.5199890136719,
        "end": 274.55999755859375,
        "text": " making dot products with the associated parameters as well to basically calculate the probabilities",
        "tokens": [
            51384,
            1455,
            5893,
            3383,
            365,
            264,
            6615,
            9834,
            382,
            731,
            281,
            1936,
            8873,
            264,
            33783,
            51636
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1875920295715332,
        "compression_ratio": 1.75,
        "no_speech_prob": 3.7636400520568714e-05
    },
    {
        "id": 57,
        "seek": 27456,
        "start": 274.55999755859375,
        "end": 281.3599853515625,
        "text": " for these new tokens. So both of these are just resizing operation. It's a very mild model surgery",
        "tokens": [
            50364,
            337,
            613,
            777,
            22667,
            13,
            407,
            1293,
            295,
            613,
            366,
            445,
            725,
            3319,
            6916,
            13,
            467,
            311,
            257,
            588,
            15154,
            2316,
            7930,
            50704
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19125483930110931,
        "compression_ratio": 1.9786477088928223,
        "no_speech_prob": 0.0024725894909352064
    },
    {
        "id": 58,
        "seek": 27456,
        "start": 281.3599853515625,
        "end": 284.8800048828125,
        "text": " and can be done fairly easily. And it's quite common that basically you would freeze the base",
        "tokens": [
            50704,
            293,
            393,
            312,
            1096,
            6457,
            3612,
            13,
            400,
            309,
            311,
            1596,
            2689,
            300,
            1936,
            291,
            576,
            15959,
            264,
            3096,
            50880
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19125483930110931,
        "compression_ratio": 1.9786477088928223,
        "no_speech_prob": 0.0024725894909352064
    },
    {
        "id": 59,
        "seek": 27456,
        "start": 284.8800048828125,
        "end": 288.9599914550781,
        "text": " model, you introduce these new parameters, and then you only train these new parameters to",
        "tokens": [
            50880,
            2316,
            11,
            291,
            5366,
            613,
            777,
            9834,
            11,
            293,
            550,
            291,
            787,
            3847,
            613,
            777,
            9834,
            281,
            51084
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19125483930110931,
        "compression_ratio": 1.9786477088928223,
        "no_speech_prob": 0.0024725894909352064
    },
    {
        "id": 60,
        "seek": 27456,
        "start": 288.9599914550781,
        "end": 294.0,
        "text": " introduce new tokens into the architecture. And so you can freeze arbitrary parts of it,",
        "tokens": [
            51084,
            5366,
            777,
            22667,
            666,
            264,
            9482,
            13,
            400,
            370,
            291,
            393,
            15959,
            23211,
            3166,
            295,
            309,
            11,
            51336
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19125483930110931,
        "compression_ratio": 1.9786477088928223,
        "no_speech_prob": 0.0024725894909352064
    },
    {
        "id": 61,
        "seek": 27456,
        "start": 294.0,
        "end": 297.9200134277344,
        "text": " or you can train arbitrary parts of it, and that's totally up to you. But basically minor",
        "tokens": [
            51336,
            420,
            291,
            393,
            3847,
            23211,
            3166,
            295,
            309,
            11,
            293,
            300,
            311,
            3879,
            493,
            281,
            291,
            13,
            583,
            1936,
            6696,
            51532
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19125483930110931,
        "compression_ratio": 1.9786477088928223,
        "no_speech_prob": 0.0024725894909352064
    },
    {
        "id": 62,
        "seek": 27456,
        "start": 297.9200134277344,
        "end": 302.239990234375,
        "text": " surgery required if you'd like to introduce new tokens. And finally, I'd like to mention that",
        "tokens": [
            51532,
            7930,
            4739,
            498,
            291,
            1116,
            411,
            281,
            5366,
            777,
            22667,
            13,
            400,
            2721,
            11,
            286,
            1116,
            411,
            281,
            2152,
            300,
            51748
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19125483930110931,
        "compression_ratio": 1.9786477088928223,
        "no_speech_prob": 0.0024725894909352064
    },
    {
        "id": 63,
        "seek": 30224,
        "start": 302.239990234375,
        "end": 306.8800048828125,
        "text": " actually there's an entire design space of applications in terms of introducing new tokens",
        "tokens": [
            50364,
            767,
            456,
            311,
            364,
            2302,
            1715,
            1901,
            295,
            5821,
            294,
            2115,
            295,
            15424,
            777,
            22667,
            50596
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2225341796875,
        "compression_ratio": 1.7469879388809204,
        "no_speech_prob": 0.00021653967269230634
    },
    {
        "id": 64,
        "seek": 30224,
        "start": 306.8800048828125,
        "end": 311.67999267578125,
        "text": " into a vocabulary that go way beyond just adding special tokens and special new functionality.",
        "tokens": [
            50596,
            666,
            257,
            19864,
            300,
            352,
            636,
            4399,
            445,
            5127,
            2121,
            22667,
            293,
            2121,
            777,
            14980,
            13,
            50836
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2225341796875,
        "compression_ratio": 1.7469879388809204,
        "no_speech_prob": 0.00021653967269230634
    },
    {
        "id": 65,
        "seek": 30224,
        "start": 311.67999267578125,
        "end": 315.3599853515625,
        "text": " So just to give you a sense of the design space, but this could be an entire video just by itself,",
        "tokens": [
            50836,
            407,
            445,
            281,
            976,
            291,
            257,
            2020,
            295,
            264,
            1715,
            1901,
            11,
            457,
            341,
            727,
            312,
            364,
            2302,
            960,
            445,
            538,
            2564,
            11,
            51020
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2225341796875,
        "compression_ratio": 1.7469879388809204,
        "no_speech_prob": 0.00021653967269230634
    },
    {
        "id": 66,
        "seek": 30224,
        "start": 316.0799865722656,
        "end": 322.0799865722656,
        "text": " this is a paper on learning to compress prompts with what they called GIST tokens. And the rough",
        "tokens": [
            51056,
            341,
            307,
            257,
            3035,
            322,
            2539,
            281,
            14778,
            41095,
            365,
            437,
            436,
            1219,
            460,
            19756,
            22667,
            13,
            400,
            264,
            5903,
            51356
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2225341796875,
        "compression_ratio": 1.7469879388809204,
        "no_speech_prob": 0.00021653967269230634
    },
    {
        "id": 67,
        "seek": 30224,
        "start": 322.0799865722656,
        "end": 326.7200012207031,
        "text": " idea is, suppose that you're using language models in a setting that requires very long prompts.",
        "tokens": [
            51356,
            1558,
            307,
            11,
            7297,
            300,
            291,
            434,
            1228,
            2856,
            5245,
            294,
            257,
            3287,
            300,
            7029,
            588,
            938,
            41095,
            13,
            51588
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2225341796875,
        "compression_ratio": 1.7469879388809204,
        "no_speech_prob": 0.00021653967269230634
    },
    {
        "id": 68,
        "seek": 30224,
        "start": 326.7200012207031,
        "end": 330.4800109863281,
        "text": " Well, these long prompts just slow everything down because you have to encode them, and then you have",
        "tokens": [
            51588,
            1042,
            11,
            613,
            938,
            41095,
            445,
            2964,
            1203,
            760,
            570,
            291,
            362,
            281,
            2058,
            1429,
            552,
            11,
            293,
            550,
            291,
            362,
            51776
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2225341796875,
        "compression_ratio": 1.7469879388809204,
        "no_speech_prob": 0.00021653967269230634
    },
    {
        "id": 69,
        "seek": 33048,
        "start": 330.4800109863281,
        "end": 335.6000061035156,
        "text": " to use them, and then you're tending over them, and it's just heavy to have very large prompts.",
        "tokens": [
            50364,
            281,
            764,
            552,
            11,
            293,
            550,
            291,
            434,
            256,
            2029,
            670,
            552,
            11,
            293,
            309,
            311,
            445,
            4676,
            281,
            362,
            588,
            2416,
            41095,
            13,
            50620
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1953125,
        "compression_ratio": 1.8582676649093628,
        "no_speech_prob": 0.000282400258583948
    },
    {
        "id": 70,
        "seek": 33048,
        "start": 336.32000732421875,
        "end": 344.0799865722656,
        "text": " So instead, what they do here in this paper is they introduce new tokens. And imagine basically",
        "tokens": [
            50656,
            407,
            2602,
            11,
            437,
            436,
            360,
            510,
            294,
            341,
            3035,
            307,
            436,
            5366,
            777,
            22667,
            13,
            400,
            3811,
            1936,
            51044
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1953125,
        "compression_ratio": 1.8582676649093628,
        "no_speech_prob": 0.000282400258583948
    },
    {
        "id": 71,
        "seek": 33048,
        "start": 344.0799865722656,
        "end": 349.0400085449219,
        "text": " having a few new tokens, you put them in a sequence, and then you train the model by",
        "tokens": [
            51044,
            1419,
            257,
            1326,
            777,
            22667,
            11,
            291,
            829,
            552,
            294,
            257,
            8310,
            11,
            293,
            550,
            291,
            3847,
            264,
            2316,
            538,
            51292
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1953125,
        "compression_ratio": 1.8582676649093628,
        "no_speech_prob": 0.000282400258583948
    },
    {
        "id": 72,
        "seek": 33048,
        "start": 349.0400085449219,
        "end": 354.1600036621094,
        "text": " distillation. So you are keeping the entire model frozen, and you're only training the representations",
        "tokens": [
            51292,
            42923,
            399,
            13,
            407,
            291,
            366,
            5145,
            264,
            2302,
            2316,
            12496,
            11,
            293,
            291,
            434,
            787,
            3097,
            264,
            33358,
            51548
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1953125,
        "compression_ratio": 1.8582676649093628,
        "no_speech_prob": 0.000282400258583948
    },
    {
        "id": 73,
        "seek": 33048,
        "start": 354.1600036621094,
        "end": 359.1199951171875,
        "text": " of the new tokens, their embeddings, and you're optimizing over the new tokens such that the",
        "tokens": [
            51548,
            295,
            264,
            777,
            22667,
            11,
            641,
            12240,
            29432,
            11,
            293,
            291,
            434,
            40425,
            670,
            264,
            777,
            22667,
            1270,
            300,
            264,
            51796
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1953125,
        "compression_ratio": 1.8582676649093628,
        "no_speech_prob": 0.000282400258583948
    },
    {
        "id": 74,
        "seek": 35912,
        "start": 359.1199951171875,
        "end": 366.55999755859375,
        "text": " behavior of the language model is identical to the model that has a very long prompt that works",
        "tokens": [
            50364,
            5223,
            295,
            264,
            2856,
            2316,
            307,
            14800,
            281,
            264,
            2316,
            300,
            575,
            257,
            588,
            938,
            12391,
            300,
            1985,
            50736
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19137167930603027,
        "compression_ratio": 1.8505747318267822,
        "no_speech_prob": 3.0241752028814517e-05
    },
    {
        "id": 75,
        "seek": 35912,
        "start": 366.55999755859375,
        "end": 370.79998779296875,
        "text": " for you. And so it's a compression technique of compressing that very long prompt into those few",
        "tokens": [
            50736,
            337,
            291,
            13,
            400,
            370,
            309,
            311,
            257,
            19355,
            6532,
            295,
            14778,
            278,
            300,
            588,
            938,
            12391,
            666,
            729,
            1326,
            50948
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19137167930603027,
        "compression_ratio": 1.8505747318267822,
        "no_speech_prob": 3.0241752028814517e-05
    },
    {
        "id": 76,
        "seek": 35912,
        "start": 370.79998779296875,
        "end": 375.5199890136719,
        "text": " new GIST tokens. And so you can train this, and then at test time you can discard your old prompt",
        "tokens": [
            50948,
            777,
            460,
            19756,
            22667,
            13,
            400,
            370,
            291,
            393,
            3847,
            341,
            11,
            293,
            550,
            412,
            1500,
            565,
            291,
            393,
            31597,
            428,
            1331,
            12391,
            51184
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19137167930603027,
        "compression_ratio": 1.8505747318267822,
        "no_speech_prob": 3.0241752028814517e-05
    },
    {
        "id": 77,
        "seek": 35912,
        "start": 375.5199890136719,
        "end": 381.0400085449219,
        "text": " and just swap in those tokens, and they sort of like stand in for that very long prompt and have",
        "tokens": [
            51184,
            293,
            445,
            18135,
            294,
            729,
            22667,
            11,
            293,
            436,
            1333,
            295,
            411,
            1463,
            294,
            337,
            300,
            588,
            938,
            12391,
            293,
            362,
            51460
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19137167930603027,
        "compression_ratio": 1.8505747318267822,
        "no_speech_prob": 3.0241752028814517e-05
    },
    {
        "id": 78,
        "seek": 35912,
        "start": 381.0400085449219,
        "end": 387.0400085449219,
        "text": " an almost identical performance. And so this is one technique in a class of parameter-efficient",
        "tokens": [
            51460,
            364,
            1920,
            14800,
            3389,
            13,
            400,
            370,
            341,
            307,
            472,
            6532,
            294,
            257,
            1508,
            295,
            13075,
            12,
            68,
            7816,
            51760
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19137167930603027,
        "compression_ratio": 1.8505747318267822,
        "no_speech_prob": 3.0241752028814517e-05
    },
    {
        "id": 79,
        "seek": 38704,
        "start": 387.0400085449219,
        "end": 391.6000061035156,
        "text": " fine-tuning techniques where most of the model is basically fixed, and there's no training of",
        "tokens": [
            50364,
            2489,
            12,
            83,
            37726,
            7512,
            689,
            881,
            295,
            264,
            2316,
            307,
            1936,
            6806,
            11,
            293,
            456,
            311,
            572,
            3097,
            295,
            50592
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16965951025485992,
        "compression_ratio": 1.8164557218551636,
        "no_speech_prob": 7.48460806789808e-05
    },
    {
        "id": 80,
        "seek": 38704,
        "start": 391.6000061035156,
        "end": 396.239990234375,
        "text": " the model weights, there's no training of LoRa or anything like that, of new parameters. The",
        "tokens": [
            50592,
            264,
            2316,
            17443,
            11,
            456,
            311,
            572,
            3097,
            295,
            6130,
            41873,
            420,
            1340,
            411,
            300,
            11,
            295,
            777,
            9834,
            13,
            440,
            50824
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16965951025485992,
        "compression_ratio": 1.8164557218551636,
        "no_speech_prob": 7.48460806789808e-05
    },
    {
        "id": 81,
        "seek": 38704,
        "start": 396.239990234375,
        "end": 401.3599853515625,
        "text": " parameters that you're training are now just the token embeddings. So that's just one example, but",
        "tokens": [
            50824,
            9834,
            300,
            291,
            434,
            3097,
            366,
            586,
            445,
            264,
            14862,
            12240,
            29432,
            13,
            407,
            300,
            311,
            445,
            472,
            1365,
            11,
            457,
            51080
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16965951025485992,
        "compression_ratio": 1.8164557218551636,
        "no_speech_prob": 7.48460806789808e-05
    },
    {
        "id": 82,
        "seek": 38704,
        "start": 401.3599853515625,
        "end": 405.0400085449219,
        "text": " this could again be like an entire video, but just to give you a sense that there's a whole design",
        "tokens": [
            51080,
            341,
            727,
            797,
            312,
            411,
            364,
            2302,
            960,
            11,
            457,
            445,
            281,
            976,
            291,
            257,
            2020,
            300,
            456,
            311,
            257,
            1379,
            1715,
            51264
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16965951025485992,
        "compression_ratio": 1.8164557218551636,
        "no_speech_prob": 7.48460806789808e-05
    },
    {
        "id": 83,
        "seek": 38704,
        "start": 405.0400085449219,
        "end": 409.1199951171875,
        "text": " space here that is potentially worth exploring in the future. The next thing I want to briefly",
        "tokens": [
            51264,
            1901,
            510,
            300,
            307,
            7263,
            3163,
            12736,
            294,
            264,
            2027,
            13,
            440,
            958,
            551,
            286,
            528,
            281,
            10515,
            51468
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16965951025485992,
        "compression_ratio": 1.8164557218551636,
        "no_speech_prob": 7.48460806789808e-05
    },
    {
        "id": 84,
        "seek": 38704,
        "start": 409.1199951171875,
        "end": 414.1600036621094,
        "text": " address is that I think recently there's a lot of momentum in how you actually could construct",
        "tokens": [
            51468,
            2985,
            307,
            300,
            286,
            519,
            3938,
            456,
            311,
            257,
            688,
            295,
            11244,
            294,
            577,
            291,
            767,
            727,
            7690,
            51720
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16965951025485992,
        "compression_ratio": 1.8164557218551636,
        "no_speech_prob": 7.48460806789808e-05
    },
    {
        "id": 85,
        "seek": 41416,
        "start": 414.1600036621094,
        "end": 418.32000732421875,
        "text": " transformers that can simultaneously process not just text as the input modality, but a lot",
        "tokens": [
            50364,
            4088,
            433,
            300,
            393,
            16561,
            1399,
            406,
            445,
            2487,
            382,
            264,
            4846,
            1072,
            1860,
            11,
            457,
            257,
            688,
            50572
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16623957455158234,
        "compression_ratio": 1.828025460243225,
        "no_speech_prob": 0.00020342602510936558
    },
    {
        "id": 86,
        "seek": 41416,
        "start": 418.32000732421875,
        "end": 423.9200134277344,
        "text": " of other modalities, so be it images, videos, audio, etc. And how do you feed in all these",
        "tokens": [
            50572,
            295,
            661,
            1072,
            16110,
            11,
            370,
            312,
            309,
            5267,
            11,
            2145,
            11,
            6278,
            11,
            5183,
            13,
            400,
            577,
            360,
            291,
            3154,
            294,
            439,
            613,
            50852
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16623957455158234,
        "compression_ratio": 1.828025460243225,
        "no_speech_prob": 0.00020342602510936558
    },
    {
        "id": 87,
        "seek": 41416,
        "start": 423.9200134277344,
        "end": 429.0400085449219,
        "text": " modalities and potentially predict these modalities from a transformer? Do you have to change the",
        "tokens": [
            50852,
            1072,
            16110,
            293,
            7263,
            6069,
            613,
            1072,
            16110,
            490,
            257,
            31782,
            30,
            1144,
            291,
            362,
            281,
            1319,
            264,
            51108
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16623957455158234,
        "compression_ratio": 1.828025460243225,
        "no_speech_prob": 0.00020342602510936558
    },
    {
        "id": 88,
        "seek": 41416,
        "start": 429.0400085449219,
        "end": 432.6400146484375,
        "text": " architecture in some fundamental way? And I think what a lot of people are starting to converge",
        "tokens": [
            51108,
            9482,
            294,
            512,
            8088,
            636,
            30,
            400,
            286,
            519,
            437,
            257,
            688,
            295,
            561,
            366,
            2891,
            281,
            41881,
            51288
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16623957455158234,
        "compression_ratio": 1.828025460243225,
        "no_speech_prob": 0.00020342602510936558
    },
    {
        "id": 89,
        "seek": 41416,
        "start": 432.6400146484375,
        "end": 436.6400146484375,
        "text": " towards is that you're not changing the architecture, you stick with the transformer, you just kind of",
        "tokens": [
            51288,
            3030,
            307,
            300,
            291,
            434,
            406,
            4473,
            264,
            9482,
            11,
            291,
            2897,
            365,
            264,
            31782,
            11,
            291,
            445,
            733,
            295,
            51488
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16623957455158234,
        "compression_ratio": 1.828025460243225,
        "no_speech_prob": 0.00020342602510936558
    },
    {
        "id": 90,
        "seek": 41416,
        "start": 436.6400146484375,
        "end": 441.6000061035156,
        "text": " tokenize your input domains, and then call it a day and pretend it's just text tokens and just",
        "tokens": [
            51488,
            14862,
            1125,
            428,
            4846,
            25514,
            11,
            293,
            550,
            818,
            309,
            257,
            786,
            293,
            11865,
            309,
            311,
            445,
            2487,
            22667,
            293,
            445,
            51736
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16623957455158234,
        "compression_ratio": 1.828025460243225,
        "no_speech_prob": 0.00020342602510936558
    },
    {
        "id": 91,
        "seek": 44160,
        "start": 441.6000061035156,
        "end": 446.6400146484375,
        "text": " do everything else in an identical manner. So here, for example, there was an early paper that",
        "tokens": [
            50364,
            360,
            1203,
            1646,
            294,
            364,
            14800,
            9060,
            13,
            407,
            510,
            11,
            337,
            1365,
            11,
            456,
            390,
            364,
            2440,
            3035,
            300,
            50616
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2592369019985199,
        "compression_ratio": 1.738916277885437,
        "no_speech_prob": 1.834286558732856e-05
    },
    {
        "id": 92,
        "seek": 44160,
        "start": 446.6400146484375,
        "end": 451.6000061035156,
        "text": " has a nice graphic for how you can take an image and you can truncate it into integers.",
        "tokens": [
            50616,
            575,
            257,
            1481,
            14089,
            337,
            577,
            291,
            393,
            747,
            364,
            3256,
            293,
            291,
            393,
            504,
            409,
            66,
            473,
            309,
            666,
            41674,
            13,
            50864
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2592369019985199,
        "compression_ratio": 1.738916277885437,
        "no_speech_prob": 1.834286558732856e-05
    },
    {
        "id": 93,
        "seek": 44160,
        "start": 452.9599914550781,
        "end": 458.8800048828125,
        "text": " So these would basically become the tokens of images, as an example, and",
        "tokens": [
            50932,
            407,
            613,
            576,
            1936,
            1813,
            264,
            22667,
            295,
            5267,
            11,
            382,
            364,
            1365,
            11,
            293,
            51228
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2592369019985199,
        "compression_ratio": 1.738916277885437,
        "no_speech_prob": 1.834286558732856e-05
    },
    {
        "id": 94,
        "seek": 44160,
        "start": 458.8800048828125,
        "end": 464.4800109863281,
        "text": " these tokens can be hard tokens where you force them to be integers. They can also be soft tokens",
        "tokens": [
            51228,
            613,
            22667,
            393,
            312,
            1152,
            22667,
            689,
            291,
            3464,
            552,
            281,
            312,
            41674,
            13,
            814,
            393,
            611,
            312,
            2787,
            22667,
            51508
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2592369019985199,
        "compression_ratio": 1.738916277885437,
        "no_speech_prob": 1.834286558732856e-05
    },
    {
        "id": 95,
        "seek": 46448,
        "start": 465.1199951171875,
        "end": 471.760009765625,
        "text": " where you sort of don't require these to be discrete, but you do force these representations",
        "tokens": [
            50396,
            689,
            291,
            1333,
            295,
            500,
            380,
            3651,
            613,
            281,
            312,
            27706,
            11,
            457,
            291,
            360,
            3464,
            613,
            33358,
            50728
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20478315651416779,
        "compression_ratio": 1.5498007535934448,
        "no_speech_prob": 0.00044421054190024734
    },
    {
        "id": 96,
        "seek": 46448,
        "start": 471.760009765625,
        "end": 477.8399963378906,
        "text": " to go through bottlenecks like in autoencoders. Also in this paper that came out from OpenAI, Sora,",
        "tokens": [
            50728,
            281,
            352,
            807,
            44641,
            2761,
            411,
            294,
            8399,
            22660,
            378,
            433,
            13,
            2743,
            294,
            341,
            3035,
            300,
            1361,
            484,
            490,
            7238,
            48698,
            11,
            46639,
            11,
            51032
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20478315651416779,
        "compression_ratio": 1.5498007535934448,
        "no_speech_prob": 0.00044421054190024734
    },
    {
        "id": 97,
        "seek": 46448,
        "start": 477.8399963378906,
        "end": 483.6000061035156,
        "text": " which I think really blew the mind of many people and inspired a lot of people in terms of what's",
        "tokens": [
            51032,
            597,
            286,
            519,
            534,
            19075,
            264,
            1575,
            295,
            867,
            561,
            293,
            7547,
            257,
            688,
            295,
            561,
            294,
            2115,
            295,
            437,
            311,
            51320
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20478315651416779,
        "compression_ratio": 1.5498007535934448,
        "no_speech_prob": 0.00044421054190024734
    },
    {
        "id": 98,
        "seek": 46448,
        "start": 483.6000061035156,
        "end": 489.67999267578125,
        "text": " possible, they have a graphic here and they talk briefly about how LLMs have text tokens, Sora has",
        "tokens": [
            51320,
            1944,
            11,
            436,
            362,
            257,
            14089,
            510,
            293,
            436,
            751,
            10515,
            466,
            577,
            441,
            43,
            26386,
            362,
            2487,
            22667,
            11,
            46639,
            575,
            51624
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20478315651416779,
        "compression_ratio": 1.5498007535934448,
        "no_speech_prob": 0.00044421054190024734
    },
    {
        "id": 99,
        "seek": 48968,
        "start": 489.67999267578125,
        "end": 495.67999267578125,
        "text": " visual patches. So again, they came up with a way to truncate videos into basically tokens with",
        "tokens": [
            50364,
            5056,
            26531,
            13,
            407,
            797,
            11,
            436,
            1361,
            493,
            365,
            257,
            636,
            281,
            504,
            409,
            66,
            473,
            2145,
            666,
            1936,
            22667,
            365,
            50664
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20915132761001587,
        "compression_ratio": 1.6793103218078613,
        "no_speech_prob": 0.0010987207060679793
    },
    {
        "id": 100,
        "seek": 48968,
        "start": 495.67999267578125,
        "end": 499.9200134277344,
        "text": " their own vocabularies, and then you can either process discrete tokens, say, with autoregressive",
        "tokens": [
            50664,
            641,
            1065,
            2329,
            455,
            1040,
            530,
            11,
            293,
            550,
            291,
            393,
            2139,
            1399,
            27706,
            22667,
            11,
            584,
            11,
            365,
            1476,
            418,
            3091,
            488,
            50876
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20915132761001587,
        "compression_ratio": 1.6793103218078613,
        "no_speech_prob": 0.0010987207060679793
    },
    {
        "id": 101,
        "seek": 48968,
        "start": 499.9200134277344,
        "end": 507.6000061035156,
        "text": " models or even soft tokens with diffusion models. And all of that is sort of being actively worked",
        "tokens": [
            50876,
            5245,
            420,
            754,
            2787,
            22667,
            365,
            25242,
            5245,
            13,
            400,
            439,
            295,
            300,
            307,
            1333,
            295,
            885,
            13022,
            2732,
            51260
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20915132761001587,
        "compression_ratio": 1.6793103218078613,
        "no_speech_prob": 0.0010987207060679793
    },
    {
        "id": 102,
        "seek": 48968,
        "start": 507.6000061035156,
        "end": 510.79998779296875,
        "text": " on, designed on, and it's beyond the scope of this video, but just something I wanted to mention",
        "tokens": [
            51260,
            322,
            11,
            4761,
            322,
            11,
            293,
            309,
            311,
            4399,
            264,
            11923,
            295,
            341,
            960,
            11,
            457,
            445,
            746,
            286,
            1415,
            281,
            2152,
            51420
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20915132761001587,
        "compression_ratio": 1.6793103218078613,
        "no_speech_prob": 0.0010987207060679793
    },
    {
        "id": 103,
        "seek": 48968,
        "start": 510.79998779296875,
        "end": 515.760009765625,
        "text": " briefly. Okay, now that we have come quite deep into the tokenization algorithm and we understand",
        "tokens": [
            51420,
            10515,
            13,
            1033,
            11,
            586,
            300,
            321,
            362,
            808,
            1596,
            2452,
            666,
            264,
            14862,
            2144,
            9284,
            293,
            321,
            1223,
            51668
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20915132761001587,
        "compression_ratio": 1.6793103218078613,
        "no_speech_prob": 0.0010987207060679793
    },
    {
        "id": 104,
        "seek": 51576,
        "start": 515.760009765625,
        "end": 520.3200073242188,
        "text": " a lot more about how it works, let's loop back around to the beginning of this video and go",
        "tokens": [
            50364,
            257,
            688,
            544,
            466,
            577,
            309,
            1985,
            11,
            718,
            311,
            6367,
            646,
            926,
            281,
            264,
            2863,
            295,
            341,
            960,
            293,
            352,
            50592
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18788740038871765,
        "compression_ratio": 1.6388888359069824,
        "no_speech_prob": 0.004829540848731995
    },
    {
        "id": 105,
        "seek": 51576,
        "start": 520.3200073242188,
        "end": 525.52001953125,
        "text": " through some of these bullet points and really see why they happen. So first of all, why can't",
        "tokens": [
            50592,
            807,
            512,
            295,
            613,
            11632,
            2793,
            293,
            534,
            536,
            983,
            436,
            1051,
            13,
            407,
            700,
            295,
            439,
            11,
            983,
            393,
            380,
            50852
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18788740038871765,
        "compression_ratio": 1.6388888359069824,
        "no_speech_prob": 0.004829540848731995
    },
    {
        "id": 106,
        "seek": 51576,
        "start": 525.52001953125,
        "end": 532.4000244140625,
        "text": " my LLM spell words very well or do other spell-related tasks? So fundamentally this is because,",
        "tokens": [
            50852,
            452,
            441,
            43,
            44,
            9827,
            2283,
            588,
            731,
            420,
            360,
            661,
            9827,
            12,
            12004,
            9608,
            30,
            407,
            17879,
            341,
            307,
            570,
            11,
            51196
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18788740038871765,
        "compression_ratio": 1.6388888359069824,
        "no_speech_prob": 0.004829540848731995
    },
    {
        "id": 107,
        "seek": 51576,
        "start": 532.4000244140625,
        "end": 537.9199829101562,
        "text": " as we saw, these characters are chunked up into tokens, and some of these tokens are actually",
        "tokens": [
            51196,
            382,
            321,
            1866,
            11,
            613,
            4342,
            366,
            16635,
            292,
            493,
            666,
            22667,
            11,
            293,
            512,
            295,
            613,
            22667,
            366,
            767,
            51472
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18788740038871765,
        "compression_ratio": 1.6388888359069824,
        "no_speech_prob": 0.004829540848731995
    },
    {
        "id": 108,
        "seek": 51576,
        "start": 537.9199829101562,
        "end": 544.1599731445312,
        "text": " fairly long. So as an example, I went to the GPT-4 vocabulary and I looked at one of the longer",
        "tokens": [
            51472,
            6457,
            938,
            13,
            407,
            382,
            364,
            1365,
            11,
            286,
            1437,
            281,
            264,
            26039,
            51,
            12,
            19,
            19864,
            293,
            286,
            2956,
            412,
            472,
            295,
            264,
            2854,
            51784
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18788740038871765,
        "compression_ratio": 1.6388888359069824,
        "no_speech_prob": 0.004829540848731995
    },
    {
        "id": 109,
        "seek": 54416,
        "start": 544.1599731445312,
        "end": 549.8400268554688,
        "text": " tokens. So .defaultstyle turns out to be a single individual token. So that's a lot of characters",
        "tokens": [
            50364,
            22667,
            13,
            407,
            2411,
            20595,
            5107,
            15014,
            4523,
            484,
            281,
            312,
            257,
            2167,
            2609,
            14862,
            13,
            407,
            300,
            311,
            257,
            688,
            295,
            4342,
            50648
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1812228113412857,
        "compression_ratio": 1.7834101915359497,
        "no_speech_prob": 0.00027802944532595575
    },
    {
        "id": 110,
        "seek": 54416,
        "start": 549.8400268554688,
        "end": 554.5599975585938,
        "text": " for a single token. So my suspicion is that there's just too much crammed into this single",
        "tokens": [
            50648,
            337,
            257,
            2167,
            14862,
            13,
            407,
            452,
            32020,
            307,
            300,
            456,
            311,
            445,
            886,
            709,
            941,
            19859,
            666,
            341,
            2167,
            50884
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1812228113412857,
        "compression_ratio": 1.7834101915359497,
        "no_speech_prob": 0.00027802944532595575
    },
    {
        "id": 111,
        "seek": 54416,
        "start": 554.5599975585938,
        "end": 561.280029296875,
        "text": " token, and my suspicion was that the model should not be very good at tasks related to spelling of",
        "tokens": [
            50884,
            14862,
            11,
            293,
            452,
            32020,
            390,
            300,
            264,
            2316,
            820,
            406,
            312,
            588,
            665,
            412,
            9608,
            4077,
            281,
            22254,
            295,
            51220
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1812228113412857,
        "compression_ratio": 1.7834101915359497,
        "no_speech_prob": 0.00027802944532595575
    },
    {
        "id": 112,
        "seek": 54416,
        "start": 561.280029296875,
        "end": 569.52001953125,
        "text": " this single token. So I asked how many letters L are there in the word .defaultstyle, and of course",
        "tokens": [
            51220,
            341,
            2167,
            14862,
            13,
            407,
            286,
            2351,
            577,
            867,
            7825,
            441,
            366,
            456,
            294,
            264,
            1349,
            2411,
            20595,
            5107,
            15014,
            11,
            293,
            295,
            1164,
            51632
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1812228113412857,
        "compression_ratio": 1.7834101915359497,
        "no_speech_prob": 0.00027802944532595575
    },
    {
        "id": 113,
        "seek": 56952,
        "start": 570.0800170898438,
        "end": 576.1599731445312,
        "text": " my prompt is intentionally done that way, and you see how .defaultstyle will be a single token. So",
        "tokens": [
            50392,
            452,
            12391,
            307,
            22062,
            1096,
            300,
            636,
            11,
            293,
            291,
            536,
            577,
            2411,
            20595,
            5107,
            15014,
            486,
            312,
            257,
            2167,
            14862,
            13,
            407,
            50696
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17793120443820953,
        "compression_ratio": 1.6385135650634766,
        "no_speech_prob": 3.120187466265634e-05
    },
    {
        "id": 114,
        "seek": 56952,
        "start": 576.1599731445312,
        "end": 580.5599975585938,
        "text": " this is what the model sees. So my suspicion is that it wouldn't be very good at this, and indeed",
        "tokens": [
            50696,
            341,
            307,
            437,
            264,
            2316,
            8194,
            13,
            407,
            452,
            32020,
            307,
            300,
            309,
            2759,
            380,
            312,
            588,
            665,
            412,
            341,
            11,
            293,
            6451,
            50916
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17793120443820953,
        "compression_ratio": 1.6385135650634766,
        "no_speech_prob": 3.120187466265634e-05
    },
    {
        "id": 115,
        "seek": 56952,
        "start": 580.5599975585938,
        "end": 584.719970703125,
        "text": " it is not. It doesn't actually know how many L's are in there. It thinks there are three, and",
        "tokens": [
            50916,
            309,
            307,
            406,
            13,
            467,
            1177,
            380,
            767,
            458,
            577,
            867,
            441,
            311,
            366,
            294,
            456,
            13,
            467,
            7309,
            456,
            366,
            1045,
            11,
            293,
            51124
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17793120443820953,
        "compression_ratio": 1.6385135650634766,
        "no_speech_prob": 3.120187466265634e-05
    },
    {
        "id": 116,
        "seek": 56952,
        "start": 584.719970703125,
        "end": 590.7999877929688,
        "text": " actually there are four, if I'm not getting this wrong myself. So that didn't go extremely well.",
        "tokens": [
            51124,
            767,
            456,
            366,
            1451,
            11,
            498,
            286,
            478,
            406,
            1242,
            341,
            2085,
            2059,
            13,
            407,
            300,
            994,
            380,
            352,
            4664,
            731,
            13,
            51428
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17793120443820953,
        "compression_ratio": 1.6385135650634766,
        "no_speech_prob": 3.120187466265634e-05
    },
    {
        "id": 117,
        "seek": 56952,
        "start": 590.7999877929688,
        "end": 598.5599975585938,
        "text": " Let's look at another kind of character-level task. So for example, here I asked GPT-4 to reverse",
        "tokens": [
            51428,
            961,
            311,
            574,
            412,
            1071,
            733,
            295,
            2517,
            12,
            12418,
            5633,
            13,
            407,
            337,
            1365,
            11,
            510,
            286,
            2351,
            26039,
            51,
            12,
            19,
            281,
            9943,
            51816
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17793120443820953,
        "compression_ratio": 1.6385135650634766,
        "no_speech_prob": 3.120187466265634e-05
    },
    {
        "id": 118,
        "seek": 59856,
        "start": 598.5599975585938,
        "end": 604.1599731445312,
        "text": " the string .defaultstyle, and it tried to use a code interpreter, and I stopped it, and I said",
        "tokens": [
            50364,
            264,
            6798,
            2411,
            20595,
            5107,
            15014,
            11,
            293,
            309,
            3031,
            281,
            764,
            257,
            3089,
            34132,
            11,
            293,
            286,
            5936,
            309,
            11,
            293,
            286,
            848,
            50644
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1929183453321457,
        "compression_ratio": 1.770909070968628,
        "no_speech_prob": 0.00016603760013822466
    },
    {
        "id": 119,
        "seek": 59856,
        "start": 604.1599731445312,
        "end": 611.52001953125,
        "text": " just do it, just try it, and it gave me jumble. So it doesn't actually really know how to reverse",
        "tokens": [
            50644,
            445,
            360,
            309,
            11,
            445,
            853,
            309,
            11,
            293,
            309,
            2729,
            385,
            361,
            16473,
            13,
            407,
            309,
            1177,
            380,
            767,
            534,
            458,
            577,
            281,
            9943,
            51012
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1929183453321457,
        "compression_ratio": 1.770909070968628,
        "no_speech_prob": 0.00016603760013822466
    },
    {
        "id": 120,
        "seek": 59856,
        "start": 611.52001953125,
        "end": 617.52001953125,
        "text": " this string going from right to left, so it gave a wrong result. So again, like working with this",
        "tokens": [
            51012,
            341,
            6798,
            516,
            490,
            558,
            281,
            1411,
            11,
            370,
            309,
            2729,
            257,
            2085,
            1874,
            13,
            407,
            797,
            11,
            411,
            1364,
            365,
            341,
            51312
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1929183453321457,
        "compression_ratio": 1.770909070968628,
        "no_speech_prob": 0.00016603760013822466
    },
    {
        "id": 121,
        "seek": 59856,
        "start": 617.52001953125,
        "end": 622.6400146484375,
        "text": " working hypothesis that maybe this is due to the tokenization, I tried a different approach. I said",
        "tokens": [
            51312,
            1364,
            17291,
            300,
            1310,
            341,
            307,
            3462,
            281,
            264,
            14862,
            2144,
            11,
            286,
            3031,
            257,
            819,
            3109,
            13,
            286,
            848,
            51568
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1929183453321457,
        "compression_ratio": 1.770909070968628,
        "no_speech_prob": 0.00016603760013822466
    },
    {
        "id": 122,
        "seek": 59856,
        "start": 622.6400146484375,
        "end": 627.8400268554688,
        "text": " okay, let's reverse the exact same string, but take the following approach. Step one, just print",
        "tokens": [
            51568,
            1392,
            11,
            718,
            311,
            9943,
            264,
            1900,
            912,
            6798,
            11,
            457,
            747,
            264,
            3480,
            3109,
            13,
            5470,
            472,
            11,
            445,
            4482,
            51828
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1929183453321457,
        "compression_ratio": 1.770909070968628,
        "no_speech_prob": 0.00016603760013822466
    },
    {
        "id": 123,
        "seek": 62784,
        "start": 627.8400268554688,
        "end": 632.9600219726562,
        "text": " out every single character separated by spaces, and then as a step two, reverse that list, and it",
        "tokens": [
            50364,
            484,
            633,
            2167,
            2517,
            12005,
            538,
            7673,
            11,
            293,
            550,
            382,
            257,
            1823,
            732,
            11,
            9943,
            300,
            1329,
            11,
            293,
            309,
            50620
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1780995875597,
        "compression_ratio": 1.9031007289886475,
        "no_speech_prob": 3.822907092398964e-05
    },
    {
        "id": 124,
        "seek": 62784,
        "start": 632.9600219726562,
        "end": 638.239990234375,
        "text": " again tried to use a tool, but when I stopped it, it first produced all the characters, and that was",
        "tokens": [
            50620,
            797,
            3031,
            281,
            764,
            257,
            2290,
            11,
            457,
            562,
            286,
            5936,
            309,
            11,
            309,
            700,
            7126,
            439,
            264,
            4342,
            11,
            293,
            300,
            390,
            50884
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1780995875597,
        "compression_ratio": 1.9031007289886475,
        "no_speech_prob": 3.822907092398964e-05
    },
    {
        "id": 125,
        "seek": 62784,
        "start": 638.239990234375,
        "end": 643.0399780273438,
        "text": " actually correct, and then it reversed them, and that was correct once it had this. So somehow it",
        "tokens": [
            50884,
            767,
            3006,
            11,
            293,
            550,
            309,
            30563,
            552,
            11,
            293,
            300,
            390,
            3006,
            1564,
            309,
            632,
            341,
            13,
            407,
            6063,
            309,
            51124
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1780995875597,
        "compression_ratio": 1.9031007289886475,
        "no_speech_prob": 3.822907092398964e-05
    },
    {
        "id": 126,
        "seek": 62784,
        "start": 643.0399780273438,
        "end": 648.47998046875,
        "text": " can't reverse it directly, but when you go just first, you know, listing it out in order, it can",
        "tokens": [
            51124,
            393,
            380,
            9943,
            309,
            3838,
            11,
            457,
            562,
            291,
            352,
            445,
            700,
            11,
            291,
            458,
            11,
            22161,
            309,
            484,
            294,
            1668,
            11,
            309,
            393,
            51396
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1780995875597,
        "compression_ratio": 1.9031007289886475,
        "no_speech_prob": 3.822907092398964e-05
    },
    {
        "id": 127,
        "seek": 62784,
        "start": 648.47998046875,
        "end": 654.0800170898438,
        "text": " do that somehow, and then it can, once it's broken up this way, this becomes all these individual",
        "tokens": [
            51396,
            360,
            300,
            6063,
            11,
            293,
            550,
            309,
            393,
            11,
            1564,
            309,
            311,
            5463,
            493,
            341,
            636,
            11,
            341,
            3643,
            439,
            613,
            2609,
            51676
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1780995875597,
        "compression_ratio": 1.9031007289886475,
        "no_speech_prob": 3.822907092398964e-05
    },
    {
        "id": 128,
        "seek": 65408,
        "start": 654.0800170898438,
        "end": 659.5999755859375,
        "text": " characters, and so now this is much easier for it to see these individual tokens and reverse them",
        "tokens": [
            50364,
            4342,
            11,
            293,
            370,
            586,
            341,
            307,
            709,
            3571,
            337,
            309,
            281,
            536,
            613,
            2609,
            22667,
            293,
            9943,
            552,
            50640
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18531586229801178,
        "compression_ratio": 1.57851243019104,
        "no_speech_prob": 8.220176096074283e-05
    },
    {
        "id": 129,
        "seek": 65408,
        "start": 659.5999755859375,
        "end": 667.6799926757812,
        "text": " and print them out. So that is kind of interesting. So let's continue now. Why are LLMs worse at",
        "tokens": [
            50640,
            293,
            4482,
            552,
            484,
            13,
            407,
            300,
            307,
            733,
            295,
            1880,
            13,
            407,
            718,
            311,
            2354,
            586,
            13,
            1545,
            366,
            441,
            43,
            26386,
            5324,
            412,
            51044
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18531586229801178,
        "compression_ratio": 1.57851243019104,
        "no_speech_prob": 8.220176096074283e-05
    },
    {
        "id": 130,
        "seek": 65408,
        "start": 668.3200073242188,
        "end": 674.3200073242188,
        "text": " non-English languages? And I briefly covered this already, but basically it's not only that",
        "tokens": [
            51076,
            2107,
            12,
            31254,
            1933,
            8650,
            30,
            400,
            286,
            10515,
            5343,
            341,
            1217,
            11,
            457,
            1936,
            309,
            311,
            406,
            787,
            300,
            51376
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18531586229801178,
        "compression_ratio": 1.57851243019104,
        "no_speech_prob": 8.220176096074283e-05
    },
    {
        "id": 131,
        "seek": 65408,
        "start": 674.3200073242188,
        "end": 679.52001953125,
        "text": " the language model sees less non-English data during training of the model parameters, but also",
        "tokens": [
            51376,
            264,
            2856,
            2316,
            8194,
            1570,
            2107,
            12,
            31254,
            1933,
            1412,
            1830,
            3097,
            295,
            264,
            2316,
            9834,
            11,
            457,
            611,
            51636
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18531586229801178,
        "compression_ratio": 1.57851243019104,
        "no_speech_prob": 8.220176096074283e-05
    },
    {
        "id": 132,
        "seek": 67952,
        "start": 679.52001953125,
        "end": 686.7999877929688,
        "text": " the tokenizer is not sufficiently trained on non-English data, and so here, for example,",
        "tokens": [
            50364,
            264,
            14862,
            6545,
            307,
            406,
            31868,
            8895,
            322,
            2107,
            12,
            31254,
            1933,
            1412,
            11,
            293,
            370,
            510,
            11,
            337,
            1365,
            11,
            50728
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25699999928474426,
        "compression_ratio": 1.8074073791503906,
        "no_speech_prob": 0.00818560365587473
    },
    {
        "id": 133,
        "seek": 67952,
        "start": 686.7999877929688,
        "end": 693.6799926757812,
        "text": " hello, how are you is five tokens, and its translation is 15 tokens, so this is a three times blow up,",
        "tokens": [
            50728,
            7751,
            11,
            577,
            366,
            291,
            307,
            1732,
            22667,
            11,
            293,
            1080,
            12853,
            307,
            2119,
            22667,
            11,
            370,
            341,
            307,
            257,
            1045,
            1413,
            6327,
            493,
            11,
            51072
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25699999928474426,
        "compression_ratio": 1.8074073791503906,
        "no_speech_prob": 0.00818560365587473
    },
    {
        "id": 134,
        "seek": 67952,
        "start": 694.47998046875,
        "end": 699.9199829101562,
        "text": " and so for example, annyeonghaseyo is just hello, basically in Korean, and that ends up being three",
        "tokens": [
            51112,
            293,
            370,
            337,
            1365,
            11,
            2324,
            18122,
            71,
            651,
            8308,
            307,
            445,
            7751,
            11,
            1936,
            294,
            6933,
            11,
            293,
            300,
            5314,
            493,
            885,
            1045,
            51384
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25699999928474426,
        "compression_ratio": 1.8074073791503906,
        "no_speech_prob": 0.00818560365587473
    },
    {
        "id": 135,
        "seek": 67952,
        "start": 699.9199829101562,
        "end": 703.5999755859375,
        "text": " tokens. I'm actually kind of surprised by that, because that is a very common phrase, and there's",
        "tokens": [
            51384,
            22667,
            13,
            286,
            478,
            767,
            733,
            295,
            6100,
            538,
            300,
            11,
            570,
            300,
            307,
            257,
            588,
            2689,
            9535,
            11,
            293,
            456,
            311,
            51568
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25699999928474426,
        "compression_ratio": 1.8074073791503906,
        "no_speech_prob": 0.00818560365587473
    },
    {
        "id": 136,
        "seek": 67952,
        "start": 703.5999755859375,
        "end": 708.3200073242188,
        "text": " just a typical greeting of like hello, and that ends up being three tokens, whereas our hello is a",
        "tokens": [
            51568,
            445,
            257,
            7476,
            28174,
            295,
            411,
            7751,
            11,
            293,
            300,
            5314,
            493,
            885,
            1045,
            22667,
            11,
            9735,
            527,
            7751,
            307,
            257,
            51804
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25699999928474426,
        "compression_ratio": 1.8074073791503906,
        "no_speech_prob": 0.00818560365587473
    },
    {
        "id": 137,
        "seek": 70832,
        "start": 708.3200073242188,
        "end": 712.4000244140625,
        "text": " single token, and so basically everything is a lot more bloated and diffuse, and this is, I think,",
        "tokens": [
            50364,
            2167,
            14862,
            11,
            293,
            370,
            1936,
            1203,
            307,
            257,
            688,
            544,
            1749,
            770,
            293,
            42165,
            11,
            293,
            341,
            307,
            11,
            286,
            519,
            11,
            50568
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17203034460544586,
        "compression_ratio": 1.5696721076965332,
        "no_speech_prob": 5.093702384328935e-06
    },
    {
        "id": 138,
        "seek": 70832,
        "start": 712.4000244140625,
        "end": 719.6799926757812,
        "text": " partly the reason that the model works worse on other languages. Coming back, why is LLM bad at",
        "tokens": [
            50568,
            17031,
            264,
            1778,
            300,
            264,
            2316,
            1985,
            5324,
            322,
            661,
            8650,
            13,
            12473,
            646,
            11,
            983,
            307,
            441,
            43,
            44,
            1578,
            412,
            50932
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17203034460544586,
        "compression_ratio": 1.5696721076965332,
        "no_speech_prob": 5.093702384328935e-06
    },
    {
        "id": 139,
        "seek": 70832,
        "start": 719.6799926757812,
        "end": 728.0800170898438,
        "text": " simple arithmetic? That has to do with the tokenization of numbers, and so you'll notice",
        "tokens": [
            50932,
            2199,
            42973,
            30,
            663,
            575,
            281,
            360,
            365,
            264,
            14862,
            2144,
            295,
            3547,
            11,
            293,
            370,
            291,
            603,
            3449,
            51352
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17203034460544586,
        "compression_ratio": 1.5696721076965332,
        "no_speech_prob": 5.093702384328935e-06
    },
    {
        "id": 140,
        "seek": 70832,
        "start": 728.0800170898438,
        "end": 733.6799926757812,
        "text": " that, for example, addition is very sort of like, there's an algorithm that is like character level",
        "tokens": [
            51352,
            300,
            11,
            337,
            1365,
            11,
            4500,
            307,
            588,
            1333,
            295,
            411,
            11,
            456,
            311,
            364,
            9284,
            300,
            307,
            411,
            2517,
            1496,
            51632
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17203034460544586,
        "compression_ratio": 1.5696721076965332,
        "no_speech_prob": 5.093702384328935e-06
    },
    {
        "id": 141,
        "seek": 73368,
        "start": 733.6799926757812,
        "end": 738.719970703125,
        "text": " for doing addition, so for example, here we would first add the ones, and then the tens, and then the",
        "tokens": [
            50364,
            337,
            884,
            4500,
            11,
            370,
            337,
            1365,
            11,
            510,
            321,
            576,
            700,
            909,
            264,
            2306,
            11,
            293,
            550,
            264,
            10688,
            11,
            293,
            550,
            264,
            50616
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19318625330924988,
        "compression_ratio": 1.7307692766189575,
        "no_speech_prob": 3.5356242733541876e-05
    },
    {
        "id": 142,
        "seek": 73368,
        "start": 738.719970703125,
        "end": 745.3599853515625,
        "text": " hundreds. You have to refer to specific parts of these digits, but these numbers are represented",
        "tokens": [
            50616,
            6779,
            13,
            509,
            362,
            281,
            2864,
            281,
            2685,
            3166,
            295,
            613,
            27011,
            11,
            457,
            613,
            3547,
            366,
            10379,
            50948
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19318625330924988,
        "compression_ratio": 1.7307692766189575,
        "no_speech_prob": 3.5356242733541876e-05
    },
    {
        "id": 143,
        "seek": 73368,
        "start": 745.3599853515625,
        "end": 749.4400024414062,
        "text": " completely arbitrarily based on whatever happened to merge or not merge during the tokenization",
        "tokens": [
            50948,
            2584,
            19071,
            3289,
            2361,
            322,
            2035,
            2011,
            281,
            22183,
            420,
            406,
            22183,
            1830,
            264,
            14862,
            2144,
            51152
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19318625330924988,
        "compression_ratio": 1.7307692766189575,
        "no_speech_prob": 3.5356242733541876e-05
    },
    {
        "id": 144,
        "seek": 73368,
        "start": 749.4400024414062,
        "end": 754.1599731445312,
        "text": " process. There's an entire blog post about this that I think is quite good. Integer tokenization",
        "tokens": [
            51152,
            1399,
            13,
            821,
            311,
            364,
            2302,
            6968,
            2183,
            466,
            341,
            300,
            286,
            519,
            307,
            1596,
            665,
            13,
            5681,
            30744,
            14862,
            2144,
            51388
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19318625330924988,
        "compression_ratio": 1.7307692766189575,
        "no_speech_prob": 3.5356242733541876e-05
    },
    {
        "id": 145,
        "seek": 73368,
        "start": 754.1599731445312,
        "end": 760.0,
        "text": " is insane, and this person basically systematically explores the tokenization of numbers in, I believe,",
        "tokens": [
            51388,
            307,
            10838,
            11,
            293,
            341,
            954,
            1936,
            39531,
            45473,
            264,
            14862,
            2144,
            295,
            3547,
            294,
            11,
            286,
            1697,
            11,
            51680
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19318625330924988,
        "compression_ratio": 1.7307692766189575,
        "no_speech_prob": 3.5356242733541876e-05
    },
    {
        "id": 146,
        "seek": 76000,
        "start": 760.0800170898438,
        "end": 766.7999877929688,
        "text": " this is GPT-2, and so they noticed that, for example, for four-digit numbers, you can take",
        "tokens": [
            50368,
            341,
            307,
            26039,
            51,
            12,
            17,
            11,
            293,
            370,
            436,
            5694,
            300,
            11,
            337,
            1365,
            11,
            337,
            1451,
            12,
            25259,
            270,
            3547,
            11,
            291,
            393,
            747,
            50704
        ],
        "temperature": 0.0,
        "avg_logprob": -0.197509765625,
        "compression_ratio": 1.9083665609359741,
        "no_speech_prob": 0.00020662952738348395
    },
    {
        "id": 147,
        "seek": 76000,
        "start": 766.7999877929688,
        "end": 773.4400024414062,
        "text": " a look at whether it is a single token, or whether it is two tokens that is a 1-3, or a 2-2, or a 3-1",
        "tokens": [
            50704,
            257,
            574,
            412,
            1968,
            309,
            307,
            257,
            2167,
            14862,
            11,
            420,
            1968,
            309,
            307,
            732,
            22667,
            300,
            307,
            257,
            502,
            12,
            18,
            11,
            420,
            257,
            568,
            12,
            17,
            11,
            420,
            257,
            805,
            12,
            16,
            51036
        ],
        "temperature": 0.0,
        "avg_logprob": -0.197509765625,
        "compression_ratio": 1.9083665609359741,
        "no_speech_prob": 0.00020662952738348395
    },
    {
        "id": 148,
        "seek": 76000,
        "start": 773.4400024414062,
        "end": 777.9199829101562,
        "text": " combination, and so all the different numbers are all the different combinations, and you can imagine",
        "tokens": [
            51036,
            6562,
            11,
            293,
            370,
            439,
            264,
            819,
            3547,
            366,
            439,
            264,
            819,
            21267,
            11,
            293,
            291,
            393,
            3811,
            51260
        ],
        "temperature": 0.0,
        "avg_logprob": -0.197509765625,
        "compression_ratio": 1.9083665609359741,
        "no_speech_prob": 0.00020662952738348395
    },
    {
        "id": 149,
        "seek": 76000,
        "start": 777.9199829101562,
        "end": 784.1599731445312,
        "text": " this is all completely arbitrarily so, and the model unfortunately sometimes sees a token",
        "tokens": [
            51260,
            341,
            307,
            439,
            2584,
            19071,
            3289,
            370,
            11,
            293,
            264,
            2316,
            7015,
            2171,
            8194,
            257,
            14862,
            51572
        ],
        "temperature": 0.0,
        "avg_logprob": -0.197509765625,
        "compression_ratio": 1.9083665609359741,
        "no_speech_prob": 0.00020662952738348395
    },
    {
        "id": 150,
        "seek": 76000,
        "start": 784.1599731445312,
        "end": 789.3599853515625,
        "text": " for all four digits, sometimes for three, sometimes for two, sometimes for one, and it's in an",
        "tokens": [
            51572,
            337,
            439,
            1451,
            27011,
            11,
            2171,
            337,
            1045,
            11,
            2171,
            337,
            732,
            11,
            2171,
            337,
            472,
            11,
            293,
            309,
            311,
            294,
            364,
            51832
        ],
        "temperature": 0.0,
        "avg_logprob": -0.197509765625,
        "compression_ratio": 1.9083665609359741,
        "no_speech_prob": 0.00020662952738348395
    },
    {
        "id": 151,
        "seek": 78936,
        "start": 789.3599853515625,
        "end": 795.52001953125,
        "text": " arbitrary manner, and so this is definitely a headwind, if you will, for the language model,",
        "tokens": [
            50364,
            23211,
            9060,
            11,
            293,
            370,
            341,
            307,
            2138,
            257,
            1378,
            12199,
            11,
            498,
            291,
            486,
            11,
            337,
            264,
            2856,
            2316,
            11,
            50672
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2181096374988556,
        "compression_ratio": 1.7105263471603394,
        "no_speech_prob": 2.7535630579222925e-05
    },
    {
        "id": 152,
        "seek": 78936,
        "start": 795.52001953125,
        "end": 799.52001953125,
        "text": " and it's kind of incredible that it can kind of do it and deal with it, but it's also kind of",
        "tokens": [
            50672,
            293,
            309,
            311,
            733,
            295,
            4651,
            300,
            309,
            393,
            733,
            295,
            360,
            309,
            293,
            2028,
            365,
            309,
            11,
            457,
            309,
            311,
            611,
            733,
            295,
            50872
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2181096374988556,
        "compression_ratio": 1.7105263471603394,
        "no_speech_prob": 2.7535630579222925e-05
    },
    {
        "id": 153,
        "seek": 78936,
        "start": 799.52001953125,
        "end": 804.6400146484375,
        "text": " not ideal, and so that's why, for example, we saw that Meta, when they trained the LLAMA2 algorithm",
        "tokens": [
            50872,
            406,
            7157,
            11,
            293,
            370,
            300,
            311,
            983,
            11,
            337,
            1365,
            11,
            321,
            1866,
            300,
            6377,
            64,
            11,
            562,
            436,
            8895,
            264,
            441,
            43,
            38136,
            17,
            9284,
            51128
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2181096374988556,
        "compression_ratio": 1.7105263471603394,
        "no_speech_prob": 2.7535630579222925e-05
    },
    {
        "id": 154,
        "seek": 78936,
        "start": 804.6400146484375,
        "end": 811.0399780273438,
        "text": " and they used sentence piece, they make sure to split up all the digits, as an example, for",
        "tokens": [
            51128,
            293,
            436,
            1143,
            8174,
            2522,
            11,
            436,
            652,
            988,
            281,
            7472,
            493,
            439,
            264,
            27011,
            11,
            382,
            364,
            1365,
            11,
            337,
            51448
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2181096374988556,
        "compression_ratio": 1.7105263471603394,
        "no_speech_prob": 2.7535630579222925e-05
    },
    {
        "id": 155,
        "seek": 78936,
        "start": 812.0,
        "end": 816.239990234375,
        "text": " LLAMA2, and this is partly to improve simple arithmetic kind of performance.",
        "tokens": [
            51496,
            441,
            43,
            38136,
            17,
            11,
            293,
            341,
            307,
            17031,
            281,
            3470,
            2199,
            42973,
            733,
            295,
            3389,
            13,
            51708
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2181096374988556,
        "compression_ratio": 1.7105263471603394,
        "no_speech_prob": 2.7535630579222925e-05
    },
    {
        "id": 156,
        "seek": 81624,
        "start": 816.5599975585938,
        "end": 822.47998046875,
        "text": " And finally, why is GPT-2 not as good in Python? Again, this is partly a modeling issue",
        "tokens": [
            50380,
            400,
            2721,
            11,
            983,
            307,
            26039,
            51,
            12,
            17,
            406,
            382,
            665,
            294,
            15329,
            30,
            3764,
            11,
            341,
            307,
            17031,
            257,
            15983,
            2734,
            50676
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25537553429603577,
        "compression_ratio": 1.7003610134124756,
        "no_speech_prob": 8.801098374533467e-06
    },
    {
        "id": 157,
        "seek": 81624,
        "start": 822.47998046875,
        "end": 826.3200073242188,
        "text": " in the architecture, and the data set, and the strength of the model, but it's also",
        "tokens": [
            50676,
            294,
            264,
            9482,
            11,
            293,
            264,
            1412,
            992,
            11,
            293,
            264,
            3800,
            295,
            264,
            2316,
            11,
            457,
            309,
            311,
            611,
            50868
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25537553429603577,
        "compression_ratio": 1.7003610134124756,
        "no_speech_prob": 8.801098374533467e-06
    },
    {
        "id": 158,
        "seek": 81624,
        "start": 826.3200073242188,
        "end": 832.5599975585938,
        "text": " partly tokenization, because as we saw here with the simple Python example, the encoding efficiency",
        "tokens": [
            50868,
            17031,
            14862,
            2144,
            11,
            570,
            382,
            321,
            1866,
            510,
            365,
            264,
            2199,
            15329,
            1365,
            11,
            264,
            43430,
            10493,
            51180
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25537553429603577,
        "compression_ratio": 1.7003610134124756,
        "no_speech_prob": 8.801098374533467e-06
    },
    {
        "id": 159,
        "seek": 81624,
        "start": 832.5599975585938,
        "end": 837.1199951171875,
        "text": " of the tokenizer for handling spaces in Python is terrible, and every single space is an individual",
        "tokens": [
            51180,
            295,
            264,
            14862,
            6545,
            337,
            13175,
            7673,
            294,
            15329,
            307,
            6237,
            11,
            293,
            633,
            2167,
            1901,
            307,
            364,
            2609,
            51408
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25537553429603577,
        "compression_ratio": 1.7003610134124756,
        "no_speech_prob": 8.801098374533467e-06
    },
    {
        "id": 160,
        "seek": 81624,
        "start": 837.1199951171875,
        "end": 842.3200073242188,
        "text": " token, and this dramatically reduces the context length that the model can attend across, so that's",
        "tokens": [
            51408,
            14862,
            11,
            293,
            341,
            17548,
            18081,
            264,
            4319,
            4641,
            300,
            264,
            2316,
            393,
            6888,
            2108,
            11,
            370,
            300,
            311,
            51668
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25537553429603577,
        "compression_ratio": 1.7003610134124756,
        "no_speech_prob": 8.801098374533467e-06
    },
    {
        "id": 161,
        "seek": 84232,
        "start": 843.280029296875,
        "end": 849.2000122070312,
        "text": " almost like a tokenization bug for GPT-2, and that was later fixed with GPT-4. Okay,",
        "tokens": [
            50412,
            1920,
            411,
            257,
            14862,
            2144,
            7426,
            337,
            26039,
            51,
            12,
            17,
            11,
            293,
            300,
            390,
            1780,
            6806,
            365,
            26039,
            51,
            12,
            19,
            13,
            1033,
            11,
            50708
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1822519153356552,
        "compression_ratio": 1.6937984228134155,
        "no_speech_prob": 0.004609499592334032
    },
    {
        "id": 162,
        "seek": 84232,
        "start": 849.2000122070312,
        "end": 853.9199829101562,
        "text": " so here's another fun one. My LLM abruptly halts when it sees the string end of text.",
        "tokens": [
            50708,
            370,
            510,
            311,
            1071,
            1019,
            472,
            13,
            1222,
            441,
            43,
            44,
            49642,
            7523,
            1373,
            562,
            309,
            8194,
            264,
            6798,
            917,
            295,
            2487,
            13,
            50944
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1822519153356552,
        "compression_ratio": 1.6937984228134155,
        "no_speech_prob": 0.004609499592334032
    },
    {
        "id": 163,
        "seek": 84232,
        "start": 854.5599975585938,
        "end": 860.8800048828125,
        "text": " So here's a very strange behavior. Print a string end of text, is what I told GPT-4,",
        "tokens": [
            50976,
            407,
            510,
            311,
            257,
            588,
            5861,
            5223,
            13,
            34439,
            257,
            6798,
            917,
            295,
            2487,
            11,
            307,
            437,
            286,
            1907,
            26039,
            51,
            12,
            19,
            11,
            51292
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1822519153356552,
        "compression_ratio": 1.6937984228134155,
        "no_speech_prob": 0.004609499592334032
    },
    {
        "id": 164,
        "seek": 84232,
        "start": 860.8800048828125,
        "end": 865.9199829101562,
        "text": " and it says, could you please specify the string? And I'm telling it, give me end of text,",
        "tokens": [
            51292,
            293,
            309,
            1619,
            11,
            727,
            291,
            1767,
            16500,
            264,
            6798,
            30,
            400,
            286,
            478,
            3585,
            309,
            11,
            976,
            385,
            917,
            295,
            2487,
            11,
            51544
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1822519153356552,
        "compression_ratio": 1.6937984228134155,
        "no_speech_prob": 0.004609499592334032
    },
    {
        "id": 165,
        "seek": 84232,
        "start": 865.9199829101562,
        "end": 870.7999877929688,
        "text": " and it seems like there's an issue. It's not seeing end of text, and then I give it end of",
        "tokens": [
            51544,
            293,
            309,
            2544,
            411,
            456,
            311,
            364,
            2734,
            13,
            467,
            311,
            406,
            2577,
            917,
            295,
            2487,
            11,
            293,
            550,
            286,
            976,
            309,
            917,
            295,
            51788
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1822519153356552,
        "compression_ratio": 1.6937984228134155,
        "no_speech_prob": 0.004609499592334032
    },
    {
        "id": 166,
        "seek": 87080,
        "start": 870.7999877929688,
        "end": 875.760009765625,
        "text": " text as the string, and then here's the string, and then it just doesn't print it. So obviously,",
        "tokens": [
            50364,
            2487,
            382,
            264,
            6798,
            11,
            293,
            550,
            510,
            311,
            264,
            6798,
            11,
            293,
            550,
            309,
            445,
            1177,
            380,
            4482,
            309,
            13,
            407,
            2745,
            11,
            50612
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2118866890668869,
        "compression_ratio": 1.8204081058502197,
        "no_speech_prob": 0.00020662917813751847
    },
    {
        "id": 167,
        "seek": 87080,
        "start": 875.760009765625,
        "end": 878.9600219726562,
        "text": " something is breaking here with respect to the handling of the special token,",
        "tokens": [
            50612,
            746,
            307,
            7697,
            510,
            365,
            3104,
            281,
            264,
            13175,
            295,
            264,
            2121,
            14862,
            11,
            50772
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2118866890668869,
        "compression_ratio": 1.8204081058502197,
        "no_speech_prob": 0.00020662917813751847
    },
    {
        "id": 168,
        "seek": 87080,
        "start": 878.9600219726562,
        "end": 883.5999755859375,
        "text": " and I don't actually know what OpenAI is doing under the hood here, and whether they are",
        "tokens": [
            50772,
            293,
            286,
            500,
            380,
            767,
            458,
            437,
            7238,
            48698,
            307,
            884,
            833,
            264,
            13376,
            510,
            11,
            293,
            1968,
            436,
            366,
            51004
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2118866890668869,
        "compression_ratio": 1.8204081058502197,
        "no_speech_prob": 0.00020662917813751847
    },
    {
        "id": 169,
        "seek": 87080,
        "start": 883.5999755859375,
        "end": 893.2000122070312,
        "text": " potentially parsing this as an actual token, instead of this just being end of text as like",
        "tokens": [
            51004,
            7263,
            21156,
            278,
            341,
            382,
            364,
            3539,
            14862,
            11,
            2602,
            295,
            341,
            445,
            885,
            917,
            295,
            2487,
            382,
            411,
            51484
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2118866890668869,
        "compression_ratio": 1.8204081058502197,
        "no_speech_prob": 0.00020662917813751847
    },
    {
        "id": 170,
        "seek": 87080,
        "start": 893.2000122070312,
        "end": 898.719970703125,
        "text": " individual sort of pieces of it, without the special token handling logic, and so it might",
        "tokens": [
            51484,
            2609,
            1333,
            295,
            3755,
            295,
            309,
            11,
            1553,
            264,
            2121,
            14862,
            13175,
            9952,
            11,
            293,
            370,
            309,
            1062,
            51760
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2118866890668869,
        "compression_ratio": 1.8204081058502197,
        "no_speech_prob": 0.00020662917813751847
    },
    {
        "id": 171,
        "seek": 89872,
        "start": 898.719970703125,
        "end": 904.239990234375,
        "text": " be that someone, when they're calling .encode, they are passing in the allowed special, and they",
        "tokens": [
            50364,
            312,
            300,
            1580,
            11,
            562,
            436,
            434,
            5141,
            2411,
            268,
            22332,
            11,
            436,
            366,
            8437,
            294,
            264,
            4350,
            2121,
            11,
            293,
            436,
            50640
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22286184132099152,
        "compression_ratio": 1.8841699361801147,
        "no_speech_prob": 0.0006986689986661077
    },
    {
        "id": 172,
        "seek": 89872,
        "start": 904.239990234375,
        "end": 909.760009765625,
        "text": " are allowing end of text as a special character in the user prompt, but the user prompt, of course,",
        "tokens": [
            50640,
            366,
            8293,
            917,
            295,
            2487,
            382,
            257,
            2121,
            2517,
            294,
            264,
            4195,
            12391,
            11,
            457,
            264,
            4195,
            12391,
            11,
            295,
            1164,
            11,
            50916
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22286184132099152,
        "compression_ratio": 1.8841699361801147,
        "no_speech_prob": 0.0006986689986661077
    },
    {
        "id": 173,
        "seek": 89872,
        "start": 909.760009765625,
        "end": 916.3200073242188,
        "text": " is a sort of attacker-controlled text, so you would hope that they don't really parse or use special",
        "tokens": [
            50916,
            307,
            257,
            1333,
            295,
            35871,
            12,
            49344,
            2487,
            11,
            370,
            291,
            576,
            1454,
            300,
            436,
            500,
            380,
            534,
            48377,
            420,
            764,
            2121,
            51244
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22286184132099152,
        "compression_ratio": 1.8841699361801147,
        "no_speech_prob": 0.0006986689986661077
    },
    {
        "id": 174,
        "seek": 89872,
        "start": 916.3200073242188,
        "end": 922.1599731445312,
        "text": " tokens from that kind of input, but it appears that there's something definitely going wrong here,",
        "tokens": [
            51244,
            22667,
            490,
            300,
            733,
            295,
            4846,
            11,
            457,
            309,
            7038,
            300,
            456,
            311,
            746,
            2138,
            516,
            2085,
            510,
            11,
            51536
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22286184132099152,
        "compression_ratio": 1.8841699361801147,
        "no_speech_prob": 0.0006986689986661077
    },
    {
        "id": 175,
        "seek": 89872,
        "start": 922.1599731445312,
        "end": 927.5999755859375,
        "text": " and so your knowledge of these special tokens ends up being an attack surface, potentially,",
        "tokens": [
            51536,
            293,
            370,
            428,
            3601,
            295,
            613,
            2121,
            22667,
            5314,
            493,
            885,
            364,
            2690,
            3753,
            11,
            7263,
            11,
            51808
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22286184132099152,
        "compression_ratio": 1.8841699361801147,
        "no_speech_prob": 0.0006986689986661077
    },
    {
        "id": 176,
        "seek": 92760,
        "start": 927.5999755859375,
        "end": 933.9199829101562,
        "text": " and so if you'd like to confuse LLMs, then just try to give them some special tokens and see if",
        "tokens": [
            50364,
            293,
            370,
            498,
            291,
            1116,
            411,
            281,
            28584,
            441,
            43,
            26386,
            11,
            550,
            445,
            853,
            281,
            976,
            552,
            512,
            2121,
            22667,
            293,
            536,
            498,
            50680
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18988560140132904,
        "compression_ratio": 1.6059322357177734,
        "no_speech_prob": 3.071816172450781e-05
    },
    {
        "id": 177,
        "seek": 92760,
        "start": 933.9199829101562,
        "end": 939.8400268554688,
        "text": " you're breaking something by chance. Okay, so this next one is a really fun one, the trailing",
        "tokens": [
            50680,
            291,
            434,
            7697,
            746,
            538,
            2931,
            13,
            1033,
            11,
            370,
            341,
            958,
            472,
            307,
            257,
            534,
            1019,
            472,
            11,
            264,
            944,
            4883,
            50976
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18988560140132904,
        "compression_ratio": 1.6059322357177734,
        "no_speech_prob": 3.071816172450781e-05
    },
    {
        "id": 178,
        "seek": 92760,
        "start": 939.8400268554688,
        "end": 947.760009765625,
        "text": " whitespace issue. So if you come to Playground, and we come here to GPT 3.5 Turbo Instruct,",
        "tokens": [
            50976,
            21909,
            17940,
            2734,
            13,
            407,
            498,
            291,
            808,
            281,
            5506,
            2921,
            11,
            293,
            321,
            808,
            510,
            281,
            26039,
            51,
            805,
            13,
            20,
            35848,
            2730,
            1757,
            11,
            51372
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18988560140132904,
        "compression_ratio": 1.6059322357177734,
        "no_speech_prob": 3.071816172450781e-05
    },
    {
        "id": 179,
        "seek": 92760,
        "start": 947.760009765625,
        "end": 952.7999877929688,
        "text": " so this is not a chat model, this is a completion model, so think of it more like it's a lot more",
        "tokens": [
            51372,
            370,
            341,
            307,
            406,
            257,
            5081,
            2316,
            11,
            341,
            307,
            257,
            19372,
            2316,
            11,
            370,
            519,
            295,
            309,
            544,
            411,
            309,
            311,
            257,
            688,
            544,
            51624
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18988560140132904,
        "compression_ratio": 1.6059322357177734,
        "no_speech_prob": 3.071816172450781e-05
    },
    {
        "id": 180,
        "seek": 95280,
        "start": 952.7999877929688,
        "end": 958.6400146484375,
        "text": " closer to a base model. It does completion, it will continue the token sequence. So here's a",
        "tokens": [
            50364,
            4966,
            281,
            257,
            3096,
            2316,
            13,
            467,
            775,
            19372,
            11,
            309,
            486,
            2354,
            264,
            14862,
            8310,
            13,
            407,
            510,
            311,
            257,
            50656
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19364033639431,
        "compression_ratio": 1.7442922592163086,
        "no_speech_prob": 0.014281274750828743
    },
    {
        "id": 181,
        "seek": 95280,
        "start": 958.6400146484375,
        "end": 964.0,
        "text": " tagline for Ice Cream Shop, and we want to continue the sequence, and so we can submit and get a bunch",
        "tokens": [
            50656,
            6162,
            1889,
            337,
            15332,
            25358,
            16319,
            11,
            293,
            321,
            528,
            281,
            2354,
            264,
            8310,
            11,
            293,
            370,
            321,
            393,
            10315,
            293,
            483,
            257,
            3840,
            50924
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19364033639431,
        "compression_ratio": 1.7442922592163086,
        "no_speech_prob": 0.014281274750828743
    },
    {
        "id": 182,
        "seek": 95280,
        "start": 964.0,
        "end": 972.4000244140625,
        "text": " of tokens. Okay, no problem, but now suppose I do this, but instead of pressing submit here, I do",
        "tokens": [
            50924,
            295,
            22667,
            13,
            1033,
            11,
            572,
            1154,
            11,
            457,
            586,
            7297,
            286,
            360,
            341,
            11,
            457,
            2602,
            295,
            12417,
            10315,
            510,
            11,
            286,
            360,
            51344
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19364033639431,
        "compression_ratio": 1.7442922592163086,
        "no_speech_prob": 0.014281274750828743
    },
    {
        "id": 183,
        "seek": 95280,
        "start": 972.4000244140625,
        "end": 977.9199829101562,
        "text": " here's a tagline for Ice Cream Shop space, so I have a space here before I click submit.",
        "tokens": [
            51344,
            510,
            311,
            257,
            6162,
            1889,
            337,
            15332,
            25358,
            16319,
            1901,
            11,
            370,
            286,
            362,
            257,
            1901,
            510,
            949,
            286,
            2052,
            10315,
            13,
            51620
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19364033639431,
        "compression_ratio": 1.7442922592163086,
        "no_speech_prob": 0.014281274750828743
    },
    {
        "id": 184,
        "seek": 97792,
        "start": 978.47998046875,
        "end": 982.5599975585938,
        "text": " We get a warning, your text ends in the trailing space, which causes worse performance due to how",
        "tokens": [
            50392,
            492,
            483,
            257,
            9164,
            11,
            428,
            2487,
            5314,
            294,
            264,
            944,
            4883,
            1901,
            11,
            597,
            7700,
            5324,
            3389,
            3462,
            281,
            577,
            50596
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2524796724319458,
        "compression_ratio": 1.7185184955596924,
        "no_speech_prob": 4.0063685446511954e-05
    },
    {
        "id": 185,
        "seek": 97792,
        "start": 982.5599975585938,
        "end": 989.1199951171875,
        "text": " API splits text into tokens. So what's happening here, it still gave us a sort of completion here,",
        "tokens": [
            50596,
            9362,
            37741,
            2487,
            666,
            22667,
            13,
            407,
            437,
            311,
            2737,
            510,
            11,
            309,
            920,
            2729,
            505,
            257,
            1333,
            295,
            19372,
            510,
            11,
            50924
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2524796724319458,
        "compression_ratio": 1.7185184955596924,
        "no_speech_prob": 4.0063685446511954e-05
    },
    {
        "id": 186,
        "seek": 97792,
        "start": 989.1199951171875,
        "end": 993.760009765625,
        "text": " but let's take a look at what's happening. So here's a tagline for an Ice Cream Shop,",
        "tokens": [
            50924,
            457,
            718,
            311,
            747,
            257,
            574,
            412,
            437,
            311,
            2737,
            13,
            407,
            510,
            311,
            257,
            6162,
            1889,
            337,
            364,
            15332,
            25358,
            16319,
            11,
            51156
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2524796724319458,
        "compression_ratio": 1.7185184955596924,
        "no_speech_prob": 4.0063685446511954e-05
    },
    {
        "id": 187,
        "seek": 97792,
        "start": 994.8800048828125,
        "end": 1000.47998046875,
        "text": " and then what does this look like in the actual training data? Suppose you found the completion",
        "tokens": [
            51212,
            293,
            550,
            437,
            775,
            341,
            574,
            411,
            294,
            264,
            3539,
            3097,
            1412,
            30,
            21360,
            291,
            1352,
            264,
            19372,
            51492
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2524796724319458,
        "compression_ratio": 1.7185184955596924,
        "no_speech_prob": 4.0063685446511954e-05
    },
    {
        "id": 188,
        "seek": 97792,
        "start": 1000.47998046875,
        "end": 1004.719970703125,
        "text": " in the training document somewhere on the internet, and the LLM trained on this data,",
        "tokens": [
            51492,
            294,
            264,
            3097,
            4166,
            4079,
            322,
            264,
            4705,
            11,
            293,
            264,
            441,
            43,
            44,
            8895,
            322,
            341,
            1412,
            11,
            51704
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2524796724319458,
        "compression_ratio": 1.7185184955596924,
        "no_speech_prob": 4.0063685446511954e-05
    },
    {
        "id": 189,
        "seek": 100472,
        "start": 1004.719970703125,
        "end": 1009.3599853515625,
        "text": " so maybe it's something like, oh yeah, maybe that's the tagline, that's a terrible tagline,",
        "tokens": [
            50364,
            370,
            1310,
            309,
            311,
            746,
            411,
            11,
            1954,
            1338,
            11,
            1310,
            300,
            311,
            264,
            6162,
            1889,
            11,
            300,
            311,
            257,
            6237,
            6162,
            1889,
            11,
            50596
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25200533866882324,
        "compression_ratio": 1.6880733966827393,
        "no_speech_prob": 0.0006986726075410843
    },
    {
        "id": 190,
        "seek": 100472,
        "start": 1009.3599853515625,
        "end": 1016.47998046875,
        "text": " but notice here that when I create O, you see that because there's the space characters always",
        "tokens": [
            50596,
            457,
            3449,
            510,
            300,
            562,
            286,
            1884,
            422,
            11,
            291,
            536,
            300,
            570,
            456,
            311,
            264,
            1901,
            4342,
            1009,
            50952
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25200533866882324,
        "compression_ratio": 1.6880733966827393,
        "no_speech_prob": 0.0006986726075410843
    },
    {
        "id": 191,
        "seek": 100472,
        "start": 1016.47998046875,
        "end": 1023.280029296875,
        "text": " a prefix to these tokens in GPT, so it's not an O token, it's a space O token. The space",
        "tokens": [
            50952,
            257,
            46969,
            281,
            613,
            22667,
            294,
            26039,
            51,
            11,
            370,
            309,
            311,
            406,
            364,
            422,
            14862,
            11,
            309,
            311,
            257,
            1901,
            422,
            14862,
            13,
            440,
            1901,
            51292
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25200533866882324,
        "compression_ratio": 1.6880733966827393,
        "no_speech_prob": 0.0006986726075410843
    },
    {
        "id": 192,
        "seek": 100472,
        "start": 1023.280029296875,
        "end": 1030.9599609375,
        "text": " is part of the O, and together they are token 8840, that's space O. So what's happening here",
        "tokens": [
            51292,
            307,
            644,
            295,
            264,
            422,
            11,
            293,
            1214,
            436,
            366,
            14862,
            24587,
            5254,
            11,
            300,
            311,
            1901,
            422,
            13,
            407,
            437,
            311,
            2737,
            510,
            51676
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25200533866882324,
        "compression_ratio": 1.6880733966827393,
        "no_speech_prob": 0.0006986726075410843
    },
    {
        "id": 193,
        "seek": 103096,
        "start": 1031.9200439453125,
        "end": 1038.8800048828125,
        "text": " is that when I just have it like this, and I let it complete the next token, it can sample the",
        "tokens": [
            50412,
            307,
            300,
            562,
            286,
            445,
            362,
            309,
            411,
            341,
            11,
            293,
            286,
            718,
            309,
            3566,
            264,
            958,
            14862,
            11,
            309,
            393,
            6889,
            264,
            50760
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20177283883094788,
        "compression_ratio": 1.6899563074111938,
        "no_speech_prob": 0.00011412216554163024
    },
    {
        "id": 194,
        "seek": 103096,
        "start": 1038.8800048828125,
        "end": 1044.6400146484375,
        "text": " space O token, but instead if I have this and I add my space, then what I'm doing here when I",
        "tokens": [
            50760,
            1901,
            422,
            14862,
            11,
            457,
            2602,
            498,
            286,
            362,
            341,
            293,
            286,
            909,
            452,
            1901,
            11,
            550,
            437,
            286,
            478,
            884,
            510,
            562,
            286,
            51048
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20177283883094788,
        "compression_ratio": 1.6899563074111938,
        "no_speech_prob": 0.00011412216554163024
    },
    {
        "id": 195,
        "seek": 103096,
        "start": 1044.6400146484375,
        "end": 1051.199951171875,
        "text": " encode this string, is I have basically, here's the tagline for an Ice Cream Shop, and this space",
        "tokens": [
            51048,
            2058,
            1429,
            341,
            6798,
            11,
            307,
            286,
            362,
            1936,
            11,
            510,
            311,
            264,
            6162,
            1889,
            337,
            364,
            15332,
            25358,
            16319,
            11,
            293,
            341,
            1901,
            51376
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20177283883094788,
        "compression_ratio": 1.6899563074111938,
        "no_speech_prob": 0.00011412216554163024
    },
    {
        "id": 196,
        "seek": 103096,
        "start": 1051.199951171875,
        "end": 1058.6400146484375,
        "text": " at the very end becomes a token 220. And so we've added token 220, and this token otherwise would be",
        "tokens": [
            51376,
            412,
            264,
            588,
            917,
            3643,
            257,
            14862,
            29387,
            13,
            400,
            370,
            321,
            600,
            3869,
            14862,
            29387,
            11,
            293,
            341,
            14862,
            5911,
            576,
            312,
            51748
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20177283883094788,
        "compression_ratio": 1.6899563074111938,
        "no_speech_prob": 0.00011412216554163024
    },
    {
        "id": 197,
        "seek": 105864,
        "start": 1058.6400146484375,
        "end": 1063.9200439453125,
        "text": " part of the tagline, because if there actually is a tagline here, so space O is the token,",
        "tokens": [
            50364,
            644,
            295,
            264,
            6162,
            1889,
            11,
            570,
            498,
            456,
            767,
            307,
            257,
            6162,
            1889,
            510,
            11,
            370,
            1901,
            422,
            307,
            264,
            14862,
            11,
            50628
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1773373931646347,
        "compression_ratio": 1.9119999408721924,
        "no_speech_prob": 7.484607340302318e-05
    },
    {
        "id": 198,
        "seek": 105864,
        "start": 1064.719970703125,
        "end": 1070.3199462890625,
        "text": " and so this is suddenly out of distribution for the model, because this space is part of the next",
        "tokens": [
            50668,
            293,
            370,
            341,
            307,
            5800,
            484,
            295,
            7316,
            337,
            264,
            2316,
            11,
            570,
            341,
            1901,
            307,
            644,
            295,
            264,
            958,
            50948
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1773373931646347,
        "compression_ratio": 1.9119999408721924,
        "no_speech_prob": 7.484607340302318e-05
    },
    {
        "id": 199,
        "seek": 105864,
        "start": 1070.3199462890625,
        "end": 1077.52001953125,
        "text": " token, but we're putting it here like this, and the model has seen very very little data of actual",
        "tokens": [
            50948,
            14862,
            11,
            457,
            321,
            434,
            3372,
            309,
            510,
            411,
            341,
            11,
            293,
            264,
            2316,
            575,
            1612,
            588,
            588,
            707,
            1412,
            295,
            3539,
            51308
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1773373931646347,
        "compression_ratio": 1.9119999408721924,
        "no_speech_prob": 7.484607340302318e-05
    },
    {
        "id": 200,
        "seek": 105864,
        "start": 1077.52001953125,
        "end": 1082.47998046875,
        "text": " space by itself, and we're asking it to complete the sequence, like add in more tokens, but the",
        "tokens": [
            51308,
            1901,
            538,
            2564,
            11,
            293,
            321,
            434,
            3365,
            309,
            281,
            3566,
            264,
            8310,
            11,
            411,
            909,
            294,
            544,
            22667,
            11,
            457,
            264,
            51556
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1773373931646347,
        "compression_ratio": 1.9119999408721924,
        "no_speech_prob": 7.484607340302318e-05
    },
    {
        "id": 201,
        "seek": 105864,
        "start": 1082.47998046875,
        "end": 1088.3199462890625,
        "text": " problem is that we've sort of begun the first token, and now it's been split up, and now we're",
        "tokens": [
            51556,
            1154,
            307,
            300,
            321,
            600,
            1333,
            295,
            16009,
            264,
            700,
            14862,
            11,
            293,
            586,
            309,
            311,
            668,
            7472,
            493,
            11,
            293,
            586,
            321,
            434,
            51848
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1773373931646347,
        "compression_ratio": 1.9119999408721924,
        "no_speech_prob": 7.484607340302318e-05
    },
    {
        "id": 202,
        "seek": 108832,
        "start": 1088.3199462890625,
        "end": 1093.43994140625,
        "text": " out of distribution, and now arbitrary bad things happen, and it's just a very rare example for it",
        "tokens": [
            50364,
            484,
            295,
            7316,
            11,
            293,
            586,
            23211,
            1578,
            721,
            1051,
            11,
            293,
            309,
            311,
            445,
            257,
            588,
            5892,
            1365,
            337,
            309,
            50620
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17372047901153564,
        "compression_ratio": 1.7536232471466064,
        "no_speech_prob": 6.30274589639157e-05
    },
    {
        "id": 203,
        "seek": 108832,
        "start": 1093.43994140625,
        "end": 1098.800048828125,
        "text": " to see something like that, and that's why we did the warning. So the fundamental issue here is of",
        "tokens": [
            50620,
            281,
            536,
            746,
            411,
            300,
            11,
            293,
            300,
            311,
            983,
            321,
            630,
            264,
            9164,
            13,
            407,
            264,
            8088,
            2734,
            510,
            307,
            295,
            50888
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17372047901153564,
        "compression_ratio": 1.7536232471466064,
        "no_speech_prob": 6.30274589639157e-05
    },
    {
        "id": 204,
        "seek": 108832,
        "start": 1098.800048828125,
        "end": 1105.199951171875,
        "text": " course that the LLM is on top of these tokens, and these tokens are text chunks, they're not",
        "tokens": [
            50888,
            1164,
            300,
            264,
            441,
            43,
            44,
            307,
            322,
            1192,
            295,
            613,
            22667,
            11,
            293,
            613,
            22667,
            366,
            2487,
            24004,
            11,
            436,
            434,
            406,
            51208
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17372047901153564,
        "compression_ratio": 1.7536232471466064,
        "no_speech_prob": 6.30274589639157e-05
    },
    {
        "id": 205,
        "seek": 108832,
        "start": 1105.199951171875,
        "end": 1110.0,
        "text": " characters in a way you and I would think of them, they are, these are the atoms of what the LLM is",
        "tokens": [
            51208,
            4342,
            294,
            257,
            636,
            291,
            293,
            286,
            576,
            519,
            295,
            552,
            11,
            436,
            366,
            11,
            613,
            366,
            264,
            16871,
            295,
            437,
            264,
            441,
            43,
            44,
            307,
            51448
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17372047901153564,
        "compression_ratio": 1.7536232471466064,
        "no_speech_prob": 6.30274589639157e-05
    },
    {
        "id": 206,
        "seek": 108832,
        "start": 1110.0,
        "end": 1114.9599609375,
        "text": " seeing, and there's a bunch of weird stuff that comes out of it. Let's go back to our default",
        "tokens": [
            51448,
            2577,
            11,
            293,
            456,
            311,
            257,
            3840,
            295,
            3657,
            1507,
            300,
            1487,
            484,
            295,
            309,
            13,
            961,
            311,
            352,
            646,
            281,
            527,
            7576,
            51696
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17372047901153564,
        "compression_ratio": 1.7536232471466064,
        "no_speech_prob": 6.30274589639157e-05
    },
    {
        "id": 207,
        "seek": 111496,
        "start": 1114.9599609375,
        "end": 1121.52001953125,
        "text": " cell style. I bet you that the model has never in its training set seen default cell star",
        "tokens": [
            50364,
            2815,
            3758,
            13,
            286,
            778,
            291,
            300,
            264,
            2316,
            575,
            1128,
            294,
            1080,
            3097,
            992,
            1612,
            7576,
            2815,
            3543,
            50692
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2147013545036316,
        "compression_ratio": 1.7242990732192993,
        "no_speech_prob": 0.00162295694462955
    },
    {
        "id": 208,
        "seek": 111496,
        "start": 1122.3199462890625,
        "end": 1128.800048828125,
        "text": " without le in there. It's always seen this as a single group, because this is some kind of a",
        "tokens": [
            50732,
            1553,
            476,
            294,
            456,
            13,
            467,
            311,
            1009,
            1612,
            341,
            382,
            257,
            2167,
            1594,
            11,
            570,
            341,
            307,
            512,
            733,
            295,
            257,
            51056
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2147013545036316,
        "compression_ratio": 1.7242990732192993,
        "no_speech_prob": 0.00162295694462955
    },
    {
        "id": 209,
        "seek": 111496,
        "start": 1128.800048828125,
        "end": 1134.4000244140625,
        "text": " function in, I don't actually know what this is part of, it's some kind of API, but I bet you",
        "tokens": [
            51056,
            2445,
            294,
            11,
            286,
            500,
            380,
            767,
            458,
            437,
            341,
            307,
            644,
            295,
            11,
            309,
            311,
            512,
            733,
            295,
            9362,
            11,
            457,
            286,
            778,
            291,
            51336
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2147013545036316,
        "compression_ratio": 1.7242990732192993,
        "no_speech_prob": 0.00162295694462955
    },
    {
        "id": 210,
        "seek": 111496,
        "start": 1134.4000244140625,
        "end": 1140.6400146484375,
        "text": " that it's never seen this combination of tokens in its training data, because, or I think it",
        "tokens": [
            51336,
            300,
            309,
            311,
            1128,
            1612,
            341,
            6562,
            295,
            22667,
            294,
            1080,
            3097,
            1412,
            11,
            570,
            11,
            420,
            286,
            519,
            309,
            51648
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2147013545036316,
        "compression_ratio": 1.7242990732192993,
        "no_speech_prob": 0.00162295694462955
    },
    {
        "id": 211,
        "seek": 114064,
        "start": 1140.6400146484375,
        "end": 1145.8399658203125,
        "text": " would be extremely rare. So I took this and I copied pasted it here, and I had, I tried to",
        "tokens": [
            50364,
            576,
            312,
            4664,
            5892,
            13,
            407,
            286,
            1890,
            341,
            293,
            286,
            25365,
            1791,
            292,
            309,
            510,
            11,
            293,
            286,
            632,
            11,
            286,
            3031,
            281,
            50624
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23603220283985138,
        "compression_ratio": 1.8376623392105103,
        "no_speech_prob": 0.0045383647084236145
    },
    {
        "id": 212,
        "seek": 114064,
        "start": 1145.8399658203125,
        "end": 1150.6400146484375,
        "text": " complete from it, and that it immediately gave me a big error, and it said the model predicted",
        "tokens": [
            50624,
            3566,
            490,
            309,
            11,
            293,
            300,
            309,
            4258,
            2729,
            385,
            257,
            955,
            6713,
            11,
            293,
            309,
            848,
            264,
            2316,
            19147,
            50864
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23603220283985138,
        "compression_ratio": 1.8376623392105103,
        "no_speech_prob": 0.0045383647084236145
    },
    {
        "id": 213,
        "seek": 114064,
        "start": 1150.6400146484375,
        "end": 1154.0799560546875,
        "text": " completion that begins with a stop sequence resulting in no output. Consider adjusting",
        "tokens": [
            50864,
            19372,
            300,
            7338,
            365,
            257,
            1590,
            8310,
            16505,
            294,
            572,
            5598,
            13,
            17416,
            23559,
            51036
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23603220283985138,
        "compression_ratio": 1.8376623392105103,
        "no_speech_prob": 0.0045383647084236145
    },
    {
        "id": 214,
        "seek": 114064,
        "start": 1154.0799560546875,
        "end": 1159.280029296875,
        "text": " your prompt or stop sequences. So what happened here when I clicked submit is that immediately",
        "tokens": [
            51036,
            428,
            12391,
            420,
            1590,
            22978,
            13,
            407,
            437,
            2011,
            510,
            562,
            286,
            23370,
            10315,
            307,
            300,
            4258,
            51296
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23603220283985138,
        "compression_ratio": 1.8376623392105103,
        "no_speech_prob": 0.0045383647084236145
    },
    {
        "id": 215,
        "seek": 114064,
        "start": 1159.280029296875,
        "end": 1164.8800048828125,
        "text": " the model emitted and sort of like end of text token, I think, or something like that, it basically",
        "tokens": [
            51296,
            264,
            2316,
            44897,
            293,
            1333,
            295,
            411,
            917,
            295,
            2487,
            14862,
            11,
            286,
            519,
            11,
            420,
            746,
            411,
            300,
            11,
            309,
            1936,
            51576
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23603220283985138,
        "compression_ratio": 1.8376623392105103,
        "no_speech_prob": 0.0045383647084236145
    },
    {
        "id": 216,
        "seek": 114064,
        "start": 1164.8800048828125,
        "end": 1169.3599853515625,
        "text": " predicted the stop sequence immediately, so it had no completion, and so this is why I'm getting a",
        "tokens": [
            51576,
            19147,
            264,
            1590,
            8310,
            4258,
            11,
            370,
            309,
            632,
            572,
            19372,
            11,
            293,
            370,
            341,
            307,
            983,
            286,
            478,
            1242,
            257,
            51800
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23603220283985138,
        "compression_ratio": 1.8376623392105103,
        "no_speech_prob": 0.0045383647084236145
    },
    {
        "id": 217,
        "seek": 116936,
        "start": 1169.3599853515625,
        "end": 1174.800048828125,
        "text": " warning again, because we're off the data distribution and the model is just predicting",
        "tokens": [
            50364,
            9164,
            797,
            11,
            570,
            321,
            434,
            766,
            264,
            1412,
            7316,
            293,
            264,
            2316,
            307,
            445,
            32884,
            50636
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22241440415382385,
        "compression_ratio": 1.7340823411941528,
        "no_speech_prob": 4.264742892701179e-05
    },
    {
        "id": 218,
        "seek": 116936,
        "start": 1175.6800537109375,
        "end": 1180.1600341796875,
        "text": " just totally arbitrary things. It's just really confused basically. This is giving it brain",
        "tokens": [
            50680,
            445,
            3879,
            23211,
            721,
            13,
            467,
            311,
            445,
            534,
            9019,
            1936,
            13,
            639,
            307,
            2902,
            309,
            3567,
            50904
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22241440415382385,
        "compression_ratio": 1.7340823411941528,
        "no_speech_prob": 4.264742892701179e-05
    },
    {
        "id": 219,
        "seek": 116936,
        "start": 1180.1600341796875,
        "end": 1184.4000244140625,
        "text": " damage. It's never seen this before. It's shocked, and it's predicting end of text or something.",
        "tokens": [
            50904,
            4344,
            13,
            467,
            311,
            1128,
            1612,
            341,
            949,
            13,
            467,
            311,
            12763,
            11,
            293,
            309,
            311,
            32884,
            917,
            295,
            2487,
            420,
            746,
            13,
            51116
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22241440415382385,
        "compression_ratio": 1.7340823411941528,
        "no_speech_prob": 4.264742892701179e-05
    },
    {
        "id": 220,
        "seek": 116936,
        "start": 1184.9599609375,
        "end": 1189.9200439453125,
        "text": " I tried it again here, and in this case it completed it, but then for some reason this",
        "tokens": [
            51144,
            286,
            3031,
            309,
            797,
            510,
            11,
            293,
            294,
            341,
            1389,
            309,
            7365,
            309,
            11,
            457,
            550,
            337,
            512,
            1778,
            341,
            51392
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22241440415382385,
        "compression_ratio": 1.7340823411941528,
        "no_speech_prob": 4.264742892701179e-05
    },
    {
        "id": 221,
        "seek": 116936,
        "start": 1189.9200439453125,
        "end": 1196.6400146484375,
        "text": " request may violate our usage policies. This was flagged. Basically something just like goes wrong,",
        "tokens": [
            51392,
            5308,
            815,
            37478,
            527,
            14924,
            7657,
            13,
            639,
            390,
            7166,
            3004,
            13,
            8537,
            746,
            445,
            411,
            1709,
            2085,
            11,
            51728
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22241440415382385,
        "compression_ratio": 1.7340823411941528,
        "no_speech_prob": 4.264742892701179e-05
    },
    {
        "id": 222,
        "seek": 119664,
        "start": 1196.6400146484375,
        "end": 1200.239990234375,
        "text": " and there's something like jank. You can just feel the jank because the model is like extremely",
        "tokens": [
            50364,
            293,
            456,
            311,
            746,
            411,
            361,
            657,
            13,
            509,
            393,
            445,
            841,
            264,
            361,
            657,
            570,
            264,
            2316,
            307,
            411,
            4664,
            50544
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21012437343597412,
        "compression_ratio": 1.8014981746673584,
        "no_speech_prob": 0.0015978323062881827
    },
    {
        "id": 223,
        "seek": 119664,
        "start": 1200.239990234375,
        "end": 1203.9200439453125,
        "text": " unhappy with just this, and it doesn't know how to complete it because it's never occurred in a",
        "tokens": [
            50544,
            22172,
            365,
            445,
            341,
            11,
            293,
            309,
            1177,
            380,
            458,
            577,
            281,
            3566,
            309,
            570,
            309,
            311,
            1128,
            11068,
            294,
            257,
            50728
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21012437343597412,
        "compression_ratio": 1.8014981746673584,
        "no_speech_prob": 0.0015978323062881827
    },
    {
        "id": 224,
        "seek": 119664,
        "start": 1203.9200439453125,
        "end": 1210.0799560546875,
        "text": " training set. In a training set it always appears like this and becomes a single token. So these",
        "tokens": [
            50728,
            3097,
            992,
            13,
            682,
            257,
            3097,
            992,
            309,
            1009,
            7038,
            411,
            341,
            293,
            3643,
            257,
            2167,
            14862,
            13,
            407,
            613,
            51036
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21012437343597412,
        "compression_ratio": 1.8014981746673584,
        "no_speech_prob": 0.0015978323062881827
    },
    {
        "id": 225,
        "seek": 119664,
        "start": 1210.0799560546875,
        "end": 1215.199951171875,
        "text": " kinds of issues where tokens are either you sort of like complete the first character of the next",
        "tokens": [
            51036,
            3685,
            295,
            2663,
            689,
            22667,
            366,
            2139,
            291,
            1333,
            295,
            411,
            3566,
            264,
            700,
            2517,
            295,
            264,
            958,
            51292
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21012437343597412,
        "compression_ratio": 1.8014981746673584,
        "no_speech_prob": 0.0015978323062881827
    },
    {
        "id": 226,
        "seek": 119664,
        "start": 1215.199951171875,
        "end": 1219.9200439453125,
        "text": " token, or you are sort of, you have long tokens that you then have just some of the characters",
        "tokens": [
            51292,
            14862,
            11,
            420,
            291,
            366,
            1333,
            295,
            11,
            291,
            362,
            938,
            22667,
            300,
            291,
            550,
            362,
            445,
            512,
            295,
            264,
            4342,
            51528
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21012437343597412,
        "compression_ratio": 1.8014981746673584,
        "no_speech_prob": 0.0015978323062881827
    },
    {
        "id": 227,
        "seek": 121992,
        "start": 1219.9200439453125,
        "end": 1225.5999755859375,
        "text": " off. All of these are kind of like issues with partial tokens is how I would describe it.",
        "tokens": [
            50364,
            766,
            13,
            1057,
            295,
            613,
            366,
            733,
            295,
            411,
            2663,
            365,
            14641,
            22667,
            307,
            577,
            286,
            576,
            6786,
            309,
            13,
            50648
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2551564574241638,
        "compression_ratio": 1.6363636255264282,
        "no_speech_prob": 0.0011695071589201689
    },
    {
        "id": 228,
        "seek": 121992,
        "start": 1226.3199462890625,
        "end": 1232.1600341796875,
        "text": " And if you actually dig into the Tuk token repository, go to the Rust code and search for",
        "tokens": [
            50684,
            400,
            498,
            291,
            767,
            2528,
            666,
            264,
            314,
            2034,
            14862,
            25841,
            11,
            352,
            281,
            264,
            34952,
            3089,
            293,
            3164,
            337,
            50976
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2551564574241638,
        "compression_ratio": 1.6363636255264282,
        "no_speech_prob": 0.0011695071589201689
    },
    {
        "id": 229,
        "seek": 121992,
        "start": 1232.1600341796875,
        "end": 1240.0799560546875,
        "text": " unstable, and you'll see in code unstable native, unstable tokens, and a lot of like special case",
        "tokens": [
            50976,
            23742,
            11,
            293,
            291,
            603,
            536,
            294,
            3089,
            23742,
            8470,
            11,
            23742,
            22667,
            11,
            293,
            257,
            688,
            295,
            411,
            2121,
            1389,
            51372
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2551564574241638,
        "compression_ratio": 1.6363636255264282,
        "no_speech_prob": 0.0011695071589201689
    },
    {
        "id": 230,
        "seek": 121992,
        "start": 1240.0799560546875,
        "end": 1245.5999755859375,
        "text": " handling. None of this stuff about unstable tokens is documented anywhere, but there's a ton of code",
        "tokens": [
            51372,
            13175,
            13,
            14492,
            295,
            341,
            1507,
            466,
            23742,
            22667,
            307,
            23007,
            4992,
            11,
            457,
            456,
            311,
            257,
            2952,
            295,
            3089,
            51648
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2551564574241638,
        "compression_ratio": 1.6363636255264282,
        "no_speech_prob": 0.0011695071589201689
    },
    {
        "id": 231,
        "seek": 124560,
        "start": 1245.5999755859375,
        "end": 1251.8399658203125,
        "text": " dealing with unstable tokens, and unstable tokens is exactly kind of like what I'm describing here.",
        "tokens": [
            50364,
            6260,
            365,
            23742,
            22667,
            11,
            293,
            23742,
            22667,
            307,
            2293,
            733,
            295,
            411,
            437,
            286,
            478,
            16141,
            510,
            13,
            50676
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2000829577445984,
        "compression_ratio": 1.862069010734558,
        "no_speech_prob": 0.0002305062225786969
    },
    {
        "id": 232,
        "seek": 124560,
        "start": 1251.8399658203125,
        "end": 1256.47998046875,
        "text": " What you would like out of a completion API is something a lot more fancy. Like if we're putting",
        "tokens": [
            50676,
            708,
            291,
            576,
            411,
            484,
            295,
            257,
            19372,
            9362,
            307,
            746,
            257,
            688,
            544,
            10247,
            13,
            1743,
            498,
            321,
            434,
            3372,
            50908
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2000829577445984,
        "compression_ratio": 1.862069010734558,
        "no_speech_prob": 0.0002305062225786969
    },
    {
        "id": 233,
        "seek": 124560,
        "start": 1256.47998046875,
        "end": 1260.9599609375,
        "text": " in default cell star, if we're asking for the next token sequence, we're not actually trying",
        "tokens": [
            50908,
            294,
            7576,
            2815,
            3543,
            11,
            498,
            321,
            434,
            3365,
            337,
            264,
            958,
            14862,
            8310,
            11,
            321,
            434,
            406,
            767,
            1382,
            51132
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2000829577445984,
        "compression_ratio": 1.862069010734558,
        "no_speech_prob": 0.0002305062225786969
    },
    {
        "id": 234,
        "seek": 124560,
        "start": 1260.9599609375,
        "end": 1266.1600341796875,
        "text": " to append the next token exactly after this list. We're actually trying to append, we're trying to",
        "tokens": [
            51132,
            281,
            34116,
            264,
            958,
            14862,
            2293,
            934,
            341,
            1329,
            13,
            492,
            434,
            767,
            1382,
            281,
            34116,
            11,
            321,
            434,
            1382,
            281,
            51392
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2000829577445984,
        "compression_ratio": 1.862069010734558,
        "no_speech_prob": 0.0002305062225786969
    },
    {
        "id": 235,
        "seek": 124560,
        "start": 1266.1600341796875,
        "end": 1274.1600341796875,
        "text": " consider lots of tokens that if we were, I guess like we're trying to search over characters that",
        "tokens": [
            51392,
            1949,
            3195,
            295,
            22667,
            300,
            498,
            321,
            645,
            11,
            286,
            2041,
            411,
            321,
            434,
            1382,
            281,
            3164,
            670,
            4342,
            300,
            51792
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2000829577445984,
        "compression_ratio": 1.862069010734558,
        "no_speech_prob": 0.0002305062225786969
    },
    {
        "id": 236,
        "seek": 127416,
        "start": 1275.1199951171875,
        "end": 1280.719970703125,
        "text": " if we re-tokenized would be of high probability, if that makes sense, so that we can actually add",
        "tokens": [
            50412,
            498,
            321,
            319,
            12,
            83,
            8406,
            1602,
            576,
            312,
            295,
            1090,
            8482,
            11,
            498,
            300,
            1669,
            2020,
            11,
            370,
            300,
            321,
            393,
            767,
            909,
            50692
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1937989741563797,
        "compression_ratio": 1.6538461446762085,
        "no_speech_prob": 0.0003799825208261609
    },
    {
        "id": 237,
        "seek": 127416,
        "start": 1280.719970703125,
        "end": 1286.1600341796875,
        "text": " a single individual character instead of just like adding the next full token that comes after",
        "tokens": [
            50692,
            257,
            2167,
            2609,
            2517,
            2602,
            295,
            445,
            411,
            5127,
            264,
            958,
            1577,
            14862,
            300,
            1487,
            934,
            50964
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1937989741563797,
        "compression_ratio": 1.6538461446762085,
        "no_speech_prob": 0.0003799825208261609
    },
    {
        "id": 238,
        "seek": 127416,
        "start": 1286.1600341796875,
        "end": 1291.5999755859375,
        "text": " this partial token list. So this is very tricky to describe, and I invite you to maybe like look",
        "tokens": [
            50964,
            341,
            14641,
            14862,
            1329,
            13,
            407,
            341,
            307,
            588,
            12414,
            281,
            6786,
            11,
            293,
            286,
            7980,
            291,
            281,
            1310,
            411,
            574,
            51236
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1937989741563797,
        "compression_ratio": 1.6538461446762085,
        "no_speech_prob": 0.0003799825208261609
    },
    {
        "id": 239,
        "seek": 127416,
        "start": 1291.5999755859375,
        "end": 1295.9200439453125,
        "text": " through this. It ends up being extremely gnarly and hairy kind of topic, and it comes from",
        "tokens": [
            51236,
            807,
            341,
            13,
            467,
            5314,
            493,
            885,
            4664,
            290,
            20062,
            356,
            293,
            42346,
            733,
            295,
            4829,
            11,
            293,
            309,
            1487,
            490,
            51452
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1937989741563797,
        "compression_ratio": 1.6538461446762085,
        "no_speech_prob": 0.0003799825208261609
    },
    {
        "id": 240,
        "seek": 127416,
        "start": 1295.9200439453125,
        "end": 1300.8800048828125,
        "text": " tokenization fundamentally, so maybe I can even spend an entire video talking about unstable",
        "tokens": [
            51452,
            14862,
            2144,
            17879,
            11,
            370,
            1310,
            286,
            393,
            754,
            3496,
            364,
            2302,
            960,
            1417,
            466,
            23742,
            51700
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1937989741563797,
        "compression_ratio": 1.6538461446762085,
        "no_speech_prob": 0.0003799825208261609
    },
    {
        "id": 241,
        "seek": 130088,
        "start": 1300.9599609375,
        "end": 1305.3599853515625,
        "text": " tokens sometime in the future. Okay, and I'm really saving the best for last. My favorite",
        "tokens": [
            50368,
            22667,
            15053,
            294,
            264,
            2027,
            13,
            1033,
            11,
            293,
            286,
            478,
            534,
            6816,
            264,
            1151,
            337,
            1036,
            13,
            1222,
            2954,
            50588
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2403971403837204,
        "compression_ratio": 1.6832579374313354,
        "no_speech_prob": 0.06656278669834137
    },
    {
        "id": 242,
        "seek": 130088,
        "start": 1305.3599853515625,
        "end": 1313.43994140625,
        "text": " one by far is this solid gold Magikarp. This comes from this blog post, solid gold Magikarp,",
        "tokens": [
            50588,
            472,
            538,
            1400,
            307,
            341,
            5100,
            3821,
            6395,
            1035,
            6529,
            13,
            639,
            1487,
            490,
            341,
            6968,
            2183,
            11,
            5100,
            3821,
            6395,
            1035,
            6529,
            11,
            50992
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2403971403837204,
        "compression_ratio": 1.6832579374313354,
        "no_speech_prob": 0.06656278669834137
    },
    {
        "id": 243,
        "seek": 130088,
        "start": 1314.239990234375,
        "end": 1321.52001953125,
        "text": " and this is internet famous now for those of us in LLMs, and basically I would advise you to",
        "tokens": [
            51032,
            293,
            341,
            307,
            4705,
            4618,
            586,
            337,
            729,
            295,
            505,
            294,
            441,
            43,
            26386,
            11,
            293,
            1936,
            286,
            576,
            18312,
            291,
            281,
            51396
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2403971403837204,
        "compression_ratio": 1.6832579374313354,
        "no_speech_prob": 0.06656278669834137
    },
    {
        "id": 244,
        "seek": 130088,
        "start": 1321.52001953125,
        "end": 1327.5999755859375,
        "text": " read this blog post in full, but basically what this person was doing is this person went to the",
        "tokens": [
            51396,
            1401,
            341,
            6968,
            2183,
            294,
            1577,
            11,
            457,
            1936,
            437,
            341,
            954,
            390,
            884,
            307,
            341,
            954,
            1437,
            281,
            264,
            51700
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2403971403837204,
        "compression_ratio": 1.6832579374313354,
        "no_speech_prob": 0.06656278669834137
    },
    {
        "id": 245,
        "seek": 132760,
        "start": 1328.0799560546875,
        "end": 1334.4000244140625,
        "text": " token embedding stable and clustered the tokens based on their embedding representation,",
        "tokens": [
            50388,
            14862,
            12240,
            3584,
            8351,
            293,
            596,
            38624,
            264,
            22667,
            2361,
            322,
            641,
            12240,
            3584,
            10290,
            11,
            50704
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2632102370262146,
        "compression_ratio": 1.837499976158142,
        "no_speech_prob": 0.00012148154928581789
    },
    {
        "id": 246,
        "seek": 132760,
        "start": 1335.280029296875,
        "end": 1339.9200439453125,
        "text": " and this person noticed that there's a cluster of tokens that look really strange.",
        "tokens": [
            50748,
            293,
            341,
            954,
            5694,
            300,
            456,
            311,
            257,
            13630,
            295,
            22667,
            300,
            574,
            534,
            5861,
            13,
            50980
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2632102370262146,
        "compression_ratio": 1.837499976158142,
        "no_speech_prob": 0.00012148154928581789
    },
    {
        "id": 247,
        "seek": 132760,
        "start": 1339.9200439453125,
        "end": 1345.199951171875,
        "text": " So there's a cluster here, at rot, eStream fame, solid gold Magikarp, signet message,",
        "tokens": [
            50980,
            407,
            456,
            311,
            257,
            13630,
            510,
            11,
            412,
            4297,
            11,
            308,
            4520,
            1572,
            16874,
            11,
            5100,
            3821,
            6395,
            1035,
            6529,
            11,
            1465,
            302,
            3636,
            11,
            51244
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2632102370262146,
        "compression_ratio": 1.837499976158142,
        "no_speech_prob": 0.00012148154928581789
    },
    {
        "id": 248,
        "seek": 132760,
        "start": 1345.199951171875,
        "end": 1352.239990234375,
        "text": " like really weird tokens in basically in this embedding cluster, and so where are these tokens,",
        "tokens": [
            51244,
            411,
            534,
            3657,
            22667,
            294,
            1936,
            294,
            341,
            12240,
            3584,
            13630,
            11,
            293,
            370,
            689,
            366,
            613,
            22667,
            11,
            51596
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2632102370262146,
        "compression_ratio": 1.837499976158142,
        "no_speech_prob": 0.00012148154928581789
    },
    {
        "id": 249,
        "seek": 132760,
        "start": 1352.239990234375,
        "end": 1355.43994140625,
        "text": " and where did they even come from? Like what is solid gold Magikarp? It makes no sense,",
        "tokens": [
            51596,
            293,
            689,
            630,
            436,
            754,
            808,
            490,
            30,
            1743,
            437,
            307,
            5100,
            3821,
            6395,
            1035,
            6529,
            30,
            467,
            1669,
            572,
            2020,
            11,
            51756
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2632102370262146,
        "compression_ratio": 1.837499976158142,
        "no_speech_prob": 0.00012148154928581789
    },
    {
        "id": 250,
        "seek": 135544,
        "start": 1356.0799560546875,
        "end": 1362.3199462890625,
        "text": " and then they found a bunch of these tokens, and then they noticed that actually the plot",
        "tokens": [
            50396,
            293,
            550,
            436,
            1352,
            257,
            3840,
            295,
            613,
            22667,
            11,
            293,
            550,
            436,
            5694,
            300,
            767,
            264,
            7542,
            50708
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20661507546901703,
        "compression_ratio": 1.6725664138793945,
        "no_speech_prob": 4.683881707023829e-05
    },
    {
        "id": 251,
        "seek": 135544,
        "start": 1362.3199462890625,
        "end": 1368.239990234375,
        "text": " thickens here, because if you ask the model about these tokens, like you ask it some very benign",
        "tokens": [
            50708,
            5060,
            694,
            510,
            11,
            570,
            498,
            291,
            1029,
            264,
            2316,
            466,
            613,
            22667,
            11,
            411,
            291,
            1029,
            309,
            512,
            588,
            3271,
            788,
            51004
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20661507546901703,
        "compression_ratio": 1.6725664138793945,
        "no_speech_prob": 4.683881707023829e-05
    },
    {
        "id": 252,
        "seek": 135544,
        "start": 1368.239990234375,
        "end": 1373.1199951171875,
        "text": " question, like please can you repeat back to me the string sold gold Magikarp, then you get a",
        "tokens": [
            51004,
            1168,
            11,
            411,
            1767,
            393,
            291,
            7149,
            646,
            281,
            385,
            264,
            6798,
            3718,
            3821,
            6395,
            1035,
            6529,
            11,
            550,
            291,
            483,
            257,
            51248
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20661507546901703,
        "compression_ratio": 1.6725664138793945,
        "no_speech_prob": 4.683881707023829e-05
    },
    {
        "id": 253,
        "seek": 135544,
        "start": 1373.1199951171875,
        "end": 1379.43994140625,
        "text": " variety of basically totally broken LLM behavior. So either you get evasion, so I'm sorry I can't",
        "tokens": [
            51248,
            5673,
            295,
            1936,
            3879,
            5463,
            441,
            43,
            44,
            5223,
            13,
            407,
            2139,
            291,
            483,
            1073,
            6822,
            11,
            370,
            286,
            478,
            2597,
            286,
            393,
            380,
            51564
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20661507546901703,
        "compression_ratio": 1.6725664138793945,
        "no_speech_prob": 4.683881707023829e-05
    },
    {
        "id": 254,
        "seek": 137944,
        "start": 1379.43994140625,
        "end": 1385.760009765625,
        "text": " hear you, or you get a bunch of hallucinations as a response. You can even get back like insults,",
        "tokens": [
            50364,
            1568,
            291,
            11,
            420,
            291,
            483,
            257,
            3840,
            295,
            35212,
            10325,
            382,
            257,
            4134,
            13,
            509,
            393,
            754,
            483,
            646,
            411,
            15285,
            82,
            11,
            50680
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2215307205915451,
        "compression_ratio": 1.6573426723480225,
        "no_speech_prob": 0.09532430768013
    },
    {
        "id": 255,
        "seek": 137944,
        "start": 1385.760009765625,
        "end": 1391.43994140625,
        "text": " so you ask it about streamer bot, and it tells the model actually just calls you names,",
        "tokens": [
            50680,
            370,
            291,
            1029,
            309,
            466,
            4309,
            260,
            10592,
            11,
            293,
            309,
            5112,
            264,
            2316,
            767,
            445,
            5498,
            291,
            5288,
            11,
            50964
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2215307205915451,
        "compression_ratio": 1.6573426723480225,
        "no_speech_prob": 0.09532430768013
    },
    {
        "id": 256,
        "seek": 137944,
        "start": 1392.719970703125,
        "end": 1397.52001953125,
        "text": " or it kind of comes up with like weird humor. Like you're actually breaking the model by asking",
        "tokens": [
            51028,
            420,
            309,
            733,
            295,
            1487,
            493,
            365,
            411,
            3657,
            14318,
            13,
            1743,
            291,
            434,
            767,
            7697,
            264,
            2316,
            538,
            3365,
            51268
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2215307205915451,
        "compression_ratio": 1.6573426723480225,
        "no_speech_prob": 0.09532430768013
    },
    {
        "id": 257,
        "seek": 137944,
        "start": 1397.52001953125,
        "end": 1402.56005859375,
        "text": " about these very simple strings like at rot and solid gold Magikarp. So like what the hell is",
        "tokens": [
            51268,
            466,
            613,
            588,
            2199,
            13985,
            411,
            412,
            4297,
            293,
            5100,
            3821,
            6395,
            1035,
            6529,
            13,
            407,
            411,
            437,
            264,
            4921,
            307,
            51520
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2215307205915451,
        "compression_ratio": 1.6573426723480225,
        "no_speech_prob": 0.09532430768013
    },
    {
        "id": 258,
        "seek": 137944,
        "start": 1402.56005859375,
        "end": 1407.5999755859375,
        "text": " happening? And there's a variety of here documented behaviors. There's a bunch of tokens, not just",
        "tokens": [
            51520,
            2737,
            30,
            400,
            456,
            311,
            257,
            5673,
            295,
            510,
            23007,
            15501,
            13,
            821,
            311,
            257,
            3840,
            295,
            22667,
            11,
            406,
            445,
            51772
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2215307205915451,
        "compression_ratio": 1.6573426723480225,
        "no_speech_prob": 0.09532430768013
    },
    {
        "id": 259,
        "seek": 140760,
        "start": 1407.5999755859375,
        "end": 1412.3199462890625,
        "text": " sold gold Magikarp, that have that kind of behavior. And so basically there's a bunch of like",
        "tokens": [
            50364,
            3718,
            3821,
            6395,
            1035,
            6529,
            11,
            300,
            362,
            300,
            733,
            295,
            5223,
            13,
            400,
            370,
            1936,
            456,
            311,
            257,
            3840,
            295,
            411,
            50600
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20002694427967072,
        "compression_ratio": 1.7340425252914429,
        "no_speech_prob": 7.722170994384214e-05
    },
    {
        "id": 260,
        "seek": 140760,
        "start": 1412.3199462890625,
        "end": 1417.0400390625,
        "text": " trigger words, and if you ask the model about these trigger words, or you just include them in your",
        "tokens": [
            50600,
            7875,
            2283,
            11,
            293,
            498,
            291,
            1029,
            264,
            2316,
            466,
            613,
            7875,
            2283,
            11,
            420,
            291,
            445,
            4090,
            552,
            294,
            428,
            50836
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20002694427967072,
        "compression_ratio": 1.7340425252914429,
        "no_speech_prob": 7.722170994384214e-05
    },
    {
        "id": 261,
        "seek": 140760,
        "start": 1417.0400390625,
        "end": 1423.0400390625,
        "text": " prompt, the model goes haywire and has all kinds of really strange behaviors, including sort of ones",
        "tokens": [
            50836,
            12391,
            11,
            264,
            2316,
            1709,
            4842,
            42689,
            293,
            575,
            439,
            3685,
            295,
            534,
            5861,
            15501,
            11,
            3009,
            1333,
            295,
            2306,
            51136
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20002694427967072,
        "compression_ratio": 1.7340425252914429,
        "no_speech_prob": 7.722170994384214e-05
    },
    {
        "id": 262,
        "seek": 140760,
        "start": 1423.0400390625,
        "end": 1428.239990234375,
        "text": " that violate typical safety guidelines and the alignment of the model, like it's swearing back at",
        "tokens": [
            51136,
            300,
            37478,
            7476,
            4514,
            12470,
            293,
            264,
            18515,
            295,
            264,
            2316,
            11,
            411,
            309,
            311,
            2484,
            1921,
            646,
            412,
            51396
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20002694427967072,
        "compression_ratio": 1.7340425252914429,
        "no_speech_prob": 7.722170994384214e-05
    },
    {
        "id": 263,
        "seek": 140760,
        "start": 1428.239990234375,
        "end": 1434.47998046875,
        "text": " you. So what is happening here, and how can this possibly be true? Well this again comes down to",
        "tokens": [
            51396,
            291,
            13,
            407,
            437,
            307,
            2737,
            510,
            11,
            293,
            577,
            393,
            341,
            6264,
            312,
            2074,
            30,
            1042,
            341,
            797,
            1487,
            760,
            281,
            51708
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20002694427967072,
        "compression_ratio": 1.7340425252914429,
        "no_speech_prob": 7.722170994384214e-05
    },
    {
        "id": 264,
        "seek": 143448,
        "start": 1434.47998046875,
        "end": 1439.5999755859375,
        "text": " tokenization. So what's happening here is that sold gold Magikarp, if you actually dig into it,",
        "tokens": [
            50364,
            14862,
            2144,
            13,
            407,
            437,
            311,
            2737,
            510,
            307,
            300,
            3718,
            3821,
            6395,
            1035,
            6529,
            11,
            498,
            291,
            767,
            2528,
            666,
            309,
            11,
            50620
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18963675200939178,
        "compression_ratio": 1.8659003973007202,
        "no_speech_prob": 0.00040448017534799874
    },
    {
        "id": 265,
        "seek": 143448,
        "start": 1439.5999755859375,
        "end": 1446.3199462890625,
        "text": " is a reddit user. So there's a u slash sold gold Magikarp, and probably what happened here,",
        "tokens": [
            50620,
            307,
            257,
            2182,
            17975,
            4195,
            13,
            407,
            456,
            311,
            257,
            344,
            17330,
            3718,
            3821,
            6395,
            1035,
            6529,
            11,
            293,
            1391,
            437,
            2011,
            510,
            11,
            50956
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18963675200939178,
        "compression_ratio": 1.8659003973007202,
        "no_speech_prob": 0.00040448017534799874
    },
    {
        "id": 266,
        "seek": 143448,
        "start": 1446.3199462890625,
        "end": 1451.3599853515625,
        "text": " even though I don't know that this has been like really definitively explored, but what is thought",
        "tokens": [
            50956,
            754,
            1673,
            286,
            500,
            380,
            458,
            300,
            341,
            575,
            668,
            411,
            534,
            28152,
            356,
            24016,
            11,
            457,
            437,
            307,
            1194,
            51208
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18963675200939178,
        "compression_ratio": 1.8659003973007202,
        "no_speech_prob": 0.00040448017534799874
    },
    {
        "id": 267,
        "seek": 143448,
        "start": 1451.3599853515625,
        "end": 1457.5999755859375,
        "text": " to have happened is that the tokenization dataset was very different from the training dataset for",
        "tokens": [
            51208,
            281,
            362,
            2011,
            307,
            300,
            264,
            14862,
            2144,
            28872,
            390,
            588,
            819,
            490,
            264,
            3097,
            28872,
            337,
            51520
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18963675200939178,
        "compression_ratio": 1.8659003973007202,
        "no_speech_prob": 0.00040448017534799874
    },
    {
        "id": 268,
        "seek": 143448,
        "start": 1457.5999755859375,
        "end": 1462.9599609375,
        "text": " the actual language model. So in the tokenization dataset there was a ton of reddit data potentially,",
        "tokens": [
            51520,
            264,
            3539,
            2856,
            2316,
            13,
            407,
            294,
            264,
            14862,
            2144,
            28872,
            456,
            390,
            257,
            2952,
            295,
            2182,
            17975,
            1412,
            7263,
            11,
            51788
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18963675200939178,
        "compression_ratio": 1.8659003973007202,
        "no_speech_prob": 0.00040448017534799874
    },
    {
        "id": 269,
        "seek": 146296,
        "start": 1462.9599609375,
        "end": 1469.0400390625,
        "text": " where the user sold gold Magikarp was mentioned in the text. Because sold gold Magikarp was a very",
        "tokens": [
            50364,
            689,
            264,
            4195,
            3718,
            3821,
            6395,
            1035,
            6529,
            390,
            2835,
            294,
            264,
            2487,
            13,
            1436,
            3718,
            3821,
            6395,
            1035,
            6529,
            390,
            257,
            588,
            50668
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18001575767993927,
        "compression_ratio": 1.87109375,
        "no_speech_prob": 1.5936595445964485e-05
    },
    {
        "id": 270,
        "seek": 146296,
        "start": 1469.0400390625,
        "end": 1474.800048828125,
        "text": " common sort of person who would post a lot, this would be a string that occurs many times in a",
        "tokens": [
            50668,
            2689,
            1333,
            295,
            954,
            567,
            576,
            2183,
            257,
            688,
            11,
            341,
            576,
            312,
            257,
            6798,
            300,
            11843,
            867,
            1413,
            294,
            257,
            50956
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18001575767993927,
        "compression_ratio": 1.87109375,
        "no_speech_prob": 1.5936595445964485e-05
    },
    {
        "id": 271,
        "seek": 146296,
        "start": 1474.800048828125,
        "end": 1480.239990234375,
        "text": " tokenization dataset. Because it occurs many times in the tokenization dataset, these tokens would",
        "tokens": [
            50956,
            14862,
            2144,
            28872,
            13,
            1436,
            309,
            11843,
            867,
            1413,
            294,
            264,
            14862,
            2144,
            28872,
            11,
            613,
            22667,
            576,
            51228
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18001575767993927,
        "compression_ratio": 1.87109375,
        "no_speech_prob": 1.5936595445964485e-05
    },
    {
        "id": 272,
        "seek": 146296,
        "start": 1480.239990234375,
        "end": 1485.760009765625,
        "text": " end up getting merged to a single individual token for that single reddit user sold gold Magikarp.",
        "tokens": [
            51228,
            917,
            493,
            1242,
            36427,
            281,
            257,
            2167,
            2609,
            14862,
            337,
            300,
            2167,
            2182,
            17975,
            4195,
            3718,
            3821,
            6395,
            1035,
            6529,
            13,
            51504
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18001575767993927,
        "compression_ratio": 1.87109375,
        "no_speech_prob": 1.5936595445964485e-05
    },
    {
        "id": 273,
        "seek": 146296,
        "start": 1485.760009765625,
        "end": 1491.199951171875,
        "text": " So they would have a dedicated token in a vocabulary of, was it 50,000 tokens in GPT-2,",
        "tokens": [
            51504,
            407,
            436,
            576,
            362,
            257,
            8374,
            14862,
            294,
            257,
            19864,
            295,
            11,
            390,
            309,
            2625,
            11,
            1360,
            22667,
            294,
            26039,
            51,
            12,
            17,
            11,
            51776
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18001575767993927,
        "compression_ratio": 1.87109375,
        "no_speech_prob": 1.5936595445964485e-05
    },
    {
        "id": 274,
        "seek": 149120,
        "start": 1491.199951171875,
        "end": 1496.9599609375,
        "text": " that is devoted to that reddit user. And then what happens is the tokenization dataset has",
        "tokens": [
            50364,
            300,
            307,
            21815,
            281,
            300,
            2182,
            17975,
            4195,
            13,
            400,
            550,
            437,
            2314,
            307,
            264,
            14862,
            2144,
            28872,
            575,
            50652
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18101415038108826,
        "compression_ratio": 1.8542510271072388,
        "no_speech_prob": 7.368576916633174e-05
    },
    {
        "id": 275,
        "seek": 149120,
        "start": 1496.9599609375,
        "end": 1501.52001953125,
        "text": " those strings, but then later when you train the model, the language model itself,",
        "tokens": [
            50652,
            729,
            13985,
            11,
            457,
            550,
            1780,
            562,
            291,
            3847,
            264,
            2316,
            11,
            264,
            2856,
            2316,
            2564,
            11,
            50880
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18101415038108826,
        "compression_ratio": 1.8542510271072388,
        "no_speech_prob": 7.368576916633174e-05
    },
    {
        "id": 276,
        "seek": 149120,
        "start": 1502.9599609375,
        "end": 1508.47998046875,
        "text": " this data from reddit was not present. And so therefore in the entire training set for the",
        "tokens": [
            50952,
            341,
            1412,
            490,
            2182,
            17975,
            390,
            406,
            1974,
            13,
            400,
            370,
            4412,
            294,
            264,
            2302,
            3097,
            992,
            337,
            264,
            51228
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18101415038108826,
        "compression_ratio": 1.8542510271072388,
        "no_speech_prob": 7.368576916633174e-05
    },
    {
        "id": 277,
        "seek": 149120,
        "start": 1508.47998046875,
        "end": 1514.719970703125,
        "text": " language model, sold gold Magikarp never occurs. That token never appears in the training set for",
        "tokens": [
            51228,
            2856,
            2316,
            11,
            3718,
            3821,
            6395,
            1035,
            6529,
            1128,
            11843,
            13,
            663,
            14862,
            1128,
            7038,
            294,
            264,
            3097,
            992,
            337,
            51540
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18101415038108826,
        "compression_ratio": 1.8542510271072388,
        "no_speech_prob": 7.368576916633174e-05
    },
    {
        "id": 278,
        "seek": 149120,
        "start": 1514.719970703125,
        "end": 1520.56005859375,
        "text": " the actual language model later. So this token never gets activated, it's initialized at random",
        "tokens": [
            51540,
            264,
            3539,
            2856,
            2316,
            1780,
            13,
            407,
            341,
            14862,
            1128,
            2170,
            18157,
            11,
            309,
            311,
            5883,
            1602,
            412,
            4974,
            51832
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18101415038108826,
        "compression_ratio": 1.8542510271072388,
        "no_speech_prob": 7.368576916633174e-05
    },
    {
        "id": 279,
        "seek": 152056,
        "start": 1520.56005859375,
        "end": 1524.800048828125,
        "text": " in the beginning of optimization. Then you have forward-backward passes and updates to the model,",
        "tokens": [
            50364,
            294,
            264,
            2863,
            295,
            19618,
            13,
            1396,
            291,
            362,
            2128,
            12,
            3207,
            1007,
            11335,
            293,
            9205,
            281,
            264,
            2316,
            11,
            50576
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18018260598182678,
        "compression_ratio": 1.8585525751113892,
        "no_speech_prob": 2.4682885850779712e-05
    },
    {
        "id": 280,
        "seek": 152056,
        "start": 1524.800048828125,
        "end": 1528.8800048828125,
        "text": " and this token is just never updated in the embedding table. That row vector never gets",
        "tokens": [
            50576,
            293,
            341,
            14862,
            307,
            445,
            1128,
            10588,
            294,
            264,
            12240,
            3584,
            3199,
            13,
            663,
            5386,
            8062,
            1128,
            2170,
            50780
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18018260598182678,
        "compression_ratio": 1.8585525751113892,
        "no_speech_prob": 2.4682885850779712e-05
    },
    {
        "id": 281,
        "seek": 152056,
        "start": 1528.8800048828125,
        "end": 1533.3599853515625,
        "text": " sampled, it never gets used, so it never gets trained, and it's completely untrained. It's",
        "tokens": [
            50780,
            3247,
            15551,
            11,
            309,
            1128,
            2170,
            1143,
            11,
            370,
            309,
            1128,
            2170,
            8895,
            11,
            293,
            309,
            311,
            2584,
            1701,
            31774,
            13,
            467,
            311,
            51004
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18018260598182678,
        "compression_ratio": 1.8585525751113892,
        "no_speech_prob": 2.4682885850779712e-05
    },
    {
        "id": 282,
        "seek": 152056,
        "start": 1533.3599853515625,
        "end": 1538.719970703125,
        "text": " kind of like unallocated memory in a typical binary program written in C or something like that.",
        "tokens": [
            51004,
            733,
            295,
            411,
            517,
            336,
            905,
            770,
            4675,
            294,
            257,
            7476,
            17434,
            1461,
            3720,
            294,
            383,
            420,
            746,
            411,
            300,
            13,
            51272
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18018260598182678,
        "compression_ratio": 1.8585525751113892,
        "no_speech_prob": 2.4682885850779712e-05
    },
    {
        "id": 283,
        "seek": 152056,
        "start": 1538.719970703125,
        "end": 1543.9200439453125,
        "text": " So it's unallocated memory. And then at test time, if you evoke this token, then you're basically",
        "tokens": [
            51272,
            407,
            309,
            311,
            517,
            336,
            905,
            770,
            4675,
            13,
            400,
            550,
            412,
            1500,
            565,
            11,
            498,
            291,
            1073,
            2949,
            341,
            14862,
            11,
            550,
            291,
            434,
            1936,
            51532
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18018260598182678,
        "compression_ratio": 1.8585525751113892,
        "no_speech_prob": 2.4682885850779712e-05
    },
    {
        "id": 284,
        "seek": 152056,
        "start": 1543.9200439453125,
        "end": 1547.6800537109375,
        "text": " plucking out a row of the embedding table that is completely untrained, and that feeds into a",
        "tokens": [
            51532,
            499,
            33260,
            484,
            257,
            5386,
            295,
            264,
            12240,
            3584,
            3199,
            300,
            307,
            2584,
            1701,
            31774,
            11,
            293,
            300,
            23712,
            666,
            257,
            51720
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18018260598182678,
        "compression_ratio": 1.8585525751113892,
        "no_speech_prob": 2.4682885850779712e-05
    },
    {
        "id": 285,
        "seek": 154768,
        "start": 1547.6800537109375,
        "end": 1551.760009765625,
        "text": " transformer and creates undefined behavior. And that's what we're seeing here, this completely",
        "tokens": [
            50364,
            31782,
            293,
            7829,
            674,
            5666,
            2001,
            5223,
            13,
            400,
            300,
            311,
            437,
            321,
            434,
            2577,
            510,
            11,
            341,
            2584,
            50568
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20969833433628082,
        "compression_ratio": 1.6939501762390137,
        "no_speech_prob": 7.843780622351915e-05
    },
    {
        "id": 286,
        "seek": 154768,
        "start": 1551.760009765625,
        "end": 1557.1199951171875,
        "text": " undefined, never-before-seen-in-a-training behavior. And so any of these kind of like weird tokens",
        "tokens": [
            50568,
            674,
            5666,
            2001,
            11,
            1128,
            12,
            45301,
            12,
            22008,
            12,
            259,
            12,
            64,
            12,
            17227,
            1760,
            5223,
            13,
            400,
            370,
            604,
            295,
            613,
            733,
            295,
            411,
            3657,
            22667,
            50836
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20969833433628082,
        "compression_ratio": 1.6939501762390137,
        "no_speech_prob": 7.843780622351915e-05
    },
    {
        "id": 287,
        "seek": 154768,
        "start": 1557.1199951171875,
        "end": 1564.800048828125,
        "text": " would evoke this behavior, because fundamentally the model is out of sample, out of distribution.",
        "tokens": [
            50836,
            576,
            1073,
            2949,
            341,
            5223,
            11,
            570,
            17879,
            264,
            2316,
            307,
            484,
            295,
            6889,
            11,
            484,
            295,
            7316,
            13,
            51220
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20969833433628082,
        "compression_ratio": 1.6939501762390137,
        "no_speech_prob": 7.843780622351915e-05
    },
    {
        "id": 288,
        "seek": 154768,
        "start": 1565.5999755859375,
        "end": 1569.0400390625,
        "text": " Okay, and the very last thing I wanted to just briefly mention and point out, although I think",
        "tokens": [
            51260,
            1033,
            11,
            293,
            264,
            588,
            1036,
            551,
            286,
            1415,
            281,
            445,
            10515,
            2152,
            293,
            935,
            484,
            11,
            4878,
            286,
            519,
            51432
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20969833433628082,
        "compression_ratio": 1.6939501762390137,
        "no_speech_prob": 7.843780622351915e-05
    },
    {
        "id": 289,
        "seek": 154768,
        "start": 1569.0400390625,
        "end": 1572.800048828125,
        "text": " a lot of people are quite aware of this, is that different kinds of formats and different",
        "tokens": [
            51432,
            257,
            688,
            295,
            561,
            366,
            1596,
            3650,
            295,
            341,
            11,
            307,
            300,
            819,
            3685,
            295,
            25879,
            293,
            819,
            51620
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20969833433628082,
        "compression_ratio": 1.6939501762390137,
        "no_speech_prob": 7.843780622351915e-05
    },
    {
        "id": 290,
        "seek": 157280,
        "start": 1572.800048828125,
        "end": 1578.3199462890625,
        "text": " representations in different languages and so on might be more or less efficient with GPT tokenizers,",
        "tokens": [
            50364,
            33358,
            294,
            819,
            8650,
            293,
            370,
            322,
            1062,
            312,
            544,
            420,
            1570,
            7148,
            365,
            26039,
            51,
            14862,
            22525,
            11,
            50640
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19986677169799805,
        "compression_ratio": 1.635193109512329,
        "no_speech_prob": 0.012431327253580093
    },
    {
        "id": 291,
        "seek": 157280,
        "start": 1579.0400390625,
        "end": 1583.43994140625,
        "text": " or any tokenizers for any other LLM for that matter. So for example, JSON is actually really",
        "tokens": [
            50676,
            420,
            604,
            14862,
            22525,
            337,
            604,
            661,
            441,
            43,
            44,
            337,
            300,
            1871,
            13,
            407,
            337,
            1365,
            11,
            31828,
            307,
            767,
            534,
            50896
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19986677169799805,
        "compression_ratio": 1.635193109512329,
        "no_speech_prob": 0.012431327253580093
    },
    {
        "id": 292,
        "seek": 157280,
        "start": 1583.43994140625,
        "end": 1590.3199462890625,
        "text": " dense in tokens, and YAML is a lot more efficient in tokens. So for example, these are",
        "tokens": [
            50896,
            18011,
            294,
            22667,
            11,
            293,
            398,
            2865,
            43,
            307,
            257,
            688,
            544,
            7148,
            294,
            22667,
            13,
            407,
            337,
            1365,
            11,
            613,
            366,
            51240
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19986677169799805,
        "compression_ratio": 1.635193109512329,
        "no_speech_prob": 0.012431327253580093
    },
    {
        "id": 293,
        "seek": 157280,
        "start": 1590.3199462890625,
        "end": 1598.4000244140625,
        "text": " the same in JSON and in YAML. The JSON is 116 and the YAML is 99, so quite a bit of an improvement.",
        "tokens": [
            51240,
            264,
            912,
            294,
            31828,
            293,
            294,
            398,
            2865,
            43,
            13,
            440,
            31828,
            307,
            2975,
            21,
            293,
            264,
            398,
            2865,
            43,
            307,
            11803,
            11,
            370,
            1596,
            257,
            857,
            295,
            364,
            10444,
            13,
            51644
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19986677169799805,
        "compression_ratio": 1.635193109512329,
        "no_speech_prob": 0.012431327253580093
    },
    {
        "id": 294,
        "seek": 159840,
        "start": 1598.8800048828125,
        "end": 1605.199951171875,
        "text": " And so in the token economy, where you are paying per token in many ways, and you are paying in the",
        "tokens": [
            50388,
            400,
            370,
            294,
            264,
            14862,
            5010,
            11,
            689,
            291,
            366,
            6229,
            680,
            14862,
            294,
            867,
            2098,
            11,
            293,
            291,
            366,
            6229,
            294,
            264,
            50704
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24047255516052246,
        "compression_ratio": 1.7553191184997559,
        "no_speech_prob": 0.0003406284376978874
    },
    {
        "id": 295,
        "seek": 159840,
        "start": 1605.199951171875,
        "end": 1610.3199462890625,
        "text": " context length, and you're paying in dollar amount for the cost of processing all this kind of",
        "tokens": [
            50704,
            4319,
            4641,
            11,
            293,
            291,
            434,
            6229,
            294,
            7241,
            2372,
            337,
            264,
            2063,
            295,
            9007,
            439,
            341,
            733,
            295,
            50960
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24047255516052246,
        "compression_ratio": 1.7553191184997559,
        "no_speech_prob": 0.0003406284376978874
    },
    {
        "id": 296,
        "seek": 159840,
        "start": 1610.3199462890625,
        "end": 1616.0,
        "text": " structured data when you have to, so I prefer to use YAMLs over JSONs. And in general, kind of like the",
        "tokens": [
            50960,
            18519,
            1412,
            562,
            291,
            362,
            281,
            11,
            370,
            286,
            4382,
            281,
            764,
            398,
            2865,
            43,
            82,
            670,
            31828,
            82,
            13,
            400,
            294,
            2674,
            11,
            733,
            295,
            411,
            264,
            51244
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24047255516052246,
        "compression_ratio": 1.7553191184997559,
        "no_speech_prob": 0.0003406284376978874
    },
    {
        "id": 297,
        "seek": 159840,
        "start": 1616.0,
        "end": 1621.280029296875,
        "text": " tokenization density is something that you have to sort of care about and worry about at all times,",
        "tokens": [
            51244,
            14862,
            2144,
            10305,
            307,
            746,
            300,
            291,
            362,
            281,
            1333,
            295,
            1127,
            466,
            293,
            3292,
            466,
            412,
            439,
            1413,
            11,
            51508
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24047255516052246,
        "compression_ratio": 1.7553191184997559,
        "no_speech_prob": 0.0003406284376978874
    },
    {
        "id": 298,
        "seek": 159840,
        "start": 1621.280029296875,
        "end": 1626.0,
        "text": " and try to find efficient encoding schemes, and spend a lot of time in TIC tokenizer and measure",
        "tokens": [
            51508,
            293,
            853,
            281,
            915,
            7148,
            43430,
            26954,
            11,
            293,
            3496,
            257,
            688,
            295,
            565,
            294,
            314,
            2532,
            14862,
            6545,
            293,
            3481,
            51744
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24047255516052246,
        "compression_ratio": 1.7553191184997559,
        "no_speech_prob": 0.0003406284376978874
    },
    {
        "id": 299,
        "seek": 162600,
        "start": 1626.0799560546875,
        "end": 1630.56005859375,
        "text": " the different token efficiencies of different formats and settings and so on. Okay, so that",
        "tokens": [
            50368,
            264,
            819,
            14862,
            4703,
            31294,
            295,
            819,
            25879,
            293,
            6257,
            293,
            370,
            322,
            13,
            1033,
            11,
            370,
            300,
            50592
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19023437798023224,
        "compression_ratio": 1.6701754331588745,
        "no_speech_prob": 0.00093990215100348
    },
    {
        "id": 300,
        "seek": 162600,
        "start": 1630.56005859375,
        "end": 1636.8800048828125,
        "text": " concludes my fairly long video on tokenization. I know it's try, I know it's annoying, I know it's",
        "tokens": [
            50592,
            24643,
            452,
            6457,
            938,
            960,
            322,
            14862,
            2144,
            13,
            286,
            458,
            309,
            311,
            256,
            627,
            11,
            286,
            458,
            309,
            311,
            11304,
            11,
            286,
            458,
            309,
            311,
            50908
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19023437798023224,
        "compression_ratio": 1.6701754331588745,
        "no_speech_prob": 0.00093990215100348
    },
    {
        "id": 301,
        "seek": 162600,
        "start": 1636.8800048828125,
        "end": 1642.239990234375,
        "text": " irritating, I personally really dislike the stage. But what I do have to say at this point is don't",
        "tokens": [
            50908,
            45971,
            11,
            286,
            5665,
            534,
            26006,
            264,
            3233,
            13,
            583,
            437,
            286,
            360,
            362,
            281,
            584,
            412,
            341,
            935,
            307,
            500,
            380,
            51176
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19023437798023224,
        "compression_ratio": 1.6701754331588745,
        "no_speech_prob": 0.00093990215100348
    },
    {
        "id": 302,
        "seek": 162600,
        "start": 1642.239990234375,
        "end": 1648.47998046875,
        "text": " brush it off. There's a lot of foot guns, sharp edges here, security issues, AI safety issues,",
        "tokens": [
            51176,
            5287,
            309,
            766,
            13,
            821,
            311,
            257,
            688,
            295,
            2671,
            10153,
            11,
            8199,
            8819,
            510,
            11,
            3825,
            2663,
            11,
            7318,
            4514,
            2663,
            11,
            51488
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19023437798023224,
        "compression_ratio": 1.6701754331588745,
        "no_speech_prob": 0.00093990215100348
    },
    {
        "id": 303,
        "seek": 162600,
        "start": 1648.47998046875,
        "end": 1654.719970703125,
        "text": " as we saw plugging in unallocated memory into language models. So it's worth understanding",
        "tokens": [
            51488,
            382,
            321,
            1866,
            42975,
            294,
            517,
            336,
            905,
            770,
            4675,
            666,
            2856,
            5245,
            13,
            407,
            309,
            311,
            3163,
            3701,
            51800
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19023437798023224,
        "compression_ratio": 1.6701754331588745,
        "no_speech_prob": 0.00093990215100348
    },
    {
        "id": 304,
        "seek": 165472,
        "start": 1654.719970703125,
        "end": 1660.8800048828125,
        "text": " this stage. That said, I will say that eternal glory goes to anyone who can get rid of it.",
        "tokens": [
            50364,
            341,
            3233,
            13,
            663,
            848,
            11,
            286,
            486,
            584,
            300,
            14503,
            11924,
            1709,
            281,
            2878,
            567,
            393,
            483,
            3973,
            295,
            309,
            13,
            50672
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20353618264198303,
        "compression_ratio": 1.6185567378997803,
        "no_speech_prob": 0.0006166115636005998
    },
    {
        "id": 305,
        "seek": 165472,
        "start": 1660.8800048828125,
        "end": 1666.4000244140625,
        "text": " I showed you one possible paper that tried to do that, and I think, I hope a lot more can follow",
        "tokens": [
            50672,
            286,
            4712,
            291,
            472,
            1944,
            3035,
            300,
            3031,
            281,
            360,
            300,
            11,
            293,
            286,
            519,
            11,
            286,
            1454,
            257,
            688,
            544,
            393,
            1524,
            50948
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20353618264198303,
        "compression_ratio": 1.6185567378997803,
        "no_speech_prob": 0.0006166115636005998
    },
    {
        "id": 306,
        "seek": 165472,
        "start": 1666.4000244140625,
        "end": 1671.3599853515625,
        "text": " over time. And my final recommendations for the application right now are if you can reuse the",
        "tokens": [
            50948,
            670,
            565,
            13,
            400,
            452,
            2572,
            10434,
            337,
            264,
            3861,
            558,
            586,
            366,
            498,
            291,
            393,
            26225,
            264,
            51196
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20353618264198303,
        "compression_ratio": 1.6185567378997803,
        "no_speech_prob": 0.0006166115636005998
    },
    {
        "id": 307,
        "seek": 165472,
        "start": 1671.3599853515625,
        "end": 1675.760009765625,
        "text": " GPT-4 tokens and vocabulary in your application, then that's something you should consider and",
        "tokens": [
            51196,
            26039,
            51,
            12,
            19,
            22667,
            293,
            19864,
            294,
            428,
            3861,
            11,
            550,
            300,
            311,
            746,
            291,
            820,
            1949,
            293,
            51416
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20353618264198303,
        "compression_ratio": 1.6185567378997803,
        "no_speech_prob": 0.0006166115636005998
    },
    {
        "id": 308,
        "seek": 165472,
        "start": 1675.760009765625,
        "end": 1682.47998046875,
        "text": " just use TIC token because it is very efficient and nice library for inference for BP. I also",
        "tokens": [
            51416,
            445,
            764,
            314,
            2532,
            14862,
            570,
            309,
            307,
            588,
            7148,
            293,
            1481,
            6405,
            337,
            38253,
            337,
            40533,
            13,
            286,
            611,
            51752
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20353618264198303,
        "compression_ratio": 1.6185567378997803,
        "no_speech_prob": 0.0006166115636005998
    },
    {
        "id": 309,
        "seek": 168248,
        "start": 1682.47998046875,
        "end": 1688.6400146484375,
        "text": " really like the byte level BP that TIC token and OpenAI uses. If you for some reason want to train",
        "tokens": [
            50364,
            534,
            411,
            264,
            40846,
            1496,
            40533,
            300,
            314,
            2532,
            14862,
            293,
            7238,
            48698,
            4960,
            13,
            759,
            291,
            337,
            512,
            1778,
            528,
            281,
            3847,
            50672
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19687500596046448,
        "compression_ratio": 1.7013888359069824,
        "no_speech_prob": 0.06852497905492783
    },
    {
        "id": 310,
        "seek": 168248,
        "start": 1688.6400146484375,
        "end": 1697.3599853515625,
        "text": " your own vocabulary from scratch, then I would use the BP with sentence piece. Oops. As I mentioned,",
        "tokens": [
            50672,
            428,
            1065,
            19864,
            490,
            8459,
            11,
            550,
            286,
            576,
            764,
            264,
            40533,
            365,
            8174,
            2522,
            13,
            21726,
            13,
            1018,
            286,
            2835,
            11,
            51108
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19687500596046448,
        "compression_ratio": 1.7013888359069824,
        "no_speech_prob": 0.06852497905492783
    },
    {
        "id": 311,
        "seek": 168248,
        "start": 1697.3599853515625,
        "end": 1703.6800537109375,
        "text": " I'm not a huge fan of sentence piece. I don't like its byte fallback. And I don't like that",
        "tokens": [
            51108,
            286,
            478,
            406,
            257,
            2603,
            3429,
            295,
            8174,
            2522,
            13,
            286,
            500,
            380,
            411,
            1080,
            40846,
            2100,
            3207,
            13,
            400,
            286,
            500,
            380,
            411,
            300,
            51424
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19687500596046448,
        "compression_ratio": 1.7013888359069824,
        "no_speech_prob": 0.06852497905492783
    },
    {
        "id": 312,
        "seek": 168248,
        "start": 1703.6800537109375,
        "end": 1708.47998046875,
        "text": " it's doing BP on Unicode code points. I think it's it also has like a million settings. And I think",
        "tokens": [
            51424,
            309,
            311,
            884,
            40533,
            322,
            1156,
            299,
            1429,
            3089,
            2793,
            13,
            286,
            519,
            309,
            311,
            309,
            611,
            575,
            411,
            257,
            2459,
            6257,
            13,
            400,
            286,
            519,
            51664
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19687500596046448,
        "compression_ratio": 1.7013888359069824,
        "no_speech_prob": 0.06852497905492783
    },
    {
        "id": 313,
        "seek": 168248,
        "start": 1708.47998046875,
        "end": 1712.0799560546875,
        "text": " there's a lot of foot guns here. And I think it's really easy to miscalibrate them. And you end up",
        "tokens": [
            51664,
            456,
            311,
            257,
            688,
            295,
            2671,
            10153,
            510,
            13,
            400,
            286,
            519,
            309,
            311,
            534,
            1858,
            281,
            3346,
            9895,
            897,
            4404,
            552,
            13,
            400,
            291,
            917,
            493,
            51844
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19687500596046448,
        "compression_ratio": 1.7013888359069824,
        "no_speech_prob": 0.06852497905492783
    },
    {
        "id": 314,
        "seek": 171208,
        "start": 1712.0799560546875,
        "end": 1716.3199462890625,
        "text": " cropping your sentences or something like that because of some type of parameter that you don't",
        "tokens": [
            50364,
            4848,
            3759,
            428,
            16579,
            420,
            746,
            411,
            300,
            570,
            295,
            512,
            2010,
            295,
            13075,
            300,
            291,
            500,
            380,
            50576
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22437787055969238,
        "compression_ratio": 1.719858169555664,
        "no_speech_prob": 0.00026947681908495724
    },
    {
        "id": 315,
        "seek": 171208,
        "start": 1716.3199462890625,
        "end": 1722.1600341796875,
        "text": " fully understand. So be very careful with the settings. Try to copy paste exactly maybe what",
        "tokens": [
            50576,
            4498,
            1223,
            13,
            407,
            312,
            588,
            5026,
            365,
            264,
            6257,
            13,
            6526,
            281,
            5055,
            9163,
            2293,
            1310,
            437,
            50868
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22437787055969238,
        "compression_ratio": 1.719858169555664,
        "no_speech_prob": 0.00026947681908495724
    },
    {
        "id": 316,
        "seek": 171208,
        "start": 1722.1600341796875,
        "end": 1727.0400390625,
        "text": " Meta did or basically spend a lot of time looking at all the hyperparameters and go through the code",
        "tokens": [
            50868,
            6377,
            64,
            630,
            420,
            1936,
            3496,
            257,
            688,
            295,
            565,
            1237,
            412,
            439,
            264,
            9848,
            2181,
            335,
            6202,
            293,
            352,
            807,
            264,
            3089,
            51112
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22437787055969238,
        "compression_ratio": 1.719858169555664,
        "no_speech_prob": 0.00026947681908495724
    },
    {
        "id": 317,
        "seek": 171208,
        "start": 1727.0400390625,
        "end": 1732.3199462890625,
        "text": " of sentence piece and make sure that you have this correct. But even if you have all the settings",
        "tokens": [
            51112,
            295,
            8174,
            2522,
            293,
            652,
            988,
            300,
            291,
            362,
            341,
            3006,
            13,
            583,
            754,
            498,
            291,
            362,
            439,
            264,
            6257,
            51376
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22437787055969238,
        "compression_ratio": 1.719858169555664,
        "no_speech_prob": 0.00026947681908495724
    },
    {
        "id": 318,
        "seek": 171208,
        "start": 1732.3199462890625,
        "end": 1737.760009765625,
        "text": " correct, I still think that the algorithm is kind of inferior to what's happening here. And maybe",
        "tokens": [
            51376,
            3006,
            11,
            286,
            920,
            519,
            300,
            264,
            9284,
            307,
            733,
            295,
            24249,
            281,
            437,
            311,
            2737,
            510,
            13,
            400,
            1310,
            51648
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22437787055969238,
        "compression_ratio": 1.719858169555664,
        "no_speech_prob": 0.00026947681908495724
    },
    {
        "id": 319,
        "seek": 173776,
        "start": 1737.8399658203125,
        "end": 1741.52001953125,
        "text": " the best if you really need to train your vocabulary, maybe the best thing is to just",
        "tokens": [
            50368,
            264,
            1151,
            498,
            291,
            534,
            643,
            281,
            3847,
            428,
            19864,
            11,
            1310,
            264,
            1151,
            551,
            307,
            281,
            445,
            50552
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23804983496665955,
        "compression_ratio": 1.7419354915618896,
        "no_speech_prob": 0.03514319285750389
    },
    {
        "id": 320,
        "seek": 173776,
        "start": 1741.52001953125,
        "end": 1747.43994140625,
        "text": " wait for minBPE to become as efficient as possible. And that's something that maybe I",
        "tokens": [
            50552,
            1699,
            337,
            923,
            33,
            5208,
            281,
            1813,
            382,
            7148,
            382,
            1944,
            13,
            400,
            300,
            311,
            746,
            300,
            1310,
            286,
            50848
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23804983496665955,
        "compression_ratio": 1.7419354915618896,
        "no_speech_prob": 0.03514319285750389
    },
    {
        "id": 321,
        "seek": 173776,
        "start": 1747.43994140625,
        "end": 1753.0400390625,
        "text": " hope to work on. And at some point, maybe we can be training basically, really what we want is we",
        "tokens": [
            50848,
            1454,
            281,
            589,
            322,
            13,
            400,
            412,
            512,
            935,
            11,
            1310,
            321,
            393,
            312,
            3097,
            1936,
            11,
            534,
            437,
            321,
            528,
            307,
            321,
            51128
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23804983496665955,
        "compression_ratio": 1.7419354915618896,
        "no_speech_prob": 0.03514319285750389
    },
    {
        "id": 322,
        "seek": 173776,
        "start": 1753.0400390625,
        "end": 1758.56005859375,
        "text": " want TIC token, but training code. And that is the ideal thing that currently does not exist.",
        "tokens": [
            51128,
            528,
            314,
            2532,
            14862,
            11,
            457,
            3097,
            3089,
            13,
            400,
            300,
            307,
            264,
            7157,
            551,
            300,
            4362,
            775,
            406,
            2514,
            13,
            51404
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23804983496665955,
        "compression_ratio": 1.7419354915618896,
        "no_speech_prob": 0.03514319285750389
    },
    {
        "id": 323,
        "seek": 173776,
        "start": 1759.199951171875,
        "end": 1763.52001953125,
        "text": " And minBPE is an implementation of it, but currently it's in Python.",
        "tokens": [
            51436,
            400,
            923,
            33,
            5208,
            307,
            364,
            11420,
            295,
            309,
            11,
            457,
            4362,
            309,
            311,
            294,
            15329,
            13,
            51652
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23804983496665955,
        "compression_ratio": 1.7419354915618896,
        "no_speech_prob": 0.03514319285750389
    },
    {
        "id": 324,
        "seek": 176352,
        "start": 1764.4000244140625,
        "end": 1769.199951171875,
        "text": " So that's currently what I have to say for tokenization. There might be an advanced video",
        "tokens": [
            50408,
            407,
            300,
            311,
            4362,
            437,
            286,
            362,
            281,
            584,
            337,
            14862,
            2144,
            13,
            821,
            1062,
            312,
            364,
            7339,
            960,
            50648
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2559027671813965,
        "compression_ratio": 1.4279279708862305,
        "no_speech_prob": 0.0009110387763939798
    },
    {
        "id": 325,
        "seek": 176352,
        "start": 1769.199951171875,
        "end": 1772.800048828125,
        "text": " that has even drier and even more detailed in the future. But for now,",
        "tokens": [
            50648,
            300,
            575,
            754,
            1630,
            260,
            293,
            754,
            544,
            9942,
            294,
            264,
            2027,
            13,
            583,
            337,
            586,
            11,
            50828
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2559027671813965,
        "compression_ratio": 1.4279279708862305,
        "no_speech_prob": 0.0009110387763939798
    },
    {
        "id": 326,
        "seek": 176352,
        "start": 1772.800048828125,
        "end": 1777.6800537109375,
        "text": " I think we're going to leave things off here and I hope that was helpful. Bye.",
        "tokens": [
            50828,
            286,
            519,
            321,
            434,
            516,
            281,
            1856,
            721,
            766,
            510,
            293,
            286,
            1454,
            300,
            390,
            4961,
            13,
            4621,
            13,
            51072
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2559027671813965,
        "compression_ratio": 1.4279279708862305,
        "no_speech_prob": 0.0009110387763939798
    },
    {
        "id": 327,
        "seek": 176352,
        "start": 1783.5999755859375,
        "end": 1791.5999755859375,
        "text": " And they increased this context size from GPT-1 of 512 to 1024 in GPT-4... 2.",
        "tokens": [
            51368,
            400,
            436,
            6505,
            341,
            4319,
            2744,
            490,
            26039,
            51,
            12,
            16,
            295,
            1025,
            4762,
            281,
            1266,
            7911,
            294,
            26039,
            51,
            12,
            19,
            485,
            568,
            13,
            51768
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2559027671813965,
        "compression_ratio": 1.4279279708862305,
        "no_speech_prob": 0.0009110387763939798
    },
    {
        "id": 328,
        "seek": 179160,
        "start": 1791.5999755859375,
        "end": 1798.6400146484375,
        "text": " The next... Okay. Next, I would like us to briefly walk through the",
        "tokens": [
            50364,
            440,
            958,
            485,
            1033,
            13,
            3087,
            11,
            286,
            576,
            411,
            505,
            281,
            10515,
            1792,
            807,
            264,
            50716
        ],
        "temperature": 0.0,
        "avg_logprob": -0.28220972418785095,
        "compression_ratio": 1.4285714626312256,
        "no_speech_prob": 0.0019267044262960553
    },
    {
        "id": 329,
        "seek": 179160,
        "start": 1798.6400146484375,
        "end": 1801.3599853515625,
        "text": " code for OpenAI on the GPT-2 encoder.py.",
        "tokens": [
            50716,
            3089,
            337,
            7238,
            48698,
            322,
            264,
            26039,
            51,
            12,
            17,
            2058,
            19866,
            13,
            8200,
            13,
            50852
        ],
        "temperature": 0.0,
        "avg_logprob": -0.28220972418785095,
        "compression_ratio": 1.4285714626312256,
        "no_speech_prob": 0.0019267044262960553
    },
    {
        "id": 330,
        "seek": 179160,
        "start": 1806.3199462890625,
        "end": 1810.3199462890625,
        "text": " I'm sorry, I'm going to sneeze. And then what's happening here is...",
        "tokens": [
            51100,
            286,
            478,
            2597,
            11,
            286,
            478,
            516,
            281,
            50076,
            13,
            400,
            550,
            437,
            311,
            2737,
            510,
            307,
            485,
            51300
        ],
        "temperature": 0.0,
        "avg_logprob": -0.28220972418785095,
        "compression_ratio": 1.4285714626312256,
        "no_speech_prob": 0.0019267044262960553
    },
    {
        "id": 331,
        "seek": 179160,
        "start": 1812.4000244140625,
        "end": 1817.8399658203125,
        "text": " This is a spurious layer that I will explain in a bit. What's happening here is...",
        "tokens": [
            51404,
            639,
            307,
            257,
            637,
            24274,
            4583,
            300,
            286,
            486,
            2903,
            294,
            257,
            857,
            13,
            708,
            311,
            2737,
            510,
            307,
            485,
            51676
        ],
        "temperature": 0.0,
        "avg_logprob": -0.28220972418785095,
        "compression_ratio": 1.4285714626312256,
        "no_speech_prob": 0.0019267044262960553
    }
]