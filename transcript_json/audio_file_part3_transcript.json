[
    {
        "id": 0,
        "seek": 0,
        "start": 0.0,
        "end": 6.360000133514404,
        "text": " Suppose we have a lot of spaces here, what's going to happen here is that these spaces",
        "tokens": [
            50364,
            21360,
            321,
            362,
            257,
            688,
            295,
            7673,
            510,
            11,
            437,
            311,
            516,
            281,
            1051,
            510,
            307,
            300,
            613,
            7673,
            50682
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2637392282485962,
        "compression_ratio": 1.9453781843185425,
        "no_speech_prob": 0.0032223311718553305
    },
    {
        "id": 1,
        "seek": 0,
        "start": 6.360000133514404,
        "end": 11.239999771118164,
        "text": " up to and not including the last character will get caught by this.",
        "tokens": [
            50682,
            493,
            281,
            293,
            406,
            3009,
            264,
            1036,
            2517,
            486,
            483,
            5415,
            538,
            341,
            13,
            50926
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2637392282485962,
        "compression_ratio": 1.9453781843185425,
        "no_speech_prob": 0.0032223311718553305
    },
    {
        "id": 2,
        "seek": 0,
        "start": 11.239999771118164,
        "end": 15.520000457763672,
        "text": " And what that will do is it will separate out the spaces up to but not including the",
        "tokens": [
            50926,
            400,
            437,
            300,
            486,
            360,
            307,
            309,
            486,
            4994,
            484,
            264,
            7673,
            493,
            281,
            457,
            406,
            3009,
            264,
            51140
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2637392282485962,
        "compression_ratio": 1.9453781843185425,
        "no_speech_prob": 0.0032223311718553305
    },
    {
        "id": 3,
        "seek": 0,
        "start": 15.520000457763672,
        "end": 21.68000030517578,
        "text": " last character, so that the last character can come here and join with the space U.",
        "tokens": [
            51140,
            1036,
            2517,
            11,
            370,
            300,
            264,
            1036,
            2517,
            393,
            808,
            510,
            293,
            3917,
            365,
            264,
            1901,
            624,
            13,
            51448
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2637392282485962,
        "compression_ratio": 1.9453781843185425,
        "no_speech_prob": 0.0032223311718553305
    },
    {
        "id": 4,
        "seek": 0,
        "start": 21.68000030517578,
        "end": 25.799999237060547,
        "text": " And the reason that's nice is because space U is the common token.",
        "tokens": [
            51448,
            400,
            264,
            1778,
            300,
            311,
            1481,
            307,
            570,
            1901,
            624,
            307,
            264,
            2689,
            14862,
            13,
            51654
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2637392282485962,
        "compression_ratio": 1.9453781843185425,
        "no_speech_prob": 0.0032223311718553305
    },
    {
        "id": 5,
        "seek": 0,
        "start": 25.799999237060547,
        "end": 29.579999923706055,
        "text": " So if I didn't have these extra spaces here, we would just have space U.",
        "tokens": [
            51654,
            407,
            498,
            286,
            994,
            380,
            362,
            613,
            2857,
            7673,
            510,
            11,
            321,
            576,
            445,
            362,
            1901,
            624,
            13,
            51843
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2637392282485962,
        "compression_ratio": 1.9453781843185425,
        "no_speech_prob": 0.0032223311718553305
    },
    {
        "id": 6,
        "seek": 2958,
        "start": 29.65999984741211,
        "end": 34.900001525878906,
        "text": " And if I add tokens, if I add spaces, we still have a space U, but now we have all this extra",
        "tokens": [
            50368,
            400,
            498,
            286,
            909,
            22667,
            11,
            498,
            286,
            909,
            7673,
            11,
            321,
            920,
            362,
            257,
            1901,
            624,
            11,
            457,
            586,
            321,
            362,
            439,
            341,
            2857,
            50630
        ],
        "temperature": 0.0,
        "avg_logprob": -0.3104509711265564,
        "compression_ratio": 1.695067286491394,
        "no_speech_prob": 7.602453842991963e-05
    },
    {
        "id": 7,
        "seek": 2958,
        "start": 34.900001525878906,
        "end": 35.900001525878906,
        "text": " white space.",
        "tokens": [
            50630,
            2418,
            1901,
            13,
            50680
        ],
        "temperature": 0.0,
        "avg_logprob": -0.3104509711265564,
        "compression_ratio": 1.695067286491394,
        "no_speech_prob": 7.602453842991963e-05
    },
    {
        "id": 8,
        "seek": 2958,
        "start": 35.900001525878906,
        "end": 41.81999969482422,
        "text": " So basically, the GPT-2 tokenizer really likes to have a space letter for numbers, and it",
        "tokens": [
            50680,
            407,
            1936,
            11,
            264,
            26039,
            51,
            12,
            17,
            14862,
            6545,
            534,
            5902,
            281,
            362,
            257,
            1901,
            5063,
            337,
            3547,
            11,
            293,
            309,
            50976
        ],
        "temperature": 0.0,
        "avg_logprob": -0.3104509711265564,
        "compression_ratio": 1.695067286491394,
        "no_speech_prob": 7.602453842991963e-05
    },
    {
        "id": 9,
        "seek": 2958,
        "start": 41.81999969482422,
        "end": 46.220001220703125,
        "text": " prepends these spaces, and this is just something that it is consistent about.",
        "tokens": [
            50976,
            2666,
            2581,
            613,
            7673,
            11,
            293,
            341,
            307,
            445,
            746,
            300,
            309,
            307,
            8398,
            466,
            13,
            51196
        ],
        "temperature": 0.0,
        "avg_logprob": -0.3104509711265564,
        "compression_ratio": 1.695067286491394,
        "no_speech_prob": 7.602453842991963e-05
    },
    {
        "id": 10,
        "seek": 2958,
        "start": 46.220001220703125,
        "end": 47.70000076293945,
        "text": " So that's what that is for.",
        "tokens": [
            51196,
            407,
            300,
            311,
            437,
            300,
            307,
            337,
            13,
            51270
        ],
        "temperature": 0.0,
        "avg_logprob": -0.3104509711265564,
        "compression_ratio": 1.695067286491394,
        "no_speech_prob": 7.602453842991963e-05
    },
    {
        "id": 11,
        "seek": 2958,
        "start": 47.70000076293945,
        "end": 53.02000045776367,
        "text": " And then finally, we have all the last fallback is white space characters.",
        "tokens": [
            51270,
            400,
            550,
            2721,
            11,
            321,
            362,
            439,
            264,
            1036,
            2100,
            3207,
            307,
            2418,
            1901,
            4342,
            13,
            51536
        ],
        "temperature": 0.0,
        "avg_logprob": -0.3104509711265564,
        "compression_ratio": 1.695067286491394,
        "no_speech_prob": 7.602453842991963e-05
    },
    {
        "id": 12,
        "seek": 5302,
        "start": 53.02000045776367,
        "end": 61.65999984741211,
        "text": " So that would be just, if that doesn't get caught, then this thing will catch any trailing",
        "tokens": [
            50364,
            407,
            300,
            576,
            312,
            445,
            11,
            498,
            300,
            1177,
            380,
            483,
            5415,
            11,
            550,
            341,
            551,
            486,
            3745,
            604,
            944,
            4883,
            50796
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24047024548053741,
        "compression_ratio": 1.6404494047164917,
        "no_speech_prob": 0.021285995841026306
    },
    {
        "id": 13,
        "seek": 5302,
        "start": 61.65999984741211,
        "end": 63.18000030517578,
        "text": " spaces and so on.",
        "tokens": [
            50796,
            7673,
            293,
            370,
            322,
            13,
            50872
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24047024548053741,
        "compression_ratio": 1.6404494047164917,
        "no_speech_prob": 0.021285995841026306
    },
    {
        "id": 14,
        "seek": 5302,
        "start": 63.18000030517578,
        "end": 66.0,
        "text": " I wanted to show one more real world example here.",
        "tokens": [
            50872,
            286,
            1415,
            281,
            855,
            472,
            544,
            957,
            1002,
            1365,
            510,
            13,
            51013
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24047024548053741,
        "compression_ratio": 1.6404494047164917,
        "no_speech_prob": 0.021285995841026306
    },
    {
        "id": 15,
        "seek": 5302,
        "start": 66.0,
        "end": 69.41999816894531,
        "text": " So if we have this string, which is a piece of Python code, and then we try to split it",
        "tokens": [
            51013,
            407,
            498,
            321,
            362,
            341,
            6798,
            11,
            597,
            307,
            257,
            2522,
            295,
            15329,
            3089,
            11,
            293,
            550,
            321,
            853,
            281,
            7472,
            309,
            51184
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24047024548053741,
        "compression_ratio": 1.6404494047164917,
        "no_speech_prob": 0.021285995841026306
    },
    {
        "id": 16,
        "seek": 5302,
        "start": 69.41999816894531,
        "end": 72.73999786376953,
        "text": " up, then this is the kind of output we get.",
        "tokens": [
            51184,
            493,
            11,
            550,
            341,
            307,
            264,
            733,
            295,
            5598,
            321,
            483,
            13,
            51350
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24047024548053741,
        "compression_ratio": 1.6404494047164917,
        "no_speech_prob": 0.021285995841026306
    },
    {
        "id": 17,
        "seek": 5302,
        "start": 72.73999786376953,
        "end": 76.26000213623047,
        "text": " So you'll notice that the list has many elements here, and that's because we are splitting",
        "tokens": [
            51350,
            407,
            291,
            603,
            3449,
            300,
            264,
            1329,
            575,
            867,
            4959,
            510,
            11,
            293,
            300,
            311,
            570,
            321,
            366,
            30348,
            51526
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24047024548053741,
        "compression_ratio": 1.6404494047164917,
        "no_speech_prob": 0.021285995841026306
    },
    {
        "id": 18,
        "seek": 5302,
        "start": 76.26000213623047,
        "end": 81.80000305175781,
        "text": " up fairly often, every time sort of a category changes.",
        "tokens": [
            51526,
            493,
            6457,
            2049,
            11,
            633,
            565,
            1333,
            295,
            257,
            7719,
            2962,
            13,
            51803
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24047024548053741,
        "compression_ratio": 1.6404494047164917,
        "no_speech_prob": 0.021285995841026306
    },
    {
        "id": 19,
        "seek": 8180,
        "start": 81.80000305175781,
        "end": 86.83999633789062,
        "text": " So there will never be any mergers within these elements, and that's what you are seeing",
        "tokens": [
            50364,
            407,
            456,
            486,
            1128,
            312,
            604,
            3551,
            9458,
            1951,
            613,
            4959,
            11,
            293,
            300,
            311,
            437,
            291,
            366,
            2577,
            50616
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24857938289642334,
        "compression_ratio": 1.625550627708435,
        "no_speech_prob": 0.0044683245941996574
    },
    {
        "id": 20,
        "seek": 8180,
        "start": 86.83999633789062,
        "end": 87.83999633789062,
        "text": " here.",
        "tokens": [
            50616,
            510,
            13,
            50666
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24857938289642334,
        "compression_ratio": 1.625550627708435,
        "no_speech_prob": 0.0044683245941996574
    },
    {
        "id": 21,
        "seek": 8180,
        "start": 87.83999633789062,
        "end": 94.5999984741211,
        "text": " Now, you might think that in order to train the tokenizer, OpenAI has used this to split",
        "tokens": [
            50666,
            823,
            11,
            291,
            1062,
            519,
            300,
            294,
            1668,
            281,
            3847,
            264,
            14862,
            6545,
            11,
            7238,
            48698,
            575,
            1143,
            341,
            281,
            7472,
            51004
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24857938289642334,
        "compression_ratio": 1.625550627708435,
        "no_speech_prob": 0.0044683245941996574
    },
    {
        "id": 22,
        "seek": 8180,
        "start": 94.5999984741211,
        "end": 100.0999984741211,
        "text": " up text into chunks, and then run just a BP algorithm within all the chunks.",
        "tokens": [
            51004,
            493,
            2487,
            666,
            24004,
            11,
            293,
            550,
            1190,
            445,
            257,
            40533,
            9284,
            1951,
            439,
            264,
            24004,
            13,
            51279
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24857938289642334,
        "compression_ratio": 1.625550627708435,
        "no_speech_prob": 0.0044683245941996574
    },
    {
        "id": 23,
        "seek": 8180,
        "start": 100.0999984741211,
        "end": 103.68000030517578,
        "text": " But that is not exactly what happened, and the reason is the following.",
        "tokens": [
            51279,
            583,
            300,
            307,
            406,
            2293,
            437,
            2011,
            11,
            293,
            264,
            1778,
            307,
            264,
            3480,
            13,
            51458
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24857938289642334,
        "compression_ratio": 1.625550627708435,
        "no_speech_prob": 0.0044683245941996574
    },
    {
        "id": 24,
        "seek": 8180,
        "start": 103.68000030517578,
        "end": 106.55999755859375,
        "text": " Notice that we have the spaces here.",
        "tokens": [
            51458,
            13428,
            300,
            321,
            362,
            264,
            7673,
            510,
            13,
            51602
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24857938289642334,
        "compression_ratio": 1.625550627708435,
        "no_speech_prob": 0.0044683245941996574
    },
    {
        "id": 25,
        "seek": 10656,
        "start": 106.55999755859375,
        "end": 112.0,
        "text": " Those spaces end up being entire elements, but these spaces never actually end up being",
        "tokens": [
            50364,
            3950,
            7673,
            917,
            493,
            885,
            2302,
            4959,
            11,
            457,
            613,
            7673,
            1128,
            767,
            917,
            493,
            885,
            50636
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2775997519493103,
        "compression_ratio": 1.65625,
        "no_speech_prob": 0.06560168415307999
    },
    {
        "id": 26,
        "seek": 10656,
        "start": 112.0,
        "end": 116.5999984741211,
        "text": " merged by OpenAI, and the way you can tell is that if you copy-paste the exact same chunk",
        "tokens": [
            50636,
            36427,
            538,
            7238,
            48698,
            11,
            293,
            264,
            636,
            291,
            393,
            980,
            307,
            300,
            498,
            291,
            5055,
            12,
            79,
            9079,
            264,
            1900,
            912,
            16635,
            50866
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2775997519493103,
        "compression_ratio": 1.65625,
        "no_speech_prob": 0.06560168415307999
    },
    {
        "id": 27,
        "seek": 10656,
        "start": 116.5999984741211,
        "end": 122.55999755859375,
        "text": " here into TickTokenizer, you see that all the spaces are kept independent, and they're",
        "tokens": [
            50866,
            510,
            666,
            314,
            618,
            51,
            8406,
            6545,
            11,
            291,
            536,
            300,
            439,
            264,
            7673,
            366,
            4305,
            6695,
            11,
            293,
            436,
            434,
            51164
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2775997519493103,
        "compression_ratio": 1.65625,
        "no_speech_prob": 0.06560168415307999
    },
    {
        "id": 28,
        "seek": 10656,
        "start": 122.55999755859375,
        "end": 125.19999694824219,
        "text": " all token 220.",
        "tokens": [
            51164,
            439,
            14862,
            29387,
            13,
            51296
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2775997519493103,
        "compression_ratio": 1.65625,
        "no_speech_prob": 0.06560168415307999
    },
    {
        "id": 29,
        "seek": 10656,
        "start": 125.19999694824219,
        "end": 130.83999633789062,
        "text": " So I think OpenAI at some point enforced some rule that these spaces would never be merged,",
        "tokens": [
            51296,
            407,
            286,
            519,
            7238,
            48698,
            412,
            512,
            935,
            40953,
            512,
            4978,
            300,
            613,
            7673,
            576,
            1128,
            312,
            36427,
            11,
            51578
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2775997519493103,
        "compression_ratio": 1.65625,
        "no_speech_prob": 0.06560168415307999
    },
    {
        "id": 30,
        "seek": 13084,
        "start": 130.83999633789062,
        "end": 137.16000366210938,
        "text": " and so there's some additional rules on top of just chunking and BP that OpenAI is not",
        "tokens": [
            50364,
            293,
            370,
            456,
            311,
            512,
            4497,
            4474,
            322,
            1192,
            295,
            445,
            16635,
            278,
            293,
            40533,
            300,
            7238,
            48698,
            307,
            406,
            50680
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2299618273973465,
        "compression_ratio": 1.77173912525177,
        "no_speech_prob": 0.150021493434906
    },
    {
        "id": 31,
        "seek": 13084,
        "start": 137.16000366210938,
        "end": 138.16000366210938,
        "text": " clear about.",
        "tokens": [
            50680,
            1850,
            466,
            13,
            50730
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2299618273973465,
        "compression_ratio": 1.77173912525177,
        "no_speech_prob": 0.150021493434906
    },
    {
        "id": 32,
        "seek": 13084,
        "start": 138.16000366210938,
        "end": 143.27999877929688,
        "text": " Now, the training code for the GPT-2 tokenizer was never released, so all we have is the",
        "tokens": [
            50730,
            823,
            11,
            264,
            3097,
            3089,
            337,
            264,
            26039,
            51,
            12,
            17,
            14862,
            6545,
            390,
            1128,
            4736,
            11,
            370,
            439,
            321,
            362,
            307,
            264,
            50986
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2299618273973465,
        "compression_ratio": 1.77173912525177,
        "no_speech_prob": 0.150021493434906
    },
    {
        "id": 33,
        "seek": 13084,
        "start": 143.27999877929688,
        "end": 147.55999755859375,
        "text": " code that I've already shown you, but this code here that they've released is only the",
        "tokens": [
            50986,
            3089,
            300,
            286,
            600,
            1217,
            4898,
            291,
            11,
            457,
            341,
            3089,
            510,
            300,
            436,
            600,
            4736,
            307,
            787,
            264,
            51200
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2299618273973465,
        "compression_ratio": 1.77173912525177,
        "no_speech_prob": 0.150021493434906
    },
    {
        "id": 34,
        "seek": 13084,
        "start": 147.55999755859375,
        "end": 150.5,
        "text": " inference code for the tokens.",
        "tokens": [
            51200,
            38253,
            3089,
            337,
            264,
            22667,
            13,
            51347
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2299618273973465,
        "compression_ratio": 1.77173912525177,
        "no_speech_prob": 0.150021493434906
    },
    {
        "id": 35,
        "seek": 13084,
        "start": 150.5,
        "end": 151.75999450683594,
        "text": " So this is not the training code.",
        "tokens": [
            51347,
            407,
            341,
            307,
            406,
            264,
            3097,
            3089,
            13,
            51410
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2299618273973465,
        "compression_ratio": 1.77173912525177,
        "no_speech_prob": 0.150021493434906
    },
    {
        "id": 36,
        "seek": 13084,
        "start": 151.75999450683594,
        "end": 154.8000030517578,
        "text": " You can't give it a piece of text and train the tokenizer.",
        "tokens": [
            51410,
            509,
            393,
            380,
            976,
            309,
            257,
            2522,
            295,
            2487,
            293,
            3847,
            264,
            14862,
            6545,
            13,
            51562
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2299618273973465,
        "compression_ratio": 1.77173912525177,
        "no_speech_prob": 0.150021493434906
    },
    {
        "id": 37,
        "seek": 13084,
        "start": 154.8000030517578,
        "end": 159.9199981689453,
        "text": " This is just the inference code, which takes the merges that we have up above and applies",
        "tokens": [
            51562,
            639,
            307,
            445,
            264,
            38253,
            3089,
            11,
            597,
            2516,
            264,
            3551,
            2880,
            300,
            321,
            362,
            493,
            3673,
            293,
            13165,
            51818
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2299618273973465,
        "compression_ratio": 1.77173912525177,
        "no_speech_prob": 0.150021493434906
    },
    {
        "id": 38,
        "seek": 15992,
        "start": 159.9199981689453,
        "end": 162.52000427246094,
        "text": " them to a new piece of text.",
        "tokens": [
            50364,
            552,
            281,
            257,
            777,
            2522,
            295,
            2487,
            13,
            50494
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21319781243801117,
        "compression_ratio": 1.642241358757019,
        "no_speech_prob": 0.03308367729187012
    },
    {
        "id": 39,
        "seek": 15992,
        "start": 162.52000427246094,
        "end": 167.8800048828125,
        "text": " And so we don't know exactly how OpenAI trained the tokenizer, but it wasn't as simple as",
        "tokens": [
            50494,
            400,
            370,
            321,
            500,
            380,
            458,
            2293,
            577,
            7238,
            48698,
            8895,
            264,
            14862,
            6545,
            11,
            457,
            309,
            2067,
            380,
            382,
            2199,
            382,
            50762
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21319781243801117,
        "compression_ratio": 1.642241358757019,
        "no_speech_prob": 0.03308367729187012
    },
    {
        "id": 40,
        "seek": 15992,
        "start": 167.8800048828125,
        "end": 171.1999969482422,
        "text": " chunk it up and BP it, whatever it was.",
        "tokens": [
            50762,
            16635,
            309,
            493,
            293,
            40533,
            309,
            11,
            2035,
            309,
            390,
            13,
            50928
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21319781243801117,
        "compression_ratio": 1.642241358757019,
        "no_speech_prob": 0.03308367729187012
    },
    {
        "id": 41,
        "seek": 15992,
        "start": 171.1999969482422,
        "end": 176.63999938964844,
        "text": " Next, I wanted to introduce you to the TickToken library from OpenAI, which is the official",
        "tokens": [
            50928,
            3087,
            11,
            286,
            1415,
            281,
            5366,
            291,
            281,
            264,
            314,
            618,
            51,
            8406,
            6405,
            490,
            7238,
            48698,
            11,
            597,
            307,
            264,
            4783,
            51200
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21319781243801117,
        "compression_ratio": 1.642241358757019,
        "no_speech_prob": 0.03308367729187012
    },
    {
        "id": 42,
        "seek": 15992,
        "start": 176.63999938964844,
        "end": 179.44000244140625,
        "text": " library for tokenization from OpenAI.",
        "tokens": [
            51200,
            6405,
            337,
            14862,
            2144,
            490,
            7238,
            48698,
            13,
            51340
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21319781243801117,
        "compression_ratio": 1.642241358757019,
        "no_speech_prob": 0.03308367729187012
    },
    {
        "id": 43,
        "seek": 15992,
        "start": 179.44000244140625,
        "end": 187.5800018310547,
        "text": " So this is TickToken, pip install TickToken, and then you can do the tokenization inference.",
        "tokens": [
            51340,
            407,
            341,
            307,
            314,
            618,
            51,
            8406,
            11,
            8489,
            3625,
            314,
            618,
            51,
            8406,
            11,
            293,
            550,
            291,
            393,
            360,
            264,
            14862,
            2144,
            38253,
            13,
            51747
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21319781243801117,
        "compression_ratio": 1.642241358757019,
        "no_speech_prob": 0.03308367729187012
    },
    {
        "id": 44,
        "seek": 18758,
        "start": 187.5800018310547,
        "end": 188.89999389648438,
        "text": " This is, again, not training code.",
        "tokens": [
            50364,
            639,
            307,
            11,
            797,
            11,
            406,
            3097,
            3089,
            13,
            50430
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22360309958457947,
        "compression_ratio": 1.701195240020752,
        "no_speech_prob": 0.002396710216999054
    },
    {
        "id": 45,
        "seek": 18758,
        "start": 188.89999389648438,
        "end": 192.25999450683594,
        "text": " This is only inference code for tokenization.",
        "tokens": [
            50430,
            639,
            307,
            787,
            38253,
            3089,
            337,
            14862,
            2144,
            13,
            50598
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22360309958457947,
        "compression_ratio": 1.701195240020752,
        "no_speech_prob": 0.002396710216999054
    },
    {
        "id": 46,
        "seek": 18758,
        "start": 192.25999450683594,
        "end": 196.5399932861328,
        "text": " I wanted to show you how you would use it, quite simple, and running this just gives",
        "tokens": [
            50598,
            286,
            1415,
            281,
            855,
            291,
            577,
            291,
            576,
            764,
            309,
            11,
            1596,
            2199,
            11,
            293,
            2614,
            341,
            445,
            2709,
            50812
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22360309958457947,
        "compression_ratio": 1.701195240020752,
        "no_speech_prob": 0.002396710216999054
    },
    {
        "id": 47,
        "seek": 18758,
        "start": 196.5399932861328,
        "end": 199.82000732421875,
        "text": " us the GPT-2 tokens or the GPT-4 tokens.",
        "tokens": [
            50812,
            505,
            264,
            26039,
            51,
            12,
            17,
            22667,
            420,
            264,
            26039,
            51,
            12,
            19,
            22667,
            13,
            50976
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22360309958457947,
        "compression_ratio": 1.701195240020752,
        "no_speech_prob": 0.002396710216999054
    },
    {
        "id": 48,
        "seek": 18758,
        "start": 199.82000732421875,
        "end": 202.66000366210938,
        "text": " So this is the tokenizer used for GPT-4.",
        "tokens": [
            50976,
            407,
            341,
            307,
            264,
            14862,
            6545,
            1143,
            337,
            26039,
            51,
            12,
            19,
            13,
            51118
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22360309958457947,
        "compression_ratio": 1.701195240020752,
        "no_speech_prob": 0.002396710216999054
    },
    {
        "id": 49,
        "seek": 18758,
        "start": 202.66000366210938,
        "end": 207.94000244140625,
        "text": " And so in particular, we see that the whitespace in GPT-2 remains unmerged, but in GPT-4, these",
        "tokens": [
            51118,
            400,
            370,
            294,
            1729,
            11,
            321,
            536,
            300,
            264,
            21909,
            17940,
            294,
            26039,
            51,
            12,
            17,
            7023,
            517,
            936,
            3004,
            11,
            457,
            294,
            26039,
            51,
            12,
            19,
            11,
            613,
            51382
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22360309958457947,
        "compression_ratio": 1.701195240020752,
        "no_speech_prob": 0.002396710216999054
    },
    {
        "id": 50,
        "seek": 18758,
        "start": 207.94000244140625,
        "end": 213.3800048828125,
        "text": " whitespaces merge, as we also saw in this one, where here they're all unmerged, but",
        "tokens": [
            51382,
            21909,
            79,
            2116,
            22183,
            11,
            382,
            321,
            611,
            1866,
            294,
            341,
            472,
            11,
            689,
            510,
            436,
            434,
            439,
            517,
            936,
            3004,
            11,
            457,
            51654
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22360309958457947,
        "compression_ratio": 1.701195240020752,
        "no_speech_prob": 0.002396710216999054
    },
    {
        "id": 51,
        "seek": 21338,
        "start": 213.3800048828125,
        "end": 219.3800048828125,
        "text": " if we go down to GPT-4, they become merged.",
        "tokens": [
            50364,
            498,
            321,
            352,
            760,
            281,
            26039,
            51,
            12,
            19,
            11,
            436,
            1813,
            36427,
            13,
            50664
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25071465969085693,
        "compression_ratio": 1.6140351295471191,
        "no_speech_prob": 0.19928725063800812
    },
    {
        "id": 52,
        "seek": 21338,
        "start": 219.3800048828125,
        "end": 226.5399932861328,
        "text": " Now in the GPT-4 tokenizer, they changed the regular expression that they use to chunk",
        "tokens": [
            50664,
            823,
            294,
            264,
            26039,
            51,
            12,
            19,
            14862,
            6545,
            11,
            436,
            3105,
            264,
            3890,
            6114,
            300,
            436,
            764,
            281,
            16635,
            51022
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25071465969085693,
        "compression_ratio": 1.6140351295471191,
        "no_speech_prob": 0.19928725063800812
    },
    {
        "id": 53,
        "seek": 21338,
        "start": 226.5399932861328,
        "end": 227.8800048828125,
        "text": " up text.",
        "tokens": [
            51022,
            493,
            2487,
            13,
            51089
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25071465969085693,
        "compression_ratio": 1.6140351295471191,
        "no_speech_prob": 0.19928725063800812
    },
    {
        "id": 54,
        "seek": 21338,
        "start": 227.8800048828125,
        "end": 233.3000030517578,
        "text": " So the way to see this is that if you come to the TickToken library, and then you go",
        "tokens": [
            51089,
            407,
            264,
            636,
            281,
            536,
            341,
            307,
            300,
            498,
            291,
            808,
            281,
            264,
            314,
            618,
            51,
            8406,
            6405,
            11,
            293,
            550,
            291,
            352,
            51360
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25071465969085693,
        "compression_ratio": 1.6140351295471191,
        "no_speech_prob": 0.19928725063800812
    },
    {
        "id": 55,
        "seek": 21338,
        "start": 233.3000030517578,
        "end": 238.3800048828125,
        "text": " to this file, TickToken.ext.openaipublic, this is where sort of like the definition",
        "tokens": [
            51360,
            281,
            341,
            3991,
            11,
            314,
            618,
            51,
            8406,
            13,
            3828,
            13,
            15752,
            64,
            647,
            3865,
            11,
            341,
            307,
            689,
            1333,
            295,
            411,
            264,
            7123,
            51614
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25071465969085693,
        "compression_ratio": 1.6140351295471191,
        "no_speech_prob": 0.19928725063800812
    },
    {
        "id": 56,
        "seek": 21338,
        "start": 238.3800048828125,
        "end": 241.9600067138672,
        "text": " of all these different tokenizers that OpenAI maintains is.",
        "tokens": [
            51614,
            295,
            439,
            613,
            819,
            14862,
            22525,
            300,
            7238,
            48698,
            33385,
            307,
            13,
            51793
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25071465969085693,
        "compression_ratio": 1.6140351295471191,
        "no_speech_prob": 0.19928725063800812
    },
    {
        "id": 57,
        "seek": 24196,
        "start": 241.9600067138672,
        "end": 245.72000122070312,
        "text": " And so necessarily to do the inference, they had to publish some of the details about the",
        "tokens": [
            50364,
            400,
            370,
            4725,
            281,
            360,
            264,
            38253,
            11,
            436,
            632,
            281,
            11374,
            512,
            295,
            264,
            4365,
            466,
            264,
            50552
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23024053871631622,
        "compression_ratio": 1.77173912525177,
        "no_speech_prob": 0.0009850000496953726
    },
    {
        "id": 58,
        "seek": 24196,
        "start": 245.72000122070312,
        "end": 247.24000549316406,
        "text": " strings.",
        "tokens": [
            50552,
            13985,
            13,
            50628
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23024053871631622,
        "compression_ratio": 1.77173912525177,
        "no_speech_prob": 0.0009850000496953726
    },
    {
        "id": 59,
        "seek": 24196,
        "start": 247.24000549316406,
        "end": 250.3000030517578,
        "text": " So this is the string that we already saw for GPT-2.",
        "tokens": [
            50628,
            407,
            341,
            307,
            264,
            6798,
            300,
            321,
            1217,
            1866,
            337,
            26039,
            51,
            12,
            17,
            13,
            50781
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23024053871631622,
        "compression_ratio": 1.77173912525177,
        "no_speech_prob": 0.0009850000496953726
    },
    {
        "id": 60,
        "seek": 24196,
        "start": 250.3000030517578,
        "end": 255.25999450683594,
        "text": " It is slightly different, but it is actually equivalent to what we discussed here.",
        "tokens": [
            50781,
            467,
            307,
            4748,
            819,
            11,
            457,
            309,
            307,
            767,
            10344,
            281,
            437,
            321,
            7152,
            510,
            13,
            51029
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23024053871631622,
        "compression_ratio": 1.77173912525177,
        "no_speech_prob": 0.0009850000496953726
    },
    {
        "id": 61,
        "seek": 24196,
        "start": 255.25999450683594,
        "end": 259.9599914550781,
        "text": " So this pattern that we discussed is equivalent to this pattern, and this one just executes",
        "tokens": [
            51029,
            407,
            341,
            5102,
            300,
            321,
            7152,
            307,
            10344,
            281,
            341,
            5102,
            11,
            293,
            341,
            472,
            445,
            4454,
            1819,
            51264
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23024053871631622,
        "compression_ratio": 1.77173912525177,
        "no_speech_prob": 0.0009850000496953726
    },
    {
        "id": 62,
        "seek": 24196,
        "start": 259.9599914550781,
        "end": 261.55999755859375,
        "text": " a little bit faster.",
        "tokens": [
            51264,
            257,
            707,
            857,
            4663,
            13,
            51344
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23024053871631622,
        "compression_ratio": 1.77173912525177,
        "no_speech_prob": 0.0009850000496953726
    },
    {
        "id": 63,
        "seek": 24196,
        "start": 261.55999755859375,
        "end": 264.3999938964844,
        "text": " So here you see a little bit of a slightly different definition, but otherwise it's the",
        "tokens": [
            51344,
            407,
            510,
            291,
            536,
            257,
            707,
            857,
            295,
            257,
            4748,
            819,
            7123,
            11,
            457,
            5911,
            309,
            311,
            264,
            51486
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23024053871631622,
        "compression_ratio": 1.77173912525177,
        "no_speech_prob": 0.0009850000496953726
    },
    {
        "id": 64,
        "seek": 24196,
        "start": 264.3999938964844,
        "end": 265.3999938964844,
        "text": " same.",
        "tokens": [
            51486,
            912,
            13,
            51536
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23024053871631622,
        "compression_ratio": 1.77173912525177,
        "no_speech_prob": 0.0009850000496953726
    },
    {
        "id": 65,
        "seek": 24196,
        "start": 265.3999938964844,
        "end": 268.3999938964844,
        "text": " We're going to go into special tokens in a bit.",
        "tokens": [
            51536,
            492,
            434,
            516,
            281,
            352,
            666,
            2121,
            22667,
            294,
            257,
            857,
            13,
            51686
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23024053871631622,
        "compression_ratio": 1.77173912525177,
        "no_speech_prob": 0.0009850000496953726
    },
    {
        "id": 66,
        "seek": 26840,
        "start": 268.3999938964844,
        "end": 274.1600036621094,
        "text": " And then if you scroll down to CL100K, this is the GPT-4 tokenizer, you see that the pattern",
        "tokens": [
            50364,
            400,
            550,
            498,
            291,
            11369,
            760,
            281,
            12855,
            6879,
            42,
            11,
            341,
            307,
            264,
            26039,
            51,
            12,
            19,
            14862,
            6545,
            11,
            291,
            536,
            300,
            264,
            5102,
            50652
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2600937485694885,
        "compression_ratio": 1.590747356414795,
        "no_speech_prob": 0.052614372223615646
    },
    {
        "id": 67,
        "seek": 26840,
        "start": 274.1600036621094,
        "end": 276.7200012207031,
        "text": " has changed.",
        "tokens": [
            50652,
            575,
            3105,
            13,
            50780
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2600937485694885,
        "compression_ratio": 1.590747356414795,
        "no_speech_prob": 0.052614372223615646
    },
    {
        "id": 68,
        "seek": 26840,
        "start": 276.7200012207031,
        "end": 281.0,
        "text": " And this is kind of like the major change in addition to a bunch of other special tokens,",
        "tokens": [
            50780,
            400,
            341,
            307,
            733,
            295,
            411,
            264,
            2563,
            1319,
            294,
            4500,
            281,
            257,
            3840,
            295,
            661,
            2121,
            22667,
            11,
            50994
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2600937485694885,
        "compression_ratio": 1.590747356414795,
        "no_speech_prob": 0.052614372223615646
    },
    {
        "id": 69,
        "seek": 26840,
        "start": 281.0,
        "end": 283.6400146484375,
        "text": " which we'll go into in a bit again.",
        "tokens": [
            50994,
            597,
            321,
            603,
            352,
            666,
            294,
            257,
            857,
            797,
            13,
            51126
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2600937485694885,
        "compression_ratio": 1.590747356414795,
        "no_speech_prob": 0.052614372223615646
    },
    {
        "id": 70,
        "seek": 26840,
        "start": 283.6400146484375,
        "end": 287.67999267578125,
        "text": " Now I'm not going to actually go into the full detail of the pattern change, because",
        "tokens": [
            51126,
            823,
            286,
            478,
            406,
            516,
            281,
            767,
            352,
            666,
            264,
            1577,
            2607,
            295,
            264,
            5102,
            1319,
            11,
            570,
            51328
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2600937485694885,
        "compression_ratio": 1.590747356414795,
        "no_speech_prob": 0.052614372223615646
    },
    {
        "id": 71,
        "seek": 26840,
        "start": 287.67999267578125,
        "end": 289.0400085449219,
        "text": " honestly this is mind-numbing.",
        "tokens": [
            51328,
            6095,
            341,
            307,
            1575,
            12,
            77,
            34236,
            13,
            51396
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2600937485694885,
        "compression_ratio": 1.590747356414795,
        "no_speech_prob": 0.052614372223615646
    },
    {
        "id": 72,
        "seek": 26840,
        "start": 289.0400085449219,
        "end": 294.3599853515625,
        "text": " I would just advise that you pull out ChatGPT and the RegEx documentation and just step",
        "tokens": [
            51396,
            286,
            576,
            445,
            18312,
            300,
            291,
            2235,
            484,
            27503,
            38,
            47,
            51,
            293,
            264,
            4791,
            11149,
            14333,
            293,
            445,
            1823,
            51662
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2600937485694885,
        "compression_ratio": 1.590747356414795,
        "no_speech_prob": 0.052614372223615646
    },
    {
        "id": 73,
        "seek": 26840,
        "start": 294.3599853515625,
        "end": 295.44000244140625,
        "text": " through it.",
        "tokens": [
            51662,
            807,
            309,
            13,
            51716
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2600937485694885,
        "compression_ratio": 1.590747356414795,
        "no_speech_prob": 0.052614372223615646
    },
    {
        "id": 74,
        "seek": 29544,
        "start": 295.4800109863281,
        "end": 300.6000061035156,
        "text": " But really the major changes are, number one, you see this I here?",
        "tokens": [
            50366,
            583,
            534,
            264,
            2563,
            2962,
            366,
            11,
            1230,
            472,
            11,
            291,
            536,
            341,
            286,
            510,
            30,
            50622
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27491533756256104,
        "compression_ratio": 1.7488986253738403,
        "no_speech_prob": 0.04084400460124016
    },
    {
        "id": 75,
        "seek": 29544,
        "start": 300.6000061035156,
        "end": 305.8800048828125,
        "text": " That means that the case sensitivity, this is case insensitive match.",
        "tokens": [
            50622,
            663,
            1355,
            300,
            264,
            1389,
            19392,
            11,
            341,
            307,
            1389,
            1028,
            34465,
            2995,
            13,
            50886
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27491533756256104,
        "compression_ratio": 1.7488986253738403,
        "no_speech_prob": 0.04084400460124016
    },
    {
        "id": 76,
        "seek": 29544,
        "start": 305.8800048828125,
        "end": 312.3599853515625,
        "text": " And so the comment that we saw earlier on, oh, we should have used re.uppercase, basically",
        "tokens": [
            50886,
            400,
            370,
            264,
            2871,
            300,
            321,
            1866,
            3071,
            322,
            11,
            1954,
            11,
            321,
            820,
            362,
            1143,
            319,
            13,
            84,
            427,
            2869,
            651,
            11,
            1936,
            51210
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27491533756256104,
        "compression_ratio": 1.7488986253738403,
        "no_speech_prob": 0.04084400460124016
    },
    {
        "id": 77,
        "seek": 29544,
        "start": 312.3599853515625,
        "end": 319.8399963378906,
        "text": " we're now going to be matching these, apostrophe s, apostrophe d, apostrophe m, etc.",
        "tokens": [
            51210,
            321,
            434,
            586,
            516,
            281,
            312,
            14324,
            613,
            11,
            19484,
            27194,
            262,
            11,
            19484,
            27194,
            274,
            11,
            19484,
            27194,
            275,
            11,
            5183,
            13,
            51584
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27491533756256104,
        "compression_ratio": 1.7488986253738403,
        "no_speech_prob": 0.04084400460124016
    },
    {
        "id": 78,
        "seek": 29544,
        "start": 319.8399963378906,
        "end": 322.8800048828125,
        "text": " We're going to be matching them both in lowercase and in uppercase.",
        "tokens": [
            51584,
            492,
            434,
            516,
            281,
            312,
            14324,
            552,
            1293,
            294,
            3126,
            9765,
            293,
            294,
            11775,
            2869,
            651,
            13,
            51736
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27491533756256104,
        "compression_ratio": 1.7488986253738403,
        "no_speech_prob": 0.04084400460124016
    },
    {
        "id": 79,
        "seek": 29544,
        "start": 322.8800048828125,
        "end": 324.3599853515625,
        "text": " So that's fixed.",
        "tokens": [
            51736,
            407,
            300,
            311,
            6806,
            13,
            51810
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27491533756256104,
        "compression_ratio": 1.7488986253738403,
        "no_speech_prob": 0.04084400460124016
    },
    {
        "id": 80,
        "seek": 32436,
        "start": 324.3599853515625,
        "end": 327.3999938964844,
        "text": " There's a bunch of different handling of the whitespace that I'm not going to go into",
        "tokens": [
            50364,
            821,
            311,
            257,
            3840,
            295,
            819,
            13175,
            295,
            264,
            21909,
            17940,
            300,
            286,
            478,
            406,
            516,
            281,
            352,
            666,
            50516
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21638725697994232,
        "compression_ratio": 1.848739504814148,
        "no_speech_prob": 0.004829670302569866
    },
    {
        "id": 81,
        "seek": 32436,
        "start": 327.3999938964844,
        "end": 329.0799865722656,
        "text": " the full details of.",
        "tokens": [
            50516,
            264,
            1577,
            4365,
            295,
            13,
            50600
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21638725697994232,
        "compression_ratio": 1.848739504814148,
        "no_speech_prob": 0.004829670302569866
    },
    {
        "id": 82,
        "seek": 32436,
        "start": 329.0799865722656,
        "end": 333.7200012207031,
        "text": " And then one more thing here is you will notice that when they match the numbers, they only",
        "tokens": [
            50600,
            400,
            550,
            472,
            544,
            551,
            510,
            307,
            291,
            486,
            3449,
            300,
            562,
            436,
            2995,
            264,
            3547,
            11,
            436,
            787,
            50832
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21638725697994232,
        "compression_ratio": 1.848739504814148,
        "no_speech_prob": 0.004829670302569866
    },
    {
        "id": 83,
        "seek": 32436,
        "start": 333.7200012207031,
        "end": 336.4800109863281,
        "text": " match one to three numbers.",
        "tokens": [
            50832,
            2995,
            472,
            281,
            1045,
            3547,
            13,
            50970
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21638725697994232,
        "compression_ratio": 1.848739504814148,
        "no_speech_prob": 0.004829670302569866
    },
    {
        "id": 84,
        "seek": 32436,
        "start": 336.4800109863281,
        "end": 343.20001220703125,
        "text": " So they will never merge numbers that are in more than three digits.",
        "tokens": [
            50970,
            407,
            436,
            486,
            1128,
            22183,
            3547,
            300,
            366,
            294,
            544,
            813,
            1045,
            27011,
            13,
            51306
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21638725697994232,
        "compression_ratio": 1.848739504814148,
        "no_speech_prob": 0.004829670302569866
    },
    {
        "id": 85,
        "seek": 32436,
        "start": 343.20001220703125,
        "end": 347.3399963378906,
        "text": " Only up to three digits of numbers will ever be merged.",
        "tokens": [
            51306,
            5686,
            493,
            281,
            1045,
            27011,
            295,
            3547,
            486,
            1562,
            312,
            36427,
            13,
            51513
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21638725697994232,
        "compression_ratio": 1.848739504814148,
        "no_speech_prob": 0.004829670302569866
    },
    {
        "id": 86,
        "seek": 32436,
        "start": 347.3399963378906,
        "end": 352.20001220703125,
        "text": " And that's one change that they made as well, to prevent tokens that are very, very long",
        "tokens": [
            51513,
            400,
            300,
            311,
            472,
            1319,
            300,
            436,
            1027,
            382,
            731,
            11,
            281,
            4871,
            22667,
            300,
            366,
            588,
            11,
            588,
            938,
            51756
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21638725697994232,
        "compression_ratio": 1.848739504814148,
        "no_speech_prob": 0.004829670302569866
    },
    {
        "id": 87,
        "seek": 35220,
        "start": 352.20001220703125,
        "end": 354.3999938964844,
        "text": " number sequences.",
        "tokens": [
            50364,
            1230,
            22978,
            13,
            50474
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2527278661727905,
        "compression_ratio": 1.519713282585144,
        "no_speech_prob": 0.008445548824965954
    },
    {
        "id": 88,
        "seek": 35220,
        "start": 354.3999938964844,
        "end": 358.0,
        "text": " But again, we don't really know why they do any of this stuff, because none of this is",
        "tokens": [
            50474,
            583,
            797,
            11,
            321,
            500,
            380,
            534,
            458,
            983,
            436,
            360,
            604,
            295,
            341,
            1507,
            11,
            570,
            6022,
            295,
            341,
            307,
            50654
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2527278661727905,
        "compression_ratio": 1.519713282585144,
        "no_speech_prob": 0.008445548824965954
    },
    {
        "id": 89,
        "seek": 35220,
        "start": 358.0,
        "end": 361.6400146484375,
        "text": " documented and it's just, we just get the pattern.",
        "tokens": [
            50654,
            23007,
            293,
            309,
            311,
            445,
            11,
            321,
            445,
            483,
            264,
            5102,
            13,
            50836
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2527278661727905,
        "compression_ratio": 1.519713282585144,
        "no_speech_prob": 0.008445548824965954
    },
    {
        "id": 90,
        "seek": 35220,
        "start": 361.6400146484375,
        "end": 364.79998779296875,
        "text": " So yeah, it is what it is.",
        "tokens": [
            50836,
            407,
            1338,
            11,
            309,
            307,
            437,
            309,
            307,
            13,
            50994
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2527278661727905,
        "compression_ratio": 1.519713282585144,
        "no_speech_prob": 0.008445548824965954
    },
    {
        "id": 91,
        "seek": 35220,
        "start": 364.79998779296875,
        "end": 367.6600036621094,
        "text": " Those are some of the changes that GPT-4 has made.",
        "tokens": [
            50994,
            3950,
            366,
            512,
            295,
            264,
            2962,
            300,
            26039,
            51,
            12,
            19,
            575,
            1027,
            13,
            51137
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2527278661727905,
        "compression_ratio": 1.519713282585144,
        "no_speech_prob": 0.008445548824965954
    },
    {
        "id": 92,
        "seek": 35220,
        "start": 367.6600036621094,
        "end": 372.67999267578125,
        "text": " And of course, the vocabulary size went from roughly 50k to roughly 100k.",
        "tokens": [
            51137,
            400,
            295,
            1164,
            11,
            264,
            19864,
            2744,
            1437,
            490,
            9810,
            2625,
            74,
            281,
            9810,
            2319,
            74,
            13,
            51388
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2527278661727905,
        "compression_ratio": 1.519713282585144,
        "no_speech_prob": 0.008445548824965954
    },
    {
        "id": 93,
        "seek": 35220,
        "start": 372.67999267578125,
        "end": 377.1199951171875,
        "text": " The next thing I would like to do very briefly is to take you through the GPT-2 encoder.py",
        "tokens": [
            51388,
            440,
            958,
            551,
            286,
            576,
            411,
            281,
            360,
            588,
            10515,
            307,
            281,
            747,
            291,
            807,
            264,
            26039,
            51,
            12,
            17,
            2058,
            19866,
            13,
            8200,
            51610
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2527278661727905,
        "compression_ratio": 1.519713282585144,
        "no_speech_prob": 0.008445548824965954
    },
    {
        "id": 94,
        "seek": 35220,
        "start": 377.1199951171875,
        "end": 379.79998779296875,
        "text": " that OpenAI has released.",
        "tokens": [
            51610,
            300,
            7238,
            48698,
            575,
            4736,
            13,
            51744
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2527278661727905,
        "compression_ratio": 1.519713282585144,
        "no_speech_prob": 0.008445548824965954
    },
    {
        "id": 95,
        "seek": 37980,
        "start": 379.79998779296875,
        "end": 382.5199890136719,
        "text": " This is the file that I already mentioned to you briefly.",
        "tokens": [
            50364,
            639,
            307,
            264,
            3991,
            300,
            286,
            1217,
            2835,
            281,
            291,
            10515,
            13,
            50500
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24691225588321686,
        "compression_ratio": 1.6692308187484741,
        "no_speech_prob": 0.03846389427781105
    },
    {
        "id": 96,
        "seek": 37980,
        "start": 382.5199890136719,
        "end": 390.55999755859375,
        "text": " Now this file is fairly short and should be relatively understandable to you at this point.",
        "tokens": [
            50500,
            823,
            341,
            3991,
            307,
            6457,
            2099,
            293,
            820,
            312,
            7226,
            25648,
            281,
            291,
            412,
            341,
            935,
            13,
            50902
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24691225588321686,
        "compression_ratio": 1.6692308187484741,
        "no_speech_prob": 0.03846389427781105
    },
    {
        "id": 97,
        "seek": 37980,
        "start": 390.55999755859375,
        "end": 396.9599914550781,
        "text": " Starting at the bottom here, they are loading two files, encoder.json and vocab.bpe.",
        "tokens": [
            50902,
            16217,
            412,
            264,
            2767,
            510,
            11,
            436,
            366,
            15114,
            732,
            7098,
            11,
            2058,
            19866,
            13,
            73,
            3015,
            293,
            2329,
            455,
            13,
            65,
            494,
            13,
            51222
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24691225588321686,
        "compression_ratio": 1.6692308187484741,
        "no_speech_prob": 0.03846389427781105
    },
    {
        "id": 98,
        "seek": 37980,
        "start": 396.9599914550781,
        "end": 398.32000732421875,
        "text": " And they do some light processing on it.",
        "tokens": [
            51222,
            400,
            436,
            360,
            512,
            1442,
            9007,
            322,
            309,
            13,
            51290
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24691225588321686,
        "compression_ratio": 1.6692308187484741,
        "no_speech_prob": 0.03846389427781105
    },
    {
        "id": 99,
        "seek": 37980,
        "start": 398.32000732421875,
        "end": 401.8800048828125,
        "text": " And then they call this encoder object, which is the tokenizer.",
        "tokens": [
            51290,
            400,
            550,
            436,
            818,
            341,
            2058,
            19866,
            2657,
            11,
            597,
            307,
            264,
            14862,
            6545,
            13,
            51468
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24691225588321686,
        "compression_ratio": 1.6692308187484741,
        "no_speech_prob": 0.03846389427781105
    },
    {
        "id": 100,
        "seek": 37980,
        "start": 401.8800048828125,
        "end": 407.7200012207031,
        "text": " Now if you'd like to inspect these two files, which together constitute their saved tokenizer,",
        "tokens": [
            51468,
            823,
            498,
            291,
            1116,
            411,
            281,
            15018,
            613,
            732,
            7098,
            11,
            597,
            1214,
            41658,
            641,
            6624,
            14862,
            6545,
            11,
            51760
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24691225588321686,
        "compression_ratio": 1.6692308187484741,
        "no_speech_prob": 0.03846389427781105
    },
    {
        "id": 101,
        "seek": 40772,
        "start": 407.7200012207031,
        "end": 411.67999267578125,
        "text": " then you can do that with a piece of code like this.",
        "tokens": [
            50364,
            550,
            291,
            393,
            360,
            300,
            365,
            257,
            2522,
            295,
            3089,
            411,
            341,
            13,
            50562
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2176230251789093,
        "compression_ratio": 1.6799999475479126,
        "no_speech_prob": 0.004755145404487848
    },
    {
        "id": 102,
        "seek": 40772,
        "start": 411.67999267578125,
        "end": 415.1199951171875,
        "text": " This is where you can download these two files and you can inspect them if you'd like.",
        "tokens": [
            50562,
            639,
            307,
            689,
            291,
            393,
            5484,
            613,
            732,
            7098,
            293,
            291,
            393,
            15018,
            552,
            498,
            291,
            1116,
            411,
            13,
            50734
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2176230251789093,
        "compression_ratio": 1.6799999475479126,
        "no_speech_prob": 0.004755145404487848
    },
    {
        "id": 103,
        "seek": 40772,
        "start": 415.1199951171875,
        "end": 419.6000061035156,
        "text": " And what you will find is that this encoder, as they call it in their code, is exactly",
        "tokens": [
            50734,
            400,
            437,
            291,
            486,
            915,
            307,
            300,
            341,
            2058,
            19866,
            11,
            382,
            436,
            818,
            309,
            294,
            641,
            3089,
            11,
            307,
            2293,
            50958
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2176230251789093,
        "compression_ratio": 1.6799999475479126,
        "no_speech_prob": 0.004755145404487848
    },
    {
        "id": 104,
        "seek": 40772,
        "start": 419.6000061035156,
        "end": 422.0,
        "text": " equivalent to our vocab.",
        "tokens": [
            50958,
            10344,
            281,
            527,
            2329,
            455,
            13,
            51078
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2176230251789093,
        "compression_ratio": 1.6799999475479126,
        "no_speech_prob": 0.004755145404487848
    },
    {
        "id": 105,
        "seek": 40772,
        "start": 422.0,
        "end": 428.3999938964844,
        "text": " So remember here, where we have this vocab object, which allowed us to decode very efficiently.",
        "tokens": [
            51078,
            407,
            1604,
            510,
            11,
            689,
            321,
            362,
            341,
            2329,
            455,
            2657,
            11,
            597,
            4350,
            505,
            281,
            979,
            1429,
            588,
            19621,
            13,
            51398
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2176230251789093,
        "compression_ratio": 1.6799999475479126,
        "no_speech_prob": 0.004755145404487848
    },
    {
        "id": 106,
        "seek": 40772,
        "start": 428.3999938964844,
        "end": 434.8800048828125,
        "text": " And basically it took us from the integer to the bytes for that integer.",
        "tokens": [
            51398,
            400,
            1936,
            309,
            1890,
            505,
            490,
            264,
            24922,
            281,
            264,
            36088,
            337,
            300,
            24922,
            13,
            51722
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2176230251789093,
        "compression_ratio": 1.6799999475479126,
        "no_speech_prob": 0.004755145404487848
    },
    {
        "id": 107,
        "seek": 43488,
        "start": 434.8800048828125,
        "end": 438.32000732421875,
        "text": " So our vocab is exactly their encoder.",
        "tokens": [
            50364,
            407,
            527,
            2329,
            455,
            307,
            2293,
            641,
            2058,
            19866,
            13,
            50536
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19962401688098907,
        "compression_ratio": 1.7487179040908813,
        "no_speech_prob": 0.011157629080116749
    },
    {
        "id": 108,
        "seek": 43488,
        "start": 438.32000732421875,
        "end": 444.2799987792969,
        "text": " And then their vocab.bpe, confusingly, is actually our merges.",
        "tokens": [
            50536,
            400,
            550,
            641,
            2329,
            455,
            13,
            65,
            494,
            11,
            13181,
            356,
            11,
            307,
            767,
            527,
            3551,
            2880,
            13,
            50834
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19962401688098907,
        "compression_ratio": 1.7487179040908813,
        "no_speech_prob": 0.011157629080116749
    },
    {
        "id": 109,
        "seek": 43488,
        "start": 444.2799987792969,
        "end": 450.239990234375,
        "text": " So their bpe.merges, which is based on the data inside vocab.bpe, ends up being equivalent",
        "tokens": [
            50834,
            407,
            641,
            272,
            494,
            13,
            936,
            2880,
            11,
            597,
            307,
            2361,
            322,
            264,
            1412,
            1854,
            2329,
            455,
            13,
            65,
            494,
            11,
            5314,
            493,
            885,
            10344,
            51132
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19962401688098907,
        "compression_ratio": 1.7487179040908813,
        "no_speech_prob": 0.011157629080116749
    },
    {
        "id": 110,
        "seek": 43488,
        "start": 450.239990234375,
        "end": 452.20001220703125,
        "text": " to our merges.",
        "tokens": [
            51132,
            281,
            527,
            3551,
            2880,
            13,
            51230
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19962401688098907,
        "compression_ratio": 1.7487179040908813,
        "no_speech_prob": 0.011157629080116749
    },
    {
        "id": 111,
        "seek": 43488,
        "start": 452.20001220703125,
        "end": 459.55999755859375,
        "text": " So basically they are saving and loading the two variables that for us are also critical,",
        "tokens": [
            51230,
            407,
            1936,
            436,
            366,
            6816,
            293,
            15114,
            264,
            732,
            9102,
            300,
            337,
            505,
            366,
            611,
            4924,
            11,
            51598
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19962401688098907,
        "compression_ratio": 1.7487179040908813,
        "no_speech_prob": 0.011157629080116749
    },
    {
        "id": 112,
        "seek": 43488,
        "start": 459.55999755859375,
        "end": 462.3999938964844,
        "text": " the merges variable and the vocab variable.",
        "tokens": [
            51598,
            264,
            3551,
            2880,
            7006,
            293,
            264,
            2329,
            455,
            7006,
            13,
            51740
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19962401688098907,
        "compression_ratio": 1.7487179040908813,
        "no_speech_prob": 0.011157629080116749
    },
    {
        "id": 113,
        "seek": 46240,
        "start": 462.3999938964844,
        "end": 466.8399963378906,
        "text": " Using just these two variables, you can represent a tokenizer and you can both do encoding and",
        "tokens": [
            50364,
            11142,
            445,
            613,
            732,
            9102,
            11,
            291,
            393,
            2906,
            257,
            14862,
            6545,
            293,
            291,
            393,
            1293,
            360,
            43430,
            293,
            50586
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2565937638282776,
        "compression_ratio": 1.727642297744751,
        "no_speech_prob": 0.026354709640145302
    },
    {
        "id": 114,
        "seek": 46240,
        "start": 466.8399963378906,
        "end": 470.32000732421875,
        "text": " decoding once you've trained this tokenizer.",
        "tokens": [
            50586,
            979,
            8616,
            1564,
            291,
            600,
            8895,
            341,
            14862,
            6545,
            13,
            50760
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2565937638282776,
        "compression_ratio": 1.727642297744751,
        "no_speech_prob": 0.026354709640145302
    },
    {
        "id": 115,
        "seek": 46240,
        "start": 470.32000732421875,
        "end": 477.1199951171875,
        "text": " Now the only thing that is actually slightly confusing inside what OpenAI does here is",
        "tokens": [
            50760,
            823,
            264,
            787,
            551,
            300,
            307,
            767,
            4748,
            13181,
            1854,
            437,
            7238,
            48698,
            775,
            510,
            307,
            51100
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2565937638282776,
        "compression_ratio": 1.727642297744751,
        "no_speech_prob": 0.026354709640145302
    },
    {
        "id": 116,
        "seek": 46240,
        "start": 477.1199951171875,
        "end": 481.55999755859375,
        "text": " that in addition to this encoder and the decoder, they also have something called a byte encoder",
        "tokens": [
            51100,
            300,
            294,
            4500,
            281,
            341,
            2058,
            19866,
            293,
            264,
            979,
            19866,
            11,
            436,
            611,
            362,
            746,
            1219,
            257,
            40846,
            2058,
            19866,
            51322
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2565937638282776,
        "compression_ratio": 1.727642297744751,
        "no_speech_prob": 0.026354709640145302
    },
    {
        "id": 117,
        "seek": 46240,
        "start": 481.55999755859375,
        "end": 483.760009765625,
        "text": " and a byte decoder.",
        "tokens": [
            51322,
            293,
            257,
            40846,
            979,
            19866,
            13,
            51432
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2565937638282776,
        "compression_ratio": 1.727642297744751,
        "no_speech_prob": 0.026354709640145302
    },
    {
        "id": 118,
        "seek": 46240,
        "start": 483.760009765625,
        "end": 489.0400085449219,
        "text": " And this is actually unfortunately just kind of a spurious implementation detail.",
        "tokens": [
            51432,
            400,
            341,
            307,
            767,
            7015,
            445,
            733,
            295,
            257,
            637,
            24274,
            11420,
            2607,
            13,
            51696
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2565937638282776,
        "compression_ratio": 1.727642297744751,
        "no_speech_prob": 0.026354709640145302
    },
    {
        "id": 119,
        "seek": 48904,
        "start": 489.0400085449219,
        "end": 492.55999755859375,
        "text": " It isn't actually deep or interesting in any way, so I'm going to skip the discussion",
        "tokens": [
            50364,
            467,
            1943,
            380,
            767,
            2452,
            420,
            1880,
            294,
            604,
            636,
            11,
            370,
            286,
            478,
            516,
            281,
            10023,
            264,
            5017,
            50540
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2124355286359787,
        "compression_ratio": 1.7447552680969238,
        "no_speech_prob": 0.17778049409389496
    },
    {
        "id": 120,
        "seek": 48904,
        "start": 492.55999755859375,
        "end": 493.55999755859375,
        "text": " of it.",
        "tokens": [
            50540,
            295,
            309,
            13,
            50590
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2124355286359787,
        "compression_ratio": 1.7447552680969238,
        "no_speech_prob": 0.17778049409389496
    },
    {
        "id": 121,
        "seek": 48904,
        "start": 493.55999755859375,
        "end": 497.4800109863281,
        "text": " But what OpenAI does here, for reasons that I don't fully understand, is that not only",
        "tokens": [
            50590,
            583,
            437,
            7238,
            48698,
            775,
            510,
            11,
            337,
            4112,
            300,
            286,
            500,
            380,
            4498,
            1223,
            11,
            307,
            300,
            406,
            787,
            50786
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2124355286359787,
        "compression_ratio": 1.7447552680969238,
        "no_speech_prob": 0.17778049409389496
    },
    {
        "id": 122,
        "seek": 48904,
        "start": 497.4800109863281,
        "end": 501.2799987792969,
        "text": " have they this tokenizer, which can encode and decode, but they have a whole separate",
        "tokens": [
            50786,
            362,
            436,
            341,
            14862,
            6545,
            11,
            597,
            393,
            2058,
            1429,
            293,
            979,
            1429,
            11,
            457,
            436,
            362,
            257,
            1379,
            4994,
            50976
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2124355286359787,
        "compression_ratio": 1.7447552680969238,
        "no_speech_prob": 0.17778049409389496
    },
    {
        "id": 123,
        "seek": 48904,
        "start": 501.2799987792969,
        "end": 505.2799987792969,
        "text": " layer here in addition that is used serially with the tokenizer.",
        "tokens": [
            50976,
            4583,
            510,
            294,
            4500,
            300,
            307,
            1143,
            816,
            2270,
            365,
            264,
            14862,
            6545,
            13,
            51176
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2124355286359787,
        "compression_ratio": 1.7447552680969238,
        "no_speech_prob": 0.17778049409389496
    },
    {
        "id": 124,
        "seek": 48904,
        "start": 505.2799987792969,
        "end": 511.8800048828125,
        "text": " And so you first do byte encode and then encode, and then you do decode and then byte decode.",
        "tokens": [
            51176,
            400,
            370,
            291,
            700,
            360,
            40846,
            2058,
            1429,
            293,
            550,
            2058,
            1429,
            11,
            293,
            550,
            291,
            360,
            979,
            1429,
            293,
            550,
            40846,
            979,
            1429,
            13,
            51506
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2124355286359787,
        "compression_ratio": 1.7447552680969238,
        "no_speech_prob": 0.17778049409389496
    },
    {
        "id": 125,
        "seek": 48904,
        "start": 511.8800048828125,
        "end": 516.3200073242188,
        "text": " So that's the loop, and they are just stacked serial on top of each other.",
        "tokens": [
            51506,
            407,
            300,
            311,
            264,
            6367,
            11,
            293,
            436,
            366,
            445,
            28867,
            17436,
            322,
            1192,
            295,
            1184,
            661,
            13,
            51728
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2124355286359787,
        "compression_ratio": 1.7447552680969238,
        "no_speech_prob": 0.17778049409389496
    },
    {
        "id": 126,
        "seek": 51632,
        "start": 516.3200073242188,
        "end": 519.4000244140625,
        "text": " And it's not that interesting, so I won't cover it, and you can step through it if you'd",
        "tokens": [
            50364,
            400,
            309,
            311,
            406,
            300,
            1880,
            11,
            370,
            286,
            1582,
            380,
            2060,
            309,
            11,
            293,
            291,
            393,
            1823,
            807,
            309,
            498,
            291,
            1116,
            50518
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2318618893623352,
        "compression_ratio": 1.7409836053848267,
        "no_speech_prob": 0.08755990117788315
    },
    {
        "id": 127,
        "seek": 51632,
        "start": 519.4000244140625,
        "end": 520.4000244140625,
        "text": " like.",
        "tokens": [
            50518,
            411,
            13,
            50568
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2318618893623352,
        "compression_ratio": 1.7409836053848267,
        "no_speech_prob": 0.08755990117788315
    },
    {
        "id": 128,
        "seek": 51632,
        "start": 520.4000244140625,
        "end": 524.760009765625,
        "text": " Otherwise, this file, if you ignore the byte encoder and the byte decoder, will be algorithmically",
        "tokens": [
            50568,
            10328,
            11,
            341,
            3991,
            11,
            498,
            291,
            11200,
            264,
            40846,
            2058,
            19866,
            293,
            264,
            40846,
            979,
            19866,
            11,
            486,
            312,
            9284,
            984,
            50786
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2318618893623352,
        "compression_ratio": 1.7409836053848267,
        "no_speech_prob": 0.08755990117788315
    },
    {
        "id": 129,
        "seek": 51632,
        "start": 524.760009765625,
        "end": 526.4199829101562,
        "text": " very familiar with you.",
        "tokens": [
            50786,
            588,
            4963,
            365,
            291,
            13,
            50869
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2318618893623352,
        "compression_ratio": 1.7409836053848267,
        "no_speech_prob": 0.08755990117788315
    },
    {
        "id": 130,
        "seek": 51632,
        "start": 526.4199829101562,
        "end": 530.3599853515625,
        "text": " And the meat of it here is what they call the BPE function.",
        "tokens": [
            50869,
            400,
            264,
            4615,
            295,
            309,
            510,
            307,
            437,
            436,
            818,
            264,
            40533,
            36,
            2445,
            13,
            51066
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2318618893623352,
        "compression_ratio": 1.7409836053848267,
        "no_speech_prob": 0.08755990117788315
    },
    {
        "id": 131,
        "seek": 51632,
        "start": 530.3599853515625,
        "end": 535.3200073242188,
        "text": " And you should recognize this loop here, which is very similar to our own while loop, where",
        "tokens": [
            51066,
            400,
            291,
            820,
            5521,
            341,
            6367,
            510,
            11,
            597,
            307,
            588,
            2531,
            281,
            527,
            1065,
            1339,
            6367,
            11,
            689,
            51314
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2318618893623352,
        "compression_ratio": 1.7409836053848267,
        "no_speech_prob": 0.08755990117788315
    },
    {
        "id": 132,
        "seek": 51632,
        "start": 535.3200073242188,
        "end": 541.280029296875,
        "text": " they're trying to identify the bigram, a pair, that they should be merging next.",
        "tokens": [
            51314,
            436,
            434,
            1382,
            281,
            5876,
            264,
            955,
            2356,
            11,
            257,
            6119,
            11,
            300,
            436,
            820,
            312,
            44559,
            958,
            13,
            51612
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2318618893623352,
        "compression_ratio": 1.7409836053848267,
        "no_speech_prob": 0.08755990117788315
    },
    {
        "id": 133,
        "seek": 51632,
        "start": 541.280029296875,
        "end": 545.5999755859375,
        "text": " And then here, just like we had, they have a for loop trying to merge this pair.",
        "tokens": [
            51612,
            400,
            550,
            510,
            11,
            445,
            411,
            321,
            632,
            11,
            436,
            362,
            257,
            337,
            6367,
            1382,
            281,
            22183,
            341,
            6119,
            13,
            51828
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2318618893623352,
        "compression_ratio": 1.7409836053848267,
        "no_speech_prob": 0.08755990117788315
    },
    {
        "id": 134,
        "seek": 54560,
        "start": 545.5999755859375,
        "end": 548.8800048828125,
        "text": " So they will go over all of the sequence, and they will merge the pair whenever they",
        "tokens": [
            50364,
            407,
            436,
            486,
            352,
            670,
            439,
            295,
            264,
            8310,
            11,
            293,
            436,
            486,
            22183,
            264,
            6119,
            5699,
            436,
            50528
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21058006584644318,
        "compression_ratio": 1.8444443941116333,
        "no_speech_prob": 0.00012533733388409019
    },
    {
        "id": 135,
        "seek": 54560,
        "start": 548.8800048828125,
        "end": 550.5,
        "text": " find it.",
        "tokens": [
            50528,
            915,
            309,
            13,
            50609
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21058006584644318,
        "compression_ratio": 1.8444443941116333,
        "no_speech_prob": 0.00012533733388409019
    },
    {
        "id": 136,
        "seek": 54560,
        "start": 550.5,
        "end": 555.47998046875,
        "text": " And they keep repeating that until they run out of possible merges in the text.",
        "tokens": [
            50609,
            400,
            436,
            1066,
            18617,
            300,
            1826,
            436,
            1190,
            484,
            295,
            1944,
            3551,
            2880,
            294,
            264,
            2487,
            13,
            50858
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21058006584644318,
        "compression_ratio": 1.8444443941116333,
        "no_speech_prob": 0.00012533733388409019
    },
    {
        "id": 137,
        "seek": 54560,
        "start": 555.47998046875,
        "end": 559.5599975585938,
        "text": " So that's the meat of this file, and there's an encode and decode function, just like we",
        "tokens": [
            50858,
            407,
            300,
            311,
            264,
            4615,
            295,
            341,
            3991,
            11,
            293,
            456,
            311,
            364,
            2058,
            1429,
            293,
            979,
            1429,
            2445,
            11,
            445,
            411,
            321,
            51062
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21058006584644318,
        "compression_ratio": 1.8444443941116333,
        "no_speech_prob": 0.00012533733388409019
    },
    {
        "id": 138,
        "seek": 54560,
        "start": 559.5599975585938,
        "end": 561.1199951171875,
        "text": " have implemented it.",
        "tokens": [
            51062,
            362,
            12270,
            309,
            13,
            51140
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21058006584644318,
        "compression_ratio": 1.8444443941116333,
        "no_speech_prob": 0.00012533733388409019
    },
    {
        "id": 139,
        "seek": 54560,
        "start": 561.1199951171875,
        "end": 564.7999877929688,
        "text": " So long story short, what I want you to take away at this point is that, unfortunately,",
        "tokens": [
            51140,
            407,
            938,
            1657,
            2099,
            11,
            437,
            286,
            528,
            291,
            281,
            747,
            1314,
            412,
            341,
            935,
            307,
            300,
            11,
            7015,
            11,
            51324
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21058006584644318,
        "compression_ratio": 1.8444443941116333,
        "no_speech_prob": 0.00012533733388409019
    },
    {
        "id": 140,
        "seek": 54560,
        "start": 564.7999877929688,
        "end": 568.1599731445312,
        "text": " it's a little bit of a messy code that they have, but algorithmically, it is identical",
        "tokens": [
            51324,
            309,
            311,
            257,
            707,
            857,
            295,
            257,
            16191,
            3089,
            300,
            436,
            362,
            11,
            457,
            9284,
            984,
            11,
            309,
            307,
            14800,
            51492
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21058006584644318,
        "compression_ratio": 1.8444443941116333,
        "no_speech_prob": 0.00012533733388409019
    },
    {
        "id": 141,
        "seek": 54560,
        "start": 568.1599731445312,
        "end": 570.5599975585938,
        "text": " to what we've built up above.",
        "tokens": [
            51492,
            281,
            437,
            321,
            600,
            3094,
            493,
            3673,
            13,
            51612
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21058006584644318,
        "compression_ratio": 1.8444443941116333,
        "no_speech_prob": 0.00012533733388409019
    },
    {
        "id": 142,
        "seek": 54560,
        "start": 570.5599975585938,
        "end": 574.5999755859375,
        "text": " And what we've built up above, if you understand it, is algorithmically what is necessary to",
        "tokens": [
            51612,
            400,
            437,
            321,
            600,
            3094,
            493,
            3673,
            11,
            498,
            291,
            1223,
            309,
            11,
            307,
            9284,
            984,
            437,
            307,
            4818,
            281,
            51814
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21058006584644318,
        "compression_ratio": 1.8444443941116333,
        "no_speech_prob": 0.00012533733388409019
    },
    {
        "id": 143,
        "seek": 57460,
        "start": 574.5999755859375,
        "end": 580.0399780273438,
        "text": " actually build a BPE tokenizer, train it, and then both encode and decode.",
        "tokens": [
            50364,
            767,
            1322,
            257,
            363,
            5208,
            14862,
            6545,
            11,
            3847,
            309,
            11,
            293,
            550,
            1293,
            2058,
            1429,
            293,
            979,
            1429,
            13,
            50636
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18876008689403534,
        "compression_ratio": 1.638596534729004,
        "no_speech_prob": 0.0026316484436392784
    },
    {
        "id": 144,
        "seek": 57460,
        "start": 580.0399780273438,
        "end": 583.5599975585938,
        "text": " The next topic I would like to turn to is that of special tokens.",
        "tokens": [
            50636,
            440,
            958,
            4829,
            286,
            576,
            411,
            281,
            1261,
            281,
            307,
            300,
            295,
            2121,
            22667,
            13,
            50812
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18876008689403534,
        "compression_ratio": 1.638596534729004,
        "no_speech_prob": 0.0026316484436392784
    },
    {
        "id": 145,
        "seek": 57460,
        "start": 583.5599975585938,
        "end": 588.6599731445312,
        "text": " So in addition to tokens that are coming from raw bytes and the BPE merges, we can insert",
        "tokens": [
            50812,
            407,
            294,
            4500,
            281,
            22667,
            300,
            366,
            1348,
            490,
            8936,
            36088,
            293,
            264,
            363,
            5208,
            3551,
            2880,
            11,
            321,
            393,
            8969,
            51067
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18876008689403534,
        "compression_ratio": 1.638596534729004,
        "no_speech_prob": 0.0026316484436392784
    },
    {
        "id": 146,
        "seek": 57460,
        "start": 588.6599731445312,
        "end": 593.1199951171875,
        "text": " all kinds of tokens that we are going to use to delimit different parts of the data or",
        "tokens": [
            51067,
            439,
            3685,
            295,
            22667,
            300,
            321,
            366,
            516,
            281,
            764,
            281,
            1103,
            332,
            270,
            819,
            3166,
            295,
            264,
            1412,
            420,
            51290
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18876008689403534,
        "compression_ratio": 1.638596534729004,
        "no_speech_prob": 0.0026316484436392784
    },
    {
        "id": 147,
        "seek": 57460,
        "start": 593.1199951171875,
        "end": 597.9600219726562,
        "text": " introduce to create a special structure of the token streams.",
        "tokens": [
            51290,
            5366,
            281,
            1884,
            257,
            2121,
            3877,
            295,
            264,
            14862,
            15842,
            13,
            51532
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18876008689403534,
        "compression_ratio": 1.638596534729004,
        "no_speech_prob": 0.0026316484436392784
    },
    {
        "id": 148,
        "seek": 57460,
        "start": 597.9600219726562,
        "end": 604.3599853515625,
        "text": " So if you look at this encoder object from OpenAI's GPT-2 right here, we mentioned this",
        "tokens": [
            51532,
            407,
            498,
            291,
            574,
            412,
            341,
            2058,
            19866,
            2657,
            490,
            7238,
            48698,
            311,
            26039,
            51,
            12,
            17,
            558,
            510,
            11,
            321,
            2835,
            341,
            51852
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18876008689403534,
        "compression_ratio": 1.638596534729004,
        "no_speech_prob": 0.0026316484436392784
    },
    {
        "id": 149,
        "seek": 60436,
        "start": 604.3599853515625,
        "end": 606.47998046875,
        "text": " is very similar to our vocab.",
        "tokens": [
            50364,
            307,
            588,
            2531,
            281,
            527,
            2329,
            455,
            13,
            50470
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2619650959968567,
        "compression_ratio": 1.6681416034698486,
        "no_speech_prob": 0.0022169819567352533
    },
    {
        "id": 150,
        "seek": 60436,
        "start": 606.47998046875,
        "end": 613.239990234375,
        "text": " You'll notice that the length of this is 50,257.",
        "tokens": [
            50470,
            509,
            603,
            3449,
            300,
            264,
            4641,
            295,
            341,
            307,
            2625,
            11,
            6074,
            22,
            13,
            50808
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2619650959968567,
        "compression_ratio": 1.6681416034698486,
        "no_speech_prob": 0.0022169819567352533
    },
    {
        "id": 151,
        "seek": 60436,
        "start": 613.239990234375,
        "end": 617.4400024414062,
        "text": " As I mentioned, it's mapping, and it's inverted from the mapping of our vocab.",
        "tokens": [
            50808,
            1018,
            286,
            2835,
            11,
            309,
            311,
            18350,
            11,
            293,
            309,
            311,
            38969,
            490,
            264,
            18350,
            295,
            527,
            2329,
            455,
            13,
            51018
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2619650959968567,
        "compression_ratio": 1.6681416034698486,
        "no_speech_prob": 0.0022169819567352533
    },
    {
        "id": 152,
        "seek": 60436,
        "start": 617.4400024414062,
        "end": 622.9600219726562,
        "text": " Our vocab goes from integer to string, and they go the other way around for no amazing",
        "tokens": [
            51018,
            2621,
            2329,
            455,
            1709,
            490,
            24922,
            281,
            6798,
            11,
            293,
            436,
            352,
            264,
            661,
            636,
            926,
            337,
            572,
            2243,
            51294
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2619650959968567,
        "compression_ratio": 1.6681416034698486,
        "no_speech_prob": 0.0022169819567352533
    },
    {
        "id": 153,
        "seek": 60436,
        "start": 622.9600219726562,
        "end": 623.9600219726562,
        "text": " reason.",
        "tokens": [
            51294,
            1778,
            13,
            51344
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2619650959968567,
        "compression_ratio": 1.6681416034698486,
        "no_speech_prob": 0.0022169819567352533
    },
    {
        "id": 154,
        "seek": 60436,
        "start": 623.9600219726562,
        "end": 629.3200073242188,
        "text": " But the thing to note here is that the mapping table here is 50,257.",
        "tokens": [
            51344,
            583,
            264,
            551,
            281,
            3637,
            510,
            307,
            300,
            264,
            18350,
            3199,
            510,
            307,
            2625,
            11,
            6074,
            22,
            13,
            51612
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2619650959968567,
        "compression_ratio": 1.6681416034698486,
        "no_speech_prob": 0.0022169819567352533
    },
    {
        "id": 155,
        "seek": 60436,
        "start": 629.3200073242188,
        "end": 632.9199829101562,
        "text": " Where does that number come from?",
        "tokens": [
            51612,
            2305,
            775,
            300,
            1230,
            808,
            490,
            30,
            51792
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2619650959968567,
        "compression_ratio": 1.6681416034698486,
        "no_speech_prob": 0.0022169819567352533
    },
    {
        "id": 156,
        "seek": 60436,
        "start": 632.9199829101562,
        "end": 633.9199829101562,
        "text": " Where are the tokens?",
        "tokens": [
            51792,
            2305,
            366,
            264,
            22667,
            30,
            51842
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2619650959968567,
        "compression_ratio": 1.6681416034698486,
        "no_speech_prob": 0.0022169819567352533
    },
    {
        "id": 157,
        "seek": 63392,
        "start": 634.47998046875,
        "end": 642.9199829101562,
        "text": " As I mentioned, there are 256 raw byte tokens, and then OpenAI actually did 50,000 merges.",
        "tokens": [
            50392,
            1018,
            286,
            2835,
            11,
            456,
            366,
            38882,
            8936,
            40846,
            22667,
            11,
            293,
            550,
            7238,
            48698,
            767,
            630,
            2625,
            11,
            1360,
            3551,
            2880,
            13,
            50814
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23589198291301727,
        "compression_ratio": 1.6186046600341797,
        "no_speech_prob": 0.08509209752082825
    },
    {
        "id": 158,
        "seek": 63392,
        "start": 642.9199829101562,
        "end": 648.280029296875,
        "text": " So those become the other tokens, but this would have been 50,256.",
        "tokens": [
            50814,
            407,
            729,
            1813,
            264,
            661,
            22667,
            11,
            457,
            341,
            576,
            362,
            668,
            2625,
            11,
            6074,
            21,
            13,
            51082
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23589198291301727,
        "compression_ratio": 1.6186046600341797,
        "no_speech_prob": 0.08509209752082825
    },
    {
        "id": 159,
        "seek": 63392,
        "start": 648.280029296875,
        "end": 650.8400268554688,
        "text": " So what is the 57th token?",
        "tokens": [
            51082,
            407,
            437,
            307,
            264,
            21423,
            392,
            14862,
            30,
            51210
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23589198291301727,
        "compression_ratio": 1.6186046600341797,
        "no_speech_prob": 0.08509209752082825
    },
    {
        "id": 160,
        "seek": 63392,
        "start": 650.8400268554688,
        "end": 654.8400268554688,
        "text": " And there is basically one special token.",
        "tokens": [
            51210,
            400,
            456,
            307,
            1936,
            472,
            2121,
            14862,
            13,
            51410
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23589198291301727,
        "compression_ratio": 1.6186046600341797,
        "no_speech_prob": 0.08509209752082825
    },
    {
        "id": 161,
        "seek": 63392,
        "start": 654.8400268554688,
        "end": 659.7999877929688,
        "text": " And that one special token, you can see, is called end-of-text.",
        "tokens": [
            51410,
            400,
            300,
            472,
            2121,
            14862,
            11,
            291,
            393,
            536,
            11,
            307,
            1219,
            917,
            12,
            2670,
            12,
            25111,
            13,
            51658
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23589198291301727,
        "compression_ratio": 1.6186046600341797,
        "no_speech_prob": 0.08509209752082825
    },
    {
        "id": 162,
        "seek": 63392,
        "start": 659.7999877929688,
        "end": 663.8800048828125,
        "text": " So this is a special token, and it's the very last token.",
        "tokens": [
            51658,
            407,
            341,
            307,
            257,
            2121,
            14862,
            11,
            293,
            309,
            311,
            264,
            588,
            1036,
            14862,
            13,
            51862
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23589198291301727,
        "compression_ratio": 1.6186046600341797,
        "no_speech_prob": 0.08509209752082825
    },
    {
        "id": 163,
        "seek": 66388,
        "start": 663.8800048828125,
        "end": 668.3200073242188,
        "text": " And this token is used to delimit documents in the training set.",
        "tokens": [
            50364,
            400,
            341,
            14862,
            307,
            1143,
            281,
            1103,
            332,
            270,
            8512,
            294,
            264,
            3097,
            992,
            13,
            50586
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20339439809322357,
        "compression_ratio": 1.7782257795333862,
        "no_speech_prob": 0.0004728526691906154
    },
    {
        "id": 164,
        "seek": 66388,
        "start": 668.3200073242188,
        "end": 672.239990234375,
        "text": " So when we're creating the training data, we have all these documents, and we tokenize",
        "tokens": [
            50586,
            407,
            562,
            321,
            434,
            4084,
            264,
            3097,
            1412,
            11,
            321,
            362,
            439,
            613,
            8512,
            11,
            293,
            321,
            14862,
            1125,
            50782
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20339439809322357,
        "compression_ratio": 1.7782257795333862,
        "no_speech_prob": 0.0004728526691906154
    },
    {
        "id": 165,
        "seek": 66388,
        "start": 672.239990234375,
        "end": 674.9199829101562,
        "text": " them and get a stream of tokens.",
        "tokens": [
            50782,
            552,
            293,
            483,
            257,
            4309,
            295,
            22667,
            13,
            50916
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20339439809322357,
        "compression_ratio": 1.7782257795333862,
        "no_speech_prob": 0.0004728526691906154
    },
    {
        "id": 166,
        "seek": 66388,
        "start": 674.9199829101562,
        "end": 679.5599975585938,
        "text": " Those tokens only range from zero to 50,256.",
        "tokens": [
            50916,
            3950,
            22667,
            787,
            3613,
            490,
            4018,
            281,
            2625,
            11,
            6074,
            21,
            13,
            51148
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20339439809322357,
        "compression_ratio": 1.7782257795333862,
        "no_speech_prob": 0.0004728526691906154
    },
    {
        "id": 167,
        "seek": 66388,
        "start": 679.5599975585938,
        "end": 685.3599853515625,
        "text": " And then in between those documents, we put special end-of-text token, and we insert that",
        "tokens": [
            51148,
            400,
            550,
            294,
            1296,
            729,
            8512,
            11,
            321,
            829,
            2121,
            917,
            12,
            2670,
            12,
            25111,
            14862,
            11,
            293,
            321,
            8969,
            300,
            51438
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20339439809322357,
        "compression_ratio": 1.7782257795333862,
        "no_speech_prob": 0.0004728526691906154
    },
    {
        "id": 168,
        "seek": 66388,
        "start": 685.3599853515625,
        "end": 687.7999877929688,
        "text": " token in between documents.",
        "tokens": [
            51438,
            14862,
            294,
            1296,
            8512,
            13,
            51560
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20339439809322357,
        "compression_ratio": 1.7782257795333862,
        "no_speech_prob": 0.0004728526691906154
    },
    {
        "id": 169,
        "seek": 66388,
        "start": 687.7999877929688,
        "end": 693.2000122070312,
        "text": " And we are using this as a signal to the language model that the document has ended, and what",
        "tokens": [
            51560,
            400,
            321,
            366,
            1228,
            341,
            382,
            257,
            6358,
            281,
            264,
            2856,
            2316,
            300,
            264,
            4166,
            575,
            4590,
            11,
            293,
            437,
            51830
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20339439809322357,
        "compression_ratio": 1.7782257795333862,
        "no_speech_prob": 0.0004728526691906154
    },
    {
        "id": 170,
        "seek": 69320,
        "start": 693.2000122070312,
        "end": 697.5599975585938,
        "text": " follows is going to be unrelated to the document previously.",
        "tokens": [
            50364,
            10002,
            307,
            516,
            281,
            312,
            38967,
            281,
            264,
            4166,
            8046,
            13,
            50582
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27743321657180786,
        "compression_ratio": 1.8107142448425293,
        "no_speech_prob": 0.00013341943849809468
    },
    {
        "id": 171,
        "seek": 69320,
        "start": 697.5599975585938,
        "end": 700.1599731445312,
        "text": " That said, the language model has to learn this from data.",
        "tokens": [
            50582,
            663,
            848,
            11,
            264,
            2856,
            2316,
            575,
            281,
            1466,
            341,
            490,
            1412,
            13,
            50712
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27743321657180786,
        "compression_ratio": 1.8107142448425293,
        "no_speech_prob": 0.00013341943849809468
    },
    {
        "id": 172,
        "seek": 69320,
        "start": 700.1599731445312,
        "end": 705.3599853515625,
        "text": " It needs to learn that this token usually means that it should wipe its memory of what",
        "tokens": [
            50712,
            467,
            2203,
            281,
            1466,
            300,
            341,
            14862,
            2673,
            1355,
            300,
            309,
            820,
            14082,
            1080,
            4675,
            295,
            437,
            50972
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27743321657180786,
        "compression_ratio": 1.8107142448425293,
        "no_speech_prob": 0.00013341943849809468
    },
    {
        "id": 173,
        "seek": 69320,
        "start": 705.3599853515625,
        "end": 709.3200073242188,
        "text": " came before, and what came before this token is not actually informative to what comes",
        "tokens": [
            50972,
            1361,
            949,
            11,
            293,
            437,
            1361,
            949,
            341,
            14862,
            307,
            406,
            767,
            27759,
            281,
            437,
            1487,
            51170
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27743321657180786,
        "compression_ratio": 1.8107142448425293,
        "no_speech_prob": 0.00013341943849809468
    },
    {
        "id": 174,
        "seek": 69320,
        "start": 709.3200073242188,
        "end": 710.3200073242188,
        "text": " next.",
        "tokens": [
            51170,
            958,
            13,
            51220
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27743321657180786,
        "compression_ratio": 1.8107142448425293,
        "no_speech_prob": 0.00013341943849809468
    },
    {
        "id": 175,
        "seek": 69320,
        "start": 710.3200073242188,
        "end": 714.5599975585938,
        "text": " But we are expecting the language model to just learn this, but we're giving it the special",
        "tokens": [
            51220,
            583,
            321,
            366,
            9650,
            264,
            2856,
            2316,
            281,
            445,
            1466,
            341,
            11,
            457,
            321,
            434,
            2902,
            309,
            264,
            2121,
            51432
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27743321657180786,
        "compression_ratio": 1.8107142448425293,
        "no_speech_prob": 0.00013341943849809468
    },
    {
        "id": 176,
        "seek": 69320,
        "start": 714.5599975585938,
        "end": 717.3599853515625,
        "text": " delimiter of these documents.",
        "tokens": [
            51432,
            1103,
            332,
            1681,
            295,
            613,
            8512,
            13,
            51572
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27743321657180786,
        "compression_ratio": 1.8107142448425293,
        "no_speech_prob": 0.00013341943849809468
    },
    {
        "id": 177,
        "seek": 69320,
        "start": 717.3599853515625,
        "end": 722.760009765625,
        "text": " We can go here to TickTokenizer, and that's the GPT to Tokenizer, our code that we've",
        "tokens": [
            51572,
            492,
            393,
            352,
            510,
            281,
            314,
            299,
            74,
            51,
            8406,
            6545,
            11,
            293,
            300,
            311,
            264,
            26039,
            51,
            281,
            314,
            8406,
            6545,
            11,
            527,
            3089,
            300,
            321,
            600,
            51842
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27743321657180786,
        "compression_ratio": 1.8107142448425293,
        "no_speech_prob": 0.00013341943849809468
    },
    {
        "id": 178,
        "seek": 72276,
        "start": 722.760009765625,
        "end": 724.3200073242188,
        "text": " been playing with before.",
        "tokens": [
            50364,
            668,
            2433,
            365,
            949,
            13,
            50442
        ],
        "temperature": 0.0,
        "avg_logprob": -0.28199586272239685,
        "compression_ratio": 1.5700483322143555,
        "no_speech_prob": 0.010328024625778198
    },
    {
        "id": 179,
        "seek": 72276,
        "start": 724.3200073242188,
        "end": 729.1199951171875,
        "text": " So we can add here, right, hello world, how are you, and we're getting different tokens.",
        "tokens": [
            50442,
            407,
            321,
            393,
            909,
            510,
            11,
            558,
            11,
            7751,
            1002,
            11,
            577,
            366,
            291,
            11,
            293,
            321,
            434,
            1242,
            819,
            22667,
            13,
            50682
        ],
        "temperature": 0.0,
        "avg_logprob": -0.28199586272239685,
        "compression_ratio": 1.5700483322143555,
        "no_speech_prob": 0.010328024625778198
    },
    {
        "id": 180,
        "seek": 72276,
        "start": 729.1199951171875,
        "end": 733.760009765625,
        "text": " But now you can see what happens if I put end-of-text.",
        "tokens": [
            50682,
            583,
            586,
            291,
            393,
            536,
            437,
            2314,
            498,
            286,
            829,
            917,
            12,
            2670,
            12,
            25111,
            13,
            50914
        ],
        "temperature": 0.0,
        "avg_logprob": -0.28199586272239685,
        "compression_ratio": 1.5700483322143555,
        "no_speech_prob": 0.010328024625778198
    },
    {
        "id": 181,
        "seek": 72276,
        "start": 733.760009765625,
        "end": 740.9600219726562,
        "text": " You see how until I finished it, these are all different tokens, end-of-text, still set",
        "tokens": [
            50914,
            509,
            536,
            577,
            1826,
            286,
            4335,
            309,
            11,
            613,
            366,
            439,
            819,
            22667,
            11,
            917,
            12,
            2670,
            12,
            25111,
            11,
            920,
            992,
            51274
        ],
        "temperature": 0.0,
        "avg_logprob": -0.28199586272239685,
        "compression_ratio": 1.5700483322143555,
        "no_speech_prob": 0.010328024625778198
    },
    {
        "id": 182,
        "seek": 72276,
        "start": 740.9600219726562,
        "end": 747.5599975585938,
        "text": " for tokens, and now when I finish it, suddenly we get token 50,256.",
        "tokens": [
            51274,
            337,
            22667,
            11,
            293,
            586,
            562,
            286,
            2413,
            309,
            11,
            5800,
            321,
            483,
            14862,
            2625,
            11,
            6074,
            21,
            13,
            51604
        ],
        "temperature": 0.0,
        "avg_logprob": -0.28199586272239685,
        "compression_ratio": 1.5700483322143555,
        "no_speech_prob": 0.010328024625778198
    },
    {
        "id": 183,
        "seek": 74756,
        "start": 747.5599975585938,
        "end": 753.280029296875,
        "text": " And the reason this works is because this didn't actually go through the BPE merges.",
        "tokens": [
            50364,
            400,
            264,
            1778,
            341,
            1985,
            307,
            570,
            341,
            994,
            380,
            767,
            352,
            807,
            264,
            363,
            5208,
            3551,
            2880,
            13,
            50650
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22884993255138397,
        "compression_ratio": 1.700440526008606,
        "no_speech_prob": 0.026353871449828148
    },
    {
        "id": 184,
        "seek": 74756,
        "start": 753.280029296875,
        "end": 760.3200073242188,
        "text": " Instead, the code that actually outputs the tokens has special case instructions for handling",
        "tokens": [
            50650,
            7156,
            11,
            264,
            3089,
            300,
            767,
            23930,
            264,
            22667,
            575,
            2121,
            1389,
            9415,
            337,
            13175,
            51002
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22884993255138397,
        "compression_ratio": 1.700440526008606,
        "no_speech_prob": 0.026353871449828148
    },
    {
        "id": 185,
        "seek": 74756,
        "start": 760.3200073242188,
        "end": 762.6400146484375,
        "text": " special tokens.",
        "tokens": [
            51002,
            2121,
            22667,
            13,
            51118
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22884993255138397,
        "compression_ratio": 1.700440526008606,
        "no_speech_prob": 0.026353871449828148
    },
    {
        "id": 186,
        "seek": 74756,
        "start": 762.6400146484375,
        "end": 768.0800170898438,
        "text": " We did not see these special instructions for handling special tokens in the encoder.py.",
        "tokens": [
            51118,
            492,
            630,
            406,
            536,
            613,
            2121,
            9415,
            337,
            13175,
            2121,
            22667,
            294,
            264,
            2058,
            19866,
            13,
            8200,
            13,
            51390
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22884993255138397,
        "compression_ratio": 1.700440526008606,
        "no_speech_prob": 0.026353871449828148
    },
    {
        "id": 187,
        "seek": 74756,
        "start": 768.0800170898438,
        "end": 769.6400146484375,
        "text": " It's absent there.",
        "tokens": [
            51390,
            467,
            311,
            25185,
            456,
            13,
            51468
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22884993255138397,
        "compression_ratio": 1.700440526008606,
        "no_speech_prob": 0.026353871449828148
    },
    {
        "id": 188,
        "seek": 74756,
        "start": 769.6400146484375,
        "end": 774.47998046875,
        "text": " But if you go to TickToken library, which is implemented in Rust, you will find all",
        "tokens": [
            51468,
            583,
            498,
            291,
            352,
            281,
            314,
            618,
            51,
            8406,
            6405,
            11,
            597,
            307,
            12270,
            294,
            34952,
            11,
            291,
            486,
            915,
            439,
            51710
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22884993255138397,
        "compression_ratio": 1.700440526008606,
        "no_speech_prob": 0.026353871449828148
    },
    {
        "id": 189,
        "seek": 77448,
        "start": 774.47998046875,
        "end": 779.9199829101562,
        "text": " kinds of special case handling for these special tokens that you can register, create,",
        "tokens": [
            50364,
            3685,
            295,
            2121,
            1389,
            13175,
            337,
            613,
            2121,
            22667,
            300,
            291,
            393,
            7280,
            11,
            1884,
            11,
            50636
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2411099076271057,
        "compression_ratio": 1.8467153310775757,
        "no_speech_prob": 0.07263010740280151
    },
    {
        "id": 190,
        "seek": 77448,
        "start": 779.9199829101562,
        "end": 784.9199829101562,
        "text": " add to the vocabulary, and then it looks for them, and whenever it sees these special tokens",
        "tokens": [
            50636,
            909,
            281,
            264,
            19864,
            11,
            293,
            550,
            309,
            1542,
            337,
            552,
            11,
            293,
            5699,
            309,
            8194,
            613,
            2121,
            22667,
            50886
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2411099076271057,
        "compression_ratio": 1.8467153310775757,
        "no_speech_prob": 0.07263010740280151
    },
    {
        "id": 191,
        "seek": 77448,
        "start": 784.9199829101562,
        "end": 789.1599731445312,
        "text": " like this, it will actually come in and swap in that special token.",
        "tokens": [
            50886,
            411,
            341,
            11,
            309,
            486,
            767,
            808,
            294,
            293,
            18135,
            294,
            300,
            2121,
            14862,
            13,
            51098
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2411099076271057,
        "compression_ratio": 1.8467153310775757,
        "no_speech_prob": 0.07263010740280151
    },
    {
        "id": 192,
        "seek": 77448,
        "start": 789.1599731445312,
        "end": 794.7999877929688,
        "text": " So these things are outside of the typical algorithm of byte-bearing coding.",
        "tokens": [
            51098,
            407,
            613,
            721,
            366,
            2380,
            295,
            264,
            7476,
            9284,
            295,
            40846,
            12,
            650,
            1921,
            17720,
            13,
            51380
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2411099076271057,
        "compression_ratio": 1.8467153310775757,
        "no_speech_prob": 0.07263010740280151
    },
    {
        "id": 193,
        "seek": 77448,
        "start": 794.7999877929688,
        "end": 799.9199829101562,
        "text": " So these special tokens are used pervasively, not just in basically base language modeling",
        "tokens": [
            51380,
            407,
            613,
            2121,
            22667,
            366,
            1143,
            680,
            7967,
            3413,
            11,
            406,
            445,
            294,
            1936,
            3096,
            2856,
            15983,
            51636
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2411099076271057,
        "compression_ratio": 1.8467153310775757,
        "no_speech_prob": 0.07263010740280151
    },
    {
        "id": 194,
        "seek": 77448,
        "start": 799.9199829101562,
        "end": 803.8400268554688,
        "text": " of predicting the next token in the sequence, but especially when it gets to later, to the",
        "tokens": [
            51636,
            295,
            32884,
            264,
            958,
            14862,
            294,
            264,
            8310,
            11,
            457,
            2318,
            562,
            309,
            2170,
            281,
            1780,
            11,
            281,
            264,
            51832
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2411099076271057,
        "compression_ratio": 1.8467153310775757,
        "no_speech_prob": 0.07263010740280151
    },
    {
        "id": 195,
        "seek": 80384,
        "start": 804.2000122070312,
        "end": 808.760009765625,
        "text": " fine-tuning stage, and all of the chat GPT sort of aspects of it.",
        "tokens": [
            50382,
            2489,
            12,
            83,
            37726,
            3233,
            11,
            293,
            439,
            295,
            264,
            5081,
            26039,
            51,
            1333,
            295,
            7270,
            295,
            309,
            13,
            50610
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24287499487400055,
        "compression_ratio": 1.6468400955200195,
        "no_speech_prob": 0.0023967113811522722
    },
    {
        "id": 196,
        "seek": 80384,
        "start": 808.760009765625,
        "end": 812.3200073242188,
        "text": " Because we don't just want to delimit documents, we want to delimit entire conversations between",
        "tokens": [
            50610,
            1436,
            321,
            500,
            380,
            445,
            528,
            281,
            1103,
            332,
            270,
            8512,
            11,
            321,
            528,
            281,
            1103,
            332,
            270,
            2302,
            7315,
            1296,
            50788
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24287499487400055,
        "compression_ratio": 1.6468400955200195,
        "no_speech_prob": 0.0023967113811522722
    },
    {
        "id": 197,
        "seek": 80384,
        "start": 812.3200073242188,
        "end": 814.3800048828125,
        "text": " an assistant and a user.",
        "tokens": [
            50788,
            364,
            10994,
            293,
            257,
            4195,
            13,
            50891
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24287499487400055,
        "compression_ratio": 1.6468400955200195,
        "no_speech_prob": 0.0023967113811522722
    },
    {
        "id": 198,
        "seek": 80384,
        "start": 814.3800048828125,
        "end": 820.4400024414062,
        "text": " So if I refresh this TickTokenizer page, the default example that they have here is using",
        "tokens": [
            50891,
            407,
            498,
            286,
            15134,
            341,
            314,
            618,
            51,
            8406,
            6545,
            3028,
            11,
            264,
            7576,
            1365,
            300,
            436,
            362,
            510,
            307,
            1228,
            51194
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24287499487400055,
        "compression_ratio": 1.6468400955200195,
        "no_speech_prob": 0.0023967113811522722
    },
    {
        "id": 199,
        "seek": 80384,
        "start": 820.4400024414062,
        "end": 827.4000244140625,
        "text": " not sort of base model encoders, but fine-tuned model sort of tokenizers.",
        "tokens": [
            51194,
            406,
            1333,
            295,
            3096,
            2316,
            2058,
            378,
            433,
            11,
            457,
            2489,
            12,
            83,
            43703,
            2316,
            1333,
            295,
            14862,
            22525,
            13,
            51542
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24287499487400055,
        "compression_ratio": 1.6468400955200195,
        "no_speech_prob": 0.0023967113811522722
    },
    {
        "id": 200,
        "seek": 80384,
        "start": 827.4000244140625,
        "end": 833.6400146484375,
        "text": " So for example, using the GPT 3.5 Turbo scheme, these here are all special tokens, imstart,",
        "tokens": [
            51542,
            407,
            337,
            1365,
            11,
            1228,
            264,
            26039,
            51,
            805,
            13,
            20,
            35848,
            12232,
            11,
            613,
            510,
            366,
            439,
            2121,
            22667,
            11,
            566,
            24419,
            11,
            51854
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24287499487400055,
        "compression_ratio": 1.6468400955200195,
        "no_speech_prob": 0.0023967113811522722
    },
    {
        "id": 201,
        "seek": 83364,
        "start": 833.6400146484375,
        "end": 836.3200073242188,
        "text": " imend, et cetera.",
        "tokens": [
            50364,
            566,
            521,
            11,
            1030,
            11458,
            13,
            50498
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27116936445236206,
        "compression_ratio": 1.7099236249923706,
        "no_speech_prob": 1.8631732018548064e-05
    },
    {
        "id": 202,
        "seek": 83364,
        "start": 836.3200073242188,
        "end": 841.1599731445312,
        "text": " This is short for imaginary malloc underscore start, by the way.",
        "tokens": [
            50498,
            639,
            307,
            2099,
            337,
            26164,
            16026,
            905,
            37556,
            722,
            11,
            538,
            264,
            636,
            13,
            50740
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27116936445236206,
        "compression_ratio": 1.7099236249923706,
        "no_speech_prob": 1.8631732018548064e-05
    },
    {
        "id": 203,
        "seek": 83364,
        "start": 841.1599731445312,
        "end": 845.1199951171875,
        "text": " But you can see here that there's a sort of start and end of every single message, and",
        "tokens": [
            50740,
            583,
            291,
            393,
            536,
            510,
            300,
            456,
            311,
            257,
            1333,
            295,
            722,
            293,
            917,
            295,
            633,
            2167,
            3636,
            11,
            293,
            50938
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27116936445236206,
        "compression_ratio": 1.7099236249923706,
        "no_speech_prob": 1.8631732018548064e-05
    },
    {
        "id": 204,
        "seek": 83364,
        "start": 845.1199951171875,
        "end": 851.5599975585938,
        "text": " there can be many other tokens, lots of tokens, in use to delimit these conversations and",
        "tokens": [
            50938,
            456,
            393,
            312,
            867,
            661,
            22667,
            11,
            3195,
            295,
            22667,
            11,
            294,
            764,
            281,
            1103,
            332,
            270,
            613,
            7315,
            293,
            51260
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27116936445236206,
        "compression_ratio": 1.7099236249923706,
        "no_speech_prob": 1.8631732018548064e-05
    },
    {
        "id": 205,
        "seek": 83364,
        "start": 851.5599975585938,
        "end": 855.1199951171875,
        "text": " kind of keep track of the flow of the messages here.",
        "tokens": [
            51260,
            733,
            295,
            1066,
            2837,
            295,
            264,
            3095,
            295,
            264,
            7897,
            510,
            13,
            51438
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27116936445236206,
        "compression_ratio": 1.7099236249923706,
        "no_speech_prob": 1.8631732018548064e-05
    },
    {
        "id": 206,
        "seek": 83364,
        "start": 855.1199951171875,
        "end": 858.1199951171875,
        "text": " Now we can go back to the TickToken library.",
        "tokens": [
            51438,
            823,
            321,
            393,
            352,
            646,
            281,
            264,
            314,
            618,
            51,
            8406,
            6405,
            13,
            51588
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27116936445236206,
        "compression_ratio": 1.7099236249923706,
        "no_speech_prob": 1.8631732018548064e-05
    },
    {
        "id": 207,
        "seek": 83364,
        "start": 858.1199951171875,
        "end": 863.1199951171875,
        "text": " And here, when you scroll to the bottom, they talk about how you can extend TickToken, and",
        "tokens": [
            51588,
            400,
            510,
            11,
            562,
            291,
            11369,
            281,
            264,
            2767,
            11,
            436,
            751,
            466,
            577,
            291,
            393,
            10101,
            314,
            618,
            51,
            8406,
            11,
            293,
            51838
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27116936445236206,
        "compression_ratio": 1.7099236249923706,
        "no_speech_prob": 1.8631732018548064e-05
    },
    {
        "id": 208,
        "seek": 86312,
        "start": 863.1199951171875,
        "end": 870.3200073242188,
        "text": " you can create, basically, you can fork the CL 100K base tokenizers in GPT 4.",
        "tokens": [
            50364,
            291,
            393,
            1884,
            11,
            1936,
            11,
            291,
            393,
            17716,
            264,
            12855,
            2319,
            42,
            3096,
            14862,
            22525,
            294,
            26039,
            51,
            1017,
            13,
            50724
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24646663665771484,
        "compression_ratio": 1.571969747543335,
        "no_speech_prob": 0.00023413519375026226
    },
    {
        "id": 209,
        "seek": 86312,
        "start": 870.3200073242188,
        "end": 873.3599853515625,
        "text": " And for example, you can extend it by adding more special tokens, and these are totally",
        "tokens": [
            50724,
            400,
            337,
            1365,
            11,
            291,
            393,
            10101,
            309,
            538,
            5127,
            544,
            2121,
            22667,
            11,
            293,
            613,
            366,
            3879,
            50876
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24646663665771484,
        "compression_ratio": 1.571969747543335,
        "no_speech_prob": 0.00023413519375026226
    },
    {
        "id": 210,
        "seek": 86312,
        "start": 873.3599853515625,
        "end": 874.3599853515625,
        "text": " up to you.",
        "tokens": [
            50876,
            493,
            281,
            291,
            13,
            50926
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24646663665771484,
        "compression_ratio": 1.571969747543335,
        "no_speech_prob": 0.00023413519375026226
    },
    {
        "id": 211,
        "seek": 86312,
        "start": 874.3599853515625,
        "end": 878.719970703125,
        "text": " You can come up with any arbitrary tokens and add them with a new ID afterwards.",
        "tokens": [
            50926,
            509,
            393,
            808,
            493,
            365,
            604,
            23211,
            22667,
            293,
            909,
            552,
            365,
            257,
            777,
            7348,
            10543,
            13,
            51144
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24646663665771484,
        "compression_ratio": 1.571969747543335,
        "no_speech_prob": 0.00023413519375026226
    },
    {
        "id": 212,
        "seek": 86312,
        "start": 878.719970703125,
        "end": 885.9199829101562,
        "text": " And the TickToken library will correctly swap them out when it sees this in the strings.",
        "tokens": [
            51144,
            400,
            264,
            314,
            618,
            51,
            8406,
            6405,
            486,
            8944,
            18135,
            552,
            484,
            562,
            309,
            8194,
            341,
            294,
            264,
            13985,
            13,
            51504
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24646663665771484,
        "compression_ratio": 1.571969747543335,
        "no_speech_prob": 0.00023413519375026226
    },
    {
        "id": 213,
        "seek": 86312,
        "start": 885.9199829101562,
        "end": 890.4000244140625,
        "text": " Now we can also go back to this file, which we looked at previously.",
        "tokens": [
            51504,
            823,
            321,
            393,
            611,
            352,
            646,
            281,
            341,
            3991,
            11,
            597,
            321,
            2956,
            412,
            8046,
            13,
            51728
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24646663665771484,
        "compression_ratio": 1.571969747543335,
        "no_speech_prob": 0.00023413519375026226
    },
    {
        "id": 214,
        "seek": 89040,
        "start": 890.4000244140625,
        "end": 896.8400268554688,
        "text": " And I mentioned that the GPT 2 in TickToken OpenAI public.py, we have the vocabulary,",
        "tokens": [
            50364,
            400,
            286,
            2835,
            300,
            264,
            26039,
            51,
            568,
            294,
            314,
            618,
            51,
            8406,
            7238,
            48698,
            1908,
            13,
            8200,
            11,
            321,
            362,
            264,
            19864,
            11,
            50686
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24157144129276276,
        "compression_ratio": 1.6916667222976685,
        "no_speech_prob": 0.014956584200263023
    },
    {
        "id": 215,
        "seek": 89040,
        "start": 896.8400268554688,
        "end": 900.9199829101562,
        "text": " we have the pattern for splitting, and then here we are registering the single special",
        "tokens": [
            50686,
            321,
            362,
            264,
            5102,
            337,
            30348,
            11,
            293,
            550,
            510,
            321,
            366,
            47329,
            264,
            2167,
            2121,
            50890
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24157144129276276,
        "compression_ratio": 1.6916667222976685,
        "no_speech_prob": 0.014956584200263023
    },
    {
        "id": 216,
        "seek": 89040,
        "start": 900.9199829101562,
        "end": 906.3599853515625,
        "text": " token in GPT 2, which was the end of text token, and we saw that it has this ID.",
        "tokens": [
            50890,
            14862,
            294,
            26039,
            51,
            568,
            11,
            597,
            390,
            264,
            917,
            295,
            2487,
            14862,
            11,
            293,
            321,
            1866,
            300,
            309,
            575,
            341,
            7348,
            13,
            51162
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24157144129276276,
        "compression_ratio": 1.6916667222976685,
        "no_speech_prob": 0.014956584200263023
    },
    {
        "id": 217,
        "seek": 89040,
        "start": 906.3599853515625,
        "end": 911.8800048828125,
        "text": " In GPT 4, when they define this here, you see that the pattern has changed, as we discussed,",
        "tokens": [
            51162,
            682,
            26039,
            51,
            1017,
            11,
            562,
            436,
            6964,
            341,
            510,
            11,
            291,
            536,
            300,
            264,
            5102,
            575,
            3105,
            11,
            382,
            321,
            7152,
            11,
            51438
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24157144129276276,
        "compression_ratio": 1.6916667222976685,
        "no_speech_prob": 0.014956584200263023
    },
    {
        "id": 218,
        "seek": 89040,
        "start": 911.8800048828125,
        "end": 914.5599975585938,
        "text": " but also the special tokens have changed in this tokenizer.",
        "tokens": [
            51438,
            457,
            611,
            264,
            2121,
            22667,
            362,
            3105,
            294,
            341,
            14862,
            6545,
            13,
            51572
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24157144129276276,
        "compression_ratio": 1.6916667222976685,
        "no_speech_prob": 0.014956584200263023
    },
    {
        "id": 219,
        "seek": 91456,
        "start": 914.5599975585938,
        "end": 920.3200073242188,
        "text": " So we, of course, have the end of text, just like in GPT 2, but we also see three, sorry,",
        "tokens": [
            50364,
            407,
            321,
            11,
            295,
            1164,
            11,
            362,
            264,
            917,
            295,
            2487,
            11,
            445,
            411,
            294,
            26039,
            51,
            568,
            11,
            457,
            321,
            611,
            536,
            1045,
            11,
            2597,
            11,
            50652
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23226386308670044,
        "compression_ratio": 1.582417607307434,
        "no_speech_prob": 0.022975977510213852
    },
    {
        "id": 220,
        "seek": 91456,
        "start": 920.3200073242188,
        "end": 924.3200073242188,
        "text": " four additional tokens here, FIM prefix, middle, and suffix.",
        "tokens": [
            50652,
            1451,
            4497,
            22667,
            510,
            11,
            479,
            6324,
            46969,
            11,
            2808,
            11,
            293,
            3889,
            970,
            13,
            50852
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23226386308670044,
        "compression_ratio": 1.582417607307434,
        "no_speech_prob": 0.022975977510213852
    },
    {
        "id": 221,
        "seek": 91456,
        "start": 924.3200073242188,
        "end": 925.3200073242188,
        "text": " What is FIM?",
        "tokens": [
            50852,
            708,
            307,
            479,
            6324,
            30,
            50902
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23226386308670044,
        "compression_ratio": 1.582417607307434,
        "no_speech_prob": 0.022975977510213852
    },
    {
        "id": 222,
        "seek": 91456,
        "start": 925.3200073242188,
        "end": 927.9600219726562,
        "text": " FIM is short for fill in the middle.",
        "tokens": [
            50902,
            479,
            6324,
            307,
            2099,
            337,
            2836,
            294,
            264,
            2808,
            13,
            51034
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23226386308670044,
        "compression_ratio": 1.582417607307434,
        "no_speech_prob": 0.022975977510213852
    },
    {
        "id": 223,
        "seek": 91456,
        "start": 927.9600219726562,
        "end": 933.0399780273438,
        "text": " And if you'd like to learn more about this idea, it comes from this paper.",
        "tokens": [
            51034,
            400,
            498,
            291,
            1116,
            411,
            281,
            1466,
            544,
            466,
            341,
            1558,
            11,
            309,
            1487,
            490,
            341,
            3035,
            13,
            51288
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23226386308670044,
        "compression_ratio": 1.582417607307434,
        "no_speech_prob": 0.022975977510213852
    },
    {
        "id": 224,
        "seek": 91456,
        "start": 933.0399780273438,
        "end": 935.9600219726562,
        "text": " And I'm not going to go into detail in this video, it's beyond this video.",
        "tokens": [
            51288,
            400,
            286,
            478,
            406,
            516,
            281,
            352,
            666,
            2607,
            294,
            341,
            960,
            11,
            309,
            311,
            4399,
            341,
            960,
            13,
            51434
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23226386308670044,
        "compression_ratio": 1.582417607307434,
        "no_speech_prob": 0.022975977510213852
    },
    {
        "id": 225,
        "seek": 91456,
        "start": 935.9600219726562,
        "end": 940.0599975585938,
        "text": " And then there's one additional SERP token here.",
        "tokens": [
            51434,
            400,
            550,
            456,
            311,
            472,
            4497,
            36772,
            47,
            14862,
            510,
            13,
            51639
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23226386308670044,
        "compression_ratio": 1.582417607307434,
        "no_speech_prob": 0.022975977510213852
    },
    {
        "id": 226,
        "seek": 91456,
        "start": 940.0599975585938,
        "end": 942.7999877929688,
        "text": " So that's that encoding as well.",
        "tokens": [
            51639,
            407,
            300,
            311,
            300,
            43430,
            382,
            731,
            13,
            51776
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23226386308670044,
        "compression_ratio": 1.582417607307434,
        "no_speech_prob": 0.022975977510213852
    },
    {
        "id": 227,
        "seek": 94280,
        "start": 942.7999877929688,
        "end": 946.2000122070312,
        "text": " So it's very common, basically, to train a language model.",
        "tokens": [
            50364,
            407,
            309,
            311,
            588,
            2689,
            11,
            1936,
            11,
            281,
            3847,
            257,
            2856,
            2316,
            13,
            50534
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2658402919769287,
        "compression_ratio": 1.7625417709350586,
        "no_speech_prob": 0.0025908597745001316
    },
    {
        "id": 228,
        "seek": 94280,
        "start": 946.2000122070312,
        "end": 950.1599731445312,
        "text": " And then if you'd like, you can add special tokens.",
        "tokens": [
            50534,
            400,
            550,
            498,
            291,
            1116,
            411,
            11,
            291,
            393,
            909,
            2121,
            22667,
            13,
            50732
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2658402919769287,
        "compression_ratio": 1.7625417709350586,
        "no_speech_prob": 0.0025908597745001316
    },
    {
        "id": 229,
        "seek": 94280,
        "start": 950.1599731445312,
        "end": 955.6799926757812,
        "text": " Now when you add special tokens, you, of course, have to do some model surgery to the transformer",
        "tokens": [
            50732,
            823,
            562,
            291,
            909,
            2121,
            22667,
            11,
            291,
            11,
            295,
            1164,
            11,
            362,
            281,
            360,
            512,
            2316,
            7930,
            281,
            264,
            31782,
            51008
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2658402919769287,
        "compression_ratio": 1.7625417709350586,
        "no_speech_prob": 0.0025908597745001316
    },
    {
        "id": 230,
        "seek": 94280,
        "start": 955.6799926757812,
        "end": 958.9600219726562,
        "text": " and all of the parameters involved in that transformer, because you are basically adding",
        "tokens": [
            51008,
            293,
            439,
            295,
            264,
            9834,
            3288,
            294,
            300,
            31782,
            11,
            570,
            291,
            366,
            1936,
            5127,
            51172
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2658402919769287,
        "compression_ratio": 1.7625417709350586,
        "no_speech_prob": 0.0025908597745001316
    },
    {
        "id": 231,
        "seek": 94280,
        "start": 958.9600219726562,
        "end": 959.9600219726562,
        "text": " an integer.",
        "tokens": [
            51172,
            364,
            24922,
            13,
            51222
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2658402919769287,
        "compression_ratio": 1.7625417709350586,
        "no_speech_prob": 0.0025908597745001316
    },
    {
        "id": 232,
        "seek": 94280,
        "start": 959.9600219726562,
        "end": 964.52001953125,
        "text": " And you want to make sure that, for example, your embedding matrix for the vocabulary tokens",
        "tokens": [
            51222,
            400,
            291,
            528,
            281,
            652,
            988,
            300,
            11,
            337,
            1365,
            11,
            428,
            12240,
            3584,
            8141,
            337,
            264,
            19864,
            22667,
            51450
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2658402919769287,
        "compression_ratio": 1.7625417709350586,
        "no_speech_prob": 0.0025908597745001316
    },
    {
        "id": 233,
        "seek": 94280,
        "start": 964.52001953125,
        "end": 967.0800170898438,
        "text": " has to be extended by adding a row.",
        "tokens": [
            51450,
            575,
            281,
            312,
            10913,
            538,
            5127,
            257,
            5386,
            13,
            51578
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2658402919769287,
        "compression_ratio": 1.7625417709350586,
        "no_speech_prob": 0.0025908597745001316
    },
    {
        "id": 234,
        "seek": 94280,
        "start": 967.0800170898438,
        "end": 970.5599975585938,
        "text": " And typically this row would be initialized with small random numbers for something like",
        "tokens": [
            51578,
            400,
            5850,
            341,
            5386,
            576,
            312,
            5883,
            1602,
            365,
            1359,
            4974,
            3547,
            337,
            746,
            411,
            51752
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2658402919769287,
        "compression_ratio": 1.7625417709350586,
        "no_speech_prob": 0.0025908597745001316
    },
    {
        "id": 235,
        "seek": 97056,
        "start": 970.5599975585938,
        "end": 975.719970703125,
        "text": " that, because we need to have a vector that now stands for that token.",
        "tokens": [
            50364,
            300,
            11,
            570,
            321,
            643,
            281,
            362,
            257,
            8062,
            300,
            586,
            7382,
            337,
            300,
            14862,
            13,
            50622
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2351152002811432,
        "compression_ratio": 1.747330904006958,
        "no_speech_prob": 0.09267871826887131
    },
    {
        "id": 236,
        "seek": 97056,
        "start": 975.719970703125,
        "end": 978.3200073242188,
        "text": " In addition to that, you have to go to the final layer of the transformer, and you have",
        "tokens": [
            50622,
            682,
            4500,
            281,
            300,
            11,
            291,
            362,
            281,
            352,
            281,
            264,
            2572,
            4583,
            295,
            264,
            31782,
            11,
            293,
            291,
            362,
            50752
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2351152002811432,
        "compression_ratio": 1.747330904006958,
        "no_speech_prob": 0.09267871826887131
    },
    {
        "id": 237,
        "seek": 97056,
        "start": 978.3200073242188,
        "end": 983.1799926757812,
        "text": " to make sure that that projection at the very end into the classifier is extended by one",
        "tokens": [
            50752,
            281,
            652,
            988,
            300,
            300,
            22743,
            412,
            264,
            588,
            917,
            666,
            264,
            1508,
            9902,
            307,
            10913,
            538,
            472,
            50995
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2351152002811432,
        "compression_ratio": 1.747330904006958,
        "no_speech_prob": 0.09267871826887131
    },
    {
        "id": 238,
        "seek": 97056,
        "start": 983.1799926757812,
        "end": 984.1799926757812,
        "text": " as well.",
        "tokens": [
            50995,
            382,
            731,
            13,
            51045
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2351152002811432,
        "compression_ratio": 1.747330904006958,
        "no_speech_prob": 0.09267871826887131
    },
    {
        "id": 239,
        "seek": 97056,
        "start": 984.1799926757812,
        "end": 988.719970703125,
        "text": " So basically, there's some model surgery involved that you have to couple with the tokenization",
        "tokens": [
            51045,
            407,
            1936,
            11,
            456,
            311,
            512,
            2316,
            7930,
            3288,
            300,
            291,
            362,
            281,
            1916,
            365,
            264,
            14862,
            2144,
            51272
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2351152002811432,
        "compression_ratio": 1.747330904006958,
        "no_speech_prob": 0.09267871826887131
    },
    {
        "id": 240,
        "seek": 97056,
        "start": 988.719970703125,
        "end": 992.0800170898438,
        "text": " changes if you are going to add special tokens.",
        "tokens": [
            51272,
            2962,
            498,
            291,
            366,
            516,
            281,
            909,
            2121,
            22667,
            13,
            51440
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2351152002811432,
        "compression_ratio": 1.747330904006958,
        "no_speech_prob": 0.09267871826887131
    },
    {
        "id": 241,
        "seek": 97056,
        "start": 992.0800170898438,
        "end": 995.47998046875,
        "text": " But this is a very common operation that people do, especially if they'd like to fine tune",
        "tokens": [
            51440,
            583,
            341,
            307,
            257,
            588,
            2689,
            6916,
            300,
            561,
            360,
            11,
            2318,
            498,
            436,
            1116,
            411,
            281,
            2489,
            10864,
            51610
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2351152002811432,
        "compression_ratio": 1.747330904006958,
        "no_speech_prob": 0.09267871826887131
    },
    {
        "id": 242,
        "seek": 99548,
        "start": 995.47998046875,
        "end": 996.47998046875,
        "text": " the model.",
        "tokens": [
            50364,
            264,
            2316,
            13,
            50414
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25502073764801025,
        "compression_ratio": 1.569023609161377,
        "no_speech_prob": 0.1258949190378189
    },
    {
        "id": 243,
        "seek": 99548,
        "start": 996.47998046875,
        "end": 1000.760009765625,
        "text": " For example, taking it from a base model to a chat model, like ChatGPT.",
        "tokens": [
            50414,
            1171,
            1365,
            11,
            1940,
            309,
            490,
            257,
            3096,
            2316,
            281,
            257,
            5081,
            2316,
            11,
            411,
            27503,
            38,
            47,
            51,
            13,
            50628
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25502073764801025,
        "compression_ratio": 1.569023609161377,
        "no_speech_prob": 0.1258949190378189
    },
    {
        "id": 244,
        "seek": 99548,
        "start": 1000.760009765625,
        "end": 1005.6400146484375,
        "text": " Okay, so at this point, you should have everything you need in order to build your own GPT-4",
        "tokens": [
            50628,
            1033,
            11,
            370,
            412,
            341,
            935,
            11,
            291,
            820,
            362,
            1203,
            291,
            643,
            294,
            1668,
            281,
            1322,
            428,
            1065,
            26039,
            51,
            12,
            19,
            50872
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25502073764801025,
        "compression_ratio": 1.569023609161377,
        "no_speech_prob": 0.1258949190378189
    },
    {
        "id": 245,
        "seek": 99548,
        "start": 1005.6400146484375,
        "end": 1006.6400146484375,
        "text": " tokenizer.",
        "tokens": [
            50872,
            14862,
            6545,
            13,
            50922
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25502073764801025,
        "compression_ratio": 1.569023609161377,
        "no_speech_prob": 0.1258949190378189
    },
    {
        "id": 246,
        "seek": 99548,
        "start": 1006.6400146484375,
        "end": 1010.1199951171875,
        "text": " Now, in the process of developing this lecture, I've done that, and I've published the code",
        "tokens": [
            50922,
            823,
            11,
            294,
            264,
            1399,
            295,
            6416,
            341,
            7991,
            11,
            286,
            600,
            1096,
            300,
            11,
            293,
            286,
            600,
            6572,
            264,
            3089,
            51096
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25502073764801025,
        "compression_ratio": 1.569023609161377,
        "no_speech_prob": 0.1258949190378189
    },
    {
        "id": 247,
        "seek": 99548,
        "start": 1010.1199951171875,
        "end": 1013.1599731445312,
        "text": " under this repository, MinBPE.",
        "tokens": [
            51096,
            833,
            341,
            25841,
            11,
            2829,
            33,
            5208,
            13,
            51248
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25502073764801025,
        "compression_ratio": 1.569023609161377,
        "no_speech_prob": 0.1258949190378189
    },
    {
        "id": 248,
        "seek": 99548,
        "start": 1013.1599731445312,
        "end": 1018.5599975585938,
        "text": " So MinBPE looks like this right now as I'm recording, but the MinBPE repository will",
        "tokens": [
            51248,
            407,
            2829,
            33,
            5208,
            1542,
            411,
            341,
            558,
            586,
            382,
            286,
            478,
            6613,
            11,
            457,
            264,
            2829,
            33,
            5208,
            25841,
            486,
            51518
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25502073764801025,
        "compression_ratio": 1.569023609161377,
        "no_speech_prob": 0.1258949190378189
    },
    {
        "id": 249,
        "seek": 99548,
        "start": 1018.5599975585938,
        "end": 1023.0,
        "text": " probably change quite a bit because I intend to continue working on it.",
        "tokens": [
            51518,
            1391,
            1319,
            1596,
            257,
            857,
            570,
            286,
            19759,
            281,
            2354,
            1364,
            322,
            309,
            13,
            51740
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25502073764801025,
        "compression_ratio": 1.569023609161377,
        "no_speech_prob": 0.1258949190378189
    },
    {
        "id": 250,
        "seek": 102300,
        "start": 1023.0,
        "end": 1027.280029296875,
        "text": " In addition to the MinBPE repository, I've published this exercise progression that you",
        "tokens": [
            50364,
            682,
            4500,
            281,
            264,
            2829,
            33,
            5208,
            25841,
            11,
            286,
            600,
            6572,
            341,
            5380,
            18733,
            300,
            291,
            50578
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1864824742078781,
        "compression_ratio": 1.6492537260055542,
        "no_speech_prob": 0.0005976599641144276
    },
    {
        "id": 251,
        "seek": 102300,
        "start": 1027.280029296875,
        "end": 1028.280029296875,
        "text": " can follow.",
        "tokens": [
            50578,
            393,
            1524,
            13,
            50628
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1864824742078781,
        "compression_ratio": 1.6492537260055542,
        "no_speech_prob": 0.0005976599641144276
    },
    {
        "id": 252,
        "seek": 102300,
        "start": 1028.280029296875,
        "end": 1034.4000244140625,
        "text": " So if you go to exercise.md here, this is sort of me breaking up the task ahead of you",
        "tokens": [
            50628,
            407,
            498,
            291,
            352,
            281,
            5380,
            13,
            76,
            67,
            510,
            11,
            341,
            307,
            1333,
            295,
            385,
            7697,
            493,
            264,
            5633,
            2286,
            295,
            291,
            50934
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1864824742078781,
        "compression_ratio": 1.6492537260055542,
        "no_speech_prob": 0.0005976599641144276
    },
    {
        "id": 253,
        "seek": 102300,
        "start": 1034.4000244140625,
        "end": 1039.8800048828125,
        "text": " into four steps that sort of build up to what can be a GPT-4 tokenizer.",
        "tokens": [
            50934,
            666,
            1451,
            4439,
            300,
            1333,
            295,
            1322,
            493,
            281,
            437,
            393,
            312,
            257,
            26039,
            51,
            12,
            19,
            14862,
            6545,
            13,
            51208
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1864824742078781,
        "compression_ratio": 1.6492537260055542,
        "no_speech_prob": 0.0005976599641144276
    },
    {
        "id": 254,
        "seek": 102300,
        "start": 1039.8800048828125,
        "end": 1043.9599609375,
        "text": " And so feel free to follow these steps exactly, and follow a little bit of the guidance that",
        "tokens": [
            51208,
            400,
            370,
            841,
            1737,
            281,
            1524,
            613,
            4439,
            2293,
            11,
            293,
            1524,
            257,
            707,
            857,
            295,
            264,
            10056,
            300,
            51412
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1864824742078781,
        "compression_ratio": 1.6492537260055542,
        "no_speech_prob": 0.0005976599641144276
    },
    {
        "id": 255,
        "seek": 102300,
        "start": 1043.9599609375,
        "end": 1045.56005859375,
        "text": " I've laid out here.",
        "tokens": [
            51412,
            286,
            600,
            9897,
            484,
            510,
            13,
            51492
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1864824742078781,
        "compression_ratio": 1.6492537260055542,
        "no_speech_prob": 0.0005976599641144276
    },
    {
        "id": 256,
        "seek": 102300,
        "start": 1045.56005859375,
        "end": 1050.280029296875,
        "text": " And anytime you feel stuck, just reference the MinBPE repository here.",
        "tokens": [
            51492,
            400,
            13038,
            291,
            841,
            5541,
            11,
            445,
            6408,
            264,
            2829,
            33,
            5208,
            25841,
            510,
            13,
            51728
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1864824742078781,
        "compression_ratio": 1.6492537260055542,
        "no_speech_prob": 0.0005976599641144276
    },
    {
        "id": 257,
        "seek": 105028,
        "start": 1050.280029296875,
        "end": 1054.5999755859375,
        "text": " So either the tests could be useful, or the MinBPE repository itself.",
        "tokens": [
            50364,
            407,
            2139,
            264,
            6921,
            727,
            312,
            4420,
            11,
            420,
            264,
            2829,
            33,
            5208,
            25841,
            2564,
            13,
            50580
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23528645932674408,
        "compression_ratio": 1.6849817037582397,
        "no_speech_prob": 0.025956355035305023
    },
    {
        "id": 258,
        "seek": 105028,
        "start": 1054.5999755859375,
        "end": 1062.239990234375,
        "text": " I try to keep the code fairly clean and understandable, and so feel free to reference it whenever",
        "tokens": [
            50580,
            286,
            853,
            281,
            1066,
            264,
            3089,
            6457,
            2541,
            293,
            25648,
            11,
            293,
            370,
            841,
            1737,
            281,
            6408,
            309,
            5699,
            50962
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23528645932674408,
        "compression_ratio": 1.6849817037582397,
        "no_speech_prob": 0.025956355035305023
    },
    {
        "id": 259,
        "seek": 105028,
        "start": 1062.239990234375,
        "end": 1064.47998046875,
        "text": " you get stuck.",
        "tokens": [
            50962,
            291,
            483,
            5541,
            13,
            51074
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23528645932674408,
        "compression_ratio": 1.6849817037582397,
        "no_speech_prob": 0.025956355035305023
    },
    {
        "id": 260,
        "seek": 105028,
        "start": 1064.47998046875,
        "end": 1068.6800537109375,
        "text": " In addition to that, basically, once you write it, you should be able to reproduce this behavior",
        "tokens": [
            51074,
            682,
            4500,
            281,
            300,
            11,
            1936,
            11,
            1564,
            291,
            2464,
            309,
            11,
            291,
            820,
            312,
            1075,
            281,
            29501,
            341,
            5223,
            51284
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23528645932674408,
        "compression_ratio": 1.6849817037582397,
        "no_speech_prob": 0.025956355035305023
    },
    {
        "id": 261,
        "seek": 105028,
        "start": 1068.6800537109375,
        "end": 1070.0799560546875,
        "text": " from the token.",
        "tokens": [
            51284,
            490,
            264,
            14862,
            13,
            51354
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23528645932674408,
        "compression_ratio": 1.6849817037582397,
        "no_speech_prob": 0.025956355035305023
    },
    {
        "id": 262,
        "seek": 105028,
        "start": 1070.0799560546875,
        "end": 1076.1600341796875,
        "text": " So getting the GPT-4 tokenizer, you can encode this string, and you should get these tokens.",
        "tokens": [
            51354,
            407,
            1242,
            264,
            26039,
            51,
            12,
            19,
            14862,
            6545,
            11,
            291,
            393,
            2058,
            1429,
            341,
            6798,
            11,
            293,
            291,
            820,
            483,
            613,
            22667,
            13,
            51658
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23528645932674408,
        "compression_ratio": 1.6849817037582397,
        "no_speech_prob": 0.025956355035305023
    },
    {
        "id": 263,
        "seek": 105028,
        "start": 1076.1600341796875,
        "end": 1079.719970703125,
        "text": " And then you can encode and decode the exact same string to recover it.",
        "tokens": [
            51658,
            400,
            550,
            291,
            393,
            2058,
            1429,
            293,
            979,
            1429,
            264,
            1900,
            912,
            6798,
            281,
            8114,
            309,
            13,
            51836
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23528645932674408,
        "compression_ratio": 1.6849817037582397,
        "no_speech_prob": 0.025956355035305023
    },
    {
        "id": 264,
        "seek": 107972,
        "start": 1080.1600341796875,
        "end": 1083.3599853515625,
        "text": " And in addition to all that, you should be able to implement your own train function,",
        "tokens": [
            50386,
            400,
            294,
            4500,
            281,
            439,
            300,
            11,
            291,
            820,
            312,
            1075,
            281,
            4445,
            428,
            1065,
            3847,
            2445,
            11,
            50546
        ],
        "temperature": 0.0,
        "avg_logprob": -0.26746249198913574,
        "compression_ratio": 1.6536965370178223,
        "no_speech_prob": 0.004829656798392534
    },
    {
        "id": 265,
        "seek": 107972,
        "start": 1083.3599853515625,
        "end": 1085.280029296875,
        "text": " which TickToken library does not provide.",
        "tokens": [
            50546,
            597,
            314,
            618,
            51,
            8406,
            6405,
            775,
            406,
            2893,
            13,
            50642
        ],
        "temperature": 0.0,
        "avg_logprob": -0.26746249198913574,
        "compression_ratio": 1.6536965370178223,
        "no_speech_prob": 0.004829656798392534
    },
    {
        "id": 266,
        "seek": 107972,
        "start": 1085.280029296875,
        "end": 1087.239990234375,
        "text": " It's, again, only inference code.",
        "tokens": [
            50642,
            467,
            311,
            11,
            797,
            11,
            787,
            38253,
            3089,
            13,
            50740
        ],
        "temperature": 0.0,
        "avg_logprob": -0.26746249198913574,
        "compression_ratio": 1.6536965370178223,
        "no_speech_prob": 0.004829656798392534
    },
    {
        "id": 267,
        "seek": 107972,
        "start": 1087.239990234375,
        "end": 1091.1600341796875,
        "text": " But you should write your own train, MinBPE does it as well.",
        "tokens": [
            50740,
            583,
            291,
            820,
            2464,
            428,
            1065,
            3847,
            11,
            2829,
            33,
            5208,
            775,
            309,
            382,
            731,
            13,
            50936
        ],
        "temperature": 0.0,
        "avg_logprob": -0.26746249198913574,
        "compression_ratio": 1.6536965370178223,
        "no_speech_prob": 0.004829656798392534
    },
    {
        "id": 268,
        "seek": 107972,
        "start": 1091.1600341796875,
        "end": 1095.0,
        "text": " And that will allow you to train your own token vocabularies.",
        "tokens": [
            50936,
            400,
            300,
            486,
            2089,
            291,
            281,
            3847,
            428,
            1065,
            14862,
            2329,
            455,
            1040,
            530,
            13,
            51128
        ],
        "temperature": 0.0,
        "avg_logprob": -0.26746249198913574,
        "compression_ratio": 1.6536965370178223,
        "no_speech_prob": 0.004829656798392534
    },
    {
        "id": 269,
        "seek": 107972,
        "start": 1095.0,
        "end": 1100.719970703125,
        "text": " So here's some of the code inside MinBPE, shows the token vocabularies that you might",
        "tokens": [
            51128,
            407,
            510,
            311,
            512,
            295,
            264,
            3089,
            1854,
            2829,
            33,
            5208,
            11,
            3110,
            264,
            14862,
            2329,
            455,
            1040,
            530,
            300,
            291,
            1062,
            51414
        ],
        "temperature": 0.0,
        "avg_logprob": -0.26746249198913574,
        "compression_ratio": 1.6536965370178223,
        "no_speech_prob": 0.004829656798392534
    },
    {
        "id": 270,
        "seek": 107972,
        "start": 1100.719970703125,
        "end": 1101.9599609375,
        "text": " obtain.",
        "tokens": [
            51414,
            12701,
            13,
            51476
        ],
        "temperature": 0.0,
        "avg_logprob": -0.26746249198913574,
        "compression_ratio": 1.6536965370178223,
        "no_speech_prob": 0.004829656798392534
    },
    {
        "id": 271,
        "seek": 107972,
        "start": 1101.9599609375,
        "end": 1106.760009765625,
        "text": " So on the left here, we have the GPT-4 merges.",
        "tokens": [
            51476,
            407,
            322,
            264,
            1411,
            510,
            11,
            321,
            362,
            264,
            26039,
            51,
            12,
            19,
            3551,
            2880,
            13,
            51716
        ],
        "temperature": 0.0,
        "avg_logprob": -0.26746249198913574,
        "compression_ratio": 1.6536965370178223,
        "no_speech_prob": 0.004829656798392534
    },
    {
        "id": 272,
        "seek": 110676,
        "start": 1106.800048828125,
        "end": 1110.3599853515625,
        "text": " So the first 256 are raw individual bytes.",
        "tokens": [
            50366,
            407,
            264,
            700,
            38882,
            366,
            8936,
            2609,
            36088,
            13,
            50544
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22107291221618652,
        "compression_ratio": 1.7847533226013184,
        "no_speech_prob": 0.004829595796763897
    },
    {
        "id": 273,
        "seek": 110676,
        "start": 1110.3599853515625,
        "end": 1114.8399658203125,
        "text": " And then here I am visualizing the merges that GPT-4 performed during its training.",
        "tokens": [
            50544,
            400,
            550,
            510,
            286,
            669,
            5056,
            3319,
            264,
            3551,
            2880,
            300,
            26039,
            51,
            12,
            19,
            10332,
            1830,
            1080,
            3097,
            13,
            50768
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22107291221618652,
        "compression_ratio": 1.7847533226013184,
        "no_speech_prob": 0.004829595796763897
    },
    {
        "id": 274,
        "seek": 110676,
        "start": 1114.8399658203125,
        "end": 1121.0400390625,
        "text": " So the very first merge that GPT-4 did was merge two spaces into a single token for two",
        "tokens": [
            50768,
            407,
            264,
            588,
            700,
            22183,
            300,
            26039,
            51,
            12,
            19,
            630,
            390,
            22183,
            732,
            7673,
            666,
            257,
            2167,
            14862,
            337,
            732,
            51078
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22107291221618652,
        "compression_ratio": 1.7847533226013184,
        "no_speech_prob": 0.004829595796763897
    },
    {
        "id": 275,
        "seek": 110676,
        "start": 1121.0400390625,
        "end": 1122.0400390625,
        "text": " spaces.",
        "tokens": [
            51078,
            7673,
            13,
            51128
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22107291221618652,
        "compression_ratio": 1.7847533226013184,
        "no_speech_prob": 0.004829595796763897
    },
    {
        "id": 276,
        "seek": 110676,
        "start": 1122.0400390625,
        "end": 1124.0999755859375,
        "text": " And that is a token 256.",
        "tokens": [
            51128,
            400,
            300,
            307,
            257,
            14862,
            38882,
            13,
            51231
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22107291221618652,
        "compression_ratio": 1.7847533226013184,
        "no_speech_prob": 0.004829595796763897
    },
    {
        "id": 277,
        "seek": 110676,
        "start": 1124.0999755859375,
        "end": 1127.47998046875,
        "text": " And so this is the order in which things merged during GPT-4 training.",
        "tokens": [
            51231,
            400,
            370,
            341,
            307,
            264,
            1668,
            294,
            597,
            721,
            36427,
            1830,
            26039,
            51,
            12,
            19,
            3097,
            13,
            51400
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22107291221618652,
        "compression_ratio": 1.7847533226013184,
        "no_speech_prob": 0.004829595796763897
    },
    {
        "id": 278,
        "seek": 110676,
        "start": 1127.47998046875,
        "end": 1133.9599609375,
        "text": " And this is the merge order that we obtained in MinBPE by training a tokenizer.",
        "tokens": [
            51400,
            400,
            341,
            307,
            264,
            22183,
            1668,
            300,
            321,
            14879,
            294,
            2829,
            33,
            5208,
            538,
            3097,
            257,
            14862,
            6545,
            13,
            51724
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22107291221618652,
        "compression_ratio": 1.7847533226013184,
        "no_speech_prob": 0.004829595796763897
    },
    {
        "id": 279,
        "seek": 113396,
        "start": 1133.9599609375,
        "end": 1137.760009765625,
        "text": " And in this case, I trained it on a Wikipedia page of Taylor Swift.",
        "tokens": [
            50364,
            400,
            294,
            341,
            1389,
            11,
            286,
            8895,
            309,
            322,
            257,
            28999,
            3028,
            295,
            12060,
            25539,
            13,
            50554
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25251588225364685,
        "compression_ratio": 1.5076923370361328,
        "no_speech_prob": 0.08034630119800568
    },
    {
        "id": 280,
        "seek": 113396,
        "start": 1137.760009765625,
        "end": 1142.52001953125,
        "text": " Not because I'm a Swifty, but because that is one of the longest Wikipedia pages apparently",
        "tokens": [
            50554,
            1726,
            570,
            286,
            478,
            257,
            3926,
            37177,
            11,
            457,
            570,
            300,
            307,
            472,
            295,
            264,
            15438,
            28999,
            7183,
            7970,
            50792
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25251588225364685,
        "compression_ratio": 1.5076923370361328,
        "no_speech_prob": 0.08034630119800568
    },
    {
        "id": 281,
        "seek": 113396,
        "start": 1142.52001953125,
        "end": 1143.760009765625,
        "text": " that's available.",
        "tokens": [
            50792,
            300,
            311,
            2435,
            13,
            50854
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25251588225364685,
        "compression_ratio": 1.5076923370361328,
        "no_speech_prob": 0.08034630119800568
    },
    {
        "id": 282,
        "seek": 113396,
        "start": 1143.760009765625,
        "end": 1145.760009765625,
        "text": " But she is pretty cool.",
        "tokens": [
            50854,
            583,
            750,
            307,
            1238,
            1627,
            13,
            50954
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25251588225364685,
        "compression_ratio": 1.5076923370361328,
        "no_speech_prob": 0.08034630119800568
    },
    {
        "id": 283,
        "seek": 113396,
        "start": 1145.760009765625,
        "end": 1149.199951171875,
        "text": " And what was I going to say?",
        "tokens": [
            50954,
            400,
            437,
            390,
            286,
            516,
            281,
            584,
            30,
            51126
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25251588225364685,
        "compression_ratio": 1.5076923370361328,
        "no_speech_prob": 0.08034630119800568
    },
    {
        "id": 284,
        "seek": 113396,
        "start": 1149.199951171875,
        "end": 1150.199951171875,
        "text": " Yeah.",
        "tokens": [
            51126,
            865,
            13,
            51176
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25251588225364685,
        "compression_ratio": 1.5076923370361328,
        "no_speech_prob": 0.08034630119800568
    },
    {
        "id": 285,
        "seek": 113396,
        "start": 1150.199951171875,
        "end": 1152.4000244140625,
        "text": " So you can compare these two vocabularies.",
        "tokens": [
            51176,
            407,
            291,
            393,
            6794,
            613,
            732,
            2329,
            455,
            1040,
            530,
            13,
            51286
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25251588225364685,
        "compression_ratio": 1.5076923370361328,
        "no_speech_prob": 0.08034630119800568
    },
    {
        "id": 286,
        "seek": 113396,
        "start": 1152.4000244140625,
        "end": 1159.199951171875,
        "text": " And so as an example, here GPT-4 merged IN to become IN.",
        "tokens": [
            51286,
            400,
            370,
            382,
            364,
            1365,
            11,
            510,
            26039,
            51,
            12,
            19,
            36427,
            6892,
            281,
            1813,
            6892,
            13,
            51626
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25251588225364685,
        "compression_ratio": 1.5076923370361328,
        "no_speech_prob": 0.08034630119800568
    },
    {
        "id": 287,
        "seek": 113396,
        "start": 1159.199951171875,
        "end": 1163.239990234375,
        "text": " And we've done the exact same thing on this token, 259.",
        "tokens": [
            51626,
            400,
            321,
            600,
            1096,
            264,
            1900,
            912,
            551,
            322,
            341,
            14862,
            11,
            3552,
            24,
            13,
            51828
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25251588225364685,
        "compression_ratio": 1.5076923370361328,
        "no_speech_prob": 0.08034630119800568
    },
    {
        "id": 288,
        "seek": 116324,
        "start": 1163.239990234375,
        "end": 1168.4000244140625,
        "text": " Here, space T becomes space T. And that happened for us a little bit later as well.",
        "tokens": [
            50364,
            1692,
            11,
            1901,
            314,
            3643,
            1901,
            314,
            13,
            400,
            300,
            2011,
            337,
            505,
            257,
            707,
            857,
            1780,
            382,
            731,
            13,
            50622
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21709217131137848,
        "compression_ratio": 1.7200000286102295,
        "no_speech_prob": 0.11278827488422394
    },
    {
        "id": 289,
        "seek": 116324,
        "start": 1168.4000244140625,
        "end": 1172.719970703125,
        "text": " So the difference here is, again, to my understanding, only a difference of the training set.",
        "tokens": [
            50622,
            407,
            264,
            2649,
            510,
            307,
            11,
            797,
            11,
            281,
            452,
            3701,
            11,
            787,
            257,
            2649,
            295,
            264,
            3097,
            992,
            13,
            50838
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21709217131137848,
        "compression_ratio": 1.7200000286102295,
        "no_speech_prob": 0.11278827488422394
    },
    {
        "id": 290,
        "seek": 116324,
        "start": 1172.719970703125,
        "end": 1176.6400146484375,
        "text": " So as an example, because I see a lot of white space, I expect that GPT-4 probably had a",
        "tokens": [
            50838,
            407,
            382,
            364,
            1365,
            11,
            570,
            286,
            536,
            257,
            688,
            295,
            2418,
            1901,
            11,
            286,
            2066,
            300,
            26039,
            51,
            12,
            19,
            1391,
            632,
            257,
            51034
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21709217131137848,
        "compression_ratio": 1.7200000286102295,
        "no_speech_prob": 0.11278827488422394
    },
    {
        "id": 291,
        "seek": 116324,
        "start": 1176.6400146484375,
        "end": 1181.9599609375,
        "text": " lot of Python code in its training set, I'm not sure, for the tokenizer.",
        "tokens": [
            51034,
            688,
            295,
            15329,
            3089,
            294,
            1080,
            3097,
            992,
            11,
            286,
            478,
            406,
            988,
            11,
            337,
            264,
            14862,
            6545,
            13,
            51300
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21709217131137848,
        "compression_ratio": 1.7200000286102295,
        "no_speech_prob": 0.11278827488422394
    },
    {
        "id": 292,
        "seek": 116324,
        "start": 1181.9599609375,
        "end": 1186.239990234375,
        "text": " And here we see much less of that, of course, in the Wikipedia page.",
        "tokens": [
            51300,
            400,
            510,
            321,
            536,
            709,
            1570,
            295,
            300,
            11,
            295,
            1164,
            11,
            294,
            264,
            28999,
            3028,
            13,
            51514
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21709217131137848,
        "compression_ratio": 1.7200000286102295,
        "no_speech_prob": 0.11278827488422394
    },
    {
        "id": 293,
        "seek": 116324,
        "start": 1186.239990234375,
        "end": 1188.0400390625,
        "text": " So roughly speaking, they look the same.",
        "tokens": [
            51514,
            407,
            9810,
            4124,
            11,
            436,
            574,
            264,
            912,
            13,
            51604
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21709217131137848,
        "compression_ratio": 1.7200000286102295,
        "no_speech_prob": 0.11278827488422394
    },
    {
        "id": 294,
        "seek": 116324,
        "start": 1188.0400390625,
        "end": 1190.719970703125,
        "text": " And they look the same because they're running the same algorithm.",
        "tokens": [
            51604,
            400,
            436,
            574,
            264,
            912,
            570,
            436,
            434,
            2614,
            264,
            912,
            9284,
            13,
            51738
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21709217131137848,
        "compression_ratio": 1.7200000286102295,
        "no_speech_prob": 0.11278827488422394
    },
    {
        "id": 295,
        "seek": 119072,
        "start": 1190.719970703125,
        "end": 1194.1600341796875,
        "text": " And when you train your own, you're probably going to get something similar, depending",
        "tokens": [
            50364,
            400,
            562,
            291,
            3847,
            428,
            1065,
            11,
            291,
            434,
            1391,
            516,
            281,
            483,
            746,
            2531,
            11,
            5413,
            50536
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2750956416130066,
        "compression_ratio": 1.7000000476837158,
        "no_speech_prob": 0.34149453043937683
    },
    {
        "id": 296,
        "seek": 119072,
        "start": 1194.1600341796875,
        "end": 1195.3199462890625,
        "text": " on what you train it on.",
        "tokens": [
            50536,
            322,
            437,
            291,
            3847,
            309,
            322,
            13,
            50594
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2750956416130066,
        "compression_ratio": 1.7000000476837158,
        "no_speech_prob": 0.34149453043937683
    },
    {
        "id": 297,
        "seek": 119072,
        "start": 1195.3199462890625,
        "end": 1199.6400146484375,
        "text": " Okay, so we are now going to move on from TICToken and the way that OpenAI tokenizes",
        "tokens": [
            50594,
            1033,
            11,
            370,
            321,
            366,
            586,
            516,
            281,
            1286,
            322,
            490,
            314,
            2532,
            51,
            8406,
            293,
            264,
            636,
            300,
            7238,
            48698,
            14862,
            5660,
            50810
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2750956416130066,
        "compression_ratio": 1.7000000476837158,
        "no_speech_prob": 0.34149453043937683
    },
    {
        "id": 298,
        "seek": 119072,
        "start": 1199.6400146484375,
        "end": 1200.6400146484375,
        "text": " its strings.",
        "tokens": [
            50810,
            1080,
            13985,
            13,
            50860
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2750956416130066,
        "compression_ratio": 1.7000000476837158,
        "no_speech_prob": 0.34149453043937683
    },
    {
        "id": 299,
        "seek": 119072,
        "start": 1200.6400146484375,
        "end": 1204.9200439453125,
        "text": " And we're going to discuss one more very commonly used library for working with tokenization",
        "tokens": [
            50860,
            400,
            321,
            434,
            516,
            281,
            2248,
            472,
            544,
            588,
            12719,
            1143,
            6405,
            337,
            1364,
            365,
            14862,
            2144,
            51074
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2750956416130066,
        "compression_ratio": 1.7000000476837158,
        "no_speech_prob": 0.34149453043937683
    },
    {
        "id": 300,
        "seek": 119072,
        "start": 1204.9200439453125,
        "end": 1208.0400390625,
        "text": " in LLMs, and that is SentencePiece.",
        "tokens": [
            51074,
            294,
            441,
            43,
            26386,
            11,
            293,
            300,
            307,
            23652,
            655,
            47,
            46566,
            13,
            51230
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2750956416130066,
        "compression_ratio": 1.7000000476837158,
        "no_speech_prob": 0.34149453043937683
    },
    {
        "id": 301,
        "seek": 119072,
        "start": 1208.0400390625,
        "end": 1213.0799560546875,
        "text": " So SentencePiece is very commonly used in language models, because unlike TICToken,",
        "tokens": [
            51230,
            407,
            23652,
            655,
            47,
            46566,
            307,
            588,
            12719,
            1143,
            294,
            2856,
            5245,
            11,
            570,
            8343,
            314,
            2532,
            51,
            8406,
            11,
            51482
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2750956416130066,
        "compression_ratio": 1.7000000476837158,
        "no_speech_prob": 0.34149453043937683
    },
    {
        "id": 302,
        "seek": 119072,
        "start": 1213.0799560546875,
        "end": 1217.56005859375,
        "text": " it can do both training and inference, and is quite efficient at both.",
        "tokens": [
            51482,
            309,
            393,
            360,
            1293,
            3097,
            293,
            38253,
            11,
            293,
            307,
            1596,
            7148,
            412,
            1293,
            13,
            51706
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2750956416130066,
        "compression_ratio": 1.7000000476837158,
        "no_speech_prob": 0.34149453043937683
    },
    {
        "id": 303,
        "seek": 121756,
        "start": 1217.56005859375,
        "end": 1222.5999755859375,
        "text": " It supports a number of algorithms for training vocabularies, but one of them is the byte-pairing",
        "tokens": [
            50364,
            467,
            9346,
            257,
            1230,
            295,
            14642,
            337,
            3097,
            2329,
            455,
            1040,
            530,
            11,
            457,
            472,
            295,
            552,
            307,
            264,
            40846,
            12,
            79,
            1246,
            278,
            50616
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2611018717288971,
        "compression_ratio": 1.6631944179534912,
        "no_speech_prob": 0.05107470229268074
    },
    {
        "id": 304,
        "seek": 121756,
        "start": 1222.5999755859375,
        "end": 1224.47998046875,
        "text": " coding algorithm that we've been looking at.",
        "tokens": [
            50616,
            17720,
            9284,
            300,
            321,
            600,
            668,
            1237,
            412,
            13,
            50710
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2611018717288971,
        "compression_ratio": 1.6631944179534912,
        "no_speech_prob": 0.05107470229268074
    },
    {
        "id": 305,
        "seek": 121756,
        "start": 1224.47998046875,
        "end": 1226.43994140625,
        "text": " So it supports it.",
        "tokens": [
            50710,
            407,
            309,
            9346,
            309,
            13,
            50808
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2611018717288971,
        "compression_ratio": 1.6631944179534912,
        "no_speech_prob": 0.05107470229268074
    },
    {
        "id": 306,
        "seek": 121756,
        "start": 1226.43994140625,
        "end": 1232.3199462890625,
        "text": " Now, SentencePiece is used both by LLAMA and Mistral Series, and many other models as well.",
        "tokens": [
            50808,
            823,
            11,
            23652,
            655,
            47,
            46566,
            307,
            1143,
            1293,
            538,
            441,
            43,
            2865,
            32,
            293,
            20166,
            2155,
            13934,
            11,
            293,
            867,
            661,
            5245,
            382,
            731,
            13,
            51102
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2611018717288971,
        "compression_ratio": 1.6631944179534912,
        "no_speech_prob": 0.05107470229268074
    },
    {
        "id": 307,
        "seek": 121756,
        "start": 1232.3199462890625,
        "end": 1236.0400390625,
        "text": " It is on GitHub under google slash SentencePiece.",
        "tokens": [
            51102,
            467,
            307,
            322,
            23331,
            833,
            20742,
            17330,
            23652,
            655,
            47,
            46566,
            13,
            51288
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2611018717288971,
        "compression_ratio": 1.6631944179534912,
        "no_speech_prob": 0.05107470229268074
    },
    {
        "id": 308,
        "seek": 121756,
        "start": 1236.0400390625,
        "end": 1239.5999755859375,
        "text": " And the big difference with SentencePiece, and we're going to look at example, because",
        "tokens": [
            51288,
            400,
            264,
            955,
            2649,
            365,
            23652,
            655,
            47,
            46566,
            11,
            293,
            321,
            434,
            516,
            281,
            574,
            412,
            1365,
            11,
            570,
            51466
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2611018717288971,
        "compression_ratio": 1.6631944179534912,
        "no_speech_prob": 0.05107470229268074
    },
    {
        "id": 309,
        "seek": 121756,
        "start": 1239.5999755859375,
        "end": 1245.47998046875,
        "text": " this is kind of hard and subtle to explain, is that they think different about the order",
        "tokens": [
            51466,
            341,
            307,
            733,
            295,
            1152,
            293,
            13743,
            281,
            2903,
            11,
            307,
            300,
            436,
            519,
            819,
            466,
            264,
            1668,
            51760
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2611018717288971,
        "compression_ratio": 1.6631944179534912,
        "no_speech_prob": 0.05107470229268074
    },
    {
        "id": 310,
        "seek": 124548,
        "start": 1245.52001953125,
        "end": 1247.760009765625,
        "text": " of operations here.",
        "tokens": [
            50366,
            295,
            7705,
            510,
            13,
            50478
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22362148761749268,
        "compression_ratio": 1.6140351295471191,
        "no_speech_prob": 0.0008040837710723281
    },
    {
        "id": 311,
        "seek": 124548,
        "start": 1247.760009765625,
        "end": 1254.1199951171875,
        "text": " So in the case of TICToken, we first take our code points in the string, we encode them",
        "tokens": [
            50478,
            407,
            294,
            264,
            1389,
            295,
            314,
            2532,
            51,
            8406,
            11,
            321,
            700,
            747,
            527,
            3089,
            2793,
            294,
            264,
            6798,
            11,
            321,
            2058,
            1429,
            552,
            50796
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22362148761749268,
        "compression_ratio": 1.6140351295471191,
        "no_speech_prob": 0.0008040837710723281
    },
    {
        "id": 312,
        "seek": 124548,
        "start": 1254.1199951171875,
        "end": 1257.0799560546875,
        "text": " using UTF-8 to bytes, and then we're merging bytes.",
        "tokens": [
            50796,
            1228,
            624,
            20527,
            12,
            23,
            281,
            36088,
            11,
            293,
            550,
            321,
            434,
            44559,
            36088,
            13,
            50944
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22362148761749268,
        "compression_ratio": 1.6140351295471191,
        "no_speech_prob": 0.0008040837710723281
    },
    {
        "id": 313,
        "seek": 124548,
        "start": 1257.0799560546875,
        "end": 1259.3199462890625,
        "text": " It's fairly straightforward.",
        "tokens": [
            50944,
            467,
            311,
            6457,
            15325,
            13,
            51056
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22362148761749268,
        "compression_ratio": 1.6140351295471191,
        "no_speech_prob": 0.0008040837710723281
    },
    {
        "id": 314,
        "seek": 124548,
        "start": 1259.3199462890625,
        "end": 1265.0,
        "text": " For SentencePiece, it works directly on the level of the code points themselves.",
        "tokens": [
            51056,
            1171,
            23652,
            655,
            47,
            46566,
            11,
            309,
            1985,
            3838,
            322,
            264,
            1496,
            295,
            264,
            3089,
            2793,
            2969,
            13,
            51340
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22362148761749268,
        "compression_ratio": 1.6140351295471191,
        "no_speech_prob": 0.0008040837710723281
    },
    {
        "id": 315,
        "seek": 124548,
        "start": 1265.0,
        "end": 1269.3599853515625,
        "text": " So it looks at whatever code points are available in your training set, and then it starts merging",
        "tokens": [
            51340,
            407,
            309,
            1542,
            412,
            2035,
            3089,
            2793,
            366,
            2435,
            294,
            428,
            3097,
            992,
            11,
            293,
            550,
            309,
            3719,
            44559,
            51558
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22362148761749268,
        "compression_ratio": 1.6140351295471191,
        "no_speech_prob": 0.0008040837710723281
    },
    {
        "id": 316,
        "seek": 126936,
        "start": 1269.3599853515625,
        "end": 1271.0799560546875,
        "text": " those code points.",
        "tokens": [
            50364,
            729,
            3089,
            2793,
            13,
            50450
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2621585726737976,
        "compression_ratio": 1.7670682668685913,
        "no_speech_prob": 0.22812776267528534
    },
    {
        "id": 317,
        "seek": 126936,
        "start": 1271.0799560546875,
        "end": 1275.8399658203125,
        "text": " And the BPE is running on the level of code points.",
        "tokens": [
            50450,
            400,
            264,
            363,
            5208,
            307,
            2614,
            322,
            264,
            1496,
            295,
            3089,
            2793,
            13,
            50688
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2621585726737976,
        "compression_ratio": 1.7670682668685913,
        "no_speech_prob": 0.22812776267528534
    },
    {
        "id": 318,
        "seek": 126936,
        "start": 1275.8399658203125,
        "end": 1280.5400390625,
        "text": " And if you happen to run out of code points, so there are maybe some rare code points that",
        "tokens": [
            50688,
            400,
            498,
            291,
            1051,
            281,
            1190,
            484,
            295,
            3089,
            2793,
            11,
            370,
            456,
            366,
            1310,
            512,
            5892,
            3089,
            2793,
            300,
            50923
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2621585726737976,
        "compression_ratio": 1.7670682668685913,
        "no_speech_prob": 0.22812776267528534
    },
    {
        "id": 319,
        "seek": 126936,
        "start": 1280.5400390625,
        "end": 1285.47998046875,
        "text": " just don't come up too often, and the rarity is determined by this character coverage hyperparameter,",
        "tokens": [
            50923,
            445,
            500,
            380,
            808,
            493,
            886,
            2049,
            11,
            293,
            264,
            367,
            17409,
            307,
            9540,
            538,
            341,
            2517,
            9645,
            9848,
            2181,
            335,
            2398,
            11,
            51170
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2621585726737976,
        "compression_ratio": 1.7670682668685913,
        "no_speech_prob": 0.22812776267528534
    },
    {
        "id": 320,
        "seek": 126936,
        "start": 1285.47998046875,
        "end": 1292.1600341796875,
        "text": " then these code points will either get mapped to a special unknown token, like UNK, or if",
        "tokens": [
            51170,
            550,
            613,
            3089,
            2793,
            486,
            2139,
            483,
            33318,
            281,
            257,
            2121,
            9841,
            14862,
            11,
            411,
            8229,
            42,
            11,
            420,
            498,
            51504
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2621585726737976,
        "compression_ratio": 1.7670682668685913,
        "no_speech_prob": 0.22812776267528534
    },
    {
        "id": 321,
        "seek": 126936,
        "start": 1292.1600341796875,
        "end": 1296.800048828125,
        "text": " you have the byte fallback option turned on, then it will take those rare code points,",
        "tokens": [
            51504,
            291,
            362,
            264,
            40846,
            2100,
            3207,
            3614,
            3574,
            322,
            11,
            550,
            309,
            486,
            747,
            729,
            5892,
            3089,
            2793,
            11,
            51736
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2621585726737976,
        "compression_ratio": 1.7670682668685913,
        "no_speech_prob": 0.22812776267528534
    },
    {
        "id": 322,
        "seek": 129680,
        "start": 1296.800048828125,
        "end": 1300.8399658203125,
        "text": " it will encode them using UTF-8, and then the individual bytes of that encoding will",
        "tokens": [
            50364,
            309,
            486,
            2058,
            1429,
            552,
            1228,
            624,
            20527,
            12,
            23,
            11,
            293,
            550,
            264,
            2609,
            36088,
            295,
            300,
            43430,
            486,
            50566
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23291060328483582,
        "compression_ratio": 1.703832745552063,
        "no_speech_prob": 0.01798575557768345
    },
    {
        "id": 323,
        "seek": 129680,
        "start": 1300.8399658203125,
        "end": 1305.719970703125,
        "text": " be translated into tokens, and there are these special byte tokens that basically get added",
        "tokens": [
            50566,
            312,
            16805,
            666,
            22667,
            11,
            293,
            456,
            366,
            613,
            2121,
            40846,
            22667,
            300,
            1936,
            483,
            3869,
            50810
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23291060328483582,
        "compression_ratio": 1.703832745552063,
        "no_speech_prob": 0.01798575557768345
    },
    {
        "id": 324,
        "seek": 129680,
        "start": 1305.719970703125,
        "end": 1307.239990234375,
        "text": " to the vocabulary.",
        "tokens": [
            50810,
            281,
            264,
            19864,
            13,
            50886
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23291060328483582,
        "compression_ratio": 1.703832745552063,
        "no_speech_prob": 0.01798575557768345
    },
    {
        "id": 325,
        "seek": 129680,
        "start": 1307.239990234375,
        "end": 1315.9599609375,
        "text": " So it uses BPE on the code points, and then it falls back to bytes for rare code points.",
        "tokens": [
            50886,
            407,
            309,
            4960,
            363,
            5208,
            322,
            264,
            3089,
            2793,
            11,
            293,
            550,
            309,
            8804,
            646,
            281,
            36088,
            337,
            5892,
            3089,
            2793,
            13,
            51322
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23291060328483582,
        "compression_ratio": 1.703832745552063,
        "no_speech_prob": 0.01798575557768345
    },
    {
        "id": 326,
        "seek": 129680,
        "start": 1315.9599609375,
        "end": 1317.3599853515625,
        "text": " And so that's kind of like the difference.",
        "tokens": [
            51322,
            400,
            370,
            300,
            311,
            733,
            295,
            411,
            264,
            2649,
            13,
            51392
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23291060328483582,
        "compression_ratio": 1.703832745552063,
        "no_speech_prob": 0.01798575557768345
    },
    {
        "id": 327,
        "seek": 129680,
        "start": 1317.3599853515625,
        "end": 1321.4000244140625,
        "text": " Personally, I find the TICToken way significantly cleaner, but it's kind of like a subtle but",
        "tokens": [
            51392,
            21079,
            11,
            286,
            915,
            264,
            314,
            2532,
            51,
            8406,
            636,
            10591,
            16532,
            11,
            457,
            309,
            311,
            733,
            295,
            411,
            257,
            13743,
            457,
            51594
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23291060328483582,
        "compression_ratio": 1.703832745552063,
        "no_speech_prob": 0.01798575557768345
    },
    {
        "id": 328,
        "seek": 129680,
        "start": 1321.4000244140625,
        "end": 1324.6800537109375,
        "text": " pretty major difference between the way they approach tokenization.",
        "tokens": [
            51594,
            1238,
            2563,
            2649,
            1296,
            264,
            636,
            436,
            3109,
            14862,
            2144,
            13,
            51758
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23291060328483582,
        "compression_ratio": 1.703832745552063,
        "no_speech_prob": 0.01798575557768345
    },
    {
        "id": 329,
        "seek": 132468,
        "start": 1324.6800537109375,
        "end": 1329.56005859375,
        "text": " Let's work with a concrete example, because otherwise this is kind of hard to get your",
        "tokens": [
            50364,
            961,
            311,
            589,
            365,
            257,
            9859,
            1365,
            11,
            570,
            5911,
            341,
            307,
            733,
            295,
            1152,
            281,
            483,
            428,
            50608
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23413045704364777,
        "compression_ratio": 1.7686567306518555,
        "no_speech_prob": 0.5388270616531372
    },
    {
        "id": 330,
        "seek": 132468,
        "start": 1329.56005859375,
        "end": 1331.0400390625,
        "text": " head around.",
        "tokens": [
            50608,
            1378,
            926,
            13,
            50682
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23413045704364777,
        "compression_ratio": 1.7686567306518555,
        "no_speech_prob": 0.5388270616531372
    },
    {
        "id": 331,
        "seek": 132468,
        "start": 1331.0400390625,
        "end": 1333.3599853515625,
        "text": " So let's work with a concrete example.",
        "tokens": [
            50682,
            407,
            718,
            311,
            589,
            365,
            257,
            9859,
            1365,
            13,
            50798
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23413045704364777,
        "compression_ratio": 1.7686567306518555,
        "no_speech_prob": 0.5388270616531372
    },
    {
        "id": 332,
        "seek": 132468,
        "start": 1333.3599853515625,
        "end": 1336.0,
        "text": " This is how we can import SentencePiece.",
        "tokens": [
            50798,
            639,
            307,
            577,
            321,
            393,
            974,
            23652,
            655,
            47,
            46566,
            13,
            50930
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23413045704364777,
        "compression_ratio": 1.7686567306518555,
        "no_speech_prob": 0.5388270616531372
    },
    {
        "id": 333,
        "seek": 132468,
        "start": 1336.0,
        "end": 1339.6400146484375,
        "text": " And then here we're going to take, I think I took the description of SentencePiece, and",
        "tokens": [
            50930,
            400,
            550,
            510,
            321,
            434,
            516,
            281,
            747,
            11,
            286,
            519,
            286,
            1890,
            264,
            3855,
            295,
            23652,
            655,
            47,
            46566,
            11,
            293,
            51112
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23413045704364777,
        "compression_ratio": 1.7686567306518555,
        "no_speech_prob": 0.5388270616531372
    },
    {
        "id": 334,
        "seek": 132468,
        "start": 1339.6400146484375,
        "end": 1341.8399658203125,
        "text": " I just created a little toy dataset.",
        "tokens": [
            51112,
            286,
            445,
            2942,
            257,
            707,
            12058,
            28872,
            13,
            51222
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23413045704364777,
        "compression_ratio": 1.7686567306518555,
        "no_speech_prob": 0.5388270616531372
    },
    {
        "id": 335,
        "seek": 132468,
        "start": 1341.8399658203125,
        "end": 1347.239990234375,
        "text": " It really likes to have a file, so I created a toy.txt file with this content.",
        "tokens": [
            51222,
            467,
            534,
            5902,
            281,
            362,
            257,
            3991,
            11,
            370,
            286,
            2942,
            257,
            12058,
            13,
            83,
            734,
            3991,
            365,
            341,
            2701,
            13,
            51492
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23413045704364777,
        "compression_ratio": 1.7686567306518555,
        "no_speech_prob": 0.5388270616531372
    },
    {
        "id": 336,
        "seek": 132468,
        "start": 1347.239990234375,
        "end": 1351.280029296875,
        "text": " Now what's kind of a little bit crazy about SentencePiece is that there's a ton of options",
        "tokens": [
            51492,
            823,
            437,
            311,
            733,
            295,
            257,
            707,
            857,
            3219,
            466,
            23652,
            655,
            47,
            46566,
            307,
            300,
            456,
            311,
            257,
            2952,
            295,
            3956,
            51694
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23413045704364777,
        "compression_ratio": 1.7686567306518555,
        "no_speech_prob": 0.5388270616531372
    },
    {
        "id": 337,
        "seek": 135128,
        "start": 1351.280029296875,
        "end": 1352.9599609375,
        "text": " and configurations.",
        "tokens": [
            50364,
            293,
            31493,
            13,
            50448
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21906617283821106,
        "compression_ratio": 1.73308265209198,
        "no_speech_prob": 0.06278743594884872
    },
    {
        "id": 338,
        "seek": 135128,
        "start": 1352.9599609375,
        "end": 1356.760009765625,
        "text": " And the reason this is so is because SentencePiece has been around, I think, for a while, and",
        "tokens": [
            50448,
            400,
            264,
            1778,
            341,
            307,
            370,
            307,
            570,
            23652,
            655,
            47,
            46566,
            575,
            668,
            926,
            11,
            286,
            519,
            11,
            337,
            257,
            1339,
            11,
            293,
            50638
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21906617283821106,
        "compression_ratio": 1.73308265209198,
        "no_speech_prob": 0.06278743594884872
    },
    {
        "id": 339,
        "seek": 135128,
        "start": 1356.760009765625,
        "end": 1360.0,
        "text": " it really tries to handle a large diversity of things.",
        "tokens": [
            50638,
            309,
            534,
            9898,
            281,
            4813,
            257,
            2416,
            8811,
            295,
            721,
            13,
            50800
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21906617283821106,
        "compression_ratio": 1.73308265209198,
        "no_speech_prob": 0.06278743594884872
    },
    {
        "id": 340,
        "seek": 135128,
        "start": 1360.0,
        "end": 1364.6800537109375,
        "text": " And because it's been around, I think it has quite a bit of accumulated historical baggage",
        "tokens": [
            50800,
            400,
            570,
            309,
            311,
            668,
            926,
            11,
            286,
            519,
            309,
            575,
            1596,
            257,
            857,
            295,
            31346,
            8584,
            41567,
            51034
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21906617283821106,
        "compression_ratio": 1.73308265209198,
        "no_speech_prob": 0.06278743594884872
    },
    {
        "id": 341,
        "seek": 135128,
        "start": 1364.6800537109375,
        "end": 1366.280029296875,
        "text": " as well.",
        "tokens": [
            51034,
            382,
            731,
            13,
            51114
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21906617283821106,
        "compression_ratio": 1.73308265209198,
        "no_speech_prob": 0.06278743594884872
    },
    {
        "id": 342,
        "seek": 135128,
        "start": 1366.280029296875,
        "end": 1369.5999755859375,
        "text": " And so in particular, there's like a ton of configuration arguments.",
        "tokens": [
            51114,
            400,
            370,
            294,
            1729,
            11,
            456,
            311,
            411,
            257,
            2952,
            295,
            11694,
            12869,
            13,
            51280
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21906617283821106,
        "compression_ratio": 1.73308265209198,
        "no_speech_prob": 0.06278743594884872
    },
    {
        "id": 343,
        "seek": 135128,
        "start": 1369.5999755859375,
        "end": 1371.43994140625,
        "text": " This is not even all of it.",
        "tokens": [
            51280,
            639,
            307,
            406,
            754,
            439,
            295,
            309,
            13,
            51372
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21906617283821106,
        "compression_ratio": 1.73308265209198,
        "no_speech_prob": 0.06278743594884872
    },
    {
        "id": 344,
        "seek": 135128,
        "start": 1371.43994140625,
        "end": 1378.47998046875,
        "text": " You can go to here to see all the training options, and there's also quite useful documentation",
        "tokens": [
            51372,
            509,
            393,
            352,
            281,
            510,
            281,
            536,
            439,
            264,
            3097,
            3956,
            11,
            293,
            456,
            311,
            611,
            1596,
            4420,
            14333,
            51724
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21906617283821106,
        "compression_ratio": 1.73308265209198,
        "no_speech_prob": 0.06278743594884872
    },
    {
        "id": 345,
        "seek": 137848,
        "start": 1378.47998046875,
        "end": 1383.4000244140625,
        "text": " when you look at the raw protobuf that is used to represent the trainer spec, and so",
        "tokens": [
            50364,
            562,
            291,
            574,
            412,
            264,
            8936,
            1742,
            996,
            2947,
            300,
            307,
            1143,
            281,
            2906,
            264,
            21110,
            1608,
            11,
            293,
            370,
            50610
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25431910157203674,
        "compression_ratio": 1.758754849433899,
        "no_speech_prob": 0.021286772564053535
    },
    {
        "id": 346,
        "seek": 137848,
        "start": 1383.4000244140625,
        "end": 1385.760009765625,
        "text": " on.",
        "tokens": [
            50610,
            322,
            13,
            50728
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25431910157203674,
        "compression_ratio": 1.758754849433899,
        "no_speech_prob": 0.021286772564053535
    },
    {
        "id": 347,
        "seek": 137848,
        "start": 1385.760009765625,
        "end": 1388.0,
        "text": " Many of these options are irrelevant to us.",
        "tokens": [
            50728,
            5126,
            295,
            613,
            3956,
            366,
            28682,
            281,
            505,
            13,
            50840
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25431910157203674,
        "compression_ratio": 1.758754849433899,
        "no_speech_prob": 0.021286772564053535
    },
    {
        "id": 348,
        "seek": 137848,
        "start": 1388.0,
        "end": 1393.280029296875,
        "text": " So maybe to point out one example, dash-dash shrinking factor, this shrinking factor is",
        "tokens": [
            50840,
            407,
            1310,
            281,
            935,
            484,
            472,
            1365,
            11,
            8240,
            12,
            67,
            1299,
            41684,
            5952,
            11,
            341,
            41684,
            5952,
            307,
            51104
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25431910157203674,
        "compression_ratio": 1.758754849433899,
        "no_speech_prob": 0.021286772564053535
    },
    {
        "id": 349,
        "seek": 137848,
        "start": 1393.280029296875,
        "end": 1395.43994140625,
        "text": " not used in the byte-pair encoding algorithm.",
        "tokens": [
            51104,
            406,
            1143,
            294,
            264,
            40846,
            12,
            79,
            1246,
            43430,
            9284,
            13,
            51212
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25431910157203674,
        "compression_ratio": 1.758754849433899,
        "no_speech_prob": 0.021286772564053535
    },
    {
        "id": 350,
        "seek": 137848,
        "start": 1395.43994140625,
        "end": 1399.0400390625,
        "text": " So this is just an argument that is irrelevant to us.",
        "tokens": [
            51212,
            407,
            341,
            307,
            445,
            364,
            6770,
            300,
            307,
            28682,
            281,
            505,
            13,
            51392
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25431910157203674,
        "compression_ratio": 1.758754849433899,
        "no_speech_prob": 0.021286772564053535
    },
    {
        "id": 351,
        "seek": 137848,
        "start": 1399.0400390625,
        "end": 1403.6800537109375,
        "text": " It applies to a different training algorithm.",
        "tokens": [
            51392,
            467,
            13165,
            281,
            257,
            819,
            3097,
            9284,
            13,
            51624
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25431910157203674,
        "compression_ratio": 1.758754849433899,
        "no_speech_prob": 0.021286772564053535
    },
    {
        "id": 352,
        "seek": 137848,
        "start": 1403.6800537109375,
        "end": 1407.9599609375,
        "text": " Now what I tried to do here is I tried to set up SentencePiece in a way that is very,",
        "tokens": [
            51624,
            823,
            437,
            286,
            3031,
            281,
            360,
            510,
            307,
            286,
            3031,
            281,
            992,
            493,
            23652,
            655,
            47,
            46566,
            294,
            257,
            636,
            300,
            307,
            588,
            11,
            51838
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25431910157203674,
        "compression_ratio": 1.758754849433899,
        "no_speech_prob": 0.021286772564053535
    },
    {
        "id": 353,
        "seek": 140796,
        "start": 1407.9599609375,
        "end": 1413.8399658203125,
        "text": " very similar, as far as I can tell, to maybe identical, hopefully, to the way that Llama2",
        "tokens": [
            50364,
            588,
            2531,
            11,
            382,
            1400,
            382,
            286,
            393,
            980,
            11,
            281,
            1310,
            14800,
            11,
            4696,
            11,
            281,
            264,
            636,
            300,
            32717,
            2404,
            17,
            50658
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2534138560295105,
        "compression_ratio": 1.7862902879714966,
        "no_speech_prob": 0.005384861957281828
    },
    {
        "id": 354,
        "seek": 140796,
        "start": 1413.8399658203125,
        "end": 1419.52001953125,
        "text": " was trained, so the way they trained their own tokenizer.",
        "tokens": [
            50658,
            390,
            8895,
            11,
            370,
            264,
            636,
            436,
            8895,
            641,
            1065,
            14862,
            6545,
            13,
            50942
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2534138560295105,
        "compression_ratio": 1.7862902879714966,
        "no_speech_prob": 0.005384861957281828
    },
    {
        "id": 355,
        "seek": 140796,
        "start": 1419.52001953125,
        "end": 1424.4000244140625,
        "text": " And the way I did this was basically you can take the tokenizer.model file that Meta released,",
        "tokens": [
            50942,
            400,
            264,
            636,
            286,
            630,
            341,
            390,
            1936,
            291,
            393,
            747,
            264,
            14862,
            6545,
            13,
            8014,
            338,
            3991,
            300,
            6377,
            64,
            4736,
            11,
            51186
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2534138560295105,
        "compression_ratio": 1.7862902879714966,
        "no_speech_prob": 0.005384861957281828
    },
    {
        "id": 356,
        "seek": 140796,
        "start": 1424.4000244140625,
        "end": 1432.3599853515625,
        "text": " and you can open it using the protobuf file that you can generate, and then you can inspect",
        "tokens": [
            51186,
            293,
            291,
            393,
            1269,
            309,
            1228,
            264,
            1742,
            996,
            2947,
            3991,
            300,
            291,
            393,
            8460,
            11,
            293,
            550,
            291,
            393,
            15018,
            51584
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2534138560295105,
        "compression_ratio": 1.7862902879714966,
        "no_speech_prob": 0.005384861957281828
    },
    {
        "id": 357,
        "seek": 140796,
        "start": 1432.3599853515625,
        "end": 1436.1600341796875,
        "text": " all the options, and I tried to copy over all the options that looked relevant.",
        "tokens": [
            51584,
            439,
            264,
            3956,
            11,
            293,
            286,
            3031,
            281,
            5055,
            670,
            439,
            264,
            3956,
            300,
            2956,
            7340,
            13,
            51774
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2534138560295105,
        "compression_ratio": 1.7862902879714966,
        "no_speech_prob": 0.005384861957281828
    },
    {
        "id": 358,
        "seek": 140796,
        "start": 1436.1600341796875,
        "end": 1437.8399658203125,
        "text": " So here we set up the input.",
        "tokens": [
            51774,
            407,
            510,
            321,
            992,
            493,
            264,
            4846,
            13,
            51858
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2534138560295105,
        "compression_ratio": 1.7862902879714966,
        "no_speech_prob": 0.005384861957281828
    },
    {
        "id": 359,
        "seek": 143784,
        "start": 1437.8399658203125,
        "end": 1440.0,
        "text": " It's raw text in this file.",
        "tokens": [
            50364,
            467,
            311,
            8936,
            2487,
            294,
            341,
            3991,
            13,
            50472
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24707449972629547,
        "compression_ratio": 1.6239999532699585,
        "no_speech_prob": 0.005220026709139347
    },
    {
        "id": 360,
        "seek": 143784,
        "start": 1440.0,
        "end": 1445.8399658203125,
        "text": " Here's going to be the output, so it's going to be protoc400.model and .vocap.",
        "tokens": [
            50472,
            1692,
            311,
            516,
            281,
            312,
            264,
            5598,
            11,
            370,
            309,
            311,
            516,
            281,
            312,
            1742,
            905,
            13741,
            13,
            8014,
            338,
            293,
            2411,
            20836,
            569,
            13,
            50764
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24707449972629547,
        "compression_ratio": 1.6239999532699585,
        "no_speech_prob": 0.005220026709139347
    },
    {
        "id": 361,
        "seek": 143784,
        "start": 1445.8399658203125,
        "end": 1450.4000244140625,
        "text": " We're saying that we're going to use the BP algorithm, and we want to vocap size of 400.",
        "tokens": [
            50764,
            492,
            434,
            1566,
            300,
            321,
            434,
            516,
            281,
            764,
            264,
            40533,
            9284,
            11,
            293,
            321,
            528,
            281,
            2329,
            569,
            2744,
            295,
            8423,
            13,
            50992
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24707449972629547,
        "compression_ratio": 1.6239999532699585,
        "no_speech_prob": 0.005220026709139347
    },
    {
        "id": 362,
        "seek": 143784,
        "start": 1450.4000244140625,
        "end": 1458.9599609375,
        "text": " And there's a ton of configurations here for basically preprocessing and normalization",
        "tokens": [
            50992,
            400,
            456,
            311,
            257,
            2952,
            295,
            31493,
            510,
            337,
            1936,
            2666,
            340,
            780,
            278,
            293,
            2710,
            2144,
            51420
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24707449972629547,
        "compression_ratio": 1.6239999532699585,
        "no_speech_prob": 0.005220026709139347
    },
    {
        "id": 363,
        "seek": 143784,
        "start": 1458.9599609375,
        "end": 1460.9599609375,
        "text": " rules, as they're called.",
        "tokens": [
            51420,
            4474,
            11,
            382,
            436,
            434,
            1219,
            13,
            51520
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24707449972629547,
        "compression_ratio": 1.6239999532699585,
        "no_speech_prob": 0.005220026709139347
    },
    {
        "id": 364,
        "seek": 143784,
        "start": 1460.9599609375,
        "end": 1464.56005859375,
        "text": " Normalization used to be very prevalent, I would say, before LLMs in natural language",
        "tokens": [
            51520,
            21277,
            2144,
            1143,
            281,
            312,
            588,
            30652,
            11,
            286,
            576,
            584,
            11,
            949,
            441,
            43,
            26386,
            294,
            3303,
            2856,
            51700
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24707449972629547,
        "compression_ratio": 1.6239999532699585,
        "no_speech_prob": 0.005220026709139347
    },
    {
        "id": 365,
        "seek": 143784,
        "start": 1464.56005859375,
        "end": 1465.56005859375,
        "text": " processing.",
        "tokens": [
            51700,
            9007,
            13,
            51750
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24707449972629547,
        "compression_ratio": 1.6239999532699585,
        "no_speech_prob": 0.005220026709139347
    },
    {
        "id": 366,
        "seek": 146556,
        "start": 1465.56005859375,
        "end": 1470.239990234375,
        "text": " So in machine translation and text classification and so on, you want to normalize and simplify",
        "tokens": [
            50364,
            407,
            294,
            3479,
            12853,
            293,
            2487,
            21538,
            293,
            370,
            322,
            11,
            291,
            528,
            281,
            2710,
            1125,
            293,
            20460,
            50598
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22149492800235748,
        "compression_ratio": 1.8193548917770386,
        "no_speech_prob": 0.03963732346892357
    },
    {
        "id": 367,
        "seek": 146556,
        "start": 1470.239990234375,
        "end": 1474.3199462890625,
        "text": " the text, and you want to turn it all lowercase, and you want to remove all double whitespace,",
        "tokens": [
            50598,
            264,
            2487,
            11,
            293,
            291,
            528,
            281,
            1261,
            309,
            439,
            3126,
            9765,
            11,
            293,
            291,
            528,
            281,
            4159,
            439,
            3834,
            21909,
            17940,
            11,
            50802
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22149492800235748,
        "compression_ratio": 1.8193548917770386,
        "no_speech_prob": 0.03963732346892357
    },
    {
        "id": 368,
        "seek": 146556,
        "start": 1474.3199462890625,
        "end": 1475.52001953125,
        "text": " et cetera.",
        "tokens": [
            50802,
            1030,
            11458,
            13,
            50862
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22149492800235748,
        "compression_ratio": 1.8193548917770386,
        "no_speech_prob": 0.03963732346892357
    },
    {
        "id": 369,
        "seek": 146556,
        "start": 1475.52001953125,
        "end": 1478.760009765625,
        "text": " And in language models, we prefer not to do any of it, or at least that is my preference",
        "tokens": [
            50862,
            400,
            294,
            2856,
            5245,
            11,
            321,
            4382,
            406,
            281,
            360,
            604,
            295,
            309,
            11,
            420,
            412,
            1935,
            300,
            307,
            452,
            17502,
            51024
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22149492800235748,
        "compression_ratio": 1.8193548917770386,
        "no_speech_prob": 0.03963732346892357
    },
    {
        "id": 370,
        "seek": 146556,
        "start": 1478.760009765625,
        "end": 1480.1199951171875,
        "text": " as a deep learning person.",
        "tokens": [
            51024,
            382,
            257,
            2452,
            2539,
            954,
            13,
            51092
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22149492800235748,
        "compression_ratio": 1.8193548917770386,
        "no_speech_prob": 0.03963732346892357
    },
    {
        "id": 371,
        "seek": 146556,
        "start": 1480.1199951171875,
        "end": 1481.56005859375,
        "text": " You want to not touch your data.",
        "tokens": [
            51092,
            509,
            528,
            281,
            406,
            2557,
            428,
            1412,
            13,
            51164
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22149492800235748,
        "compression_ratio": 1.8193548917770386,
        "no_speech_prob": 0.03963732346892357
    },
    {
        "id": 372,
        "seek": 146556,
        "start": 1481.56005859375,
        "end": 1487.43994140625,
        "text": " You want to keep the raw data as much as possible in a raw form.",
        "tokens": [
            51164,
            509,
            528,
            281,
            1066,
            264,
            8936,
            1412,
            382,
            709,
            382,
            1944,
            294,
            257,
            8936,
            1254,
            13,
            51458
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22149492800235748,
        "compression_ratio": 1.8193548917770386,
        "no_speech_prob": 0.03963732346892357
    },
    {
        "id": 373,
        "seek": 146556,
        "start": 1487.43994140625,
        "end": 1490.9599609375,
        "text": " So you're basically trying to turn off a lot of this, if you can.",
        "tokens": [
            51458,
            407,
            291,
            434,
            1936,
            1382,
            281,
            1261,
            766,
            257,
            688,
            295,
            341,
            11,
            498,
            291,
            393,
            13,
            51634
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22149492800235748,
        "compression_ratio": 1.8193548917770386,
        "no_speech_prob": 0.03963732346892357
    },
    {
        "id": 374,
        "seek": 146556,
        "start": 1490.9599609375,
        "end": 1494.9200439453125,
        "text": " The other thing that sentence piece does is that it has this concept of sentences.",
        "tokens": [
            51634,
            440,
            661,
            551,
            300,
            8174,
            2522,
            775,
            307,
            300,
            309,
            575,
            341,
            3410,
            295,
            16579,
            13,
            51832
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22149492800235748,
        "compression_ratio": 1.8193548917770386,
        "no_speech_prob": 0.03963732346892357
    },
    {
        "id": 375,
        "seek": 149492,
        "start": 1494.9200439453125,
        "end": 1502.760009765625,
        "text": " So sentence piece, it kind of was developed, I think, early in the days where there was",
        "tokens": [
            50364,
            407,
            8174,
            2522,
            11,
            309,
            733,
            295,
            390,
            4743,
            11,
            286,
            519,
            11,
            2440,
            294,
            264,
            1708,
            689,
            456,
            390,
            50756
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2916666567325592,
        "compression_ratio": 1.7116104364395142,
        "no_speech_prob": 0.0007672898354940116
    },
    {
        "id": 376,
        "seek": 149492,
        "start": 1502.760009765625,
        "end": 1506.97998046875,
        "text": " an idea that you're training a tokenizer on a bunch of independent sentences.",
        "tokens": [
            50756,
            364,
            1558,
            300,
            291,
            434,
            3097,
            257,
            14862,
            6545,
            322,
            257,
            3840,
            295,
            6695,
            16579,
            13,
            50967
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2916666567325592,
        "compression_ratio": 1.7116104364395142,
        "no_speech_prob": 0.0007672898354940116
    },
    {
        "id": 377,
        "seek": 149492,
        "start": 1506.97998046875,
        "end": 1511.6400146484375,
        "text": " So it has a lot of how many sentences you're going to train on, what is the maximum sentence",
        "tokens": [
            50967,
            407,
            309,
            575,
            257,
            688,
            295,
            577,
            867,
            16579,
            291,
            434,
            516,
            281,
            3847,
            322,
            11,
            437,
            307,
            264,
            6674,
            8174,
            51200
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2916666567325592,
        "compression_ratio": 1.7116104364395142,
        "no_speech_prob": 0.0007672898354940116
    },
    {
        "id": 378,
        "seek": 149492,
        "start": 1511.6400146484375,
        "end": 1518.3199462890625,
        "text": " length, shuffling sentences, and so for it, sentences are kind of like the individual",
        "tokens": [
            51200,
            4641,
            11,
            402,
            1245,
            1688,
            16579,
            11,
            293,
            370,
            337,
            309,
            11,
            16579,
            366,
            733,
            295,
            411,
            264,
            2609,
            51534
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2916666567325592,
        "compression_ratio": 1.7116104364395142,
        "no_speech_prob": 0.0007672898354940116
    },
    {
        "id": 379,
        "seek": 149492,
        "start": 1518.3199462890625,
        "end": 1519.3199462890625,
        "text": " training examples.",
        "tokens": [
            51534,
            3097,
            5110,
            13,
            51584
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2916666567325592,
        "compression_ratio": 1.7116104364395142,
        "no_speech_prob": 0.0007672898354940116
    },
    {
        "id": 380,
        "seek": 149492,
        "start": 1519.3199462890625,
        "end": 1524.52001953125,
        "text": " But again, in the context of LLMs, I find that this is a very spurious and weird distinction.",
        "tokens": [
            51584,
            583,
            797,
            11,
            294,
            264,
            4319,
            295,
            441,
            43,
            26386,
            11,
            286,
            915,
            300,
            341,
            307,
            257,
            588,
            637,
            24274,
            293,
            3657,
            16844,
            13,
            51844
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2916666567325592,
        "compression_ratio": 1.7116104364395142,
        "no_speech_prob": 0.0007672898354940116
    },
    {
        "id": 381,
        "seek": 152452,
        "start": 1525.52001953125,
        "end": 1528.5999755859375,
        "text": " Sentences are just like, don't touch the raw data.",
        "tokens": [
            50414,
            23652,
            2667,
            366,
            445,
            411,
            11,
            500,
            380,
            2557,
            264,
            8936,
            1412,
            13,
            50568
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27188992500305176,
        "compression_ratio": 1.7175573110580444,
        "no_speech_prob": 0.0028009098023176193
    },
    {
        "id": 382,
        "seek": 152452,
        "start": 1528.5999755859375,
        "end": 1530.1199951171875,
        "text": " Sentences happen to exist.",
        "tokens": [
            50568,
            23652,
            2667,
            1051,
            281,
            2514,
            13,
            50644
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27188992500305176,
        "compression_ratio": 1.7175573110580444,
        "no_speech_prob": 0.0028009098023176193
    },
    {
        "id": 383,
        "seek": 152452,
        "start": 1530.1199951171875,
        "end": 1535.0400390625,
        "text": " But in the raw datasets, there are a lot of in-betweens, like what exactly is a sentence?",
        "tokens": [
            50644,
            583,
            294,
            264,
            8936,
            42856,
            11,
            456,
            366,
            257,
            688,
            295,
            294,
            12,
            10671,
            826,
            694,
            11,
            411,
            437,
            2293,
            307,
            257,
            8174,
            30,
            50890
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27188992500305176,
        "compression_ratio": 1.7175573110580444,
        "no_speech_prob": 0.0028009098023176193
    },
    {
        "id": 384,
        "seek": 152452,
        "start": 1535.0400390625,
        "end": 1537.43994140625,
        "text": " What isn't a sentence?",
        "tokens": [
            50890,
            708,
            1943,
            380,
            257,
            8174,
            30,
            51010
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27188992500305176,
        "compression_ratio": 1.7175573110580444,
        "no_speech_prob": 0.0028009098023176193
    },
    {
        "id": 385,
        "seek": 152452,
        "start": 1537.43994140625,
        "end": 1541.43994140625,
        "text": " And so I think it's really hard to define what an actual sentence is if you really dig",
        "tokens": [
            51010,
            400,
            370,
            286,
            519,
            309,
            311,
            534,
            1152,
            281,
            6964,
            437,
            364,
            3539,
            8174,
            307,
            498,
            291,
            534,
            2528,
            51210
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27188992500305176,
        "compression_ratio": 1.7175573110580444,
        "no_speech_prob": 0.0028009098023176193
    },
    {
        "id": 386,
        "seek": 152452,
        "start": 1541.43994140625,
        "end": 1545.47998046875,
        "text": " into it, and there could be different concepts of it in different languages or something",
        "tokens": [
            51210,
            666,
            309,
            11,
            293,
            456,
            727,
            312,
            819,
            10392,
            295,
            309,
            294,
            819,
            8650,
            420,
            746,
            51412
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27188992500305176,
        "compression_ratio": 1.7175573110580444,
        "no_speech_prob": 0.0028009098023176193
    },
    {
        "id": 387,
        "seek": 152452,
        "start": 1545.47998046875,
        "end": 1546.47998046875,
        "text": " like that.",
        "tokens": [
            51412,
            411,
            300,
            13,
            51462
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27188992500305176,
        "compression_ratio": 1.7175573110580444,
        "no_speech_prob": 0.0028009098023176193
    },
    {
        "id": 388,
        "seek": 152452,
        "start": 1546.47998046875,
        "end": 1547.8800048828125,
        "text": " So why even introduce the concept?",
        "tokens": [
            51462,
            407,
            983,
            754,
            5366,
            264,
            3410,
            30,
            51532
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27188992500305176,
        "compression_ratio": 1.7175573110580444,
        "no_speech_prob": 0.0028009098023176193
    },
    {
        "id": 389,
        "seek": 152452,
        "start": 1547.8800048828125,
        "end": 1549.6800537109375,
        "text": " It doesn't honestly make sense to me.",
        "tokens": [
            51532,
            467,
            1177,
            380,
            6095,
            652,
            2020,
            281,
            385,
            13,
            51622
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27188992500305176,
        "compression_ratio": 1.7175573110580444,
        "no_speech_prob": 0.0028009098023176193
    },
    {
        "id": 390,
        "seek": 154968,
        "start": 1549.6800537109375,
        "end": 1554.5999755859375,
        "text": " I would just prefer to treat a file as a giant stream of bytes.",
        "tokens": [
            50364,
            286,
            576,
            445,
            4382,
            281,
            2387,
            257,
            3991,
            382,
            257,
            7410,
            4309,
            295,
            36088,
            13,
            50610
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25270670652389526,
        "compression_ratio": 1.6462584733963013,
        "no_speech_prob": 0.3521304428577423
    },
    {
        "id": 391,
        "seek": 154968,
        "start": 1554.5999755859375,
        "end": 1558.719970703125,
        "text": " It has a lot of treatment around rare word characters, and when I say word, I mean code",
        "tokens": [
            50610,
            467,
            575,
            257,
            688,
            295,
            5032,
            926,
            5892,
            1349,
            4342,
            11,
            293,
            562,
            286,
            584,
            1349,
            11,
            286,
            914,
            3089,
            50816
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25270670652389526,
        "compression_ratio": 1.6462584733963013,
        "no_speech_prob": 0.3521304428577423
    },
    {
        "id": 392,
        "seek": 154968,
        "start": 1558.719970703125,
        "end": 1559.719970703125,
        "text": " points.",
        "tokens": [
            50816,
            2793,
            13,
            50866
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25270670652389526,
        "compression_ratio": 1.6462584733963013,
        "no_speech_prob": 0.3521304428577423
    },
    {
        "id": 393,
        "seek": 154968,
        "start": 1559.719970703125,
        "end": 1561.5400390625,
        "text": " We're going to come back to this in a second.",
        "tokens": [
            50866,
            492,
            434,
            516,
            281,
            808,
            646,
            281,
            341,
            294,
            257,
            1150,
            13,
            50957
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25270670652389526,
        "compression_ratio": 1.6462584733963013,
        "no_speech_prob": 0.3521304428577423
    },
    {
        "id": 394,
        "seek": 154968,
        "start": 1561.5400390625,
        "end": 1567.800048828125,
        "text": " And it has a lot of other rules for basically splitting digits, splitting white space and",
        "tokens": [
            50957,
            400,
            309,
            575,
            257,
            688,
            295,
            661,
            4474,
            337,
            1936,
            30348,
            27011,
            11,
            30348,
            2418,
            1901,
            293,
            51270
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25270670652389526,
        "compression_ratio": 1.6462584733963013,
        "no_speech_prob": 0.3521304428577423
    },
    {
        "id": 395,
        "seek": 154968,
        "start": 1567.800048828125,
        "end": 1569.760009765625,
        "text": " numbers and how you deal with that.",
        "tokens": [
            51270,
            3547,
            293,
            577,
            291,
            2028,
            365,
            300,
            13,
            51368
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25270670652389526,
        "compression_ratio": 1.6462584733963013,
        "no_speech_prob": 0.3521304428577423
    },
    {
        "id": 396,
        "seek": 154968,
        "start": 1569.760009765625,
        "end": 1572.3199462890625,
        "text": " So these are some kind of like merge rules.",
        "tokens": [
            51368,
            407,
            613,
            366,
            512,
            733,
            295,
            411,
            22183,
            4474,
            13,
            51496
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25270670652389526,
        "compression_ratio": 1.6462584733963013,
        "no_speech_prob": 0.3521304428577423
    },
    {
        "id": 397,
        "seek": 154968,
        "start": 1572.3199462890625,
        "end": 1576.8399658203125,
        "text": " So I think this is a little bit equivalent to TickToken using the regular expression",
        "tokens": [
            51496,
            407,
            286,
            519,
            341,
            307,
            257,
            707,
            857,
            10344,
            281,
            314,
            618,
            51,
            8406,
            1228,
            264,
            3890,
            6114,
            51722
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25270670652389526,
        "compression_ratio": 1.6462584733963013,
        "no_speech_prob": 0.3521304428577423
    },
    {
        "id": 398,
        "seek": 154968,
        "start": 1576.8399658203125,
        "end": 1578.8399658203125,
        "text": " to split up categories.",
        "tokens": [
            51722,
            281,
            7472,
            493,
            10479,
            13,
            51822
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25270670652389526,
        "compression_ratio": 1.6462584733963013,
        "no_speech_prob": 0.3521304428577423
    },
    {
        "id": 399,
        "seek": 157884,
        "start": 1579.0,
        "end": 1583.280029296875,
        "text": " There's kind of an equivalence of it, if you squint at it, in SentenceBees, where you can",
        "tokens": [
            50372,
            821,
            311,
            733,
            295,
            364,
            9052,
            655,
            295,
            309,
            11,
            498,
            291,
            2339,
            686,
            412,
            309,
            11,
            294,
            23652,
            655,
            33,
            4031,
            11,
            689,
            291,
            393,
            50586
        ],
        "temperature": 0.0,
        "avg_logprob": -0.30374306440353394,
        "compression_ratio": 1.6078431606292725,
        "no_speech_prob": 0.03258789703249931
    },
    {
        "id": 400,
        "seek": 157884,
        "start": 1583.280029296875,
        "end": 1590.1600341796875,
        "text": " also, for example, split up the digits and so on.",
        "tokens": [
            50586,
            611,
            11,
            337,
            1365,
            11,
            7472,
            493,
            264,
            27011,
            293,
            370,
            322,
            13,
            50930
        ],
        "temperature": 0.0,
        "avg_logprob": -0.30374306440353394,
        "compression_ratio": 1.6078431606292725,
        "no_speech_prob": 0.03258789703249931
    },
    {
        "id": 401,
        "seek": 157884,
        "start": 1590.1600341796875,
        "end": 1592.800048828125,
        "text": " There's a few more things here that I'll come back to in a bit, and then there are some",
        "tokens": [
            50930,
            821,
            311,
            257,
            1326,
            544,
            721,
            510,
            300,
            286,
            603,
            808,
            646,
            281,
            294,
            257,
            857,
            11,
            293,
            550,
            456,
            366,
            512,
            51062
        ],
        "temperature": 0.0,
        "avg_logprob": -0.30374306440353394,
        "compression_ratio": 1.6078431606292725,
        "no_speech_prob": 0.03258789703249931
    },
    {
        "id": 402,
        "seek": 157884,
        "start": 1592.800048828125,
        "end": 1594.9599609375,
        "text": " special tokens that you can indicate.",
        "tokens": [
            51062,
            2121,
            22667,
            300,
            291,
            393,
            13330,
            13,
            51170
        ],
        "temperature": 0.0,
        "avg_logprob": -0.30374306440353394,
        "compression_ratio": 1.6078431606292725,
        "no_speech_prob": 0.03258789703249931
    },
    {
        "id": 403,
        "seek": 157884,
        "start": 1594.9599609375,
        "end": 1602.0400390625,
        "text": " And it hardcodes the UNCTOKEN, the beginning of sentence, end of sentence, and the PADTOKEN.",
        "tokens": [
            51170,
            400,
            309,
            1152,
            66,
            4789,
            264,
            8229,
            10259,
            9443,
            2195,
            11,
            264,
            2863,
            295,
            8174,
            11,
            917,
            295,
            8174,
            11,
            293,
            264,
            430,
            6112,
            51,
            9443,
            2195,
            13,
            51524
        ],
        "temperature": 0.0,
        "avg_logprob": -0.30374306440353394,
        "compression_ratio": 1.6078431606292725,
        "no_speech_prob": 0.03258789703249931
    },
    {
        "id": 404,
        "seek": 157884,
        "start": 1602.0400390625,
        "end": 1605.800048828125,
        "text": " And the UNCTOKEN must exist, from my understanding.",
        "tokens": [
            51524,
            400,
            264,
            8229,
            10259,
            9443,
            2195,
            1633,
            2514,
            11,
            490,
            452,
            3701,
            13,
            51712
        ],
        "temperature": 0.0,
        "avg_logprob": -0.30374306440353394,
        "compression_ratio": 1.6078431606292725,
        "no_speech_prob": 0.03258789703249931
    },
    {
        "id": 405,
        "seek": 160580,
        "start": 1606.800048828125,
        "end": 1613.6800537109375,
        "text": " So we can train, and when I press train, it's going to create this file, TOC400.model and",
        "tokens": [
            50414,
            407,
            321,
            393,
            3847,
            11,
            293,
            562,
            286,
            1886,
            3847,
            11,
            309,
            311,
            516,
            281,
            1884,
            341,
            3991,
            11,
            8232,
            34,
            13741,
            13,
            8014,
            338,
            293,
            50758
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2805418074131012,
        "compression_ratio": 1.6173468828201294,
        "no_speech_prob": 0.150015726685524
    },
    {
        "id": 406,
        "seek": 160580,
        "start": 1613.6800537109375,
        "end": 1615.52001953125,
        "text": " TOC400.vocab.",
        "tokens": [
            50758,
            8232,
            34,
            13741,
            13,
            20836,
            455,
            13,
            50850
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2805418074131012,
        "compression_ratio": 1.6173468828201294,
        "no_speech_prob": 0.150015726685524
    },
    {
        "id": 407,
        "seek": 160580,
        "start": 1615.52001953125,
        "end": 1620.6800537109375,
        "text": " I can then load the model file, and I can inspect the vocabulary of it.",
        "tokens": [
            50850,
            286,
            393,
            550,
            3677,
            264,
            2316,
            3991,
            11,
            293,
            286,
            393,
            15018,
            264,
            19864,
            295,
            309,
            13,
            51108
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2805418074131012,
        "compression_ratio": 1.6173468828201294,
        "no_speech_prob": 0.150015726685524
    },
    {
        "id": 408,
        "seek": 160580,
        "start": 1620.6800537109375,
        "end": 1626.47998046875,
        "text": " And so we trained vocab size 400 on this text here.",
        "tokens": [
            51108,
            400,
            370,
            321,
            8895,
            2329,
            455,
            2744,
            8423,
            322,
            341,
            2487,
            510,
            13,
            51398
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2805418074131012,
        "compression_ratio": 1.6173468828201294,
        "no_speech_prob": 0.150015726685524
    },
    {
        "id": 409,
        "seek": 160580,
        "start": 1626.47998046875,
        "end": 1631.239990234375,
        "text": " And these are the individual pieces, the individual tokens that SentenceBees will create.",
        "tokens": [
            51398,
            400,
            613,
            366,
            264,
            2609,
            3755,
            11,
            264,
            2609,
            22667,
            300,
            23652,
            655,
            33,
            4031,
            486,
            1884,
            13,
            51636
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2805418074131012,
        "compression_ratio": 1.6173468828201294,
        "no_speech_prob": 0.150015726685524
    },
    {
        "id": 410,
        "seek": 163124,
        "start": 1631.239990234375,
        "end": 1636.280029296875,
        "text": " So in the beginning, we see that we have the UNCTOKEN with the ID 0.",
        "tokens": [
            50364,
            407,
            294,
            264,
            2863,
            11,
            321,
            536,
            300,
            321,
            362,
            264,
            8229,
            10259,
            9443,
            2195,
            365,
            264,
            7348,
            1958,
            13,
            50616
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2530052363872528,
        "compression_ratio": 1.5825687646865845,
        "no_speech_prob": 0.008847104385495186
    },
    {
        "id": 411,
        "seek": 163124,
        "start": 1636.280029296875,
        "end": 1641.0,
        "text": " Then we have the beginning of sequence, end of sequence, 1 and 2.",
        "tokens": [
            50616,
            1396,
            321,
            362,
            264,
            2863,
            295,
            8310,
            11,
            917,
            295,
            8310,
            11,
            502,
            293,
            568,
            13,
            50852
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2530052363872528,
        "compression_ratio": 1.5825687646865845,
        "no_speech_prob": 0.008847104385495186
    },
    {
        "id": 412,
        "seek": 163124,
        "start": 1641.0,
        "end": 1645.0400390625,
        "text": " And then we said that the PADID is negative 1, so we chose not to use it.",
        "tokens": [
            50852,
            400,
            550,
            321,
            848,
            300,
            264,
            430,
            6112,
            2777,
            307,
            3671,
            502,
            11,
            370,
            321,
            5111,
            406,
            281,
            764,
            309,
            13,
            51054
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2530052363872528,
        "compression_ratio": 1.5825687646865845,
        "no_speech_prob": 0.008847104385495186
    },
    {
        "id": 413,
        "seek": 163124,
        "start": 1645.0400390625,
        "end": 1647.760009765625,
        "text": " So there's no PADID here.",
        "tokens": [
            51054,
            407,
            456,
            311,
            572,
            430,
            6112,
            2777,
            510,
            13,
            51190
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2530052363872528,
        "compression_ratio": 1.5825687646865845,
        "no_speech_prob": 0.008847104385495186
    },
    {
        "id": 414,
        "seek": 163124,
        "start": 1647.760009765625,
        "end": 1651.0999755859375,
        "text": " Then these are individual BYTE tokens.",
        "tokens": [
            51190,
            1396,
            613,
            366,
            2609,
            26930,
            13639,
            22667,
            13,
            51357
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2530052363872528,
        "compression_ratio": 1.5825687646865845,
        "no_speech_prob": 0.008847104385495186
    },
    {
        "id": 415,
        "seek": 163124,
        "start": 1651.0999755859375,
        "end": 1656.699951171875,
        "text": " So here we saw that BYTE fallback in Llama was turned on, so it's true.",
        "tokens": [
            51357,
            407,
            510,
            321,
            1866,
            300,
            26930,
            13639,
            2100,
            3207,
            294,
            32717,
            2404,
            390,
            3574,
            322,
            11,
            370,
            309,
            311,
            2074,
            13,
            51637
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2530052363872528,
        "compression_ratio": 1.5825687646865845,
        "no_speech_prob": 0.008847104385495186
    },
    {
        "id": 416,
        "seek": 165670,
        "start": 1656.699951171875,
        "end": 1666.0999755859375,
        "text": " So what follows are going to be the 256 BYTE tokens, and these are their IDs.",
        "tokens": [
            50364,
            407,
            437,
            10002,
            366,
            516,
            281,
            312,
            264,
            38882,
            26930,
            13639,
            22667,
            11,
            293,
            613,
            366,
            641,
            48212,
            13,
            50834
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2455119639635086,
        "compression_ratio": 1.8556150197982788,
        "no_speech_prob": 0.0008558951085433364
    },
    {
        "id": 417,
        "seek": 165670,
        "start": 1666.0999755859375,
        "end": 1672.02001953125,
        "text": " And then at the bottom, after the BYTE tokens, come the merges.",
        "tokens": [
            50834,
            400,
            550,
            412,
            264,
            2767,
            11,
            934,
            264,
            26930,
            13639,
            22667,
            11,
            808,
            264,
            3551,
            2880,
            13,
            51130
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2455119639635086,
        "compression_ratio": 1.8556150197982788,
        "no_speech_prob": 0.0008558951085433364
    },
    {
        "id": 418,
        "seek": 165670,
        "start": 1672.02001953125,
        "end": 1674.780029296875,
        "text": " And these are the parent nodes in the merges.",
        "tokens": [
            51130,
            400,
            613,
            366,
            264,
            2596,
            13891,
            294,
            264,
            3551,
            2880,
            13,
            51268
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2455119639635086,
        "compression_ratio": 1.8556150197982788,
        "no_speech_prob": 0.0008558951085433364
    },
    {
        "id": 419,
        "seek": 165670,
        "start": 1674.780029296875,
        "end": 1678.9000244140625,
        "text": " So we're not seeing the children, we're just seeing the parents and their ID.",
        "tokens": [
            51268,
            407,
            321,
            434,
            406,
            2577,
            264,
            2227,
            11,
            321,
            434,
            445,
            2577,
            264,
            3152,
            293,
            641,
            7348,
            13,
            51474
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2455119639635086,
        "compression_ratio": 1.8556150197982788,
        "no_speech_prob": 0.0008558951085433364
    },
    {
        "id": 420,
        "seek": 165670,
        "start": 1678.9000244140625,
        "end": 1686.3399658203125,
        "text": " And then after the merges comes, eventually, the individual tokens and their IDs.",
        "tokens": [
            51474,
            400,
            550,
            934,
            264,
            3551,
            2880,
            1487,
            11,
            4728,
            11,
            264,
            2609,
            22667,
            293,
            641,
            48212,
            13,
            51846
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2455119639635086,
        "compression_ratio": 1.8556150197982788,
        "no_speech_prob": 0.0008558951085433364
    },
    {
        "id": 421,
        "seek": 168634,
        "start": 1686.3399658203125,
        "end": 1690.5799560546875,
        "text": " And so these are the individual tokens, so these are the individual code point tokens,",
        "tokens": [
            50364,
            400,
            370,
            613,
            366,
            264,
            2609,
            22667,
            11,
            370,
            613,
            366,
            264,
            2609,
            3089,
            935,
            22667,
            11,
            50576
        ],
        "temperature": 0.0,
        "avg_logprob": -0.26410001516342163,
        "compression_ratio": 1.9759615659713745,
        "no_speech_prob": 0.00034062619670294225
    },
    {
        "id": 422,
        "seek": 168634,
        "start": 1690.5799560546875,
        "end": 1691.5799560546875,
        "text": " if you will.",
        "tokens": [
            50576,
            498,
            291,
            486,
            13,
            50626
        ],
        "temperature": 0.0,
        "avg_logprob": -0.26410001516342163,
        "compression_ratio": 1.9759615659713745,
        "no_speech_prob": 0.00034062619670294225
    },
    {
        "id": 423,
        "seek": 168634,
        "start": 1691.5799560546875,
        "end": 1692.5799560546875,
        "text": " And they come at the end.",
        "tokens": [
            50626,
            400,
            436,
            808,
            412,
            264,
            917,
            13,
            50676
        ],
        "temperature": 0.0,
        "avg_logprob": -0.26410001516342163,
        "compression_ratio": 1.9759615659713745,
        "no_speech_prob": 0.00034062619670294225
    },
    {
        "id": 424,
        "seek": 168634,
        "start": 1692.5799560546875,
        "end": 1697.06005859375,
        "text": " So that is the ordering with which SentenceBees represents its vocabularies.",
        "tokens": [
            50676,
            407,
            300,
            307,
            264,
            21739,
            365,
            597,
            23652,
            655,
            33,
            4031,
            8855,
            1080,
            2329,
            455,
            1040,
            530,
            13,
            50900
        ],
        "temperature": 0.0,
        "avg_logprob": -0.26410001516342163,
        "compression_ratio": 1.9759615659713745,
        "no_speech_prob": 0.00034062619670294225
    },
    {
        "id": 425,
        "seek": 168634,
        "start": 1697.06005859375,
        "end": 1701.4599609375,
        "text": " It starts with special tokens, then the BYTE tokens, then the merge tokens, and then the",
        "tokens": [
            50900,
            467,
            3719,
            365,
            2121,
            22667,
            11,
            550,
            264,
            26930,
            13639,
            22667,
            11,
            550,
            264,
            22183,
            22667,
            11,
            293,
            550,
            264,
            51120
        ],
        "temperature": 0.0,
        "avg_logprob": -0.26410001516342163,
        "compression_ratio": 1.9759615659713745,
        "no_speech_prob": 0.00034062619670294225
    },
    {
        "id": 426,
        "seek": 168634,
        "start": 1701.4599609375,
        "end": 1704.06005859375,
        "text": " individual code point tokens.",
        "tokens": [
            51120,
            2609,
            3089,
            935,
            22667,
            13,
            51250
        ],
        "temperature": 0.0,
        "avg_logprob": -0.26410001516342163,
        "compression_ratio": 1.9759615659713745,
        "no_speech_prob": 0.00034062619670294225
    },
    {
        "id": 427,
        "seek": 168634,
        "start": 1704.06005859375,
        "end": 1710.3800048828125,
        "text": " And all these raw code point tokens are the ones that it encountered in the training set.",
        "tokens": [
            51250,
            400,
            439,
            613,
            8936,
            3089,
            935,
            22667,
            366,
            264,
            2306,
            300,
            309,
            20381,
            294,
            264,
            3097,
            992,
            13,
            51566
        ],
        "temperature": 0.0,
        "avg_logprob": -0.26410001516342163,
        "compression_ratio": 1.9759615659713745,
        "no_speech_prob": 0.00034062619670294225
    },
    {
        "id": 428,
        "seek": 171038,
        "start": 1710.3800048828125,
        "end": 1718.719970703125,
        "text": " So those individual code points are the entire set of code points that occurred here.",
        "tokens": [
            50364,
            407,
            729,
            2609,
            3089,
            2793,
            366,
            264,
            2302,
            992,
            295,
            3089,
            2793,
            300,
            11068,
            510,
            13,
            50781
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27989640831947327,
        "compression_ratio": 1.6915887594223022,
        "no_speech_prob": 0.025955982506275177
    },
    {
        "id": 429,
        "seek": 171038,
        "start": 1718.719970703125,
        "end": 1720.699951171875,
        "text": " So those all get put in there.",
        "tokens": [
            50781,
            407,
            729,
            439,
            483,
            829,
            294,
            456,
            13,
            50880
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27989640831947327,
        "compression_ratio": 1.6915887594223022,
        "no_speech_prob": 0.025955982506275177
    },
    {
        "id": 430,
        "seek": 171038,
        "start": 1720.699951171875,
        "end": 1724.1199951171875,
        "text": " And then those are extremely rare, as determined by character coverage.",
        "tokens": [
            50880,
            400,
            550,
            729,
            366,
            4664,
            5892,
            11,
            382,
            9540,
            538,
            2517,
            9645,
            13,
            51051
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27989640831947327,
        "compression_ratio": 1.6915887594223022,
        "no_speech_prob": 0.025955982506275177
    },
    {
        "id": 431,
        "seek": 171038,
        "start": 1724.1199951171875,
        "end": 1728.5799560546875,
        "text": " So if a code point occurred only a single time out of like a million sentences or something",
        "tokens": [
            51051,
            407,
            498,
            257,
            3089,
            935,
            11068,
            787,
            257,
            2167,
            565,
            484,
            295,
            411,
            257,
            2459,
            16579,
            420,
            746,
            51274
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27989640831947327,
        "compression_ratio": 1.6915887594223022,
        "no_speech_prob": 0.025955982506275177
    },
    {
        "id": 432,
        "seek": 171038,
        "start": 1728.5799560546875,
        "end": 1735.4200439453125,
        "text": " like that, then it would be ignored, and it would not be added to our vocabulary.",
        "tokens": [
            51274,
            411,
            300,
            11,
            550,
            309,
            576,
            312,
            19735,
            11,
            293,
            309,
            576,
            406,
            312,
            3869,
            281,
            527,
            19864,
            13,
            51616
        ],
        "temperature": 0.0,
        "avg_logprob": -0.27989640831947327,
        "compression_ratio": 1.6915887594223022,
        "no_speech_prob": 0.025955982506275177
    },
    {
        "id": 433,
        "seek": 173542,
        "start": 1735.4200439453125,
        "end": 1741.699951171875,
        "text": " Once we have a vocabulary, we can encode into IDs, and we can sort of get a list.",
        "tokens": [
            50364,
            3443,
            321,
            362,
            257,
            19864,
            11,
            321,
            393,
            2058,
            1429,
            666,
            48212,
            11,
            293,
            321,
            393,
            1333,
            295,
            483,
            257,
            1329,
            13,
            50678
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2866586446762085,
        "compression_ratio": 1.559999942779541,
        "no_speech_prob": 0.02886323444545269
    },
    {
        "id": 434,
        "seek": 173542,
        "start": 1741.699951171875,
        "end": 1749.8199462890625,
        "text": " And then here, I am also decoding the individual tokens back into little pieces, as they call it.",
        "tokens": [
            50678,
            400,
            550,
            510,
            11,
            286,
            669,
            611,
            979,
            8616,
            264,
            2609,
            22667,
            646,
            666,
            707,
            3755,
            11,
            382,
            436,
            818,
            309,
            13,
            51084
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2866586446762085,
        "compression_ratio": 1.559999942779541,
        "no_speech_prob": 0.02886323444545269
    },
    {
        "id": 435,
        "seek": 173542,
        "start": 1749.8199462890625,
        "end": 1752.0999755859375,
        "text": " So let's take a look at what happened here.",
        "tokens": [
            51084,
            407,
            718,
            311,
            747,
            257,
            574,
            412,
            437,
            2011,
            510,
            13,
            51198
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2866586446762085,
        "compression_ratio": 1.559999942779541,
        "no_speech_prob": 0.02886323444545269
    },
    {
        "id": 436,
        "seek": 173542,
        "start": 1752.0999755859375,
        "end": 1755.300048828125,
        "text": " Hello, space, annyeonghaseyo.",
        "tokens": [
            51198,
            2425,
            11,
            1901,
            11,
            2324,
            18122,
            71,
            651,
            8308,
            13,
            51358
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2866586446762085,
        "compression_ratio": 1.559999942779541,
        "no_speech_prob": 0.02886323444545269
    },
    {
        "id": 437,
        "seek": 173542,
        "start": 1755.300048828125,
        "end": 1758.93994140625,
        "text": " So these are the token IDs we got back.",
        "tokens": [
            51358,
            407,
            613,
            366,
            264,
            14862,
            48212,
            321,
            658,
            646,
            13,
            51540
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2866586446762085,
        "compression_ratio": 1.559999942779541,
        "no_speech_prob": 0.02886323444545269
    },
    {
        "id": 438,
        "seek": 173542,
        "start": 1758.93994140625,
        "end": 1764.5999755859375,
        "text": " And when we look here, a few things sort of jump to mind.",
        "tokens": [
            51540,
            400,
            562,
            321,
            574,
            510,
            11,
            257,
            1326,
            721,
            1333,
            295,
            3012,
            281,
            1575,
            13,
            51823
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2866586446762085,
        "compression_ratio": 1.559999942779541,
        "no_speech_prob": 0.02886323444545269
    },
    {
        "id": 439,
        "seek": 176460,
        "start": 1764.5999755859375,
        "end": 1767.3599853515625,
        "text": " For one, take a look at these characters.",
        "tokens": [
            50364,
            1171,
            472,
            11,
            747,
            257,
            574,
            412,
            613,
            4342,
            13,
            50502
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24041613936424255,
        "compression_ratio": 1.6810344457626343,
        "no_speech_prob": 0.0017004238907247782
    },
    {
        "id": 440,
        "seek": 176460,
        "start": 1767.3599853515625,
        "end": 1770.3599853515625,
        "text": " The Korean characters, of course, were not part of the training set.",
        "tokens": [
            50502,
            440,
            6933,
            4342,
            11,
            295,
            1164,
            11,
            645,
            406,
            644,
            295,
            264,
            3097,
            992,
            13,
            50652
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24041613936424255,
        "compression_ratio": 1.6810344457626343,
        "no_speech_prob": 0.0017004238907247782
    },
    {
        "id": 441,
        "seek": 176460,
        "start": 1770.3599853515625,
        "end": 1775.4000244140625,
        "text": " So SentenceBees is encountering code points that it has not seen during training time.",
        "tokens": [
            50652,
            407,
            23652,
            655,
            33,
            4031,
            307,
            8593,
            278,
            3089,
            2793,
            300,
            309,
            575,
            406,
            1612,
            1830,
            3097,
            565,
            13,
            50904
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24041613936424255,
        "compression_ratio": 1.6810344457626343,
        "no_speech_prob": 0.0017004238907247782
    },
    {
        "id": 442,
        "seek": 176460,
        "start": 1775.4000244140625,
        "end": 1779.0799560546875,
        "text": " And those code points do not have a token associated with them.",
        "tokens": [
            50904,
            400,
            729,
            3089,
            2793,
            360,
            406,
            362,
            257,
            14862,
            6615,
            365,
            552,
            13,
            51088
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24041613936424255,
        "compression_ratio": 1.6810344457626343,
        "no_speech_prob": 0.0017004238907247782
    },
    {
        "id": 443,
        "seek": 176460,
        "start": 1779.0799560546875,
        "end": 1783.0400390625,
        "text": " So suddenly, these are unk tokens, unknown tokens.",
        "tokens": [
            51088,
            407,
            5800,
            11,
            613,
            366,
            517,
            74,
            22667,
            11,
            9841,
            22667,
            13,
            51286
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24041613936424255,
        "compression_ratio": 1.6810344457626343,
        "no_speech_prob": 0.0017004238907247782
    },
    {
        "id": 444,
        "seek": 176460,
        "start": 1783.0400390625,
        "end": 1789.0,
        "text": " But because byte fallback is true, instead, SentenceBees falls back to bytes.",
        "tokens": [
            51286,
            583,
            570,
            40846,
            2100,
            3207,
            307,
            2074,
            11,
            2602,
            11,
            23652,
            655,
            33,
            4031,
            8804,
            646,
            281,
            36088,
            13,
            51584
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24041613936424255,
        "compression_ratio": 1.6810344457626343,
        "no_speech_prob": 0.0017004238907247782
    },
    {
        "id": 445,
        "seek": 178900,
        "start": 1789.0,
        "end": 1796.280029296875,
        "text": " And so it takes this, it encodes it with UTF-8, and then it uses these tokens to represent",
        "tokens": [
            50364,
            400,
            370,
            309,
            2516,
            341,
            11,
            309,
            2058,
            4789,
            309,
            365,
            624,
            20527,
            12,
            23,
            11,
            293,
            550,
            309,
            4960,
            613,
            22667,
            281,
            2906,
            50728
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24649453163146973,
        "compression_ratio": 1.554973840713501,
        "no_speech_prob": 0.06852884590625763
    },
    {
        "id": 446,
        "seek": 178900,
        "start": 1796.280029296875,
        "end": 1798.3199462890625,
        "text": " those bytes.",
        "tokens": [
            50728,
            729,
            36088,
            13,
            50830
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24649453163146973,
        "compression_ratio": 1.554973840713501,
        "no_speech_prob": 0.06852884590625763
    },
    {
        "id": 447,
        "seek": 178900,
        "start": 1798.3199462890625,
        "end": 1801.1600341796875,
        "text": " And that's what we are getting sort of here.",
        "tokens": [
            50830,
            400,
            300,
            311,
            437,
            321,
            366,
            1242,
            1333,
            295,
            510,
            13,
            50972
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24649453163146973,
        "compression_ratio": 1.554973840713501,
        "no_speech_prob": 0.06852884590625763
    },
    {
        "id": 448,
        "seek": 178900,
        "start": 1801.1600341796875,
        "end": 1809.199951171875,
        "text": " This is the UTF-8 encoding, and it is shifted by three, because of these special tokens",
        "tokens": [
            50972,
            639,
            307,
            264,
            624,
            20527,
            12,
            23,
            43430,
            11,
            293,
            309,
            307,
            18892,
            538,
            1045,
            11,
            570,
            295,
            613,
            2121,
            22667,
            51374
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24649453163146973,
        "compression_ratio": 1.554973840713501,
        "no_speech_prob": 0.06852884590625763
    },
    {
        "id": 449,
        "seek": 178900,
        "start": 1809.199951171875,
        "end": 1811.6400146484375,
        "text": " here that have IDs earlier on.",
        "tokens": [
            51374,
            510,
            300,
            362,
            48212,
            3071,
            322,
            13,
            51496
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24649453163146973,
        "compression_ratio": 1.554973840713501,
        "no_speech_prob": 0.06852884590625763
    },
    {
        "id": 450,
        "seek": 178900,
        "start": 1811.6400146484375,
        "end": 1813.239990234375,
        "text": " So that's what happened here.",
        "tokens": [
            51496,
            407,
            300,
            311,
            437,
            2011,
            510,
            13,
            51576
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24649453163146973,
        "compression_ratio": 1.554973840713501,
        "no_speech_prob": 0.06852884590625763
    },
    {
        "id": 451,
        "seek": 181324,
        "start": 1813.56005859375,
        "end": 1820.1600341796875,
        "text": " Now, one more thing that, well, first, before I go on, with respect to the byte fallback,",
        "tokens": [
            50380,
            823,
            11,
            472,
            544,
            551,
            300,
            11,
            731,
            11,
            700,
            11,
            949,
            286,
            352,
            322,
            11,
            365,
            3104,
            281,
            264,
            40846,
            2100,
            3207,
            11,
            50710
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24084700644016266,
        "compression_ratio": 1.7857142686843872,
        "no_speech_prob": 0.05581901594996452
    },
    {
        "id": 452,
        "seek": 181324,
        "start": 1820.1600341796875,
        "end": 1822.0,
        "text": " let me remove byte fallback.",
        "tokens": [
            50710,
            718,
            385,
            4159,
            40846,
            2100,
            3207,
            13,
            50802
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24084700644016266,
        "compression_ratio": 1.7857142686843872,
        "no_speech_prob": 0.05581901594996452
    },
    {
        "id": 453,
        "seek": 181324,
        "start": 1822.0,
        "end": 1824.9599609375,
        "text": " If this is false, what's going to happen?",
        "tokens": [
            50802,
            759,
            341,
            307,
            7908,
            11,
            437,
            311,
            516,
            281,
            1051,
            30,
            50950
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24084700644016266,
        "compression_ratio": 1.7857142686843872,
        "no_speech_prob": 0.05581901594996452
    },
    {
        "id": 454,
        "seek": 181324,
        "start": 1824.9599609375,
        "end": 1826.8800048828125,
        "text": " Let's retrain.",
        "tokens": [
            50950,
            961,
            311,
            1533,
            7146,
            13,
            51046
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24084700644016266,
        "compression_ratio": 1.7857142686843872,
        "no_speech_prob": 0.05581901594996452
    },
    {
        "id": 455,
        "seek": 181324,
        "start": 1826.8800048828125,
        "end": 1830.5999755859375,
        "text": " So the first thing that happened is all the byte tokens disappeared, right?",
        "tokens": [
            51046,
            407,
            264,
            700,
            551,
            300,
            2011,
            307,
            439,
            264,
            40846,
            22667,
            13954,
            11,
            558,
            30,
            51232
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24084700644016266,
        "compression_ratio": 1.7857142686843872,
        "no_speech_prob": 0.05581901594996452
    },
    {
        "id": 456,
        "seek": 181324,
        "start": 1830.5999755859375,
        "end": 1833.9200439453125,
        "text": " And now we just have the merges, and we have a lot more merges now, because we have a lot",
        "tokens": [
            51232,
            400,
            586,
            321,
            445,
            362,
            264,
            3551,
            2880,
            11,
            293,
            321,
            362,
            257,
            688,
            544,
            3551,
            2880,
            586,
            11,
            570,
            321,
            362,
            257,
            688,
            51398
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24084700644016266,
        "compression_ratio": 1.7857142686843872,
        "no_speech_prob": 0.05581901594996452
    },
    {
        "id": 457,
        "seek": 181324,
        "start": 1833.9200439453125,
        "end": 1840.239990234375,
        "text": " more space, because we're not taking up space in the vocab size with all the bytes.",
        "tokens": [
            51398,
            544,
            1901,
            11,
            570,
            321,
            434,
            406,
            1940,
            493,
            1901,
            294,
            264,
            2329,
            455,
            2744,
            365,
            439,
            264,
            36088,
            13,
            51714
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24084700644016266,
        "compression_ratio": 1.7857142686843872,
        "no_speech_prob": 0.05581901594996452
    },
    {
        "id": 458,
        "seek": 184024,
        "start": 1840.239990234375,
        "end": 1845.239990234375,
        "text": " And now if we encode this, we get a zero.",
        "tokens": [
            50364,
            400,
            586,
            498,
            321,
            2058,
            1429,
            341,
            11,
            321,
            483,
            257,
            4018,
            13,
            50614
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2074548453092575,
        "compression_ratio": 1.6731517314910889,
        "no_speech_prob": 0.0025909687392413616
    },
    {
        "id": 459,
        "seek": 184024,
        "start": 1845.239990234375,
        "end": 1849.9599609375,
        "text": " So this entire string here, suddenly, there's no byte fallback, so this is unknown, and",
        "tokens": [
            50614,
            407,
            341,
            2302,
            6798,
            510,
            11,
            5800,
            11,
            456,
            311,
            572,
            40846,
            2100,
            3207,
            11,
            370,
            341,
            307,
            9841,
            11,
            293,
            50850
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2074548453092575,
        "compression_ratio": 1.6731517314910889,
        "no_speech_prob": 0.0025909687392413616
    },
    {
        "id": 460,
        "seek": 184024,
        "start": 1849.9599609375,
        "end": 1852.3199462890625,
        "text": " unknown is UNK.",
        "tokens": [
            50850,
            9841,
            307,
            8229,
            42,
            13,
            50968
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2074548453092575,
        "compression_ratio": 1.6731517314910889,
        "no_speech_prob": 0.0025909687392413616
    },
    {
        "id": 461,
        "seek": 184024,
        "start": 1852.3199462890625,
        "end": 1857.43994140625,
        "text": " And so this is zero, because the UNK token is token zero.",
        "tokens": [
            50968,
            400,
            370,
            341,
            307,
            4018,
            11,
            570,
            264,
            8229,
            42,
            14862,
            307,
            14862,
            4018,
            13,
            51224
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2074548453092575,
        "compression_ratio": 1.6731517314910889,
        "no_speech_prob": 0.0025909687392413616
    },
    {
        "id": 462,
        "seek": 184024,
        "start": 1857.43994140625,
        "end": 1861.0,
        "text": " And you have to keep in mind that this would feed into your language model.",
        "tokens": [
            51224,
            400,
            291,
            362,
            281,
            1066,
            294,
            1575,
            300,
            341,
            576,
            3154,
            666,
            428,
            2856,
            2316,
            13,
            51402
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2074548453092575,
        "compression_ratio": 1.6731517314910889,
        "no_speech_prob": 0.0025909687392413616
    },
    {
        "id": 463,
        "seek": 184024,
        "start": 1861.0,
        "end": 1864.56005859375,
        "text": " So what is a language model supposed to do when all kinds of different things that are",
        "tokens": [
            51402,
            407,
            437,
            307,
            257,
            2856,
            2316,
            3442,
            281,
            360,
            562,
            439,
            3685,
            295,
            819,
            721,
            300,
            366,
            51580
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2074548453092575,
        "compression_ratio": 1.6731517314910889,
        "no_speech_prob": 0.0025909687392413616
    },
    {
        "id": 464,
        "seek": 184024,
        "start": 1864.56005859375,
        "end": 1868.1600341796875,
        "text": " unrecognized because they're rare just end up mapping into UNK?",
        "tokens": [
            51580,
            517,
            13867,
            2912,
            1602,
            570,
            436,
            434,
            5892,
            445,
            917,
            493,
            18350,
            666,
            8229,
            42,
            30,
            51760
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2074548453092575,
        "compression_ratio": 1.6731517314910889,
        "no_speech_prob": 0.0025909687392413616
    },
    {
        "id": 465,
        "seek": 186816,
        "start": 1868.1600341796875,
        "end": 1870.3199462890625,
        "text": " It's not exactly the property that you want.",
        "tokens": [
            50364,
            467,
            311,
            406,
            2293,
            264,
            4707,
            300,
            291,
            528,
            13,
            50472
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25977060198783875,
        "compression_ratio": 1.588477373123169,
        "no_speech_prob": 0.018546344712376595
    },
    {
        "id": 466,
        "seek": 186816,
        "start": 1870.3199462890625,
        "end": 1876.0799560546875,
        "text": " So that's why I think Llama correctly used byte fallback true, because we definitely",
        "tokens": [
            50472,
            407,
            300,
            311,
            983,
            286,
            519,
            32717,
            2404,
            8944,
            1143,
            40846,
            2100,
            3207,
            2074,
            11,
            570,
            321,
            2138,
            50760
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25977060198783875,
        "compression_ratio": 1.588477373123169,
        "no_speech_prob": 0.018546344712376595
    },
    {
        "id": 467,
        "seek": 186816,
        "start": 1876.0799560546875,
        "end": 1881.8399658203125,
        "text": " want to feed these unknown or rare code points into the model in some manner.",
        "tokens": [
            50760,
            528,
            281,
            3154,
            613,
            9841,
            420,
            5892,
            3089,
            2793,
            666,
            264,
            2316,
            294,
            512,
            9060,
            13,
            51048
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25977060198783875,
        "compression_ratio": 1.588477373123169,
        "no_speech_prob": 0.018546344712376595
    },
    {
        "id": 468,
        "seek": 186816,
        "start": 1881.8399658203125,
        "end": 1885.0799560546875,
        "text": " The next thing I want to show you is the following.",
        "tokens": [
            51048,
            440,
            958,
            551,
            286,
            528,
            281,
            855,
            291,
            307,
            264,
            3480,
            13,
            51210
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25977060198783875,
        "compression_ratio": 1.588477373123169,
        "no_speech_prob": 0.018546344712376595
    },
    {
        "id": 469,
        "seek": 186816,
        "start": 1885.0799560546875,
        "end": 1891.0799560546875,
        "text": " Notice here when we are decoding all the individual tokens, you see how spaces, space here, ends",
        "tokens": [
            51210,
            13428,
            510,
            562,
            321,
            366,
            979,
            8616,
            439,
            264,
            2609,
            22667,
            11,
            291,
            536,
            577,
            7673,
            11,
            1901,
            510,
            11,
            5314,
            51510
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25977060198783875,
        "compression_ratio": 1.588477373123169,
        "no_speech_prob": 0.018546344712376595
    },
    {
        "id": 470,
        "seek": 186816,
        "start": 1891.0799560546875,
        "end": 1894.0400390625,
        "text": " up being this bold underline.",
        "tokens": [
            51510,
            493,
            885,
            341,
            11928,
            833,
            1889,
            13,
            51658
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25977060198783875,
        "compression_ratio": 1.588477373123169,
        "no_speech_prob": 0.018546344712376595
    },
    {
        "id": 471,
        "seek": 189404,
        "start": 1894.0400390625,
        "end": 1899.0400390625,
        "text": " I'm not 100% sure, by the way, why SentenceBees switches whitespace into these bold underscore",
        "tokens": [
            50364,
            286,
            478,
            406,
            2319,
            4,
            988,
            11,
            538,
            264,
            636,
            11,
            983,
            23652,
            655,
            33,
            4031,
            19458,
            21909,
            17940,
            666,
            613,
            11928,
            37556,
            50614
        ],
        "temperature": 0.0,
        "avg_logprob": -0.3343994617462158,
        "compression_ratio": 1.5135135650634766,
        "no_speech_prob": 0.048854630440473557
    },
    {
        "id": 472,
        "seek": 189404,
        "start": 1899.0400390625,
        "end": 1900.0400390625,
        "text": " characters.",
        "tokens": [
            50614,
            4342,
            13,
            50664
        ],
        "temperature": 0.0,
        "avg_logprob": -0.3343994617462158,
        "compression_ratio": 1.5135135650634766,
        "no_speech_prob": 0.048854630440473557
    },
    {
        "id": 473,
        "seek": 189404,
        "start": 1900.0400390625,
        "end": 1901.0400390625,
        "text": " Maybe it's for visualization.",
        "tokens": [
            50664,
            2704,
            309,
            311,
            337,
            25801,
            13,
            50714
        ],
        "temperature": 0.0,
        "avg_logprob": -0.3343994617462158,
        "compression_ratio": 1.5135135650634766,
        "no_speech_prob": 0.048854630440473557
    },
    {
        "id": 474,
        "seek": 189404,
        "start": 1901.0400390625,
        "end": 1904.280029296875,
        "text": " I'm not 100% sure why that happens.",
        "tokens": [
            50714,
            286,
            478,
            406,
            2319,
            4,
            988,
            983,
            300,
            2314,
            13,
            50876
        ],
        "temperature": 0.0,
        "avg_logprob": -0.3343994617462158,
        "compression_ratio": 1.5135135650634766,
        "no_speech_prob": 0.048854630440473557
    },
    {
        "id": 475,
        "seek": 189404,
        "start": 1904.280029296875,
        "end": 1905.280029296875,
        "text": " But notice this.",
        "tokens": [
            50876,
            583,
            3449,
            341,
            13,
            50926
        ],
        "temperature": 0.0,
        "avg_logprob": -0.3343994617462158,
        "compression_ratio": 1.5135135650634766,
        "no_speech_prob": 0.048854630440473557
    },
    {
        "id": 476,
        "seek": 189404,
        "start": 1905.280029296875,
        "end": 1910.6800537109375,
        "text": " Why do we have an extra space in the front of hello?",
        "tokens": [
            50926,
            1545,
            360,
            321,
            362,
            364,
            2857,
            1901,
            294,
            264,
            1868,
            295,
            7751,
            30,
            51196
        ],
        "temperature": 0.0,
        "avg_logprob": -0.3343994617462158,
        "compression_ratio": 1.5135135650634766,
        "no_speech_prob": 0.048854630440473557
    },
    {
        "id": 477,
        "seek": 189404,
        "start": 1910.6800537109375,
        "end": 1913.3599853515625,
        "text": " Where is this coming from?",
        "tokens": [
            51196,
            2305,
            307,
            341,
            1348,
            490,
            30,
            51330
        ],
        "temperature": 0.0,
        "avg_logprob": -0.3343994617462158,
        "compression_ratio": 1.5135135650634766,
        "no_speech_prob": 0.048854630440473557
    },
    {
        "id": 478,
        "seek": 189404,
        "start": 1913.3599853515625,
        "end": 1919.43994140625,
        "text": " Well, it's coming from this option here.",
        "tokens": [
            51330,
            1042,
            11,
            309,
            311,
            1348,
            490,
            341,
            3614,
            510,
            13,
            51634
        ],
        "temperature": 0.0,
        "avg_logprob": -0.3343994617462158,
        "compression_ratio": 1.5135135650634766,
        "no_speech_prob": 0.048854630440473557
    },
    {
        "id": 479,
        "seek": 189404,
        "start": 1919.43994140625,
        "end": 1921.3599853515625,
        "text": " Add dummy prefix is true.",
        "tokens": [
            51634,
            5349,
            35064,
            46969,
            307,
            2074,
            13,
            51730
        ],
        "temperature": 0.0,
        "avg_logprob": -0.3343994617462158,
        "compression_ratio": 1.5135135650634766,
        "no_speech_prob": 0.048854630440473557
    },
    {
        "id": 480,
        "seek": 192136,
        "start": 1921.3599853515625,
        "end": 1925.760009765625,
        "text": " And when you go to the documentation, add dummy whitespace at the beginning of text",
        "tokens": [
            50364,
            400,
            562,
            291,
            352,
            281,
            264,
            14333,
            11,
            909,
            35064,
            21909,
            17940,
            412,
            264,
            2863,
            295,
            2487,
            50584
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25880682468414307,
        "compression_ratio": 1.6356275081634521,
        "no_speech_prob": 0.08755865693092346
    },
    {
        "id": 481,
        "seek": 192136,
        "start": 1925.760009765625,
        "end": 1930.52001953125,
        "text": " in order to treat world in world and hello world in the exact same way.",
        "tokens": [
            50584,
            294,
            1668,
            281,
            2387,
            1002,
            294,
            1002,
            293,
            7751,
            1002,
            294,
            264,
            1900,
            912,
            636,
            13,
            50822
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25880682468414307,
        "compression_ratio": 1.6356275081634521,
        "no_speech_prob": 0.08755865693092346
    },
    {
        "id": 482,
        "seek": 192136,
        "start": 1930.52001953125,
        "end": 1933.56005859375,
        "text": " So what this is trying to do is the following.",
        "tokens": [
            50822,
            407,
            437,
            341,
            307,
            1382,
            281,
            360,
            307,
            264,
            3480,
            13,
            50974
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25880682468414307,
        "compression_ratio": 1.6356275081634521,
        "no_speech_prob": 0.08755865693092346
    },
    {
        "id": 483,
        "seek": 192136,
        "start": 1933.56005859375,
        "end": 1941.6800537109375,
        "text": " If we go back to our TickTokenizer, world as a token by itself has a different ID than",
        "tokens": [
            50974,
            759,
            321,
            352,
            646,
            281,
            527,
            314,
            618,
            51,
            8406,
            6545,
            11,
            1002,
            382,
            257,
            14862,
            538,
            2564,
            575,
            257,
            819,
            7348,
            813,
            51380
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25880682468414307,
        "compression_ratio": 1.6356275081634521,
        "no_speech_prob": 0.08755865693092346
    },
    {
        "id": 484,
        "seek": 192136,
        "start": 1941.6800537109375,
        "end": 1943.3599853515625,
        "text": " space world.",
        "tokens": [
            51380,
            1901,
            1002,
            13,
            51464
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25880682468414307,
        "compression_ratio": 1.6356275081634521,
        "no_speech_prob": 0.08755865693092346
    },
    {
        "id": 485,
        "seek": 192136,
        "start": 1943.3599853515625,
        "end": 1948.0,
        "text": " So this is 1917, but this is 14, et cetera.",
        "tokens": [
            51464,
            407,
            341,
            307,
            42757,
            11,
            457,
            341,
            307,
            3499,
            11,
            1030,
            11458,
            13,
            51696
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25880682468414307,
        "compression_ratio": 1.6356275081634521,
        "no_speech_prob": 0.08755865693092346
    },
    {
        "id": 486,
        "seek": 192136,
        "start": 1948.0,
        "end": 1950.280029296875,
        "text": " So these are two different tokens for the language model.",
        "tokens": [
            51696,
            407,
            613,
            366,
            732,
            819,
            22667,
            337,
            264,
            2856,
            2316,
            13,
            51810
        ],
        "temperature": 0.0,
        "avg_logprob": -0.25880682468414307,
        "compression_ratio": 1.6356275081634521,
        "no_speech_prob": 0.08755865693092346
    },
    {
        "id": 487,
        "seek": 195028,
        "start": 1950.280029296875,
        "end": 1952.8800048828125,
        "text": " And the language model has to learn from data that they are actually kind of like a",
        "tokens": [
            50364,
            400,
            264,
            2856,
            2316,
            575,
            281,
            1466,
            490,
            1412,
            300,
            436,
            366,
            767,
            733,
            295,
            411,
            257,
            50494
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2425176352262497,
        "compression_ratio": 1.844897985458374,
        "no_speech_prob": 0.01495687197893858
    },
    {
        "id": 488,
        "seek": 195028,
        "start": 1952.8800048828125,
        "end": 1954.43994140625,
        "text": " very similar concept.",
        "tokens": [
            50494,
            588,
            2531,
            3410,
            13,
            50572
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2425176352262497,
        "compression_ratio": 1.844897985458374,
        "no_speech_prob": 0.01495687197893858
    },
    {
        "id": 489,
        "seek": 195028,
        "start": 1954.43994140625,
        "end": 1959.5999755859375,
        "text": " So to the language model in the TickToken world, basically words in the beginning of",
        "tokens": [
            50572,
            407,
            281,
            264,
            2856,
            2316,
            294,
            264,
            314,
            618,
            51,
            8406,
            1002,
            11,
            1936,
            2283,
            294,
            264,
            2863,
            295,
            50830
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2425176352262497,
        "compression_ratio": 1.844897985458374,
        "no_speech_prob": 0.01495687197893858
    },
    {
        "id": 490,
        "seek": 195028,
        "start": 1959.5999755859375,
        "end": 1964.8399658203125,
        "text": " sentences and words in the middle of sentences actually look completely different.",
        "tokens": [
            50830,
            16579,
            293,
            2283,
            294,
            264,
            2808,
            295,
            16579,
            767,
            574,
            2584,
            819,
            13,
            51092
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2425176352262497,
        "compression_ratio": 1.844897985458374,
        "no_speech_prob": 0.01495687197893858
    },
    {
        "id": 491,
        "seek": 195028,
        "start": 1964.8399658203125,
        "end": 1967.6800537109375,
        "text": " And it has learned that they are roughly the same.",
        "tokens": [
            51092,
            400,
            309,
            575,
            3264,
            300,
            436,
            366,
            9810,
            264,
            912,
            13,
            51234
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2425176352262497,
        "compression_ratio": 1.844897985458374,
        "no_speech_prob": 0.01495687197893858
    },
    {
        "id": 492,
        "seek": 195028,
        "start": 1967.6800537109375,
        "end": 1971.719970703125,
        "text": " So this add dummy prefix is trying to fight that a little bit.",
        "tokens": [
            51234,
            407,
            341,
            909,
            35064,
            46969,
            307,
            1382,
            281,
            2092,
            300,
            257,
            707,
            857,
            13,
            51436
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2425176352262497,
        "compression_ratio": 1.844897985458374,
        "no_speech_prob": 0.01495687197893858
    },
    {
        "id": 493,
        "seek": 195028,
        "start": 1971.719970703125,
        "end": 1977.199951171875,
        "text": " And the way that works is that it basically adds a dummy prefix.",
        "tokens": [
            51436,
            400,
            264,
            636,
            300,
            1985,
            307,
            300,
            309,
            1936,
            10860,
            257,
            35064,
            46969,
            13,
            51710
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2425176352262497,
        "compression_ratio": 1.844897985458374,
        "no_speech_prob": 0.01495687197893858
    },
    {
        "id": 494,
        "seek": 197720,
        "start": 1977.199951171875,
        "end": 1984.3599853515625,
        "text": " So as a part of preprocessing, it will take the string and it will add a space.",
        "tokens": [
            50364,
            407,
            382,
            257,
            644,
            295,
            2666,
            340,
            780,
            278,
            11,
            309,
            486,
            747,
            264,
            6798,
            293,
            309,
            486,
            909,
            257,
            1901,
            13,
            50722
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2453491985797882,
        "compression_ratio": 1.7238494157791138,
        "no_speech_prob": 0.017711324617266655
    },
    {
        "id": 495,
        "seek": 197720,
        "start": 1984.3599853515625,
        "end": 1986.280029296875,
        "text": " It will do this.",
        "tokens": [
            50722,
            467,
            486,
            360,
            341,
            13,
            50818
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2453491985797882,
        "compression_ratio": 1.7238494157791138,
        "no_speech_prob": 0.017711324617266655
    },
    {
        "id": 496,
        "seek": 197720,
        "start": 1986.280029296875,
        "end": 1990.800048828125,
        "text": " And that's done in an effort to make this world and that world the same.",
        "tokens": [
            50818,
            400,
            300,
            311,
            1096,
            294,
            364,
            4630,
            281,
            652,
            341,
            1002,
            293,
            300,
            1002,
            264,
            912,
            13,
            51044
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2453491985797882,
        "compression_ratio": 1.7238494157791138,
        "no_speech_prob": 0.017711324617266655
    },
    {
        "id": 497,
        "seek": 197720,
        "start": 1990.800048828125,
        "end": 1993.239990234375,
        "text": " They will both be space world.",
        "tokens": [
            51044,
            814,
            486,
            1293,
            312,
            1901,
            1002,
            13,
            51166
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2453491985797882,
        "compression_ratio": 1.7238494157791138,
        "no_speech_prob": 0.017711324617266655
    },
    {
        "id": 498,
        "seek": 197720,
        "start": 1993.239990234375,
        "end": 1996.719970703125,
        "text": " So that's one other kind of preprocessing option that is turned on.",
        "tokens": [
            51166,
            407,
            300,
            311,
            472,
            661,
            733,
            295,
            2666,
            340,
            780,
            278,
            3614,
            300,
            307,
            3574,
            322,
            13,
            51340
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2453491985797882,
        "compression_ratio": 1.7238494157791138,
        "no_speech_prob": 0.017711324617266655
    },
    {
        "id": 499,
        "seek": 197720,
        "start": 1996.719970703125,
        "end": 2000.0799560546875,
        "text": " And Lama 2 also uses this option.",
        "tokens": [
            51340,
            400,
            441,
            2404,
            568,
            611,
            4960,
            341,
            3614,
            13,
            51508
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2453491985797882,
        "compression_ratio": 1.7238494157791138,
        "no_speech_prob": 0.017711324617266655
    },
    {
        "id": 500,
        "seek": 197720,
        "start": 2000.0799560546875,
        "end": 2003.0,
        "text": " And that's, I think, everything that I want to say from my preview of sentence piece and",
        "tokens": [
            51508,
            400,
            300,
            311,
            11,
            286,
            519,
            11,
            1203,
            300,
            286,
            528,
            281,
            584,
            490,
            452,
            14281,
            295,
            8174,
            2522,
            293,
            51654
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2453491985797882,
        "compression_ratio": 1.7238494157791138,
        "no_speech_prob": 0.017711324617266655
    },
    {
        "id": 501,
        "seek": 197720,
        "start": 2003.0,
        "end": 2005.280029296875,
        "text": " how it is different.",
        "tokens": [
            51654,
            577,
            309,
            307,
            819,
            13,
            51768
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2453491985797882,
        "compression_ratio": 1.7238494157791138,
        "no_speech_prob": 0.017711324617266655
    },
    {
        "id": 502,
        "seek": 200528,
        "start": 2005.280029296875,
        "end": 2011.56005859375,
        "text": " Basically here, what I've done is I just put in the raw protocol buffer representation",
        "tokens": [
            50364,
            8537,
            510,
            11,
            437,
            286,
            600,
            1096,
            307,
            286,
            445,
            829,
            294,
            264,
            8936,
            10336,
            21762,
            10290,
            50678
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24387499690055847,
        "compression_ratio": 1.6868327856063843,
        "no_speech_prob": 0.35568198561668396
    },
    {
        "id": 503,
        "seek": 200528,
        "start": 2011.56005859375,
        "end": 2015.56005859375,
        "text": " basically of the tokenizer, the Lama 2 trained.",
        "tokens": [
            50678,
            1936,
            295,
            264,
            14862,
            6545,
            11,
            264,
            441,
            2404,
            568,
            8895,
            13,
            50878
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24387499690055847,
        "compression_ratio": 1.6868327856063843,
        "no_speech_prob": 0.35568198561668396
    },
    {
        "id": 504,
        "seek": 200528,
        "start": 2015.56005859375,
        "end": 2017.760009765625,
        "text": " So feel free to sort of step through this.",
        "tokens": [
            50878,
            407,
            841,
            1737,
            281,
            1333,
            295,
            1823,
            807,
            341,
            13,
            50988
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24387499690055847,
        "compression_ratio": 1.6868327856063843,
        "no_speech_prob": 0.35568198561668396
    },
    {
        "id": 505,
        "seek": 200528,
        "start": 2017.760009765625,
        "end": 2023.5999755859375,
        "text": " And if you would like your tokenization to look identical to that of the meta Lama 2,",
        "tokens": [
            50988,
            400,
            498,
            291,
            576,
            411,
            428,
            14862,
            2144,
            281,
            574,
            14800,
            281,
            300,
            295,
            264,
            19616,
            441,
            2404,
            568,
            11,
            51280
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24387499690055847,
        "compression_ratio": 1.6868327856063843,
        "no_speech_prob": 0.35568198561668396
    },
    {
        "id": 506,
        "seek": 200528,
        "start": 2023.5999755859375,
        "end": 2027.5999755859375,
        "text": " then you would be copy pasting these settings as I've tried to do up above.",
        "tokens": [
            51280,
            550,
            291,
            576,
            312,
            5055,
            1791,
            278,
            613,
            6257,
            382,
            286,
            600,
            3031,
            281,
            360,
            493,
            3673,
            13,
            51480
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24387499690055847,
        "compression_ratio": 1.6868327856063843,
        "no_speech_prob": 0.35568198561668396
    },
    {
        "id": 507,
        "seek": 200528,
        "start": 2027.5999755859375,
        "end": 2030.8800048828125,
        "text": " And yeah, I think that's it for this section.",
        "tokens": [
            51480,
            400,
            1338,
            11,
            286,
            519,
            300,
            311,
            309,
            337,
            341,
            3541,
            13,
            51644
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24387499690055847,
        "compression_ratio": 1.6868327856063843,
        "no_speech_prob": 0.35568198561668396
    },
    {
        "id": 508,
        "seek": 200528,
        "start": 2030.8800048828125,
        "end": 2035.260009765625,
        "text": " I think my summary for sentence piece from all this is, number one, I think that there's",
        "tokens": [
            51644,
            286,
            519,
            452,
            12691,
            337,
            8174,
            2522,
            490,
            439,
            341,
            307,
            11,
            1230,
            472,
            11,
            286,
            519,
            300,
            456,
            311,
            51863
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24387499690055847,
        "compression_ratio": 1.6868327856063843,
        "no_speech_prob": 0.35568198561668396
    },
    {
        "id": 509,
        "seek": 203526,
        "start": 2035.260009765625,
        "end": 2039.1800537109375,
        "text": " a lot of historical baggage in sentence piece, a lot of concepts that I think are slightly",
        "tokens": [
            50364,
            257,
            688,
            295,
            8584,
            41567,
            294,
            8174,
            2522,
            11,
            257,
            688,
            295,
            10392,
            300,
            286,
            519,
            366,
            4748,
            50560
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2784569263458252,
        "compression_ratio": 1.6697248220443726,
        "no_speech_prob": 6.961911367397988e-06
    },
    {
        "id": 510,
        "seek": 203526,
        "start": 2039.1800537109375,
        "end": 2043.6199951171875,
        "text": " confusing and I think potentially contain foot guns, like this concept of a sentence",
        "tokens": [
            50560,
            13181,
            293,
            286,
            519,
            7263,
            5304,
            2671,
            10153,
            11,
            411,
            341,
            3410,
            295,
            257,
            8174,
            50782
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2784569263458252,
        "compression_ratio": 1.6697248220443726,
        "no_speech_prob": 6.961911367397988e-06
    },
    {
        "id": 511,
        "seek": 203526,
        "start": 2043.6199951171875,
        "end": 2046.1800537109375,
        "text": " and its maximum length and stuff like that.",
        "tokens": [
            50782,
            293,
            1080,
            6674,
            4641,
            293,
            1507,
            411,
            300,
            13,
            50910
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2784569263458252,
        "compression_ratio": 1.6697248220443726,
        "no_speech_prob": 6.961911367397988e-06
    },
    {
        "id": 512,
        "seek": 203526,
        "start": 2046.1800537109375,
        "end": 2052.419921875,
        "text": " Otherwise, it is fairly commonly used in the industry because it is efficient and can do",
        "tokens": [
            50910,
            10328,
            11,
            309,
            307,
            6457,
            12719,
            1143,
            294,
            264,
            3518,
            570,
            309,
            307,
            7148,
            293,
            393,
            360,
            51222
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2784569263458252,
        "compression_ratio": 1.6697248220443726,
        "no_speech_prob": 6.961911367397988e-06
    },
    {
        "id": 513,
        "seek": 203526,
        "start": 2052.419921875,
        "end": 2054.3798828125,
        "text": " both training and inference.",
        "tokens": [
            51222,
            1293,
            3097,
            293,
            38253,
            13,
            51320
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2784569263458252,
        "compression_ratio": 1.6697248220443726,
        "no_speech_prob": 6.961911367397988e-06
    },
    {
        "id": 514,
        "seek": 203526,
        "start": 2054.3798828125,
        "end": 2058.699951171875,
        "text": " It has a few quirks, like for example, UNCTOKEN must exist and the way the byte fallbacks",
        "tokens": [
            51320,
            467,
            575,
            257,
            1326,
            35645,
            1694,
            11,
            411,
            337,
            1365,
            11,
            8229,
            10259,
            46,
            8522,
            45,
            1633,
            2514,
            293,
            264,
            636,
            264,
            40846,
            2100,
            17758,
            51536
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2784569263458252,
        "compression_ratio": 1.6697248220443726,
        "no_speech_prob": 6.961911367397988e-06
    },
    {
        "id": 515,
        "seek": 203526,
        "start": 2058.699951171875,
        "end": 2061.6201171875,
        "text": " are done and so on, I don't find particularly elegant.",
        "tokens": [
            51536,
            366,
            1096,
            293,
            370,
            322,
            11,
            286,
            500,
            380,
            915,
            4098,
            21117,
            13,
            51682
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2784569263458252,
        "compression_ratio": 1.6697248220443726,
        "no_speech_prob": 6.961911367397988e-06
    },
    {
        "id": 516,
        "seek": 203526,
        "start": 2061.6201171875,
        "end": 2063.580078125,
        "text": " And unfortunately, I have to say it's not very well documented.",
        "tokens": [
            51682,
            400,
            7015,
            11,
            286,
            362,
            281,
            584,
            309,
            311,
            406,
            588,
            731,
            23007,
            13,
            51780
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2784569263458252,
        "compression_ratio": 1.6697248220443726,
        "no_speech_prob": 6.961911367397988e-06
    }
]