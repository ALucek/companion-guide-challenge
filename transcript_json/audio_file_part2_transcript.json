[
    {
        "id": 0,
        "seek": 0,
        "start": 0.0,
        "end": 6.800000190734863,
        "text": " times do we do it for? Well, that's totally up to us as a hyperparameter. The more steps we take,",
        "tokens": [
            50364,
            1413,
            360,
            321,
            360,
            309,
            337,
            30,
            1042,
            11,
            300,
            311,
            3879,
            493,
            281,
            505,
            382,
            257,
            9848,
            2181,
            335,
            2398,
            13,
            440,
            544,
            4439,
            321,
            747,
            11,
            50704
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21594518423080444,
        "compression_ratio": 1.6333333253860474,
        "no_speech_prob": 0.0022169023286551237
    },
    {
        "id": 1,
        "seek": 0,
        "start": 6.800000190734863,
        "end": 12.319999694824219,
        "text": " the larger will be our vocabulary, and the shorter will be our sequence. And there is some sweet spot",
        "tokens": [
            50704,
            264,
            4833,
            486,
            312,
            527,
            19864,
            11,
            293,
            264,
            11639,
            486,
            312,
            527,
            8310,
            13,
            400,
            456,
            307,
            512,
            3844,
            4008,
            50980
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21594518423080444,
        "compression_ratio": 1.6333333253860474,
        "no_speech_prob": 0.0022169023286551237
    },
    {
        "id": 2,
        "seek": 0,
        "start": 12.319999694824219,
        "end": 17.600000381469727,
        "text": " that we usually find works the best in practice. And so this is kind of a hyperparameter, and we",
        "tokens": [
            50980,
            300,
            321,
            2673,
            915,
            1985,
            264,
            1151,
            294,
            3124,
            13,
            400,
            370,
            341,
            307,
            733,
            295,
            257,
            9848,
            2181,
            335,
            2398,
            11,
            293,
            321,
            51244
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21594518423080444,
        "compression_ratio": 1.6333333253860474,
        "no_speech_prob": 0.0022169023286551237
    },
    {
        "id": 3,
        "seek": 0,
        "start": 17.600000381469727,
        "end": 23.360000610351562,
        "text": " tune it, and we find good vocabulary sizes. As an example, GPT-4 currently uses roughly 100,000",
        "tokens": [
            51244,
            10864,
            309,
            11,
            293,
            321,
            915,
            665,
            19864,
            11602,
            13,
            1018,
            364,
            1365,
            11,
            26039,
            51,
            12,
            19,
            4362,
            4960,
            9810,
            2319,
            11,
            1360,
            51532
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21594518423080444,
        "compression_ratio": 1.6333333253860474,
        "no_speech_prob": 0.0022169023286551237
    },
    {
        "id": 4,
        "seek": 2336,
        "start": 23.360000610351562,
        "end": 29.040000915527344,
        "text": " tokens. And ballpark, those are reasonable numbers currently instead of the archaeological",
        "tokens": [
            50364,
            22667,
            13,
            400,
            2594,
            31239,
            11,
            729,
            366,
            10585,
            3547,
            4362,
            2602,
            295,
            264,
            42139,
            50648
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23309426009655,
        "compression_ratio": 1.7049180269241333,
        "no_speech_prob": 0.3737161457538605
    },
    {
        "id": 5,
        "seek": 2336,
        "start": 29.040000915527344,
        "end": 35.119998931884766,
        "text": " models. So let me now write, putting it all together and iterating these steps.",
        "tokens": [
            50648,
            5245,
            13,
            407,
            718,
            385,
            586,
            2464,
            11,
            3372,
            309,
            439,
            1214,
            293,
            17138,
            990,
            613,
            4439,
            13,
            50952
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23309426009655,
        "compression_ratio": 1.7049180269241333,
        "no_speech_prob": 0.3737161457538605
    },
    {
        "id": 6,
        "seek": 2336,
        "start": 35.119998931884766,
        "end": 39.599998474121094,
        "text": " Okay, now before we dive into the while loop, I wanted to add one more cell here,",
        "tokens": [
            50952,
            1033,
            11,
            586,
            949,
            321,
            9192,
            666,
            264,
            1339,
            6367,
            11,
            286,
            1415,
            281,
            909,
            472,
            544,
            2815,
            510,
            11,
            51176
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23309426009655,
        "compression_ratio": 1.7049180269241333,
        "no_speech_prob": 0.3737161457538605
    },
    {
        "id": 7,
        "seek": 2336,
        "start": 39.599998474121094,
        "end": 43.52000045776367,
        "text": " where I went to the blog post, and instead of grabbing just the first paragraph or two,",
        "tokens": [
            51176,
            689,
            286,
            1437,
            281,
            264,
            6968,
            2183,
            11,
            293,
            2602,
            295,
            23771,
            445,
            264,
            700,
            18865,
            420,
            732,
            11,
            51372
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23309426009655,
        "compression_ratio": 1.7049180269241333,
        "no_speech_prob": 0.3737161457538605
    },
    {
        "id": 8,
        "seek": 2336,
        "start": 43.52000045776367,
        "end": 48.31999969482422,
        "text": " I took the entire blog post, and I stretched it out in a single line. And basically just using",
        "tokens": [
            51372,
            286,
            1890,
            264,
            2302,
            6968,
            2183,
            11,
            293,
            286,
            23563,
            309,
            484,
            294,
            257,
            2167,
            1622,
            13,
            400,
            1936,
            445,
            1228,
            51612
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23309426009655,
        "compression_ratio": 1.7049180269241333,
        "no_speech_prob": 0.3737161457538605
    },
    {
        "id": 9,
        "seek": 2336,
        "start": 48.31999969482422,
        "end": 52.400001525878906,
        "text": " longer text will allow us to have more representative statistics for the byte pairs,",
        "tokens": [
            51612,
            2854,
            2487,
            486,
            2089,
            505,
            281,
            362,
            544,
            12424,
            12523,
            337,
            264,
            40846,
            15494,
            11,
            51816
        ],
        "temperature": 0.0,
        "avg_logprob": -0.23309426009655,
        "compression_ratio": 1.7049180269241333,
        "no_speech_prob": 0.3737161457538605
    },
    {
        "id": 10,
        "seek": 5240,
        "start": 52.400001525878906,
        "end": 56.47999954223633,
        "text": " and we'll just get more sensible results out of it because it's longer text.",
        "tokens": [
            50364,
            293,
            321,
            603,
            445,
            483,
            544,
            25380,
            3542,
            484,
            295,
            309,
            570,
            309,
            311,
            2854,
            2487,
            13,
            50568
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17885707318782806,
        "compression_ratio": 1.6266666650772095,
        "no_speech_prob": 5.771918495156569e-06
    },
    {
        "id": 11,
        "seek": 5240,
        "start": 57.91999816894531,
        "end": 64.95999908447266,
        "text": " So here we have the raw text. We encode it into bytes using the UTF-8 encoding. And then here,",
        "tokens": [
            50640,
            407,
            510,
            321,
            362,
            264,
            8936,
            2487,
            13,
            492,
            2058,
            1429,
            309,
            666,
            36088,
            1228,
            264,
            624,
            20527,
            12,
            23,
            43430,
            13,
            400,
            550,
            510,
            11,
            50992
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17885707318782806,
        "compression_ratio": 1.6266666650772095,
        "no_speech_prob": 5.771918495156569e-06
    },
    {
        "id": 12,
        "seek": 5240,
        "start": 64.95999908447266,
        "end": 69.91999816894531,
        "text": " as before, we are just changing it into a list of integers in Python, just so it's easier to work",
        "tokens": [
            50992,
            382,
            949,
            11,
            321,
            366,
            445,
            4473,
            309,
            666,
            257,
            1329,
            295,
            41674,
            294,
            15329,
            11,
            445,
            370,
            309,
            311,
            3571,
            281,
            589,
            51240
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17885707318782806,
        "compression_ratio": 1.6266666650772095,
        "no_speech_prob": 5.771918495156569e-06
    },
    {
        "id": 13,
        "seek": 5240,
        "start": 69.91999816894531,
        "end": 78.55999755859375,
        "text": " with instead of the raw bytes objects. And then this is the code that I came up with to actually",
        "tokens": [
            51240,
            365,
            2602,
            295,
            264,
            8936,
            36088,
            6565,
            13,
            400,
            550,
            341,
            307,
            264,
            3089,
            300,
            286,
            1361,
            493,
            365,
            281,
            767,
            51672
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17885707318782806,
        "compression_ratio": 1.6266666650772095,
        "no_speech_prob": 5.771918495156569e-06
    },
    {
        "id": 14,
        "seek": 7856,
        "start": 78.55999755859375,
        "end": 83.5999984741211,
        "text": " do the merging and loop. These two functions here are identical to what we had above. I only",
        "tokens": [
            50364,
            360,
            264,
            44559,
            293,
            6367,
            13,
            1981,
            732,
            6828,
            510,
            366,
            14800,
            281,
            437,
            321,
            632,
            3673,
            13,
            286,
            787,
            50616
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1910400688648224,
        "compression_ratio": 1.7200000286102295,
        "no_speech_prob": 0.08035187423229218
    },
    {
        "id": 15,
        "seek": 7856,
        "start": 83.5999984741211,
        "end": 90.4800033569336,
        "text": " included them here just so that you have the point of reference here. So these two are identical,",
        "tokens": [
            50616,
            5556,
            552,
            510,
            445,
            370,
            300,
            291,
            362,
            264,
            935,
            295,
            6408,
            510,
            13,
            407,
            613,
            732,
            366,
            14800,
            11,
            50960
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1910400688648224,
        "compression_ratio": 1.7200000286102295,
        "no_speech_prob": 0.08035187423229218
    },
    {
        "id": 16,
        "seek": 7856,
        "start": 90.4800033569336,
        "end": 94.31999969482422,
        "text": " and then this is the new code that I added. So the first thing we want to do is we want to",
        "tokens": [
            50960,
            293,
            550,
            341,
            307,
            264,
            777,
            3089,
            300,
            286,
            3869,
            13,
            407,
            264,
            700,
            551,
            321,
            528,
            281,
            360,
            307,
            321,
            528,
            281,
            51152
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1910400688648224,
        "compression_ratio": 1.7200000286102295,
        "no_speech_prob": 0.08035187423229218
    },
    {
        "id": 17,
        "seek": 7856,
        "start": 94.31999969482422,
        "end": 99.5999984741211,
        "text": " decide on a final vocabulary size that we want our tokenizer to have. And as I mentioned, this",
        "tokens": [
            51152,
            4536,
            322,
            257,
            2572,
            19864,
            2744,
            300,
            321,
            528,
            527,
            14862,
            6545,
            281,
            362,
            13,
            400,
            382,
            286,
            2835,
            11,
            341,
            51416
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1910400688648224,
        "compression_ratio": 1.7200000286102295,
        "no_speech_prob": 0.08035187423229218
    },
    {
        "id": 18,
        "seek": 7856,
        "start": 99.5999984741211,
        "end": 104.4800033569336,
        "text": " is a hyperparameter, and you set it in some way depending on your best performance. So let's say",
        "tokens": [
            51416,
            307,
            257,
            9848,
            2181,
            335,
            2398,
            11,
            293,
            291,
            992,
            309,
            294,
            512,
            636,
            5413,
            322,
            428,
            1151,
            3389,
            13,
            407,
            718,
            311,
            584,
            51660
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1910400688648224,
        "compression_ratio": 1.7200000286102295,
        "no_speech_prob": 0.08035187423229218
    },
    {
        "id": 19,
        "seek": 10448,
        "start": 104.4800033569336,
        "end": 109.36000061035156,
        "text": " for us, we're going to use 276 because that way we're going to be doing exactly 20 merges.",
        "tokens": [
            50364,
            337,
            505,
            11,
            321,
            434,
            516,
            281,
            764,
            7634,
            21,
            570,
            300,
            636,
            321,
            434,
            516,
            281,
            312,
            884,
            2293,
            945,
            3551,
            2880,
            13,
            50608
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17915655672550201,
        "compression_ratio": 1.6454545259475708,
        "no_speech_prob": 0.0053847786039114
    },
    {
        "id": 20,
        "seek": 10448,
        "start": 110.4800033569336,
        "end": 118.23999786376953,
        "text": " And 20 merges because we already have 256 tokens for the raw bytes. And to reach 276,",
        "tokens": [
            50664,
            400,
            945,
            3551,
            2880,
            570,
            321,
            1217,
            362,
            38882,
            22667,
            337,
            264,
            8936,
            36088,
            13,
            400,
            281,
            2524,
            7634,
            21,
            11,
            51052
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17915655672550201,
        "compression_ratio": 1.6454545259475708,
        "no_speech_prob": 0.0053847786039114
    },
    {
        "id": 21,
        "seek": 10448,
        "start": 118.23999786376953,
        "end": 125.91999816894531,
        "text": " we have to do 20 merges to add 20 new tokens. Here, this is one way in Python to just create",
        "tokens": [
            51052,
            321,
            362,
            281,
            360,
            945,
            3551,
            2880,
            281,
            909,
            945,
            777,
            22667,
            13,
            1692,
            11,
            341,
            307,
            472,
            636,
            294,
            15329,
            281,
            445,
            1884,
            51436
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17915655672550201,
        "compression_ratio": 1.6454545259475708,
        "no_speech_prob": 0.0053847786039114
    },
    {
        "id": 22,
        "seek": 10448,
        "start": 125.91999816894531,
        "end": 132.39999389648438,
        "text": " a copy of a list. So I'm taking the tokens list, and by wrapping it in the list, Python will",
        "tokens": [
            51436,
            257,
            5055,
            295,
            257,
            1329,
            13,
            407,
            286,
            478,
            1940,
            264,
            22667,
            1329,
            11,
            293,
            538,
            21993,
            309,
            294,
            264,
            1329,
            11,
            15329,
            486,
            51760
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17915655672550201,
        "compression_ratio": 1.6454545259475708,
        "no_speech_prob": 0.0053847786039114
    },
    {
        "id": 23,
        "seek": 13240,
        "start": 132.39999389648438,
        "end": 135.9199981689453,
        "text": " construct a new list of all the individual elements. So this is just a copy operation.",
        "tokens": [
            50364,
            7690,
            257,
            777,
            1329,
            295,
            439,
            264,
            2609,
            4959,
            13,
            407,
            341,
            307,
            445,
            257,
            5055,
            6916,
            13,
            50540
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17056196928024292,
        "compression_ratio": 1.7200000286102295,
        "no_speech_prob": 3.219232530682348e-05
    },
    {
        "id": 24,
        "seek": 13240,
        "start": 137.44000244140625,
        "end": 142.63999938964844,
        "text": " Then here, I'm creating a merges dictionary. So this merges dictionary is going to maintain",
        "tokens": [
            50616,
            1396,
            510,
            11,
            286,
            478,
            4084,
            257,
            3551,
            2880,
            25890,
            13,
            407,
            341,
            3551,
            2880,
            25890,
            307,
            516,
            281,
            6909,
            50876
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17056196928024292,
        "compression_ratio": 1.7200000286102295,
        "no_speech_prob": 3.219232530682348e-05
    },
    {
        "id": 25,
        "seek": 13240,
        "start": 142.63999938964844,
        "end": 149.83999633789062,
        "text": " basically the child one, child two mapping to a new token. And so what we're going to be building",
        "tokens": [
            50876,
            1936,
            264,
            1440,
            472,
            11,
            1440,
            732,
            18350,
            281,
            257,
            777,
            14862,
            13,
            400,
            370,
            437,
            321,
            434,
            516,
            281,
            312,
            2390,
            51236
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17056196928024292,
        "compression_ratio": 1.7200000286102295,
        "no_speech_prob": 3.219232530682348e-05
    },
    {
        "id": 26,
        "seek": 13240,
        "start": 149.83999633789062,
        "end": 156.0,
        "text": " up here is a binary tree of merges. But actually, it's not exactly a tree because a tree would have",
        "tokens": [
            51236,
            493,
            510,
            307,
            257,
            17434,
            4230,
            295,
            3551,
            2880,
            13,
            583,
            767,
            11,
            309,
            311,
            406,
            2293,
            257,
            4230,
            570,
            257,
            4230,
            576,
            362,
            51544
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17056196928024292,
        "compression_ratio": 1.7200000286102295,
        "no_speech_prob": 3.219232530682348e-05
    },
    {
        "id": 27,
        "seek": 13240,
        "start": 156.0,
        "end": 161.1199951171875,
        "text": " a single root node with a bunch of leaves. For us, we're starting with the leaves on the bottom,",
        "tokens": [
            51544,
            257,
            2167,
            5593,
            9984,
            365,
            257,
            3840,
            295,
            5510,
            13,
            1171,
            505,
            11,
            321,
            434,
            2891,
            365,
            264,
            5510,
            322,
            264,
            2767,
            11,
            51800
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17056196928024292,
        "compression_ratio": 1.7200000286102295,
        "no_speech_prob": 3.219232530682348e-05
    },
    {
        "id": 28,
        "seek": 16112,
        "start": 161.1199951171875,
        "end": 166.24000549316406,
        "text": " which are the individual bytes. Those are the starting 256 tokens. And then we're starting to",
        "tokens": [
            50364,
            597,
            366,
            264,
            2609,
            36088,
            13,
            3950,
            366,
            264,
            2891,
            38882,
            22667,
            13,
            400,
            550,
            321,
            434,
            2891,
            281,
            50620
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18515871465206146,
        "compression_ratio": 1.7351598739624023,
        "no_speech_prob": 4.264733797754161e-05
    },
    {
        "id": 29,
        "seek": 16112,
        "start": 166.24000549316406,
        "end": 173.67999267578125,
        "text": " merge two of them at a time. And so it's not a tree. It's more like a forest as we merge these",
        "tokens": [
            50620,
            22183,
            732,
            295,
            552,
            412,
            257,
            565,
            13,
            400,
            370,
            309,
            311,
            406,
            257,
            4230,
            13,
            467,
            311,
            544,
            411,
            257,
            6719,
            382,
            321,
            22183,
            613,
            50992
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18515871465206146,
        "compression_ratio": 1.7351598739624023,
        "no_speech_prob": 4.264733797754161e-05
    },
    {
        "id": 30,
        "seek": 16112,
        "start": 173.67999267578125,
        "end": 181.83999633789062,
        "text": " elements. So for 20 merges, we're going to find the most commonly occurring pair. We're going to",
        "tokens": [
            50992,
            4959,
            13,
            407,
            337,
            945,
            3551,
            2880,
            11,
            321,
            434,
            516,
            281,
            915,
            264,
            881,
            12719,
            18386,
            6119,
            13,
            492,
            434,
            516,
            281,
            51400
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18515871465206146,
        "compression_ratio": 1.7351598739624023,
        "no_speech_prob": 4.264733797754161e-05
    },
    {
        "id": 31,
        "seek": 16112,
        "start": 181.83999633789062,
        "end": 187.67999267578125,
        "text": " mint a new token integer for it. So I here will start at zero. So we're going to start at 256.",
        "tokens": [
            51400,
            18189,
            257,
            777,
            14862,
            24922,
            337,
            309,
            13,
            407,
            286,
            510,
            486,
            722,
            412,
            4018,
            13,
            407,
            321,
            434,
            516,
            281,
            722,
            412,
            38882,
            13,
            51692
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18515871465206146,
        "compression_ratio": 1.7351598739624023,
        "no_speech_prob": 4.264733797754161e-05
    },
    {
        "id": 32,
        "seek": 18768,
        "start": 188.32000732421875,
        "end": 192.72000122070312,
        "text": " We're going to print that we're merging it. And we're going to replace all the occurrences of that",
        "tokens": [
            50396,
            492,
            434,
            516,
            281,
            4482,
            300,
            321,
            434,
            44559,
            309,
            13,
            400,
            321,
            434,
            516,
            281,
            7406,
            439,
            264,
            5160,
            38983,
            295,
            300,
            50616
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19864627718925476,
        "compression_ratio": 1.7411167621612549,
        "no_speech_prob": 6.108848174335435e-05
    },
    {
        "id": 33,
        "seek": 18768,
        "start": 192.72000122070312,
        "end": 199.52000427246094,
        "text": " pair with the new newly minted token. And we're going to record that this pair of integers",
        "tokens": [
            50616,
            6119,
            365,
            264,
            777,
            15109,
            18189,
            292,
            14862,
            13,
            400,
            321,
            434,
            516,
            281,
            2136,
            300,
            341,
            6119,
            295,
            41674,
            50956
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19864627718925476,
        "compression_ratio": 1.7411167621612549,
        "no_speech_prob": 6.108848174335435e-05
    },
    {
        "id": 34,
        "seek": 18768,
        "start": 199.52000427246094,
        "end": 206.24000549316406,
        "text": " merged into this new integer. So running this gives us the following output.",
        "tokens": [
            50956,
            36427,
            666,
            341,
            777,
            24922,
            13,
            407,
            2614,
            341,
            2709,
            505,
            264,
            3480,
            5598,
            13,
            51292
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19864627718925476,
        "compression_ratio": 1.7411167621612549,
        "no_speech_prob": 6.108848174335435e-05
    },
    {
        "id": 35,
        "seek": 18768,
        "start": 208.63999938964844,
        "end": 214.0,
        "text": " So we did 20 merges. And for example, the first merge was exactly as before,",
        "tokens": [
            51412,
            407,
            321,
            630,
            945,
            3551,
            2880,
            13,
            400,
            337,
            1365,
            11,
            264,
            700,
            22183,
            390,
            2293,
            382,
            949,
            11,
            51680
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19864627718925476,
        "compression_ratio": 1.7411167621612549,
        "no_speech_prob": 6.108848174335435e-05
    },
    {
        "id": 36,
        "seek": 21400,
        "start": 214.0,
        "end": 220.9600067138672,
        "text": " the 101, 32 tokens merging into a new token 256. Now keep in mind that the individual",
        "tokens": [
            50364,
            264,
            21055,
            11,
            8858,
            22667,
            44559,
            666,
            257,
            777,
            14862,
            38882,
            13,
            823,
            1066,
            294,
            1575,
            300,
            264,
            2609,
            50712
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20624999701976776,
        "compression_ratio": 1.624454140663147,
        "no_speech_prob": 6.014142854837701e-05
    },
    {
        "id": 37,
        "seek": 21400,
        "start": 221.60000610351562,
        "end": 227.36000061035156,
        "text": " tokens 101 and 32 can still occur in the sequence after merging. It's only when they occur exactly",
        "tokens": [
            50744,
            22667,
            21055,
            293,
            8858,
            393,
            920,
            5160,
            294,
            264,
            8310,
            934,
            44559,
            13,
            467,
            311,
            787,
            562,
            436,
            5160,
            2293,
            51032
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20624999701976776,
        "compression_ratio": 1.624454140663147,
        "no_speech_prob": 6.014142854837701e-05
    },
    {
        "id": 38,
        "seek": 21400,
        "start": 227.36000061035156,
        "end": 234.24000549316406,
        "text": " consecutively that that becomes 256 now. And in particular, the other thing to notice here is",
        "tokens": [
            51032,
            27154,
            3413,
            300,
            300,
            3643,
            38882,
            586,
            13,
            400,
            294,
            1729,
            11,
            264,
            661,
            551,
            281,
            3449,
            510,
            307,
            51376
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20624999701976776,
        "compression_ratio": 1.624454140663147,
        "no_speech_prob": 6.014142854837701e-05
    },
    {
        "id": 39,
        "seek": 21400,
        "start": 234.24000549316406,
        "end": 239.83999633789062,
        "text": " that the token 256, which is the newly minted token, is also eligible for merging. So here on",
        "tokens": [
            51376,
            300,
            264,
            14862,
            38882,
            11,
            597,
            307,
            264,
            15109,
            18189,
            292,
            14862,
            11,
            307,
            611,
            14728,
            337,
            44559,
            13,
            407,
            510,
            322,
            51656
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20624999701976776,
        "compression_ratio": 1.624454140663147,
        "no_speech_prob": 6.014142854837701e-05
    },
    {
        "id": 40,
        "seek": 23984,
        "start": 239.83999633789062,
        "end": 248.32000732421875,
        "text": " the bottom, the 20th merge was a merge of 256 and 259 becoming 275. So every time we replace these",
        "tokens": [
            50364,
            264,
            2767,
            11,
            264,
            945,
            392,
            22183,
            390,
            257,
            22183,
            295,
            38882,
            293,
            3552,
            24,
            5617,
            7634,
            20,
            13,
            407,
            633,
            565,
            321,
            7406,
            613,
            50788
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16507159173488617,
        "compression_ratio": 1.6166666746139526,
        "no_speech_prob": 0.0003982102789450437
    },
    {
        "id": 41,
        "seek": 23984,
        "start": 248.32000732421875,
        "end": 252.8800048828125,
        "text": " tokens, they become eligible for merging in the next round of the iteration. So that's why we're",
        "tokens": [
            50788,
            22667,
            11,
            436,
            1813,
            14728,
            337,
            44559,
            294,
            264,
            958,
            3098,
            295,
            264,
            24784,
            13,
            407,
            300,
            311,
            983,
            321,
            434,
            51016
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16507159173488617,
        "compression_ratio": 1.6166666746139526,
        "no_speech_prob": 0.0003982102789450437
    },
    {
        "id": 42,
        "seek": 23984,
        "start": 252.8800048828125,
        "end": 258.3999938964844,
        "text": " building up a small sort of binary forest instead of a single individual tree. One thing we can take",
        "tokens": [
            51016,
            2390,
            493,
            257,
            1359,
            1333,
            295,
            17434,
            6719,
            2602,
            295,
            257,
            2167,
            2609,
            4230,
            13,
            1485,
            551,
            321,
            393,
            747,
            51292
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16507159173488617,
        "compression_ratio": 1.6166666746139526,
        "no_speech_prob": 0.0003982102789450437
    },
    {
        "id": 43,
        "seek": 23984,
        "start": 258.3999938964844,
        "end": 262.79998779296875,
        "text": " a look at as well is we can take a look at the compression ratio that we've achieved. So in",
        "tokens": [
            51292,
            257,
            574,
            412,
            382,
            731,
            307,
            321,
            393,
            747,
            257,
            574,
            412,
            264,
            19355,
            8509,
            300,
            321,
            600,
            11042,
            13,
            407,
            294,
            51512
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16507159173488617,
        "compression_ratio": 1.6166666746139526,
        "no_speech_prob": 0.0003982102789450437
    },
    {
        "id": 44,
        "seek": 26280,
        "start": 262.79998779296875,
        "end": 269.2799987792969,
        "text": " particular, we started off with this tokens list. So we started off with 24,000 bytes,",
        "tokens": [
            50364,
            1729,
            11,
            321,
            1409,
            766,
            365,
            341,
            22667,
            1329,
            13,
            407,
            321,
            1409,
            766,
            365,
            4022,
            11,
            1360,
            36088,
            11,
            50688
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1813497394323349,
        "compression_ratio": 1.6163792610168457,
        "no_speech_prob": 0.003945226781070232
    },
    {
        "id": 45,
        "seek": 26280,
        "start": 269.9200134277344,
        "end": 278.79998779296875,
        "text": " and after merging 20 times, we now have only 19,000 tokens. And so therefore, the compression",
        "tokens": [
            50720,
            293,
            934,
            44559,
            945,
            1413,
            11,
            321,
            586,
            362,
            787,
            1294,
            11,
            1360,
            22667,
            13,
            400,
            370,
            4412,
            11,
            264,
            19355,
            51164
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1813497394323349,
        "compression_ratio": 1.6163792610168457,
        "no_speech_prob": 0.003945226781070232
    },
    {
        "id": 46,
        "seek": 26280,
        "start": 278.79998779296875,
        "end": 284.1600036621094,
        "text": " ratio of simply just dividing the two is roughly 1.27. So that's the amount of compression we're",
        "tokens": [
            51164,
            8509,
            295,
            2935,
            445,
            26764,
            264,
            732,
            307,
            9810,
            502,
            13,
            10076,
            13,
            407,
            300,
            311,
            264,
            2372,
            295,
            19355,
            321,
            434,
            51432
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1813497394323349,
        "compression_ratio": 1.6163792610168457,
        "no_speech_prob": 0.003945226781070232
    },
    {
        "id": 47,
        "seek": 26280,
        "start": 284.1600036621094,
        "end": 290.8800048828125,
        "text": " able to achieve of this text with only 20 merges. And of course, the more vocabulary elements you",
        "tokens": [
            51432,
            1075,
            281,
            4584,
            295,
            341,
            2487,
            365,
            787,
            945,
            3551,
            2880,
            13,
            400,
            295,
            1164,
            11,
            264,
            544,
            19864,
            4959,
            291,
            51768
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1813497394323349,
        "compression_ratio": 1.6163792610168457,
        "no_speech_prob": 0.003945226781070232
    },
    {
        "id": 48,
        "seek": 29088,
        "start": 290.8800048828125,
        "end": 298.32000732421875,
        "text": " add, the greater the compression ratio here would be. Finally, so that's kind of like",
        "tokens": [
            50364,
            909,
            11,
            264,
            5044,
            264,
            19355,
            8509,
            510,
            576,
            312,
            13,
            6288,
            11,
            370,
            300,
            311,
            733,
            295,
            411,
            50736
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1905735582113266,
        "compression_ratio": 1.7003744840621948,
        "no_speech_prob": 1.2606910786416847e-05
    },
    {
        "id": 49,
        "seek": 29088,
        "start": 300.1600036621094,
        "end": 304.7200012207031,
        "text": " the training of the tokenizer, if you will. Now, one point that I wanted to make is that,",
        "tokens": [
            50828,
            264,
            3097,
            295,
            264,
            14862,
            6545,
            11,
            498,
            291,
            486,
            13,
            823,
            11,
            472,
            935,
            300,
            286,
            1415,
            281,
            652,
            307,
            300,
            11,
            51056
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1905735582113266,
        "compression_ratio": 1.7003744840621948,
        "no_speech_prob": 1.2606910786416847e-05
    },
    {
        "id": 50,
        "seek": 29088,
        "start": 304.7200012207031,
        "end": 310.239990234375,
        "text": " and maybe this is a diagram that can help kind of illustrate, is that tokenizer is a completely",
        "tokens": [
            51056,
            293,
            1310,
            341,
            307,
            257,
            10686,
            300,
            393,
            854,
            733,
            295,
            23221,
            11,
            307,
            300,
            14862,
            6545,
            307,
            257,
            2584,
            51332
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1905735582113266,
        "compression_ratio": 1.7003744840621948,
        "no_speech_prob": 1.2606910786416847e-05
    },
    {
        "id": 51,
        "seek": 29088,
        "start": 310.239990234375,
        "end": 314.4800109863281,
        "text": " separate object from the large language model itself. So everything in this lecture, we're",
        "tokens": [
            51332,
            4994,
            2657,
            490,
            264,
            2416,
            2856,
            2316,
            2564,
            13,
            407,
            1203,
            294,
            341,
            7991,
            11,
            321,
            434,
            51544
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1905735582113266,
        "compression_ratio": 1.7003744840621948,
        "no_speech_prob": 1.2606910786416847e-05
    },
    {
        "id": 52,
        "seek": 29088,
        "start": 314.4800109863281,
        "end": 318.8800048828125,
        "text": " not really touching the LLM itself. We're just training the tokenizer. This is a completely",
        "tokens": [
            51544,
            406,
            534,
            11175,
            264,
            441,
            43,
            44,
            2564,
            13,
            492,
            434,
            445,
            3097,
            264,
            14862,
            6545,
            13,
            639,
            307,
            257,
            2584,
            51764
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1905735582113266,
        "compression_ratio": 1.7003744840621948,
        "no_speech_prob": 1.2606910786416847e-05
    },
    {
        "id": 53,
        "seek": 31888,
        "start": 318.8800048828125,
        "end": 323.8399963378906,
        "text": " separate preprocessing stage, usually. So the tokenizer will have its own training set,",
        "tokens": [
            50364,
            4994,
            2666,
            340,
            780,
            278,
            3233,
            11,
            2673,
            13,
            407,
            264,
            14862,
            6545,
            486,
            362,
            1080,
            1065,
            3097,
            992,
            11,
            50612
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1877143532037735,
        "compression_ratio": 1.884297490119934,
        "no_speech_prob": 0.0005193014512769878
    },
    {
        "id": 54,
        "seek": 31888,
        "start": 323.8399963378906,
        "end": 328.79998779296875,
        "text": " just like a large language model has a potentially different training set. So the tokenizer has a",
        "tokens": [
            50612,
            445,
            411,
            257,
            2416,
            2856,
            2316,
            575,
            257,
            7263,
            819,
            3097,
            992,
            13,
            407,
            264,
            14862,
            6545,
            575,
            257,
            50860
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1877143532037735,
        "compression_ratio": 1.884297490119934,
        "no_speech_prob": 0.0005193014512769878
    },
    {
        "id": 55,
        "seek": 31888,
        "start": 328.79998779296875,
        "end": 334.6400146484375,
        "text": " training set of documents on which you're going to train the tokenizer. And then we're performing",
        "tokens": [
            50860,
            3097,
            992,
            295,
            8512,
            322,
            597,
            291,
            434,
            516,
            281,
            3847,
            264,
            14862,
            6545,
            13,
            400,
            550,
            321,
            434,
            10205,
            51152
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1877143532037735,
        "compression_ratio": 1.884297490119934,
        "no_speech_prob": 0.0005193014512769878
    },
    {
        "id": 56,
        "seek": 31888,
        "start": 334.6400146484375,
        "end": 339.20001220703125,
        "text": " the byte-pair encoding algorithm, as we saw above, to train the vocabulary of this tokenizer.",
        "tokens": [
            51152,
            264,
            40846,
            12,
            79,
            1246,
            43430,
            9284,
            11,
            382,
            321,
            1866,
            3673,
            11,
            281,
            3847,
            264,
            19864,
            295,
            341,
            14862,
            6545,
            13,
            51380
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1877143532037735,
        "compression_ratio": 1.884297490119934,
        "no_speech_prob": 0.0005193014512769878
    },
    {
        "id": 57,
        "seek": 31888,
        "start": 340.0799865722656,
        "end": 343.67999267578125,
        "text": " So it has its own training set. It is a preprocessing stage that you would run",
        "tokens": [
            51424,
            407,
            309,
            575,
            1080,
            1065,
            3097,
            992,
            13,
            467,
            307,
            257,
            2666,
            340,
            780,
            278,
            3233,
            300,
            291,
            576,
            1190,
            51604
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1877143532037735,
        "compression_ratio": 1.884297490119934,
        "no_speech_prob": 0.0005193014512769878
    },
    {
        "id": 58,
        "seek": 34368,
        "start": 343.67999267578125,
        "end": 349.6000061035156,
        "text": " a single time in the beginning. And the tokenizer is trained using byte-pair encoding algorithm.",
        "tokens": [
            50364,
            257,
            2167,
            565,
            294,
            264,
            2863,
            13,
            400,
            264,
            14862,
            6545,
            307,
            8895,
            1228,
            40846,
            12,
            79,
            1246,
            43430,
            9284,
            13,
            50660
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19127893447875977,
        "compression_ratio": 1.7260273694992065,
        "no_speech_prob": 0.0033765423577278852
    },
    {
        "id": 59,
        "seek": 34368,
        "start": 350.239990234375,
        "end": 354.1600036621094,
        "text": " Once you have the tokenizer, once it's trained, and you have the vocabulary, and you have the",
        "tokens": [
            50692,
            3443,
            291,
            362,
            264,
            14862,
            6545,
            11,
            1564,
            309,
            311,
            8895,
            11,
            293,
            291,
            362,
            264,
            19864,
            11,
            293,
            291,
            362,
            264,
            50888
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19127893447875977,
        "compression_ratio": 1.7260273694992065,
        "no_speech_prob": 0.0033765423577278852
    },
    {
        "id": 60,
        "seek": 34368,
        "start": 354.1600036621094,
        "end": 361.9200134277344,
        "text": " merges, we can do both encoding and decoding. So these two arrows here. So the tokenizer is a",
        "tokens": [
            50888,
            3551,
            2880,
            11,
            321,
            393,
            360,
            1293,
            43430,
            293,
            979,
            8616,
            13,
            407,
            613,
            732,
            19669,
            510,
            13,
            407,
            264,
            14862,
            6545,
            307,
            257,
            51276
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19127893447875977,
        "compression_ratio": 1.7260273694992065,
        "no_speech_prob": 0.0033765423577278852
    },
    {
        "id": 61,
        "seek": 34368,
        "start": 361.9200134277344,
        "end": 368.239990234375,
        "text": " translation layer between raw text, which is, as we saw, the sequence of Unicode code points.",
        "tokens": [
            51276,
            12853,
            4583,
            1296,
            8936,
            2487,
            11,
            597,
            307,
            11,
            382,
            321,
            1866,
            11,
            264,
            8310,
            295,
            1156,
            299,
            1429,
            3089,
            2793,
            13,
            51592
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19127893447875977,
        "compression_ratio": 1.7260273694992065,
        "no_speech_prob": 0.0033765423577278852
    },
    {
        "id": 62,
        "seek": 36824,
        "start": 368.239990234375,
        "end": 373.3599853515625,
        "text": " It can take raw text and turn it into a token sequence. And vice versa, it can take a token",
        "tokens": [
            50364,
            467,
            393,
            747,
            8936,
            2487,
            293,
            1261,
            309,
            666,
            257,
            14862,
            8310,
            13,
            400,
            11964,
            25650,
            11,
            309,
            393,
            747,
            257,
            14862,
            50620
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1854878067970276,
        "compression_ratio": 1.9710743427276611,
        "no_speech_prob": 0.0002571395307313651
    },
    {
        "id": 63,
        "seek": 36824,
        "start": 373.3599853515625,
        "end": 380.55999755859375,
        "text": " sequence and translate it back into raw text. So now that we have trained the tokenizer,",
        "tokens": [
            50620,
            8310,
            293,
            13799,
            309,
            646,
            666,
            8936,
            2487,
            13,
            407,
            586,
            300,
            321,
            362,
            8895,
            264,
            14862,
            6545,
            11,
            50980
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1854878067970276,
        "compression_ratio": 1.9710743427276611,
        "no_speech_prob": 0.0002571395307313651
    },
    {
        "id": 64,
        "seek": 36824,
        "start": 380.55999755859375,
        "end": 385.8399963378906,
        "text": " and we have these merges, we are going to turn to how we can do the encoding and the decoding step.",
        "tokens": [
            50980,
            293,
            321,
            362,
            613,
            3551,
            2880,
            11,
            321,
            366,
            516,
            281,
            1261,
            281,
            577,
            321,
            393,
            360,
            264,
            43430,
            293,
            264,
            979,
            8616,
            1823,
            13,
            51244
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1854878067970276,
        "compression_ratio": 1.9710743427276611,
        "no_speech_prob": 0.0002571395307313651
    },
    {
        "id": 65,
        "seek": 36824,
        "start": 385.8399963378906,
        "end": 390.0,
        "text": " If you give me text, here are the tokens. And vice versa, if you give me tokens, here's the text.",
        "tokens": [
            51244,
            759,
            291,
            976,
            385,
            2487,
            11,
            510,
            366,
            264,
            22667,
            13,
            400,
            11964,
            25650,
            11,
            498,
            291,
            976,
            385,
            22667,
            11,
            510,
            311,
            264,
            2487,
            13,
            51452
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1854878067970276,
        "compression_ratio": 1.9710743427276611,
        "no_speech_prob": 0.0002571395307313651
    },
    {
        "id": 66,
        "seek": 36824,
        "start": 390.6400146484375,
        "end": 395.1199951171875,
        "text": " Once we have that, we can translate between these two realms. And then the language model is going",
        "tokens": [
            51484,
            3443,
            321,
            362,
            300,
            11,
            321,
            393,
            13799,
            1296,
            613,
            732,
            42824,
            13,
            400,
            550,
            264,
            2856,
            2316,
            307,
            516,
            51708
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1854878067970276,
        "compression_ratio": 1.9710743427276611,
        "no_speech_prob": 0.0002571395307313651
    },
    {
        "id": 67,
        "seek": 39512,
        "start": 395.1199951171875,
        "end": 401.67999267578125,
        "text": " to be trained as a step two afterwards. And typically, in a state-of-the-art application,",
        "tokens": [
            50364,
            281,
            312,
            8895,
            382,
            257,
            1823,
            732,
            10543,
            13,
            400,
            5850,
            11,
            294,
            257,
            1785,
            12,
            2670,
            12,
            3322,
            12,
            446,
            3861,
            11,
            50692
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2145843207836151,
        "compression_ratio": 1.7894736528396606,
        "no_speech_prob": 0.0021156705915927887
    },
    {
        "id": 68,
        "seek": 39512,
        "start": 401.67999267578125,
        "end": 405.2799987792969,
        "text": " you might stake all of your training data for the language model, and you might run it through the",
        "tokens": [
            50692,
            291,
            1062,
            10407,
            439,
            295,
            428,
            3097,
            1412,
            337,
            264,
            2856,
            2316,
            11,
            293,
            291,
            1062,
            1190,
            309,
            807,
            264,
            50872
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2145843207836151,
        "compression_ratio": 1.7894736528396606,
        "no_speech_prob": 0.0021156705915927887
    },
    {
        "id": 69,
        "seek": 39512,
        "start": 405.2799987792969,
        "end": 410.4800109863281,
        "text": " tokenizer and translate everything into a massive token sequence. And then you can throw away the",
        "tokens": [
            50872,
            14862,
            6545,
            293,
            13799,
            1203,
            666,
            257,
            5994,
            14862,
            8310,
            13,
            400,
            550,
            291,
            393,
            3507,
            1314,
            264,
            51132
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2145843207836151,
        "compression_ratio": 1.7894736528396606,
        "no_speech_prob": 0.0021156705915927887
    },
    {
        "id": 70,
        "seek": 39512,
        "start": 410.4800109863281,
        "end": 415.5199890136719,
        "text": " raw text. You're just left with the tokens themselves. And those are stored on disk.",
        "tokens": [
            51132,
            8936,
            2487,
            13,
            509,
            434,
            445,
            1411,
            365,
            264,
            22667,
            2969,
            13,
            400,
            729,
            366,
            12187,
            322,
            12355,
            13,
            51384
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2145843207836151,
        "compression_ratio": 1.7894736528396606,
        "no_speech_prob": 0.0021156705915927887
    },
    {
        "id": 71,
        "seek": 39512,
        "start": 415.5199890136719,
        "end": 419.2799987792969,
        "text": " And that is what the large language model is actually reading when it's training on them.",
        "tokens": [
            51384,
            400,
            300,
            307,
            437,
            264,
            2416,
            2856,
            2316,
            307,
            767,
            3760,
            562,
            309,
            311,
            3097,
            322,
            552,
            13,
            51572
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2145843207836151,
        "compression_ratio": 1.7894736528396606,
        "no_speech_prob": 0.0021156705915927887
    },
    {
        "id": 72,
        "seek": 39512,
        "start": 419.2799987792969,
        "end": 423.0400085449219,
        "text": " So that's one approach that you can take as a single massive pre-processing stage.",
        "tokens": [
            51572,
            407,
            300,
            311,
            472,
            3109,
            300,
            291,
            393,
            747,
            382,
            257,
            2167,
            5994,
            659,
            12,
            41075,
            278,
            3233,
            13,
            51760
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2145843207836151,
        "compression_ratio": 1.7894736528396606,
        "no_speech_prob": 0.0021156705915927887
    },
    {
        "id": 73,
        "seek": 42512,
        "start": 425.44000244140625,
        "end": 428.79998779296875,
        "text": " So yeah, basically, I think the most important thing I want to get across is that this is a",
        "tokens": [
            50380,
            407,
            1338,
            11,
            1936,
            11,
            286,
            519,
            264,
            881,
            1021,
            551,
            286,
            528,
            281,
            483,
            2108,
            307,
            300,
            341,
            307,
            257,
            50548
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1932251900434494,
        "compression_ratio": 1.8745874166488647,
        "no_speech_prob": 6.814823427703232e-05
    },
    {
        "id": 74,
        "seek": 42512,
        "start": 428.79998779296875,
        "end": 433.67999267578125,
        "text": " completely separate stage. It usually has its own entire training set. You may want to have those",
        "tokens": [
            50548,
            2584,
            4994,
            3233,
            13,
            467,
            2673,
            575,
            1080,
            1065,
            2302,
            3097,
            992,
            13,
            509,
            815,
            528,
            281,
            362,
            729,
            50792
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1932251900434494,
        "compression_ratio": 1.8745874166488647,
        "no_speech_prob": 6.814823427703232e-05
    },
    {
        "id": 75,
        "seek": 42512,
        "start": 433.67999267578125,
        "end": 437.67999267578125,
        "text": " training sets be different between the tokenizer and the large language model. So for example,",
        "tokens": [
            50792,
            3097,
            6352,
            312,
            819,
            1296,
            264,
            14862,
            6545,
            293,
            264,
            2416,
            2856,
            2316,
            13,
            407,
            337,
            1365,
            11,
            50992
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1932251900434494,
        "compression_ratio": 1.8745874166488647,
        "no_speech_prob": 6.814823427703232e-05
    },
    {
        "id": 76,
        "seek": 42512,
        "start": 437.67999267578125,
        "end": 442.0,
        "text": " when you're training the tokenizer, as I mentioned, we don't just care about the performance",
        "tokens": [
            50992,
            562,
            291,
            434,
            3097,
            264,
            14862,
            6545,
            11,
            382,
            286,
            2835,
            11,
            321,
            500,
            380,
            445,
            1127,
            466,
            264,
            3389,
            51208
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1932251900434494,
        "compression_ratio": 1.8745874166488647,
        "no_speech_prob": 6.814823427703232e-05
    },
    {
        "id": 77,
        "seek": 42512,
        "start": 442.0,
        "end": 447.44000244140625,
        "text": " of English text. We care about many different languages. And we also care about code or not",
        "tokens": [
            51208,
            295,
            3669,
            2487,
            13,
            492,
            1127,
            466,
            867,
            819,
            8650,
            13,
            400,
            321,
            611,
            1127,
            466,
            3089,
            420,
            406,
            51480
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1932251900434494,
        "compression_ratio": 1.8745874166488647,
        "no_speech_prob": 6.814823427703232e-05
    },
    {
        "id": 78,
        "seek": 42512,
        "start": 447.44000244140625,
        "end": 452.7200012207031,
        "text": " code. So you may want to look into different kinds of mixtures of different kinds of languages and",
        "tokens": [
            51480,
            3089,
            13,
            407,
            291,
            815,
            528,
            281,
            574,
            666,
            819,
            3685,
            295,
            2752,
            37610,
            295,
            819,
            3685,
            295,
            8650,
            293,
            51744
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1932251900434494,
        "compression_ratio": 1.8745874166488647,
        "no_speech_prob": 6.814823427703232e-05
    },
    {
        "id": 79,
        "seek": 45272,
        "start": 452.7200012207031,
        "end": 458.0799865722656,
        "text": " different amounts of code and things like that, because the amount of different language that you",
        "tokens": [
            50364,
            819,
            11663,
            295,
            3089,
            293,
            721,
            411,
            300,
            11,
            570,
            264,
            2372,
            295,
            819,
            2856,
            300,
            291,
            50632
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2047313153743744,
        "compression_ratio": 1.9043824672698975,
        "no_speech_prob": 0.0034295490477234125
    },
    {
        "id": 80,
        "seek": 45272,
        "start": 458.0799865722656,
        "end": 464.239990234375,
        "text": " have in your tokenizer training set will determine how many merges of it there will be. And therefore,",
        "tokens": [
            50632,
            362,
            294,
            428,
            14862,
            6545,
            3097,
            992,
            486,
            6997,
            577,
            867,
            3551,
            2880,
            295,
            309,
            456,
            486,
            312,
            13,
            400,
            4412,
            11,
            50940
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2047313153743744,
        "compression_ratio": 1.9043824672698975,
        "no_speech_prob": 0.0034295490477234125
    },
    {
        "id": 81,
        "seek": 45272,
        "start": 464.239990234375,
        "end": 471.9200134277344,
        "text": " that determines the density with which this type of data sort of has in the token space.",
        "tokens": [
            50940,
            300,
            24799,
            264,
            10305,
            365,
            597,
            341,
            2010,
            295,
            1412,
            1333,
            295,
            575,
            294,
            264,
            14862,
            1901,
            13,
            51324
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2047313153743744,
        "compression_ratio": 1.9043824672698975,
        "no_speech_prob": 0.0034295490477234125
    },
    {
        "id": 82,
        "seek": 45272,
        "start": 472.7200012207031,
        "end": 477.44000244140625,
        "text": " And so roughly speaking intuitively, if you add some amount of data, like say you have a ton of",
        "tokens": [
            51364,
            400,
            370,
            9810,
            4124,
            46506,
            11,
            498,
            291,
            909,
            512,
            2372,
            295,
            1412,
            11,
            411,
            584,
            291,
            362,
            257,
            2952,
            295,
            51600
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2047313153743744,
        "compression_ratio": 1.9043824672698975,
        "no_speech_prob": 0.0034295490477234125
    },
    {
        "id": 83,
        "seek": 45272,
        "start": 477.44000244140625,
        "end": 482.3999938964844,
        "text": " Japanese data in your tokenizer training set, then that means that more Japanese tokens will",
        "tokens": [
            51600,
            5433,
            1412,
            294,
            428,
            14862,
            6545,
            3097,
            992,
            11,
            550,
            300,
            1355,
            300,
            544,
            5433,
            22667,
            486,
            51848
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2047313153743744,
        "compression_ratio": 1.9043824672698975,
        "no_speech_prob": 0.0034295490477234125
    },
    {
        "id": 84,
        "seek": 48240,
        "start": 482.3999938964844,
        "end": 487.6000061035156,
        "text": " get merged. And therefore, Japanese will have shorter sequences. And that's going to be beneficial",
        "tokens": [
            50364,
            483,
            36427,
            13,
            400,
            4412,
            11,
            5433,
            486,
            362,
            11639,
            22978,
            13,
            400,
            300,
            311,
            516,
            281,
            312,
            14072,
            50624
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1688012331724167,
        "compression_ratio": 1.7681159973144531,
        "no_speech_prob": 2.885700268961955e-05
    },
    {
        "id": 85,
        "seek": 48240,
        "start": 487.6000061035156,
        "end": 492.7200012207031,
        "text": " for the large language model, which has a finite context length on which it can work on in the",
        "tokens": [
            50624,
            337,
            264,
            2416,
            2856,
            2316,
            11,
            597,
            575,
            257,
            19362,
            4319,
            4641,
            322,
            597,
            309,
            393,
            589,
            322,
            294,
            264,
            50880
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1688012331724167,
        "compression_ratio": 1.7681159973144531,
        "no_speech_prob": 2.885700268961955e-05
    },
    {
        "id": 86,
        "seek": 48240,
        "start": 492.7200012207031,
        "end": 498.55999755859375,
        "text": " token space. So hopefully that makes sense. So we're now going to turn to encoding and decoding",
        "tokens": [
            50880,
            14862,
            1901,
            13,
            407,
            4696,
            300,
            1669,
            2020,
            13,
            407,
            321,
            434,
            586,
            516,
            281,
            1261,
            281,
            43430,
            293,
            979,
            8616,
            51172
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1688012331724167,
        "compression_ratio": 1.7681159973144531,
        "no_speech_prob": 2.885700268961955e-05
    },
    {
        "id": 87,
        "seek": 48240,
        "start": 498.55999755859375,
        "end": 503.760009765625,
        "text": " now that we have trained a tokenizer. So we have our merges. And now how do we do encoding and",
        "tokens": [
            51172,
            586,
            300,
            321,
            362,
            8895,
            257,
            14862,
            6545,
            13,
            407,
            321,
            362,
            527,
            3551,
            2880,
            13,
            400,
            586,
            577,
            360,
            321,
            360,
            43430,
            293,
            51432
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1688012331724167,
        "compression_ratio": 1.7681159973144531,
        "no_speech_prob": 2.885700268961955e-05
    },
    {
        "id": 88,
        "seek": 48240,
        "start": 503.760009765625,
        "end": 509.67999267578125,
        "text": " decoding? Okay, so let's begin with decoding, which is this arrow over here. So given a token sequence,",
        "tokens": [
            51432,
            979,
            8616,
            30,
            1033,
            11,
            370,
            718,
            311,
            1841,
            365,
            979,
            8616,
            11,
            597,
            307,
            341,
            11610,
            670,
            510,
            13,
            407,
            2212,
            257,
            14862,
            8310,
            11,
            51728
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1688012331724167,
        "compression_ratio": 1.7681159973144531,
        "no_speech_prob": 2.885700268961955e-05
    },
    {
        "id": 89,
        "seek": 50968,
        "start": 510.239990234375,
        "end": 516.0,
        "text": " let's go through the tokenizer to get back a Python string object, so the raw text. So this",
        "tokens": [
            50392,
            718,
            311,
            352,
            807,
            264,
            14862,
            6545,
            281,
            483,
            646,
            257,
            15329,
            6798,
            2657,
            11,
            370,
            264,
            8936,
            2487,
            13,
            407,
            341,
            50680
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17423115670681,
        "compression_ratio": 1.6538461446762085,
        "no_speech_prob": 0.0006771831540390849
    },
    {
        "id": 90,
        "seek": 50968,
        "start": 516.0,
        "end": 520.47998046875,
        "text": " is the function that we'd like to implement. We're given a list of integers, and we want to return a",
        "tokens": [
            50680,
            307,
            264,
            2445,
            300,
            321,
            1116,
            411,
            281,
            4445,
            13,
            492,
            434,
            2212,
            257,
            1329,
            295,
            41674,
            11,
            293,
            321,
            528,
            281,
            2736,
            257,
            50904
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17423115670681,
        "compression_ratio": 1.6538461446762085,
        "no_speech_prob": 0.0006771831540390849
    },
    {
        "id": 91,
        "seek": 50968,
        "start": 520.47998046875,
        "end": 525.0399780273438,
        "text": " Python string. If you'd like, try to implement this function yourself. It's a fun exercise.",
        "tokens": [
            50904,
            15329,
            6798,
            13,
            759,
            291,
            1116,
            411,
            11,
            853,
            281,
            4445,
            341,
            2445,
            1803,
            13,
            467,
            311,
            257,
            1019,
            5380,
            13,
            51132
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17423115670681,
        "compression_ratio": 1.6538461446762085,
        "no_speech_prob": 0.0006771831540390849
    },
    {
        "id": 92,
        "seek": 50968,
        "start": 525.0399780273438,
        "end": 530.6400146484375,
        "text": " Otherwise, I'm going to start pasting in my own solution. So there are many different ways to do",
        "tokens": [
            51132,
            10328,
            11,
            286,
            478,
            516,
            281,
            722,
            1791,
            278,
            294,
            452,
            1065,
            3827,
            13,
            407,
            456,
            366,
            867,
            819,
            2098,
            281,
            360,
            51412
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17423115670681,
        "compression_ratio": 1.6538461446762085,
        "no_speech_prob": 0.0006771831540390849
    },
    {
        "id": 93,
        "seek": 50968,
        "start": 530.6400146484375,
        "end": 537.0399780273438,
        "text": " it. Here's one way. I will create a kind of pre-processing variable that I will call vocab.",
        "tokens": [
            51412,
            309,
            13,
            1692,
            311,
            472,
            636,
            13,
            286,
            486,
            1884,
            257,
            733,
            295,
            659,
            12,
            41075,
            278,
            7006,
            300,
            286,
            486,
            818,
            2329,
            455,
            13,
            51732
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17423115670681,
        "compression_ratio": 1.6538461446762085,
        "no_speech_prob": 0.0006771831540390849
    },
    {
        "id": 94,
        "seek": 53704,
        "start": 537.9199829101562,
        "end": 546.4000244140625,
        "text": " And vocab is a mapping or dictionary in Python from the token ID to the bytes object for that",
        "tokens": [
            50408,
            400,
            2329,
            455,
            307,
            257,
            18350,
            420,
            25890,
            294,
            15329,
            490,
            264,
            14862,
            7348,
            281,
            264,
            36088,
            2657,
            337,
            300,
            50832
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22981779277324677,
        "compression_ratio": 1.484536051750183,
        "no_speech_prob": 0.0004442164208739996
    },
    {
        "id": 95,
        "seek": 53704,
        "start": 546.4000244140625,
        "end": 553.5999755859375,
        "text": " token. So we begin with the raw bytes for tokens from 0 to 255. And then we go in order of all the",
        "tokens": [
            50832,
            14862,
            13,
            407,
            321,
            1841,
            365,
            264,
            8936,
            36088,
            337,
            22667,
            490,
            1958,
            281,
            3552,
            20,
            13,
            400,
            550,
            321,
            352,
            294,
            1668,
            295,
            439,
            264,
            51192
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22981779277324677,
        "compression_ratio": 1.484536051750183,
        "no_speech_prob": 0.0004442164208739996
    },
    {
        "id": 96,
        "seek": 53704,
        "start": 553.5999755859375,
        "end": 561.6799926757812,
        "text": " merges, and we sort of populate this vocab list by doing an addition here. So this is basically",
        "tokens": [
            51192,
            3551,
            2880,
            11,
            293,
            321,
            1333,
            295,
            1665,
            5256,
            341,
            2329,
            455,
            1329,
            538,
            884,
            364,
            4500,
            510,
            13,
            407,
            341,
            307,
            1936,
            51596
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22981779277324677,
        "compression_ratio": 1.484536051750183,
        "no_speech_prob": 0.0004442164208739996
    },
    {
        "id": 97,
        "seek": 56168,
        "start": 562.239990234375,
        "end": 566.8800048828125,
        "text": " the bytes representation of the first child followed by the second one. And remember,",
        "tokens": [
            50392,
            264,
            36088,
            10290,
            295,
            264,
            700,
            1440,
            6263,
            538,
            264,
            1150,
            472,
            13,
            400,
            1604,
            11,
            50624
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1605902761220932,
        "compression_ratio": 1.707692265510559,
        "no_speech_prob": 0.00036829672171734273
    },
    {
        "id": 98,
        "seek": 56168,
        "start": 566.8800048828125,
        "end": 571.6799926757812,
        "text": " these are bytes objects. So this addition here is an addition of two bytes objects,",
        "tokens": [
            50624,
            613,
            366,
            36088,
            6565,
            13,
            407,
            341,
            4500,
            510,
            307,
            364,
            4500,
            295,
            732,
            36088,
            6565,
            11,
            50864
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1605902761220932,
        "compression_ratio": 1.707692265510559,
        "no_speech_prob": 0.00036829672171734273
    },
    {
        "id": 99,
        "seek": 56168,
        "start": 571.6799926757812,
        "end": 577.6799926757812,
        "text": " just concatenation. So that's what we get here. One tricky thing to be careful with,",
        "tokens": [
            50864,
            445,
            1588,
            7186,
            399,
            13,
            407,
            300,
            311,
            437,
            321,
            483,
            510,
            13,
            1485,
            12414,
            551,
            281,
            312,
            5026,
            365,
            11,
            51164
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1605902761220932,
        "compression_ratio": 1.707692265510559,
        "no_speech_prob": 0.00036829672171734273
    },
    {
        "id": 100,
        "seek": 56168,
        "start": 577.6799926757812,
        "end": 583.9199829101562,
        "text": " by the way, is that I'm iterating a dictionary in Python using a dot items. And it really matters",
        "tokens": [
            51164,
            538,
            264,
            636,
            11,
            307,
            300,
            286,
            478,
            17138,
            990,
            257,
            25890,
            294,
            15329,
            1228,
            257,
            5893,
            4754,
            13,
            400,
            309,
            534,
            7001,
            51476
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1605902761220932,
        "compression_ratio": 1.707692265510559,
        "no_speech_prob": 0.00036829672171734273
    },
    {
        "id": 101,
        "seek": 56168,
        "start": 583.9199829101562,
        "end": 590.0800170898438,
        "text": " that this runs in the order in which we inserted items into the merges dictionary. Luckily,",
        "tokens": [
            51476,
            300,
            341,
            6676,
            294,
            264,
            1668,
            294,
            597,
            321,
            27992,
            4754,
            666,
            264,
            3551,
            2880,
            25890,
            13,
            19726,
            11,
            51784
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1605902761220932,
        "compression_ratio": 1.707692265510559,
        "no_speech_prob": 0.00036829672171734273
    },
    {
        "id": 102,
        "seek": 59008,
        "start": 590.0800170898438,
        "end": 595.3599853515625,
        "text": " starting with Python 3.7, this is guaranteed to be the case. But before Python 3.7, this iteration",
        "tokens": [
            50364,
            2891,
            365,
            15329,
            805,
            13,
            22,
            11,
            341,
            307,
            18031,
            281,
            312,
            264,
            1389,
            13,
            583,
            949,
            15329,
            805,
            13,
            22,
            11,
            341,
            24784,
            50628
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1670992076396942,
        "compression_ratio": 1.6391304731369019,
        "no_speech_prob": 0.0002571405784692615
    },
    {
        "id": 103,
        "seek": 59008,
        "start": 595.3599853515625,
        "end": 599.8400268554688,
        "text": " may have been out of order with respect to how we inserted elements into merges, and this may",
        "tokens": [
            50628,
            815,
            362,
            668,
            484,
            295,
            1668,
            365,
            3104,
            281,
            577,
            321,
            27992,
            4959,
            666,
            3551,
            2880,
            11,
            293,
            341,
            815,
            50852
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1670992076396942,
        "compression_ratio": 1.6391304731369019,
        "no_speech_prob": 0.0002571405784692615
    },
    {
        "id": 104,
        "seek": 59008,
        "start": 599.8400268554688,
        "end": 607.6799926757812,
        "text": " not have worked. But we are using modern Python, so we're okay. And then here, given the IDs,",
        "tokens": [
            50852,
            406,
            362,
            2732,
            13,
            583,
            321,
            366,
            1228,
            4363,
            15329,
            11,
            370,
            321,
            434,
            1392,
            13,
            400,
            550,
            510,
            11,
            2212,
            264,
            48212,
            11,
            51244
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1670992076396942,
        "compression_ratio": 1.6391304731369019,
        "no_speech_prob": 0.0002571405784692615
    },
    {
        "id": 105,
        "seek": 59008,
        "start": 607.6799926757812,
        "end": 614.1599731445312,
        "text": " the first thing we're going to do is get the tokens. So the way I implemented this here is",
        "tokens": [
            51244,
            264,
            700,
            551,
            321,
            434,
            516,
            281,
            360,
            307,
            483,
            264,
            22667,
            13,
            407,
            264,
            636,
            286,
            12270,
            341,
            510,
            307,
            51568
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1670992076396942,
        "compression_ratio": 1.6391304731369019,
        "no_speech_prob": 0.0002571405784692615
    },
    {
        "id": 106,
        "seek": 61416,
        "start": 614.719970703125,
        "end": 620.0800170898438,
        "text": " I'm iterating over all the IDs. I'm using vocab to look up their bytes. And then here,",
        "tokens": [
            50392,
            286,
            478,
            17138,
            990,
            670,
            439,
            264,
            48212,
            13,
            286,
            478,
            1228,
            2329,
            455,
            281,
            574,
            493,
            641,
            36088,
            13,
            400,
            550,
            510,
            11,
            50660
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20054268836975098,
        "compression_ratio": 1.663636326789856,
        "no_speech_prob": 0.03789011761546135
    },
    {
        "id": 107,
        "seek": 61416,
        "start": 620.0800170898438,
        "end": 625.52001953125,
        "text": " this is one way in Python to concatenate all these bytes together to create our tokens.",
        "tokens": [
            50660,
            341,
            307,
            472,
            636,
            294,
            15329,
            281,
            1588,
            7186,
            473,
            439,
            613,
            36088,
            1214,
            281,
            1884,
            527,
            22667,
            13,
            50932
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20054268836975098,
        "compression_ratio": 1.663636326789856,
        "no_speech_prob": 0.03789011761546135
    },
    {
        "id": 108,
        "seek": 61416,
        "start": 626.1599731445312,
        "end": 633.3599853515625,
        "text": " And then these tokens here at this point are raw bytes. So I have to decode using UTF-8 now",
        "tokens": [
            50964,
            400,
            550,
            613,
            22667,
            510,
            412,
            341,
            935,
            366,
            8936,
            36088,
            13,
            407,
            286,
            362,
            281,
            979,
            1429,
            1228,
            624,
            20527,
            12,
            23,
            586,
            51324
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20054268836975098,
        "compression_ratio": 1.663636326789856,
        "no_speech_prob": 0.03789011761546135
    },
    {
        "id": 109,
        "seek": 61416,
        "start": 633.3599853515625,
        "end": 639.52001953125,
        "text": " back into Python strings. So previously, we called that encode on a string object to get the bytes,",
        "tokens": [
            51324,
            646,
            666,
            15329,
            13985,
            13,
            407,
            8046,
            11,
            321,
            1219,
            300,
            2058,
            1429,
            322,
            257,
            6798,
            2657,
            281,
            483,
            264,
            36088,
            11,
            51632
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20054268836975098,
        "compression_ratio": 1.663636326789856,
        "no_speech_prob": 0.03789011761546135
    },
    {
        "id": 110,
        "seek": 63952,
        "start": 639.52001953125,
        "end": 644.9600219726562,
        "text": " and now we're doing it opposite. We're taking the bytes and calling a decode on the bytes object",
        "tokens": [
            50364,
            293,
            586,
            321,
            434,
            884,
            309,
            6182,
            13,
            492,
            434,
            1940,
            264,
            36088,
            293,
            5141,
            257,
            979,
            1429,
            322,
            264,
            36088,
            2657,
            50636
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16691954433918,
        "compression_ratio": 1.6515836715698242,
        "no_speech_prob": 2.1112477043061517e-05
    },
    {
        "id": 111,
        "seek": 63952,
        "start": 644.9600219726562,
        "end": 653.280029296875,
        "text": " to get a string in Python. And then we can return text. So this is how we can do it.",
        "tokens": [
            50636,
            281,
            483,
            257,
            6798,
            294,
            15329,
            13,
            400,
            550,
            321,
            393,
            2736,
            2487,
            13,
            407,
            341,
            307,
            577,
            321,
            393,
            360,
            309,
            13,
            51052
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16691954433918,
        "compression_ratio": 1.6515836715698242,
        "no_speech_prob": 2.1112477043061517e-05
    },
    {
        "id": 112,
        "seek": 63952,
        "start": 653.280029296875,
        "end": 659.1199951171875,
        "text": " Now, this actually has an issue in the way I implemented it, and this could actually throw",
        "tokens": [
            51052,
            823,
            11,
            341,
            767,
            575,
            364,
            2734,
            294,
            264,
            636,
            286,
            12270,
            309,
            11,
            293,
            341,
            727,
            767,
            3507,
            51344
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16691954433918,
        "compression_ratio": 1.6515836715698242,
        "no_speech_prob": 2.1112477043061517e-05
    },
    {
        "id": 113,
        "seek": 63952,
        "start": 659.1199951171875,
        "end": 664.9600219726562,
        "text": " an error. So try to figure out why this code could actually result in an error if we plug in",
        "tokens": [
            51344,
            364,
            6713,
            13,
            407,
            853,
            281,
            2573,
            484,
            983,
            341,
            3089,
            727,
            767,
            1874,
            294,
            364,
            6713,
            498,
            321,
            5452,
            294,
            51636
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16691954433918,
        "compression_ratio": 1.6515836715698242,
        "no_speech_prob": 2.1112477043061517e-05
    },
    {
        "id": 114,
        "seek": 66496,
        "start": 665.4400024414062,
        "end": 672.7999877929688,
        "text": " some sequence of IDs that is unlucky. So let me demonstrate the issue. When I try to decode",
        "tokens": [
            50388,
            512,
            8310,
            295,
            48212,
            300,
            307,
            38838,
            13,
            407,
            718,
            385,
            11698,
            264,
            2734,
            13,
            1133,
            286,
            853,
            281,
            979,
            1429,
            50756
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20939302444458008,
        "compression_ratio": 1.4725275039672852,
        "no_speech_prob": 9.761547698872164e-05
    },
    {
        "id": 115,
        "seek": 66496,
        "start": 672.7999877929688,
        "end": 679.1199951171875,
        "text": " just something like 97, I'm going to get a letter A here back. So nothing too crazy happening.",
        "tokens": [
            50756,
            445,
            746,
            411,
            23399,
            11,
            286,
            478,
            516,
            281,
            483,
            257,
            5063,
            316,
            510,
            646,
            13,
            407,
            1825,
            886,
            3219,
            2737,
            13,
            51072
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20939302444458008,
        "compression_ratio": 1.4725275039672852,
        "no_speech_prob": 9.761547698872164e-05
    },
    {
        "id": 116,
        "seek": 66496,
        "start": 680.0800170898438,
        "end": 687.280029296875,
        "text": " But when I try to decode 128 as a single element, the token 128 is what in string",
        "tokens": [
            51120,
            583,
            562,
            286,
            853,
            281,
            979,
            1429,
            29810,
            382,
            257,
            2167,
            4478,
            11,
            264,
            14862,
            29810,
            307,
            437,
            294,
            6798,
            51480
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20939302444458008,
        "compression_ratio": 1.4725275039672852,
        "no_speech_prob": 9.761547698872164e-05
    },
    {
        "id": 117,
        "seek": 68728,
        "start": 687.3599853515625,
        "end": 696.5599975585938,
        "text": " or in Python object, Unicode decoder. UTF-8 can't decode byte 0x80, which is this in hex,",
        "tokens": [
            50368,
            420,
            294,
            15329,
            2657,
            11,
            1156,
            299,
            1429,
            979,
            19866,
            13,
            624,
            20527,
            12,
            23,
            393,
            380,
            979,
            1429,
            40846,
            1958,
            87,
            4702,
            11,
            597,
            307,
            341,
            294,
            23291,
            11,
            50828
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1732998937368393,
        "compression_ratio": 1.5204918384552002,
        "no_speech_prob": 5.738753316109069e-05
    },
    {
        "id": 118,
        "seek": 68728,
        "start": 696.5599975585938,
        "end": 700.5599975585938,
        "text": " in position zero, invalid start byte. What does that mean? Well, to understand what this means,",
        "tokens": [
            50828,
            294,
            2535,
            4018,
            11,
            34702,
            722,
            40846,
            13,
            708,
            775,
            300,
            914,
            30,
            1042,
            11,
            281,
            1223,
            437,
            341,
            1355,
            11,
            51028
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1732998937368393,
        "compression_ratio": 1.5204918384552002,
        "no_speech_prob": 5.738753316109069e-05
    },
    {
        "id": 119,
        "seek": 68728,
        "start": 700.5599975585938,
        "end": 706.0,
        "text": " we have to go back to our UTF-8 page that I briefly showed earlier. And this is Wikipedia",
        "tokens": [
            51028,
            321,
            362,
            281,
            352,
            646,
            281,
            527,
            624,
            20527,
            12,
            23,
            3028,
            300,
            286,
            10515,
            4712,
            3071,
            13,
            400,
            341,
            307,
            28999,
            51300
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1732998937368393,
        "compression_ratio": 1.5204918384552002,
        "no_speech_prob": 5.738753316109069e-05
    },
    {
        "id": 120,
        "seek": 68728,
        "start": 706.0,
        "end": 713.52001953125,
        "text": " UTF-8. And basically, there's a specific schema that UTF-8 bytes take. So in particular, if you",
        "tokens": [
            51300,
            624,
            20527,
            12,
            23,
            13,
            400,
            1936,
            11,
            456,
            311,
            257,
            2685,
            34078,
            300,
            624,
            20527,
            12,
            23,
            36088,
            747,
            13,
            407,
            294,
            1729,
            11,
            498,
            291,
            51676
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1732998937368393,
        "compression_ratio": 1.5204918384552002,
        "no_speech_prob": 5.738753316109069e-05
    },
    {
        "id": 121,
        "seek": 71352,
        "start": 713.52001953125,
        "end": 718.8800048828125,
        "text": " have a multi-byte object for some of the Unicode characters, they have to have this special sort",
        "tokens": [
            50364,
            362,
            257,
            4825,
            12,
            2322,
            975,
            2657,
            337,
            512,
            295,
            264,
            1156,
            299,
            1429,
            4342,
            11,
            436,
            362,
            281,
            362,
            341,
            2121,
            1333,
            50632
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18383152782917023,
        "compression_ratio": 1.7105263471603394,
        "no_speech_prob": 1.8058512068819255e-05
    },
    {
        "id": 122,
        "seek": 71352,
        "start": 718.8800048828125,
        "end": 723.5999755859375,
        "text": " of envelope in how the encoding works. And so what's happening here is that",
        "tokens": [
            50632,
            295,
            19989,
            294,
            577,
            264,
            43430,
            1985,
            13,
            400,
            370,
            437,
            311,
            2737,
            510,
            307,
            300,
            50868
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18383152782917023,
        "compression_ratio": 1.7105263471603394,
        "no_speech_prob": 1.8058512068819255e-05
    },
    {
        "id": 123,
        "seek": 71352,
        "start": 724.8800048828125,
        "end": 732.0,
        "text": " invalid start byte, that's because 128, the binary representation of it is one followed by all zeros.",
        "tokens": [
            50932,
            34702,
            722,
            40846,
            11,
            300,
            311,
            570,
            29810,
            11,
            264,
            17434,
            10290,
            295,
            309,
            307,
            472,
            6263,
            538,
            439,
            35193,
            13,
            51288
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18383152782917023,
        "compression_ratio": 1.7105263471603394,
        "no_speech_prob": 1.8058512068819255e-05
    },
    {
        "id": 124,
        "seek": 71352,
        "start": 732.8800048828125,
        "end": 737.5999755859375,
        "text": " So we have one and then all zero. And we see here that that doesn't conform to the format",
        "tokens": [
            51332,
            407,
            321,
            362,
            472,
            293,
            550,
            439,
            4018,
            13,
            400,
            321,
            536,
            510,
            300,
            300,
            1177,
            380,
            18975,
            281,
            264,
            7877,
            51568
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18383152782917023,
        "compression_ratio": 1.7105263471603394,
        "no_speech_prob": 1.8058512068819255e-05
    },
    {
        "id": 125,
        "seek": 71352,
        "start": 737.5999755859375,
        "end": 742.6400146484375,
        "text": " because one followed by all zero just doesn't fit any of these rules, so to speak. So it's",
        "tokens": [
            51568,
            570,
            472,
            6263,
            538,
            439,
            4018,
            445,
            1177,
            380,
            3318,
            604,
            295,
            613,
            4474,
            11,
            370,
            281,
            1710,
            13,
            407,
            309,
            311,
            51820
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18383152782917023,
        "compression_ratio": 1.7105263471603394,
        "no_speech_prob": 1.8058512068819255e-05
    },
    {
        "id": 126,
        "seek": 74264,
        "start": 742.6400146484375,
        "end": 749.280029296875,
        "text": " an invalid start byte, which is byte one. This one must have a one following it, and then a zero",
        "tokens": [
            50364,
            364,
            34702,
            722,
            40846,
            11,
            597,
            307,
            40846,
            472,
            13,
            639,
            472,
            1633,
            362,
            257,
            472,
            3480,
            309,
            11,
            293,
            550,
            257,
            4018,
            50696
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19178920984268188,
        "compression_ratio": 1.6403508186340332,
        "no_speech_prob": 3.4268472518306226e-05
    },
    {
        "id": 127,
        "seek": 74264,
        "start": 749.280029296875,
        "end": 755.6799926757812,
        "text": " following it, and then the content of your Unicode in hexes here. So basically, we don't exactly",
        "tokens": [
            50696,
            3480,
            309,
            11,
            293,
            550,
            264,
            2701,
            295,
            428,
            1156,
            299,
            1429,
            294,
            23291,
            279,
            510,
            13,
            407,
            1936,
            11,
            321,
            500,
            380,
            2293,
            51016
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19178920984268188,
        "compression_ratio": 1.6403508186340332,
        "no_speech_prob": 3.4268472518306226e-05
    },
    {
        "id": 128,
        "seek": 74264,
        "start": 755.6799926757812,
        "end": 762.47998046875,
        "text": " follow the UTF-8 standard, and this cannot be decoded. And so the way to fix this is to use",
        "tokens": [
            51016,
            1524,
            264,
            624,
            20527,
            12,
            23,
            3832,
            11,
            293,
            341,
            2644,
            312,
            979,
            12340,
            13,
            400,
            370,
            264,
            636,
            281,
            3191,
            341,
            307,
            281,
            764,
            51356
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19178920984268188,
        "compression_ratio": 1.6403508186340332,
        "no_speech_prob": 3.4268472518306226e-05
    },
    {
        "id": 129,
        "seek": 74264,
        "start": 763.760009765625,
        "end": 771.280029296875,
        "text": " this errors equals in bytes.decode function of Python. And by default, errors is strict,",
        "tokens": [
            51420,
            341,
            13603,
            6915,
            294,
            36088,
            13,
            1479,
            22332,
            2445,
            295,
            15329,
            13,
            400,
            538,
            7576,
            11,
            13603,
            307,
            10910,
            11,
            51796
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19178920984268188,
        "compression_ratio": 1.6403508186340332,
        "no_speech_prob": 3.4268472518306226e-05
    },
    {
        "id": 130,
        "seek": 77128,
        "start": 771.280029296875,
        "end": 777.8400268554688,
        "text": " so we will throw an error if it's not valid UTF-8 bytes encoding. But there are many different",
        "tokens": [
            50364,
            370,
            321,
            486,
            3507,
            364,
            6713,
            498,
            309,
            311,
            406,
            7363,
            624,
            20527,
            12,
            23,
            36088,
            43430,
            13,
            583,
            456,
            366,
            867,
            819,
            50692
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18471463024616241,
        "compression_ratio": 1.6371681690216064,
        "no_speech_prob": 3.1875640615908196e-06
    },
    {
        "id": 131,
        "seek": 77128,
        "start": 777.8400268554688,
        "end": 782.1599731445312,
        "text": " things that you could put here on error handling. This is the full list of all the errors that you",
        "tokens": [
            50692,
            721,
            300,
            291,
            727,
            829,
            510,
            322,
            6713,
            13175,
            13,
            639,
            307,
            264,
            1577,
            1329,
            295,
            439,
            264,
            13603,
            300,
            291,
            50908
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18471463024616241,
        "compression_ratio": 1.6371681690216064,
        "no_speech_prob": 3.1875640615908196e-06
    },
    {
        "id": 132,
        "seek": 77128,
        "start": 782.1599731445312,
        "end": 788.3200073242188,
        "text": " can use. And in particular, instead of strict, let's change it to replace. And that will replace",
        "tokens": [
            50908,
            393,
            764,
            13,
            400,
            294,
            1729,
            11,
            2602,
            295,
            10910,
            11,
            718,
            311,
            1319,
            309,
            281,
            7406,
            13,
            400,
            300,
            486,
            7406,
            51216
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18471463024616241,
        "compression_ratio": 1.6371681690216064,
        "no_speech_prob": 3.1875640615908196e-06
    },
    {
        "id": 133,
        "seek": 77128,
        "start": 788.3200073242188,
        "end": 795.8400268554688,
        "text": " with this special marker, this replacement character. So errors equals replace,",
        "tokens": [
            51216,
            365,
            341,
            2121,
            15247,
            11,
            341,
            14419,
            2517,
            13,
            407,
            13603,
            6915,
            7406,
            11,
            51592
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18471463024616241,
        "compression_ratio": 1.6371681690216064,
        "no_speech_prob": 3.1875640615908196e-06
    },
    {
        "id": 134,
        "seek": 79584,
        "start": 796.7999877929688,
        "end": 804.0800170898438,
        "text": " and now we just get that character back. So basically, not every single byte sequence is",
        "tokens": [
            50412,
            293,
            586,
            321,
            445,
            483,
            300,
            2517,
            646,
            13,
            407,
            1936,
            11,
            406,
            633,
            2167,
            40846,
            8310,
            307,
            50776
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16784793138504028,
        "compression_ratio": 1.5666667222976685,
        "no_speech_prob": 2.931151720986236e-05
    },
    {
        "id": 135,
        "seek": 79584,
        "start": 804.0800170898438,
        "end": 810.8800048828125,
        "text": " valid UTF-8. And if it happens that your large language model, for example, predicts your tokens",
        "tokens": [
            50776,
            7363,
            624,
            20527,
            12,
            23,
            13,
            400,
            498,
            309,
            2314,
            300,
            428,
            2416,
            2856,
            2316,
            11,
            337,
            1365,
            11,
            6069,
            82,
            428,
            22667,
            51116
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16784793138504028,
        "compression_ratio": 1.5666667222976685,
        "no_speech_prob": 2.931151720986236e-05
    },
    {
        "id": 136,
        "seek": 79584,
        "start": 810.8800048828125,
        "end": 817.760009765625,
        "text": " in a bad manner, then they might not fall into valid UTF-8, and then we won't be able to decode",
        "tokens": [
            51116,
            294,
            257,
            1578,
            9060,
            11,
            550,
            436,
            1062,
            406,
            2100,
            666,
            7363,
            624,
            20527,
            12,
            23,
            11,
            293,
            550,
            321,
            1582,
            380,
            312,
            1075,
            281,
            979,
            1429,
            51460
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16784793138504028,
        "compression_ratio": 1.5666667222976685,
        "no_speech_prob": 2.931151720986236e-05
    },
    {
        "id": 137,
        "seek": 79584,
        "start": 817.760009765625,
        "end": 824.1599731445312,
        "text": " them. So the standard practice is to basically use errors equals replace, and this is what you",
        "tokens": [
            51460,
            552,
            13,
            407,
            264,
            3832,
            3124,
            307,
            281,
            1936,
            764,
            13603,
            6915,
            7406,
            11,
            293,
            341,
            307,
            437,
            291,
            51780
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16784793138504028,
        "compression_ratio": 1.5666667222976685,
        "no_speech_prob": 2.931151720986236e-05
    },
    {
        "id": 138,
        "seek": 82416,
        "start": 824.1599731445312,
        "end": 829.6799926757812,
        "text": " will also find in the OpenAI code that they released as well. But basically, whenever you",
        "tokens": [
            50364,
            486,
            611,
            915,
            294,
            264,
            7238,
            48698,
            3089,
            300,
            436,
            4736,
            382,
            731,
            13,
            583,
            1936,
            11,
            5699,
            291,
            50640
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2038998007774353,
        "compression_ratio": 1.7007042169570923,
        "no_speech_prob": 9.915236296365038e-05
    },
    {
        "id": 139,
        "seek": 82416,
        "start": 829.6799926757812,
        "end": 834.8800048828125,
        "text": " see this kind of a character in your output, in that case, something went wrong, and the LM output",
        "tokens": [
            50640,
            536,
            341,
            733,
            295,
            257,
            2517,
            294,
            428,
            5598,
            11,
            294,
            300,
            1389,
            11,
            746,
            1437,
            2085,
            11,
            293,
            264,
            46529,
            5598,
            50900
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2038998007774353,
        "compression_ratio": 1.7007042169570923,
        "no_speech_prob": 9.915236296365038e-05
    },
    {
        "id": 140,
        "seek": 82416,
        "start": 834.8800048828125,
        "end": 841.760009765625,
        "text": " was not valid sequence of tokens. Okay, and now we're going to go the other way. So we are going",
        "tokens": [
            50900,
            390,
            406,
            7363,
            8310,
            295,
            22667,
            13,
            1033,
            11,
            293,
            586,
            321,
            434,
            516,
            281,
            352,
            264,
            661,
            636,
            13,
            407,
            321,
            366,
            516,
            51244
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2038998007774353,
        "compression_ratio": 1.7007042169570923,
        "no_speech_prob": 9.915236296365038e-05
    },
    {
        "id": 141,
        "seek": 82416,
        "start": 841.760009765625,
        "end": 846.47998046875,
        "text": " to implement this error right here, where we are going to be given a string, and we want to encode",
        "tokens": [
            51244,
            281,
            4445,
            341,
            6713,
            558,
            510,
            11,
            689,
            321,
            366,
            516,
            281,
            312,
            2212,
            257,
            6798,
            11,
            293,
            321,
            528,
            281,
            2058,
            1429,
            51480
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2038998007774353,
        "compression_ratio": 1.7007042169570923,
        "no_speech_prob": 9.915236296365038e-05
    },
    {
        "id": 142,
        "seek": 82416,
        "start": 846.47998046875,
        "end": 853.8400268554688,
        "text": " it into tokens. So this is the signature of the function that we're interested in. And this should",
        "tokens": [
            51480,
            309,
            666,
            22667,
            13,
            407,
            341,
            307,
            264,
            13397,
            295,
            264,
            2445,
            300,
            321,
            434,
            3102,
            294,
            13,
            400,
            341,
            820,
            51848
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2038998007774353,
        "compression_ratio": 1.7007042169570923,
        "no_speech_prob": 9.915236296365038e-05
    },
    {
        "id": 143,
        "seek": 85384,
        "start": 854.0800170898438,
        "end": 859.9199829101562,
        "text": " print a list of integers of the tokens. So again, try to maybe implement this yourself if you'd like",
        "tokens": [
            50376,
            4482,
            257,
            1329,
            295,
            41674,
            295,
            264,
            22667,
            13,
            407,
            797,
            11,
            853,
            281,
            1310,
            4445,
            341,
            1803,
            498,
            291,
            1116,
            411,
            50668
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20041339099407196,
        "compression_ratio": 1.5922746658325195,
        "no_speech_prob": 8.750276901992038e-05
    },
    {
        "id": 144,
        "seek": 85384,
        "start": 859.9199829101562,
        "end": 863.9199829101562,
        "text": " a fun exercise, and pause here. Otherwise, I'm going to start putting in my solution.",
        "tokens": [
            50668,
            257,
            1019,
            5380,
            11,
            293,
            10465,
            510,
            13,
            10328,
            11,
            286,
            478,
            516,
            281,
            722,
            3372,
            294,
            452,
            3827,
            13,
            50868
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20041339099407196,
        "compression_ratio": 1.5922746658325195,
        "no_speech_prob": 8.750276901992038e-05
    },
    {
        "id": 145,
        "seek": 85384,
        "start": 865.52001953125,
        "end": 872.47998046875,
        "text": " So again, there are many ways to do this. So this is one of the ways that I came up with.",
        "tokens": [
            50948,
            407,
            797,
            11,
            456,
            366,
            867,
            2098,
            281,
            360,
            341,
            13,
            407,
            341,
            307,
            472,
            295,
            264,
            2098,
            300,
            286,
            1361,
            493,
            365,
            13,
            51296
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20041339099407196,
        "compression_ratio": 1.5922746658325195,
        "no_speech_prob": 8.750276901992038e-05
    },
    {
        "id": 146,
        "seek": 85384,
        "start": 873.9199829101562,
        "end": 880.0800170898438,
        "text": " So the first thing we're going to do is we are going to take our text, encode it into UTF-8 to",
        "tokens": [
            51368,
            407,
            264,
            700,
            551,
            321,
            434,
            516,
            281,
            360,
            307,
            321,
            366,
            516,
            281,
            747,
            527,
            2487,
            11,
            2058,
            1429,
            309,
            666,
            624,
            20527,
            12,
            23,
            281,
            51676
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20041339099407196,
        "compression_ratio": 1.5922746658325195,
        "no_speech_prob": 8.750276901992038e-05
    },
    {
        "id": 147,
        "seek": 88008,
        "start": 880.0800170898438,
        "end": 884.719970703125,
        "text": " get the raw bytes. And then as before, we're going to call list on the bytes object to get",
        "tokens": [
            50364,
            483,
            264,
            8936,
            36088,
            13,
            400,
            550,
            382,
            949,
            11,
            321,
            434,
            516,
            281,
            818,
            1329,
            322,
            264,
            36088,
            2657,
            281,
            483,
            50596
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16940511763095856,
        "compression_ratio": 1.8373016119003296,
        "no_speech_prob": 0.0004373332194518298
    },
    {
        "id": 148,
        "seek": 88008,
        "start": 884.719970703125,
        "end": 890.7999877929688,
        "text": " a list of integers of those bytes. So those are the starting tokens. Those are the raw bytes of",
        "tokens": [
            50596,
            257,
            1329,
            295,
            41674,
            295,
            729,
            36088,
            13,
            407,
            729,
            366,
            264,
            2891,
            22667,
            13,
            3950,
            366,
            264,
            8936,
            36088,
            295,
            50900
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16940511763095856,
        "compression_ratio": 1.8373016119003296,
        "no_speech_prob": 0.0004373332194518298
    },
    {
        "id": 149,
        "seek": 88008,
        "start": 890.7999877929688,
        "end": 896.0800170898438,
        "text": " our sequence. But now, of course, according to the merges dictionary above, and recall,",
        "tokens": [
            50900,
            527,
            8310,
            13,
            583,
            586,
            11,
            295,
            1164,
            11,
            4650,
            281,
            264,
            3551,
            2880,
            25890,
            3673,
            11,
            293,
            9901,
            11,
            51164
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16940511763095856,
        "compression_ratio": 1.8373016119003296,
        "no_speech_prob": 0.0004373332194518298
    },
    {
        "id": 150,
        "seek": 88008,
        "start": 896.0800170898438,
        "end": 902.9600219726562,
        "text": " this was the merges, some of the bytes may be merged, according to this lookup. In addition",
        "tokens": [
            51164,
            341,
            390,
            264,
            3551,
            2880,
            11,
            512,
            295,
            264,
            36088,
            815,
            312,
            36427,
            11,
            4650,
            281,
            341,
            574,
            1010,
            13,
            682,
            4500,
            51508
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16940511763095856,
        "compression_ratio": 1.8373016119003296,
        "no_speech_prob": 0.0004373332194518298
    },
    {
        "id": 151,
        "seek": 88008,
        "start": 902.9600219726562,
        "end": 906.8800048828125,
        "text": " to that, remember that the merges was built from top to bottom. And this is sort of the order in",
        "tokens": [
            51508,
            281,
            300,
            11,
            1604,
            300,
            264,
            3551,
            2880,
            390,
            3094,
            490,
            1192,
            281,
            2767,
            13,
            400,
            341,
            307,
            1333,
            295,
            264,
            1668,
            294,
            51704
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16940511763095856,
        "compression_ratio": 1.8373016119003296,
        "no_speech_prob": 0.0004373332194518298
    },
    {
        "id": 152,
        "seek": 90688,
        "start": 906.9600219726562,
        "end": 912.239990234375,
        "text": " which we inserted stuff into merges. And so we prefer to do all these merges in the beginning",
        "tokens": [
            50368,
            597,
            321,
            27992,
            1507,
            666,
            3551,
            2880,
            13,
            400,
            370,
            321,
            4382,
            281,
            360,
            439,
            613,
            3551,
            2880,
            294,
            264,
            2863,
            50632
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17873874306678772,
        "compression_ratio": 1.7027027606964111,
        "no_speech_prob": 0.009708280675113201
    },
    {
        "id": 153,
        "seek": 90688,
        "start": 912.239990234375,
        "end": 917.9199829101562,
        "text": " before we do these merges later. Because, for example, this merge over here relies on the",
        "tokens": [
            50632,
            949,
            321,
            360,
            613,
            3551,
            2880,
            1780,
            13,
            1436,
            11,
            337,
            1365,
            11,
            341,
            22183,
            670,
            510,
            30910,
            322,
            264,
            50916
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17873874306678772,
        "compression_ratio": 1.7027027606964111,
        "no_speech_prob": 0.009708280675113201
    },
    {
        "id": 154,
        "seek": 90688,
        "start": 917.9199829101562,
        "end": 924.1599731445312,
        "text": " 256, which got merged here. So we have to go in the order from top to bottom, sort of, if we are",
        "tokens": [
            50916,
            38882,
            11,
            597,
            658,
            36427,
            510,
            13,
            407,
            321,
            362,
            281,
            352,
            294,
            264,
            1668,
            490,
            1192,
            281,
            2767,
            11,
            1333,
            295,
            11,
            498,
            321,
            366,
            51228
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17873874306678772,
        "compression_ratio": 1.7027027606964111,
        "no_speech_prob": 0.009708280675113201
    },
    {
        "id": 155,
        "seek": 90688,
        "start": 924.1599731445312,
        "end": 929.6799926757812,
        "text": " going to be merging anything. Now, we expect to be doing a few merges. So we're going to be doing",
        "tokens": [
            51228,
            516,
            281,
            312,
            44559,
            1340,
            13,
            823,
            11,
            321,
            2066,
            281,
            312,
            884,
            257,
            1326,
            3551,
            2880,
            13,
            407,
            321,
            434,
            516,
            281,
            312,
            884,
            51504
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17873874306678772,
        "compression_ratio": 1.7027027606964111,
        "no_speech_prob": 0.009708280675113201
    },
    {
        "id": 156,
        "seek": 92968,
        "start": 929.6799926757812,
        "end": 937.52001953125,
        "text": " while true. And now we want to find a pair of bytes that is consecutive that we are allowed",
        "tokens": [
            50364,
            1339,
            2074,
            13,
            400,
            586,
            321,
            528,
            281,
            915,
            257,
            6119,
            295,
            36088,
            300,
            307,
            30497,
            300,
            321,
            366,
            4350,
            50756
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20697703957557678,
        "compression_ratio": 1.6347825527191162,
        "no_speech_prob": 0.001187883666716516
    },
    {
        "id": 157,
        "seek": 92968,
        "start": 937.52001953125,
        "end": 942.6400146484375,
        "text": " to merge according to this. In order to reuse some of the functionality that we've already written,",
        "tokens": [
            50756,
            281,
            22183,
            4650,
            281,
            341,
            13,
            682,
            1668,
            281,
            26225,
            512,
            295,
            264,
            14980,
            300,
            321,
            600,
            1217,
            3720,
            11,
            51012
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20697703957557678,
        "compression_ratio": 1.6347825527191162,
        "no_speech_prob": 0.001187883666716516
    },
    {
        "id": 158,
        "seek": 92968,
        "start": 942.6400146484375,
        "end": 949.6799926757812,
        "text": " I'm going to reuse the function getStats. So recall that getStats will give us the, will",
        "tokens": [
            51012,
            286,
            478,
            516,
            281,
            26225,
            264,
            2445,
            483,
            4520,
            1720,
            13,
            407,
            9901,
            300,
            483,
            4520,
            1720,
            486,
            976,
            505,
            264,
            11,
            486,
            51364
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20697703957557678,
        "compression_ratio": 1.6347825527191162,
        "no_speech_prob": 0.001187883666716516
    },
    {
        "id": 159,
        "seek": 92968,
        "start": 949.6799926757812,
        "end": 955.3599853515625,
        "text": " basically count up how many times every single pair occurs in our sequence of tokens and return",
        "tokens": [
            51364,
            1936,
            1207,
            493,
            577,
            867,
            1413,
            633,
            2167,
            6119,
            11843,
            294,
            527,
            8310,
            295,
            22667,
            293,
            2736,
            51648
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20697703957557678,
        "compression_ratio": 1.6347825527191162,
        "no_speech_prob": 0.001187883666716516
    },
    {
        "id": 160,
        "seek": 95536,
        "start": 955.3599853515625,
        "end": 963.1199951171875,
        "text": " that as a dictionary. And the dictionary was a mapping from all the different byte pairs to the",
        "tokens": [
            50364,
            300,
            382,
            257,
            25890,
            13,
            400,
            264,
            25890,
            390,
            257,
            18350,
            490,
            439,
            264,
            819,
            40846,
            15494,
            281,
            264,
            50752
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17787452042102814,
        "compression_ratio": 1.803030252456665,
        "no_speech_prob": 0.003593549830839038
    },
    {
        "id": 161,
        "seek": 95536,
        "start": 963.1199951171875,
        "end": 968.3200073242188,
        "text": " number of times that they occur, right? At this point, we don't actually care how many times",
        "tokens": [
            50752,
            1230,
            295,
            1413,
            300,
            436,
            5160,
            11,
            558,
            30,
            1711,
            341,
            935,
            11,
            321,
            500,
            380,
            767,
            1127,
            577,
            867,
            1413,
            51012
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17787452042102814,
        "compression_ratio": 1.803030252456665,
        "no_speech_prob": 0.003593549830839038
    },
    {
        "id": 162,
        "seek": 95536,
        "start": 968.3200073242188,
        "end": 973.6799926757812,
        "text": " they occur in the sequence. We only care what the raw pairs are in that sequence. And so I'm only",
        "tokens": [
            51012,
            436,
            5160,
            294,
            264,
            8310,
            13,
            492,
            787,
            1127,
            437,
            264,
            8936,
            15494,
            366,
            294,
            300,
            8310,
            13,
            400,
            370,
            286,
            478,
            787,
            51280
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17787452042102814,
        "compression_ratio": 1.803030252456665,
        "no_speech_prob": 0.003593549830839038
    },
    {
        "id": 163,
        "seek": 95536,
        "start": 973.6799926757812,
        "end": 978.3200073242188,
        "text": " going to be using basically the keys of this dictionary. I only care about the set of possible",
        "tokens": [
            51280,
            516,
            281,
            312,
            1228,
            1936,
            264,
            9317,
            295,
            341,
            25890,
            13,
            286,
            787,
            1127,
            466,
            264,
            992,
            295,
            1944,
            51512
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17787452042102814,
        "compression_ratio": 1.803030252456665,
        "no_speech_prob": 0.003593549830839038
    },
    {
        "id": 164,
        "seek": 95536,
        "start": 978.3200073242188,
        "end": 983.3599853515625,
        "text": " merge candidates, if that makes sense. Now we want to identify the pair that we're going to be",
        "tokens": [
            51512,
            22183,
            11255,
            11,
            498,
            300,
            1669,
            2020,
            13,
            823,
            321,
            528,
            281,
            5876,
            264,
            6119,
            300,
            321,
            434,
            516,
            281,
            312,
            51764
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17787452042102814,
        "compression_ratio": 1.803030252456665,
        "no_speech_prob": 0.003593549830839038
    },
    {
        "id": 165,
        "seek": 98336,
        "start": 983.3599853515625,
        "end": 989.8400268554688,
        "text": " merging at this stage of the loop. So what do we want? We want to find the pair or like a key",
        "tokens": [
            50364,
            44559,
            412,
            341,
            3233,
            295,
            264,
            6367,
            13,
            407,
            437,
            360,
            321,
            528,
            30,
            492,
            528,
            281,
            915,
            264,
            6119,
            420,
            411,
            257,
            2141,
            50688
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16157716512680054,
        "compression_ratio": 1.694690227508545,
        "no_speech_prob": 1.520681325928308e-05
    },
    {
        "id": 166,
        "seek": 98336,
        "start": 989.8400268554688,
        "end": 996.8800048828125,
        "text": " inside stats that has the lowest index in the merges dictionary, because we want to do all the",
        "tokens": [
            50688,
            1854,
            18152,
            300,
            575,
            264,
            12437,
            8186,
            294,
            264,
            3551,
            2880,
            25890,
            11,
            570,
            321,
            528,
            281,
            360,
            439,
            264,
            51040
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16157716512680054,
        "compression_ratio": 1.694690227508545,
        "no_speech_prob": 1.520681325928308e-05
    },
    {
        "id": 167,
        "seek": 98336,
        "start": 996.8800048828125,
        "end": 1002.0800170898438,
        "text": " early merges before we work our way to the late merges. So again, there are many different ways",
        "tokens": [
            51040,
            2440,
            3551,
            2880,
            949,
            321,
            589,
            527,
            636,
            281,
            264,
            3469,
            3551,
            2880,
            13,
            407,
            797,
            11,
            456,
            366,
            867,
            819,
            2098,
            51300
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16157716512680054,
        "compression_ratio": 1.694690227508545,
        "no_speech_prob": 1.520681325928308e-05
    },
    {
        "id": 168,
        "seek": 98336,
        "start": 1002.0800170898438,
        "end": 1009.9199829101562,
        "text": " to implement this, but I'm going to do something a little bit fancy here. So I'm going to be using",
        "tokens": [
            51300,
            281,
            4445,
            341,
            11,
            457,
            286,
            478,
            516,
            281,
            360,
            746,
            257,
            707,
            857,
            10247,
            510,
            13,
            407,
            286,
            478,
            516,
            281,
            312,
            1228,
            51692
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16157716512680054,
        "compression_ratio": 1.694690227508545,
        "no_speech_prob": 1.520681325928308e-05
    },
    {
        "id": 169,
        "seek": 100992,
        "start": 1009.9199829101562,
        "end": 1015.52001953125,
        "text": " the min over an iterator. In Python, when you call min on an iterator, and stats here is a",
        "tokens": [
            50364,
            264,
            923,
            670,
            364,
            17138,
            1639,
            13,
            682,
            15329,
            11,
            562,
            291,
            818,
            923,
            322,
            364,
            17138,
            1639,
            11,
            293,
            18152,
            510,
            307,
            257,
            50644
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17100541293621063,
        "compression_ratio": 1.8627450466156006,
        "no_speech_prob": 0.001151390722952783
    },
    {
        "id": 170,
        "seek": 100992,
        "start": 1015.52001953125,
        "end": 1021.52001953125,
        "text": " dictionary, we're going to be iterating the keys of this dictionary in Python. So we're looking at",
        "tokens": [
            50644,
            25890,
            11,
            321,
            434,
            516,
            281,
            312,
            17138,
            990,
            264,
            9317,
            295,
            341,
            25890,
            294,
            15329,
            13,
            407,
            321,
            434,
            1237,
            412,
            50944
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17100541293621063,
        "compression_ratio": 1.8627450466156006,
        "no_speech_prob": 0.001151390722952783
    },
    {
        "id": 171,
        "seek": 100992,
        "start": 1021.52001953125,
        "end": 1028.0799560546875,
        "text": " all the pairs inside stats, which are all the consecutive pairs. And we're going to be taking",
        "tokens": [
            50944,
            439,
            264,
            15494,
            1854,
            18152,
            11,
            597,
            366,
            439,
            264,
            30497,
            15494,
            13,
            400,
            321,
            434,
            516,
            281,
            312,
            1940,
            51272
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17100541293621063,
        "compression_ratio": 1.8627450466156006,
        "no_speech_prob": 0.001151390722952783
    },
    {
        "id": 172,
        "seek": 100992,
        "start": 1028.0799560546875,
        "end": 1035.9200439453125,
        "text": " the consecutive pair inside tokens that has the minimum what the min takes a key, which gives us",
        "tokens": [
            51272,
            264,
            30497,
            6119,
            1854,
            22667,
            300,
            575,
            264,
            7285,
            437,
            264,
            923,
            2516,
            257,
            2141,
            11,
            597,
            2709,
            505,
            51664
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17100541293621063,
        "compression_ratio": 1.8627450466156006,
        "no_speech_prob": 0.001151390722952783
    },
    {
        "id": 173,
        "seek": 103592,
        "start": 1035.9200439453125,
        "end": 1041.3599853515625,
        "text": " the function that is going to return a value over which we're going to do the min. And the one we",
        "tokens": [
            50364,
            264,
            2445,
            300,
            307,
            516,
            281,
            2736,
            257,
            2158,
            670,
            597,
            321,
            434,
            516,
            281,
            360,
            264,
            923,
            13,
            400,
            264,
            472,
            321,
            50636
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16939975321292877,
        "compression_ratio": 1.7117117643356323,
        "no_speech_prob": 0.008847109973430634
    },
    {
        "id": 174,
        "seek": 103592,
        "start": 1041.3599853515625,
        "end": 1052.0799560546875,
        "text": " care about is we care about taking merges, and basically getting that pair's index. So basically,",
        "tokens": [
            50636,
            1127,
            466,
            307,
            321,
            1127,
            466,
            1940,
            3551,
            2880,
            11,
            293,
            1936,
            1242,
            300,
            6119,
            311,
            8186,
            13,
            407,
            1936,
            11,
            51172
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16939975321292877,
        "compression_ratio": 1.7117117643356323,
        "no_speech_prob": 0.008847109973430634
    },
    {
        "id": 175,
        "seek": 103592,
        "start": 1052.0799560546875,
        "end": 1059.3599853515625,
        "text": " for any pair inside stats, we are going to be looking into merges at what index it has. And",
        "tokens": [
            51172,
            337,
            604,
            6119,
            1854,
            18152,
            11,
            321,
            366,
            516,
            281,
            312,
            1237,
            666,
            3551,
            2880,
            412,
            437,
            8186,
            309,
            575,
            13,
            400,
            51536
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16939975321292877,
        "compression_ratio": 1.7117117643356323,
        "no_speech_prob": 0.008847109973430634
    },
    {
        "id": 176,
        "seek": 103592,
        "start": 1059.3599853515625,
        "end": 1064.800048828125,
        "text": " we want to get the pair with the min number. So as an example, if there's a pair 101 and 32,",
        "tokens": [
            51536,
            321,
            528,
            281,
            483,
            264,
            6119,
            365,
            264,
            923,
            1230,
            13,
            407,
            382,
            364,
            1365,
            11,
            498,
            456,
            311,
            257,
            6119,
            21055,
            293,
            8858,
            11,
            51808
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16939975321292877,
        "compression_ratio": 1.7117117643356323,
        "no_speech_prob": 0.008847109973430634
    },
    {
        "id": 177,
        "seek": 106480,
        "start": 1064.800048828125,
        "end": 1069.6800537109375,
        "text": " we definitely want to get that pair. We want to identify it here and return it. And pair would",
        "tokens": [
            50364,
            321,
            2138,
            528,
            281,
            483,
            300,
            6119,
            13,
            492,
            528,
            281,
            5876,
            309,
            510,
            293,
            2736,
            309,
            13,
            400,
            6119,
            576,
            50608
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20814605057239532,
        "compression_ratio": 1.6331877708435059,
        "no_speech_prob": 7.722178270341828e-05
    },
    {
        "id": 178,
        "seek": 106480,
        "start": 1069.6800537109375,
        "end": 1077.8399658203125,
        "text": " become 101 32 if it occurs. And the reason that I'm putting a float inf here as a fallback is that",
        "tokens": [
            50608,
            1813,
            21055,
            8858,
            498,
            309,
            11843,
            13,
            400,
            264,
            1778,
            300,
            286,
            478,
            3372,
            257,
            15706,
            1536,
            510,
            382,
            257,
            2100,
            3207,
            307,
            300,
            51016
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20814605057239532,
        "compression_ratio": 1.6331877708435059,
        "no_speech_prob": 7.722178270341828e-05
    },
    {
        "id": 179,
        "seek": 106480,
        "start": 1077.8399658203125,
        "end": 1084.1600341796875,
        "text": " in the get function, when we basically consider a pair that doesn't occur in the merges,",
        "tokens": [
            51016,
            294,
            264,
            483,
            2445,
            11,
            562,
            321,
            1936,
            1949,
            257,
            6119,
            300,
            1177,
            380,
            5160,
            294,
            264,
            3551,
            2880,
            11,
            51332
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20814605057239532,
        "compression_ratio": 1.6331877708435059,
        "no_speech_prob": 7.722178270341828e-05
    },
    {
        "id": 180,
        "seek": 106480,
        "start": 1084.8800048828125,
        "end": 1090.0,
        "text": " then that pair is not eligible to be merged. So if in the token sequence, there's some pair",
        "tokens": [
            51368,
            550,
            300,
            6119,
            307,
            406,
            14728,
            281,
            312,
            36427,
            13,
            407,
            498,
            294,
            264,
            14862,
            8310,
            11,
            456,
            311,
            512,
            6119,
            51624
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20814605057239532,
        "compression_ratio": 1.6331877708435059,
        "no_speech_prob": 7.722178270341828e-05
    },
    {
        "id": 181,
        "seek": 109000,
        "start": 1090.0,
        "end": 1095.0400390625,
        "text": " that is not a merging pair, it cannot be merged, then it doesn't actually occur here,",
        "tokens": [
            50364,
            300,
            307,
            406,
            257,
            44559,
            6119,
            11,
            309,
            2644,
            312,
            36427,
            11,
            550,
            309,
            1177,
            380,
            767,
            5160,
            510,
            11,
            50616
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1738324910402298,
        "compression_ratio": 1.7615385055541992,
        "no_speech_prob": 0.007937708869576454
    },
    {
        "id": 182,
        "seek": 109000,
        "start": 1095.0400390625,
        "end": 1099.9200439453125,
        "text": " and it doesn't have an index, and it cannot be merged, which we will denote as float inf.",
        "tokens": [
            50616,
            293,
            309,
            1177,
            380,
            362,
            364,
            8186,
            11,
            293,
            309,
            2644,
            312,
            36427,
            11,
            597,
            321,
            486,
            45708,
            382,
            15706,
            1536,
            13,
            50860
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1738324910402298,
        "compression_ratio": 1.7615385055541992,
        "no_speech_prob": 0.007937708869576454
    },
    {
        "id": 183,
        "seek": 109000,
        "start": 1100.56005859375,
        "end": 1104.56005859375,
        "text": " And the reason infinity is nice here is because for sure we're guaranteed that it's not going to",
        "tokens": [
            50892,
            400,
            264,
            1778,
            13202,
            307,
            1481,
            510,
            307,
            570,
            337,
            988,
            321,
            434,
            18031,
            300,
            309,
            311,
            406,
            516,
            281,
            51092
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1738324910402298,
        "compression_ratio": 1.7615385055541992,
        "no_speech_prob": 0.007937708869576454
    },
    {
        "id": 184,
        "seek": 109000,
        "start": 1104.56005859375,
        "end": 1110.47998046875,
        "text": " participate in the list of candidates when we do the min. So this is one way to do it.",
        "tokens": [
            51092,
            8197,
            294,
            264,
            1329,
            295,
            11255,
            562,
            321,
            360,
            264,
            923,
            13,
            407,
            341,
            307,
            472,
            636,
            281,
            360,
            309,
            13,
            51388
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1738324910402298,
        "compression_ratio": 1.7615385055541992,
        "no_speech_prob": 0.007937708869576454
    },
    {
        "id": 185,
        "seek": 109000,
        "start": 1111.52001953125,
        "end": 1117.5999755859375,
        "text": " So basically, long story short, this returns the most eligible emerging candidate pair that occurs",
        "tokens": [
            51440,
            407,
            1936,
            11,
            938,
            1657,
            2099,
            11,
            341,
            11247,
            264,
            881,
            14728,
            14989,
            11532,
            6119,
            300,
            11843,
            51744
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1738324910402298,
        "compression_ratio": 1.7615385055541992,
        "no_speech_prob": 0.007937708869576454
    },
    {
        "id": 186,
        "seek": 111760,
        "start": 1117.5999755859375,
        "end": 1124.9599609375,
        "text": " in the tokens. Now, one thing to be careful with here is this function here might fail in the",
        "tokens": [
            50364,
            294,
            264,
            22667,
            13,
            823,
            11,
            472,
            551,
            281,
            312,
            5026,
            365,
            510,
            307,
            341,
            2445,
            510,
            1062,
            3061,
            294,
            264,
            50732
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18498508632183075,
        "compression_ratio": 1.7136363983154297,
        "no_speech_prob": 6.108851084718481e-05
    },
    {
        "id": 187,
        "seek": 111760,
        "start": 1124.9599609375,
        "end": 1134.1600341796875,
        "text": " following way. If there's nothing to merge, then there's nothing in merges that is satisfied",
        "tokens": [
            50732,
            3480,
            636,
            13,
            759,
            456,
            311,
            1825,
            281,
            22183,
            11,
            550,
            456,
            311,
            1825,
            294,
            3551,
            2880,
            300,
            307,
            11239,
            51192
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18498508632183075,
        "compression_ratio": 1.7136363983154297,
        "no_speech_prob": 6.108851084718481e-05
    },
    {
        "id": 188,
        "seek": 111760,
        "start": 1134.1600341796875,
        "end": 1139.52001953125,
        "text": " anymore. There's nothing to merge. Everything just returns float infs. And then the pair,",
        "tokens": [
            51192,
            3602,
            13,
            821,
            311,
            1825,
            281,
            22183,
            13,
            5471,
            445,
            11247,
            15706,
            1536,
            82,
            13,
            400,
            550,
            264,
            6119,
            11,
            51460
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18498508632183075,
        "compression_ratio": 1.7136363983154297,
        "no_speech_prob": 6.108851084718481e-05
    },
    {
        "id": 189,
        "seek": 111760,
        "start": 1139.52001953125,
        "end": 1145.3599853515625,
        "text": " I think, will just become the very first element of stats. But this pair is not actually a mergeable",
        "tokens": [
            51460,
            286,
            519,
            11,
            486,
            445,
            1813,
            264,
            588,
            700,
            4478,
            295,
            18152,
            13,
            583,
            341,
            6119,
            307,
            406,
            767,
            257,
            22183,
            712,
            51752
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18498508632183075,
        "compression_ratio": 1.7136363983154297,
        "no_speech_prob": 6.108851084718481e-05
    },
    {
        "id": 190,
        "seek": 114536,
        "start": 1145.3599853515625,
        "end": 1151.199951171875,
        "text": " pair. It just becomes the first pair inside stats arbitrarily because all these pairs evaluate to",
        "tokens": [
            50364,
            6119,
            13,
            467,
            445,
            3643,
            264,
            700,
            6119,
            1854,
            18152,
            19071,
            3289,
            570,
            439,
            613,
            15494,
            13059,
            281,
            50656
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19275107979774475,
        "compression_ratio": 1.6929824352264404,
        "no_speech_prob": 0.0002269336546305567
    },
    {
        "id": 191,
        "seek": 114536,
        "start": 1151.199951171875,
        "end": 1157.43994140625,
        "text": " float inf for the merging criterion. So basically, it could be that this doesn't succeed because",
        "tokens": [
            50656,
            15706,
            1536,
            337,
            264,
            44559,
            46691,
            13,
            407,
            1936,
            11,
            309,
            727,
            312,
            300,
            341,
            1177,
            380,
            7754,
            570,
            50968
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19275107979774475,
        "compression_ratio": 1.6929824352264404,
        "no_speech_prob": 0.0002269336546305567
    },
    {
        "id": 192,
        "seek": 114536,
        "start": 1157.43994140625,
        "end": 1162.9599609375,
        "text": " there's no more merging pairs. So if this pair is not in merges that was returned, then this is a",
        "tokens": [
            50968,
            456,
            311,
            572,
            544,
            44559,
            15494,
            13,
            407,
            498,
            341,
            6119,
            307,
            406,
            294,
            3551,
            2880,
            300,
            390,
            8752,
            11,
            550,
            341,
            307,
            257,
            51244
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19275107979774475,
        "compression_ratio": 1.6929824352264404,
        "no_speech_prob": 0.0002269336546305567
    },
    {
        "id": 193,
        "seek": 114536,
        "start": 1162.9599609375,
        "end": 1167.9200439453125,
        "text": " signal for us that actually there was nothing to merge. No single pair can be merged anymore.",
        "tokens": [
            51244,
            6358,
            337,
            505,
            300,
            767,
            456,
            390,
            1825,
            281,
            22183,
            13,
            883,
            2167,
            6119,
            393,
            312,
            36427,
            3602,
            13,
            51492
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19275107979774475,
        "compression_ratio": 1.6929824352264404,
        "no_speech_prob": 0.0002269336546305567
    },
    {
        "id": 194,
        "seek": 116792,
        "start": 1167.9200439453125,
        "end": 1172.719970703125,
        "text": " In that case, we will break out. Nothing else can be merged.",
        "tokens": [
            50364,
            682,
            300,
            1389,
            11,
            321,
            486,
            1821,
            484,
            13,
            6693,
            1646,
            393,
            312,
            36427,
            13,
            50604
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18676015734672546,
        "compression_ratio": 1.6398104429244995,
        "no_speech_prob": 0.0002571412769611925
    },
    {
        "id": 195,
        "seek": 116792,
        "start": 1175.43994140625,
        "end": 1179.199951171875,
        "text": " You may come up with a different implementation, by the way. This is kind of like really trying",
        "tokens": [
            50740,
            509,
            815,
            808,
            493,
            365,
            257,
            819,
            11420,
            11,
            538,
            264,
            636,
            13,
            639,
            307,
            733,
            295,
            411,
            534,
            1382,
            50928
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18676015734672546,
        "compression_ratio": 1.6398104429244995,
        "no_speech_prob": 0.0002571412769611925
    },
    {
        "id": 196,
        "seek": 116792,
        "start": 1179.199951171875,
        "end": 1185.1199951171875,
        "text": " hard in Python. But really, we're just trying to find a pair that can be merged with a lowest",
        "tokens": [
            50928,
            1152,
            294,
            15329,
            13,
            583,
            534,
            11,
            321,
            434,
            445,
            1382,
            281,
            915,
            257,
            6119,
            300,
            393,
            312,
            36427,
            365,
            257,
            12437,
            51224
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18676015734672546,
        "compression_ratio": 1.6398104429244995,
        "no_speech_prob": 0.0002571412769611925
    },
    {
        "id": 197,
        "seek": 116792,
        "start": 1185.1199951171875,
        "end": 1193.0400390625,
        "text": " index here. Now, if we did find a pair that is inside merges with the lowest index, then we can",
        "tokens": [
            51224,
            8186,
            510,
            13,
            823,
            11,
            498,
            321,
            630,
            915,
            257,
            6119,
            300,
            307,
            1854,
            3551,
            2880,
            365,
            264,
            12437,
            8186,
            11,
            550,
            321,
            393,
            51620
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18676015734672546,
        "compression_ratio": 1.6398104429244995,
        "no_speech_prob": 0.0002571412769611925
    },
    {
        "id": 198,
        "seek": 119304,
        "start": 1193.0400390625,
        "end": 1201.6800537109375,
        "text": " merge it. So we're going to look into the mergers dictionary for that pair to look up the index.",
        "tokens": [
            50364,
            22183,
            309,
            13,
            407,
            321,
            434,
            516,
            281,
            574,
            666,
            264,
            3551,
            9458,
            25890,
            337,
            300,
            6119,
            281,
            574,
            493,
            264,
            8186,
            13,
            50796
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20067934691905975,
        "compression_ratio": 2.0754716396331787,
        "no_speech_prob": 0.0004728532221633941
    },
    {
        "id": 199,
        "seek": 119304,
        "start": 1201.6800537109375,
        "end": 1206.6400146484375,
        "text": " And we're going to now merge that into that index. So we're going to do tokens equals,",
        "tokens": [
            50796,
            400,
            321,
            434,
            516,
            281,
            586,
            22183,
            300,
            666,
            300,
            8186,
            13,
            407,
            321,
            434,
            516,
            281,
            360,
            22667,
            6915,
            11,
            51044
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20067934691905975,
        "compression_ratio": 2.0754716396331787,
        "no_speech_prob": 0.0004728532221633941
    },
    {
        "id": 200,
        "seek": 119304,
        "start": 1206.6400146484375,
        "end": 1213.0400390625,
        "text": " and we're going to replace the original tokens. We're going to be replacing the pair pair,",
        "tokens": [
            51044,
            293,
            321,
            434,
            516,
            281,
            7406,
            264,
            3380,
            22667,
            13,
            492,
            434,
            516,
            281,
            312,
            19139,
            264,
            6119,
            6119,
            11,
            51364
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20067934691905975,
        "compression_ratio": 2.0754716396331787,
        "no_speech_prob": 0.0004728532221633941
    },
    {
        "id": 201,
        "seek": 119304,
        "start": 1213.0400390625,
        "end": 1218.0799560546875,
        "text": " and we're going to be replacing it with index idx. And this returns a new list of tokens",
        "tokens": [
            51364,
            293,
            321,
            434,
            516,
            281,
            312,
            19139,
            309,
            365,
            8186,
            4496,
            87,
            13,
            400,
            341,
            11247,
            257,
            777,
            1329,
            295,
            22667,
            51616
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20067934691905975,
        "compression_ratio": 2.0754716396331787,
        "no_speech_prob": 0.0004728532221633941
    },
    {
        "id": 202,
        "seek": 119304,
        "start": 1218.0799560546875,
        "end": 1221.760009765625,
        "text": " where every occurrence of pair is replaced with idx. So we're doing a merge.",
        "tokens": [
            51616,
            689,
            633,
            36122,
            295,
            6119,
            307,
            10772,
            365,
            4496,
            87,
            13,
            407,
            321,
            434,
            884,
            257,
            22183,
            13,
            51800
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20067934691905975,
        "compression_ratio": 2.0754716396331787,
        "no_speech_prob": 0.0004728532221633941
    },
    {
        "id": 203,
        "seek": 122176,
        "start": 1222.719970703125,
        "end": 1226.1600341796875,
        "text": " And we're going to be continuing this until eventually nothing can be merged. We'll come",
        "tokens": [
            50412,
            400,
            321,
            434,
            516,
            281,
            312,
            9289,
            341,
            1826,
            4728,
            1825,
            393,
            312,
            36427,
            13,
            492,
            603,
            808,
            50584
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19254031777381897,
        "compression_ratio": 1.619217038154602,
        "no_speech_prob": 9.761544788489118e-05
    },
    {
        "id": 204,
        "seek": 122176,
        "start": 1226.1600341796875,
        "end": 1232.4000244140625,
        "text": " out here, and we'll break out. And here we just return tokens. And so that's the",
        "tokens": [
            50584,
            484,
            510,
            11,
            293,
            321,
            603,
            1821,
            484,
            13,
            400,
            510,
            321,
            445,
            2736,
            22667,
            13,
            400,
            370,
            300,
            311,
            264,
            50896
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19254031777381897,
        "compression_ratio": 1.619217038154602,
        "no_speech_prob": 9.761544788489118e-05
    },
    {
        "id": 205,
        "seek": 122176,
        "start": 1232.4000244140625,
        "end": 1239.52001953125,
        "text": " implementation, I think. So hopefully this runs. Okay, cool. Yeah, and this looks reasonable. So",
        "tokens": [
            50896,
            11420,
            11,
            286,
            519,
            13,
            407,
            4696,
            341,
            6676,
            13,
            1033,
            11,
            1627,
            13,
            865,
            11,
            293,
            341,
            1542,
            10585,
            13,
            407,
            51252
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19254031777381897,
        "compression_ratio": 1.619217038154602,
        "no_speech_prob": 9.761544788489118e-05
    },
    {
        "id": 206,
        "seek": 122176,
        "start": 1239.52001953125,
        "end": 1247.1199951171875,
        "text": " for example, 32 is a space in ASCII, so that's here. So this looks like it worked. Great.",
        "tokens": [
            51252,
            337,
            1365,
            11,
            8858,
            307,
            257,
            1901,
            294,
            7469,
            34,
            9503,
            11,
            370,
            300,
            311,
            510,
            13,
            407,
            341,
            1542,
            411,
            309,
            2732,
            13,
            3769,
            13,
            51632
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19254031777381897,
        "compression_ratio": 1.619217038154602,
        "no_speech_prob": 9.761544788489118e-05
    },
    {
        "id": 207,
        "seek": 122176,
        "start": 1247.1199951171875,
        "end": 1251.43994140625,
        "text": " Okay, so let's wrap up this section of the video, at least. I wanted to point out that this is not",
        "tokens": [
            51632,
            1033,
            11,
            370,
            718,
            311,
            7019,
            493,
            341,
            3541,
            295,
            264,
            960,
            11,
            412,
            1935,
            13,
            286,
            1415,
            281,
            935,
            484,
            300,
            341,
            307,
            406,
            51848
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19254031777381897,
        "compression_ratio": 1.619217038154602,
        "no_speech_prob": 9.761544788489118e-05
    },
    {
        "id": 208,
        "seek": 125144,
        "start": 1251.43994140625,
        "end": 1256.0,
        "text": " quite the right implementation just yet, because we are leaving out a special case. So in particular,",
        "tokens": [
            50364,
            1596,
            264,
            558,
            11420,
            445,
            1939,
            11,
            570,
            321,
            366,
            5012,
            484,
            257,
            2121,
            1389,
            13,
            407,
            294,
            1729,
            11,
            50592
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19179023802280426,
        "compression_ratio": 1.7698112726211548,
        "no_speech_prob": 0.0008426379645243287
    },
    {
        "id": 209,
        "seek": 125144,
        "start": 1256.0,
        "end": 1262.47998046875,
        "text": " if we try to do this, this would give us an error. And the issue is that if we only have a single",
        "tokens": [
            50592,
            498,
            321,
            853,
            281,
            360,
            341,
            11,
            341,
            576,
            976,
            505,
            364,
            6713,
            13,
            400,
            264,
            2734,
            307,
            300,
            498,
            321,
            787,
            362,
            257,
            2167,
            50916
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19179023802280426,
        "compression_ratio": 1.7698112726211548,
        "no_speech_prob": 0.0008426379645243287
    },
    {
        "id": 210,
        "seek": 125144,
        "start": 1262.47998046875,
        "end": 1266.9599609375,
        "text": " character or an empty string, then stats is empty, and that causes an issue inside min.",
        "tokens": [
            50916,
            2517,
            420,
            364,
            6707,
            6798,
            11,
            550,
            18152,
            307,
            6707,
            11,
            293,
            300,
            7700,
            364,
            2734,
            1854,
            923,
            13,
            51140
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19179023802280426,
        "compression_ratio": 1.7698112726211548,
        "no_speech_prob": 0.0008426379645243287
    },
    {
        "id": 211,
        "seek": 125144,
        "start": 1267.52001953125,
        "end": 1273.760009765625,
        "text": " So one way to fight this is if len of tokens is at least two, because if it's less than two,",
        "tokens": [
            51168,
            407,
            472,
            636,
            281,
            2092,
            341,
            307,
            498,
            40116,
            295,
            22667,
            307,
            412,
            1935,
            732,
            11,
            570,
            498,
            309,
            311,
            1570,
            813,
            732,
            11,
            51480
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19179023802280426,
        "compression_ratio": 1.7698112726211548,
        "no_speech_prob": 0.0008426379645243287
    },
    {
        "id": 212,
        "seek": 125144,
        "start": 1273.760009765625,
        "end": 1278.239990234375,
        "text": " it's just a single token or no tokens, then there's nothing to merge, so we just return.",
        "tokens": [
            51480,
            309,
            311,
            445,
            257,
            2167,
            14862,
            420,
            572,
            22667,
            11,
            550,
            456,
            311,
            1825,
            281,
            22183,
            11,
            370,
            321,
            445,
            2736,
            13,
            51704
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19179023802280426,
        "compression_ratio": 1.7698112726211548,
        "no_speech_prob": 0.0008426379645243287
    },
    {
        "id": 213,
        "seek": 127824,
        "start": 1278.800048828125,
        "end": 1285.9200439453125,
        "text": " So that would fix that case. Okay, and then second, I have a few test cases here for us as well.",
        "tokens": [
            50392,
            407,
            300,
            576,
            3191,
            300,
            1389,
            13,
            1033,
            11,
            293,
            550,
            1150,
            11,
            286,
            362,
            257,
            1326,
            1500,
            3331,
            510,
            337,
            505,
            382,
            731,
            13,
            50748
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1957782357931137,
        "compression_ratio": 1.6880733966827393,
        "no_speech_prob": 0.00026530149625614285
    },
    {
        "id": 214,
        "seek": 127824,
        "start": 1286.47998046875,
        "end": 1293.1199951171875,
        "text": " So first, let's make sure about, or let's note the following. If we take a string, and we try",
        "tokens": [
            50776,
            407,
            700,
            11,
            718,
            311,
            652,
            988,
            466,
            11,
            420,
            718,
            311,
            3637,
            264,
            3480,
            13,
            759,
            321,
            747,
            257,
            6798,
            11,
            293,
            321,
            853,
            51108
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1957782357931137,
        "compression_ratio": 1.6880733966827393,
        "no_speech_prob": 0.00026530149625614285
    },
    {
        "id": 215,
        "seek": 127824,
        "start": 1293.1199951171875,
        "end": 1296.8800048828125,
        "text": " to encode it, and then decode it back, you'd expect to get the same string back, right?",
        "tokens": [
            51108,
            281,
            2058,
            1429,
            309,
            11,
            293,
            550,
            979,
            1429,
            309,
            646,
            11,
            291,
            1116,
            2066,
            281,
            483,
            264,
            912,
            6798,
            646,
            11,
            558,
            30,
            51296
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1957782357931137,
        "compression_ratio": 1.6880733966827393,
        "no_speech_prob": 0.00026530149625614285
    },
    {
        "id": 216,
        "seek": 127824,
        "start": 1297.52001953125,
        "end": 1304.8800048828125,
        "text": " Is that true for all strings? So I think, so here it is the case, and I think in general,",
        "tokens": [
            51328,
            1119,
            300,
            2074,
            337,
            439,
            13985,
            30,
            407,
            286,
            519,
            11,
            370,
            510,
            309,
            307,
            264,
            1389,
            11,
            293,
            286,
            519,
            294,
            2674,
            11,
            51696
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1957782357931137,
        "compression_ratio": 1.6880733966827393,
        "no_speech_prob": 0.00026530149625614285
    },
    {
        "id": 217,
        "seek": 130488,
        "start": 1304.8800048828125,
        "end": 1311.760009765625,
        "text": " this is probably the case. But notice that going backwards is not, you're not going to have an",
        "tokens": [
            50364,
            341,
            307,
            1391,
            264,
            1389,
            13,
            583,
            3449,
            300,
            516,
            12204,
            307,
            406,
            11,
            291,
            434,
            406,
            516,
            281,
            362,
            364,
            50708
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18793289363384247,
        "compression_ratio": 1.5957447290420532,
        "no_speech_prob": 9.915258851833642e-05
    },
    {
        "id": 218,
        "seek": 130488,
        "start": 1311.760009765625,
        "end": 1320.0,
        "text": " identity going backwards, because as I mentioned, not all token sequences are valid UTF-8 sort of",
        "tokens": [
            50708,
            6575,
            516,
            12204,
            11,
            570,
            382,
            286,
            2835,
            11,
            406,
            439,
            14862,
            22978,
            366,
            7363,
            624,
            20527,
            12,
            23,
            1333,
            295,
            51120
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18793289363384247,
        "compression_ratio": 1.5957447290420532,
        "no_speech_prob": 9.915258851833642e-05
    },
    {
        "id": 219,
        "seek": 130488,
        "start": 1320.0,
        "end": 1326.47998046875,
        "text": " byte streams. And so therefore, some of them can't even be decodable. So this only goes in",
        "tokens": [
            51120,
            40846,
            15842,
            13,
            400,
            370,
            4412,
            11,
            512,
            295,
            552,
            393,
            380,
            754,
            312,
            979,
            378,
            712,
            13,
            407,
            341,
            787,
            1709,
            294,
            51444
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18793289363384247,
        "compression_ratio": 1.5957447290420532,
        "no_speech_prob": 9.915258851833642e-05
    },
    {
        "id": 220,
        "seek": 130488,
        "start": 1326.47998046875,
        "end": 1332.0799560546875,
        "text": " one direction. But for that one direction, we can check here, if we take the training text,",
        "tokens": [
            51444,
            472,
            3513,
            13,
            583,
            337,
            300,
            472,
            3513,
            11,
            321,
            393,
            1520,
            510,
            11,
            498,
            321,
            747,
            264,
            3097,
            2487,
            11,
            51724
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18793289363384247,
        "compression_ratio": 1.5957447290420532,
        "no_speech_prob": 9.915258851833642e-05
    },
    {
        "id": 221,
        "seek": 133208,
        "start": 1332.0799560546875,
        "end": 1335.8399658203125,
        "text": " which is the text that we trained the tokenizer on, we can make sure that when we encode and decode,",
        "tokens": [
            50364,
            597,
            307,
            264,
            2487,
            300,
            321,
            8895,
            264,
            14862,
            6545,
            322,
            11,
            321,
            393,
            652,
            988,
            300,
            562,
            321,
            2058,
            1429,
            293,
            979,
            1429,
            11,
            50552
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17607508599758148,
        "compression_ratio": 1.8074073791503906,
        "no_speech_prob": 0.00014653119433205575
    },
    {
        "id": 222,
        "seek": 133208,
        "start": 1335.8399658203125,
        "end": 1340.9599609375,
        "text": " we get the same thing back, which is true. And here, I took some validation data. So I went to,",
        "tokens": [
            50552,
            321,
            483,
            264,
            912,
            551,
            646,
            11,
            597,
            307,
            2074,
            13,
            400,
            510,
            11,
            286,
            1890,
            512,
            24071,
            1412,
            13,
            407,
            286,
            1437,
            281,
            11,
            50808
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17607508599758148,
        "compression_ratio": 1.8074073791503906,
        "no_speech_prob": 0.00014653119433205575
    },
    {
        "id": 223,
        "seek": 133208,
        "start": 1340.9599609375,
        "end": 1345.8399658203125,
        "text": " I think, this web page, and I grabbed some text. So this is text that the tokenizer has not seen,",
        "tokens": [
            50808,
            286,
            519,
            11,
            341,
            3670,
            3028,
            11,
            293,
            286,
            18607,
            512,
            2487,
            13,
            407,
            341,
            307,
            2487,
            300,
            264,
            14862,
            6545,
            575,
            406,
            1612,
            11,
            51052
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17607508599758148,
        "compression_ratio": 1.8074073791503906,
        "no_speech_prob": 0.00014653119433205575
    },
    {
        "id": 224,
        "seek": 133208,
        "start": 1345.8399658203125,
        "end": 1350.719970703125,
        "text": " and we can make sure that this also works. Okay, so that gives us some confidence that this was",
        "tokens": [
            51052,
            293,
            321,
            393,
            652,
            988,
            300,
            341,
            611,
            1985,
            13,
            1033,
            11,
            370,
            300,
            2709,
            505,
            512,
            6687,
            300,
            341,
            390,
            51296
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17607508599758148,
        "compression_ratio": 1.8074073791503906,
        "no_speech_prob": 0.00014653119433205575
    },
    {
        "id": 225,
        "seek": 133208,
        "start": 1350.719970703125,
        "end": 1356.47998046875,
        "text": " correctly implemented. So those are the basics of the byte pair encoding algorithm. We saw how we",
        "tokens": [
            51296,
            8944,
            12270,
            13,
            407,
            729,
            366,
            264,
            14688,
            295,
            264,
            40846,
            6119,
            43430,
            9284,
            13,
            492,
            1866,
            577,
            321,
            51584
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17607508599758148,
        "compression_ratio": 1.8074073791503906,
        "no_speech_prob": 0.00014653119433205575
    },
    {
        "id": 226,
        "seek": 135648,
        "start": 1356.47998046875,
        "end": 1362.47998046875,
        "text": " can take some training set, train a tokenizer. The parameters of this tokenizer really are just",
        "tokens": [
            50364,
            393,
            747,
            512,
            3097,
            992,
            11,
            3847,
            257,
            14862,
            6545,
            13,
            440,
            9834,
            295,
            341,
            14862,
            6545,
            534,
            366,
            445,
            50664
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16761483252048492,
        "compression_ratio": 1.7090909481048584,
        "no_speech_prob": 0.005819701123982668
    },
    {
        "id": 227,
        "seek": 135648,
        "start": 1362.47998046875,
        "end": 1367.280029296875,
        "text": " this dictionary of merges. And that basically creates the little binary forest on top of raw",
        "tokens": [
            50664,
            341,
            25890,
            295,
            3551,
            2880,
            13,
            400,
            300,
            1936,
            7829,
            264,
            707,
            17434,
            6719,
            322,
            1192,
            295,
            8936,
            50904
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16761483252048492,
        "compression_ratio": 1.7090909481048584,
        "no_speech_prob": 0.005819701123982668
    },
    {
        "id": 228,
        "seek": 135648,
        "start": 1367.280029296875,
        "end": 1374.0,
        "text": " bytes. Once we have this, the merges table, we can both encode and decode between raw text and",
        "tokens": [
            50904,
            36088,
            13,
            3443,
            321,
            362,
            341,
            11,
            264,
            3551,
            2880,
            3199,
            11,
            321,
            393,
            1293,
            2058,
            1429,
            293,
            979,
            1429,
            1296,
            8936,
            2487,
            293,
            51240
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16761483252048492,
        "compression_ratio": 1.7090909481048584,
        "no_speech_prob": 0.005819701123982668
    },
    {
        "id": 229,
        "seek": 135648,
        "start": 1374.0,
        "end": 1379.6800537109375,
        "text": " token sequences. So that's the simplest setting of the tokenizer. What we're going to do now,",
        "tokens": [
            51240,
            14862,
            22978,
            13,
            407,
            300,
            311,
            264,
            22811,
            3287,
            295,
            264,
            14862,
            6545,
            13,
            708,
            321,
            434,
            516,
            281,
            360,
            586,
            11,
            51524
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16761483252048492,
        "compression_ratio": 1.7090909481048584,
        "no_speech_prob": 0.005819701123982668
    },
    {
        "id": 230,
        "seek": 135648,
        "start": 1379.6800537109375,
        "end": 1383.199951171875,
        "text": " though, is we're going to look at some of the state-of-the-art large language models and the",
        "tokens": [
            51524,
            1673,
            11,
            307,
            321,
            434,
            516,
            281,
            574,
            412,
            512,
            295,
            264,
            1785,
            12,
            2670,
            12,
            3322,
            12,
            446,
            2416,
            2856,
            5245,
            293,
            264,
            51700
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16761483252048492,
        "compression_ratio": 1.7090909481048584,
        "no_speech_prob": 0.005819701123982668
    },
    {
        "id": 231,
        "seek": 138320,
        "start": 1383.199951171875,
        "end": 1386.9599609375,
        "text": " kinds of tokenizers that they use. And we're going to see that this picture complexifies very",
        "tokens": [
            50364,
            3685,
            295,
            14862,
            22525,
            300,
            436,
            764,
            13,
            400,
            321,
            434,
            516,
            281,
            536,
            300,
            341,
            3036,
            3997,
            11221,
            588,
            50552
        ],
        "temperature": 0.0,
        "avg_logprob": -0.169189453125,
        "compression_ratio": 1.6849817037582397,
        "no_speech_prob": 0.006388198118656874
    },
    {
        "id": 232,
        "seek": 138320,
        "start": 1386.9599609375,
        "end": 1392.800048828125,
        "text": " quickly. So we're going to go through the details of this complexification one at a time.",
        "tokens": [
            50552,
            2661,
            13,
            407,
            321,
            434,
            516,
            281,
            352,
            807,
            264,
            4365,
            295,
            341,
            3997,
            3774,
            472,
            412,
            257,
            565,
            13,
            50844
        ],
        "temperature": 0.0,
        "avg_logprob": -0.169189453125,
        "compression_ratio": 1.6849817037582397,
        "no_speech_prob": 0.006388198118656874
    },
    {
        "id": 233,
        "seek": 138320,
        "start": 1393.760009765625,
        "end": 1398.47998046875,
        "text": " So let's kick things off by looking at the GPT series. So in particular, I have the GPT-2 paper",
        "tokens": [
            50892,
            407,
            718,
            311,
            4437,
            721,
            766,
            538,
            1237,
            412,
            264,
            26039,
            51,
            2638,
            13,
            407,
            294,
            1729,
            11,
            286,
            362,
            264,
            26039,
            51,
            12,
            17,
            3035,
            51128
        ],
        "temperature": 0.0,
        "avg_logprob": -0.169189453125,
        "compression_ratio": 1.6849817037582397,
        "no_speech_prob": 0.006388198118656874
    },
    {
        "id": 234,
        "seek": 138320,
        "start": 1398.47998046875,
        "end": 1405.760009765625,
        "text": " here. And this paper is from 2019 or so, so five years ago. And let's scroll down to",
        "tokens": [
            51128,
            510,
            13,
            400,
            341,
            3035,
            307,
            490,
            6071,
            420,
            370,
            11,
            370,
            1732,
            924,
            2057,
            13,
            400,
            718,
            311,
            11369,
            760,
            281,
            51492
        ],
        "temperature": 0.0,
        "avg_logprob": -0.169189453125,
        "compression_ratio": 1.6849817037582397,
        "no_speech_prob": 0.006388198118656874
    },
    {
        "id": 235,
        "seek": 138320,
        "start": 1406.3199462890625,
        "end": 1410.800048828125,
        "text": " input representation. This is where they talk about the tokenizer that they're using for GPT-2.",
        "tokens": [
            51520,
            4846,
            10290,
            13,
            639,
            307,
            689,
            436,
            751,
            466,
            264,
            14862,
            6545,
            300,
            436,
            434,
            1228,
            337,
            26039,
            51,
            12,
            17,
            13,
            51744
        ],
        "temperature": 0.0,
        "avg_logprob": -0.169189453125,
        "compression_ratio": 1.6849817037582397,
        "no_speech_prob": 0.006388198118656874
    },
    {
        "id": 236,
        "seek": 141080,
        "start": 1411.5999755859375,
        "end": 1416.719970703125,
        "text": " Now, this is all fairly readable, so I encourage you to pause and read this yourself. But this is",
        "tokens": [
            50404,
            823,
            11,
            341,
            307,
            439,
            6457,
            49857,
            11,
            370,
            286,
            5373,
            291,
            281,
            10465,
            293,
            1401,
            341,
            1803,
            13,
            583,
            341,
            307,
            50660
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18456196784973145,
        "compression_ratio": 1.7785978317260742,
        "no_speech_prob": 0.0010987240821123123
    },
    {
        "id": 237,
        "seek": 141080,
        "start": 1416.719970703125,
        "end": 1422.9599609375,
        "text": " where they motivate the use of the byte-pair encoding algorithm on the byte-level representation",
        "tokens": [
            50660,
            689,
            436,
            28497,
            264,
            764,
            295,
            264,
            40846,
            12,
            79,
            1246,
            43430,
            9284,
            322,
            264,
            40846,
            12,
            12418,
            10290,
            50972
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18456196784973145,
        "compression_ratio": 1.7785978317260742,
        "no_speech_prob": 0.0010987240821123123
    },
    {
        "id": 238,
        "seek": 141080,
        "start": 1422.9599609375,
        "end": 1428.3199462890625,
        "text": " of UTF-8 encoding. So this is where they motivate it, and they talk about the vocabulary sizes",
        "tokens": [
            50972,
            295,
            624,
            20527,
            12,
            23,
            43430,
            13,
            407,
            341,
            307,
            689,
            436,
            28497,
            309,
            11,
            293,
            436,
            751,
            466,
            264,
            19864,
            11602,
            51240
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18456196784973145,
        "compression_ratio": 1.7785978317260742,
        "no_speech_prob": 0.0010987240821123123
    },
    {
        "id": 239,
        "seek": 141080,
        "start": 1428.3199462890625,
        "end": 1433.1199951171875,
        "text": " and everything. Now, everything here is exactly as we've covered it so far, but things start to",
        "tokens": [
            51240,
            293,
            1203,
            13,
            823,
            11,
            1203,
            510,
            307,
            2293,
            382,
            321,
            600,
            5343,
            309,
            370,
            1400,
            11,
            457,
            721,
            722,
            281,
            51480
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18456196784973145,
        "compression_ratio": 1.7785978317260742,
        "no_speech_prob": 0.0010987240821123123
    },
    {
        "id": 240,
        "seek": 141080,
        "start": 1433.1199951171875,
        "end": 1438.800048828125,
        "text": " depart around here. So what they mention is that they don't just apply the Naive algorithm as we",
        "tokens": [
            51480,
            9110,
            926,
            510,
            13,
            407,
            437,
            436,
            2152,
            307,
            300,
            436,
            500,
            380,
            445,
            3079,
            264,
            6056,
            488,
            9284,
            382,
            321,
            51764
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18456196784973145,
        "compression_ratio": 1.7785978317260742,
        "no_speech_prob": 0.0010987240821123123
    },
    {
        "id": 241,
        "seek": 143880,
        "start": 1438.800048828125,
        "end": 1443.52001953125,
        "text": " have done it. And in particular, here's a motivating example. Suppose that you have common",
        "tokens": [
            50364,
            362,
            1096,
            309,
            13,
            400,
            294,
            1729,
            11,
            510,
            311,
            257,
            41066,
            1365,
            13,
            21360,
            300,
            291,
            362,
            2689,
            50600
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1805555522441864,
        "compression_ratio": 1.7484076023101807,
        "no_speech_prob": 0.0004442148201633245
    },
    {
        "id": 242,
        "seek": 143880,
        "start": 1443.52001953125,
        "end": 1449.0400390625,
        "text": " words like dog. What will happen is that dog, of course, occurs very frequently in the text,",
        "tokens": [
            50600,
            2283,
            411,
            3000,
            13,
            708,
            486,
            1051,
            307,
            300,
            3000,
            11,
            295,
            1164,
            11,
            11843,
            588,
            10374,
            294,
            264,
            2487,
            11,
            50876
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1805555522441864,
        "compression_ratio": 1.7484076023101807,
        "no_speech_prob": 0.0004442148201633245
    },
    {
        "id": 243,
        "seek": 143880,
        "start": 1449.0400390625,
        "end": 1454.0,
        "text": " and it occurs right next to all kinds of punctuation, as an example. So dog dot,",
        "tokens": [
            50876,
            293,
            309,
            11843,
            558,
            958,
            281,
            439,
            3685,
            295,
            27006,
            16073,
            11,
            382,
            364,
            1365,
            13,
            407,
            3000,
            5893,
            11,
            51124
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1805555522441864,
        "compression_ratio": 1.7484076023101807,
        "no_speech_prob": 0.0004442148201633245
    },
    {
        "id": 244,
        "seek": 143880,
        "start": 1454.0,
        "end": 1459.5999755859375,
        "text": " dog exclamation mark, dog question mark, et cetera. And naively, you might imagine that the BP",
        "tokens": [
            51124,
            3000,
            1624,
            43233,
            1491,
            11,
            3000,
            1168,
            1491,
            11,
            1030,
            11458,
            13,
            400,
            1667,
            3413,
            11,
            291,
            1062,
            3811,
            300,
            264,
            40533,
            51404
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1805555522441864,
        "compression_ratio": 1.7484076023101807,
        "no_speech_prob": 0.0004442148201633245
    },
    {
        "id": 245,
        "seek": 143880,
        "start": 1459.5999755859375,
        "end": 1464.0,
        "text": " algorithm could merge these to be single tokens. And then you end up with lots of tokens that are",
        "tokens": [
            51404,
            9284,
            727,
            22183,
            613,
            281,
            312,
            2167,
            22667,
            13,
            400,
            550,
            291,
            917,
            493,
            365,
            3195,
            295,
            22667,
            300,
            366,
            51624
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1805555522441864,
        "compression_ratio": 1.7484076023101807,
        "no_speech_prob": 0.0004442148201633245
    },
    {
        "id": 246,
        "seek": 143880,
        "start": 1464.0,
        "end": 1467.9200439453125,
        "text": " just like dog with a slightly different punctuation. And so it feels like you're clustering",
        "tokens": [
            51624,
            445,
            411,
            3000,
            365,
            257,
            4748,
            819,
            27006,
            16073,
            13,
            400,
            370,
            309,
            3417,
            411,
            291,
            434,
            596,
            48673,
            51820
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1805555522441864,
        "compression_ratio": 1.7484076023101807,
        "no_speech_prob": 0.0004442148201633245
    },
    {
        "id": 247,
        "seek": 146792,
        "start": 1467.9200439453125,
        "end": 1471.760009765625,
        "text": " things that shouldn't be clustered. You're combining semantics with punctuation.",
        "tokens": [
            50364,
            721,
            300,
            4659,
            380,
            312,
            596,
            38624,
            13,
            509,
            434,
            21928,
            4361,
            45298,
            365,
            27006,
            16073,
            13,
            50556
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18328803777694702,
        "compression_ratio": 1.7622641324996948,
        "no_speech_prob": 8.61461230670102e-05
    },
    {
        "id": 248,
        "seek": 146792,
        "start": 1473.0400390625,
        "end": 1478.6400146484375,
        "text": " And this feels suboptimal. And indeed, they also say that this is suboptimal according to some of",
        "tokens": [
            50620,
            400,
            341,
            3417,
            1422,
            5747,
            10650,
            13,
            400,
            6451,
            11,
            436,
            611,
            584,
            300,
            341,
            307,
            1422,
            5747,
            10650,
            4650,
            281,
            512,
            295,
            50900
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18328803777694702,
        "compression_ratio": 1.7622641324996948,
        "no_speech_prob": 8.61461230670102e-05
    },
    {
        "id": 249,
        "seek": 146792,
        "start": 1478.6400146484375,
        "end": 1484.0,
        "text": " the experiments. So what they want to do is they want to top down in a manual way, enforce that",
        "tokens": [
            50900,
            264,
            12050,
            13,
            407,
            437,
            436,
            528,
            281,
            360,
            307,
            436,
            528,
            281,
            1192,
            760,
            294,
            257,
            9688,
            636,
            11,
            24825,
            300,
            51168
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18328803777694702,
        "compression_ratio": 1.7622641324996948,
        "no_speech_prob": 8.61461230670102e-05
    },
    {
        "id": 250,
        "seek": 146792,
        "start": 1484.0,
        "end": 1490.800048828125,
        "text": " some types of characters should never be merged together. So they want to enforce these merging",
        "tokens": [
            51168,
            512,
            3467,
            295,
            4342,
            820,
            1128,
            312,
            36427,
            1214,
            13,
            407,
            436,
            528,
            281,
            24825,
            613,
            44559,
            51508
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18328803777694702,
        "compression_ratio": 1.7622641324996948,
        "no_speech_prob": 8.61461230670102e-05
    },
    {
        "id": 251,
        "seek": 146792,
        "start": 1490.800048828125,
        "end": 1496.9599609375,
        "text": " rules on top of the byte-pair encoding algorithm. So let's take a look at their code and see how",
        "tokens": [
            51508,
            4474,
            322,
            1192,
            295,
            264,
            40846,
            12,
            79,
            1246,
            43430,
            9284,
            13,
            407,
            718,
            311,
            747,
            257,
            574,
            412,
            641,
            3089,
            293,
            536,
            577,
            51816
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18328803777694702,
        "compression_ratio": 1.7622641324996948,
        "no_speech_prob": 8.61461230670102e-05
    },
    {
        "id": 252,
        "seek": 149696,
        "start": 1496.9599609375,
        "end": 1501.5999755859375,
        "text": " they actually enforce this and what kinds of mergers they actually do perform. So I have the",
        "tokens": [
            50364,
            436,
            767,
            24825,
            341,
            293,
            437,
            3685,
            295,
            3551,
            9458,
            436,
            767,
            360,
            2042,
            13,
            407,
            286,
            362,
            264,
            50596
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17390094697475433,
        "compression_ratio": 1.7576923370361328,
        "no_speech_prob": 0.006387903355062008
    },
    {
        "id": 253,
        "seek": 149696,
        "start": 1501.5999755859375,
        "end": 1509.52001953125,
        "text": " tab open here for GPT-2 under OpenAI on GitHub. And when we go to source, there is an encoder.py.",
        "tokens": [
            50596,
            4421,
            1269,
            510,
            337,
            26039,
            51,
            12,
            17,
            833,
            7238,
            48698,
            322,
            23331,
            13,
            400,
            562,
            321,
            352,
            281,
            4009,
            11,
            456,
            307,
            364,
            2058,
            19866,
            13,
            8200,
            13,
            50992
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17390094697475433,
        "compression_ratio": 1.7576923370361328,
        "no_speech_prob": 0.006387903355062008
    },
    {
        "id": 254,
        "seek": 149696,
        "start": 1510.47998046875,
        "end": 1514.56005859375,
        "text": " Now, I don't personally love that they called encoder.py because this is the tokenizer.",
        "tokens": [
            51040,
            823,
            11,
            286,
            500,
            380,
            5665,
            959,
            300,
            436,
            1219,
            2058,
            19866,
            13,
            8200,
            570,
            341,
            307,
            264,
            14862,
            6545,
            13,
            51244
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17390094697475433,
        "compression_ratio": 1.7576923370361328,
        "no_speech_prob": 0.006387903355062008
    },
    {
        "id": 255,
        "seek": 149696,
        "start": 1514.56005859375,
        "end": 1519.0400390625,
        "text": " And the tokenizer can do both encode and decode. So it feels kind of awkward to me that it's",
        "tokens": [
            51244,
            400,
            264,
            14862,
            6545,
            393,
            360,
            1293,
            2058,
            1429,
            293,
            979,
            1429,
            13,
            407,
            309,
            3417,
            733,
            295,
            11411,
            281,
            385,
            300,
            309,
            311,
            51468
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17390094697475433,
        "compression_ratio": 1.7576923370361328,
        "no_speech_prob": 0.006387903355062008
    },
    {
        "id": 256,
        "seek": 149696,
        "start": 1519.0400390625,
        "end": 1523.3599853515625,
        "text": " called encoder, but that is the tokenizer. And there's a lot going on here, and we're",
        "tokens": [
            51468,
            1219,
            2058,
            19866,
            11,
            457,
            300,
            307,
            264,
            14862,
            6545,
            13,
            400,
            456,
            311,
            257,
            688,
            516,
            322,
            510,
            11,
            293,
            321,
            434,
            51684
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17390094697475433,
        "compression_ratio": 1.7576923370361328,
        "no_speech_prob": 0.006387903355062008
    },
    {
        "id": 257,
        "seek": 152336,
        "start": 1523.3599853515625,
        "end": 1528.6400146484375,
        "text": " going to step through it in detail at one point. For now, I just want to focus on this part here.",
        "tokens": [
            50364,
            516,
            281,
            1823,
            807,
            309,
            294,
            2607,
            412,
            472,
            935,
            13,
            1171,
            586,
            11,
            286,
            445,
            528,
            281,
            1879,
            322,
            341,
            644,
            510,
            13,
            50628
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1871279776096344,
        "compression_ratio": 1.7188612222671509,
        "no_speech_prob": 0.06008060649037361
    },
    {
        "id": 258,
        "seek": 152336,
        "start": 1529.5999755859375,
        "end": 1533.52001953125,
        "text": " They create a regex pattern here that looks very complicated, and we're going to go through it",
        "tokens": [
            50676,
            814,
            1884,
            257,
            319,
            432,
            87,
            5102,
            510,
            300,
            1542,
            588,
            6179,
            11,
            293,
            321,
            434,
            516,
            281,
            352,
            807,
            309,
            50872
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1871279776096344,
        "compression_ratio": 1.7188612222671509,
        "no_speech_prob": 0.06008060649037361
    },
    {
        "id": 259,
        "seek": 152336,
        "start": 1533.52001953125,
        "end": 1541.0400390625,
        "text": " in a bit. But this is the core part that allows them to enforce rules for what parts of the text",
        "tokens": [
            50872,
            294,
            257,
            857,
            13,
            583,
            341,
            307,
            264,
            4965,
            644,
            300,
            4045,
            552,
            281,
            24825,
            4474,
            337,
            437,
            3166,
            295,
            264,
            2487,
            51248
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1871279776096344,
        "compression_ratio": 1.7188612222671509,
        "no_speech_prob": 0.06008060649037361
    },
    {
        "id": 260,
        "seek": 152336,
        "start": 1541.0400390625,
        "end": 1547.0400390625,
        "text": " will never be merged for sure. Now, notice that re.compile here is a little bit misleading because",
        "tokens": [
            51248,
            486,
            1128,
            312,
            36427,
            337,
            988,
            13,
            823,
            11,
            3449,
            300,
            319,
            13,
            21541,
            794,
            510,
            307,
            257,
            707,
            857,
            36429,
            570,
            51548
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1871279776096344,
        "compression_ratio": 1.7188612222671509,
        "no_speech_prob": 0.06008060649037361
    },
    {
        "id": 261,
        "seek": 152336,
        "start": 1547.0400390625,
        "end": 1552.0799560546875,
        "text": " we're not just doing import re, which is the Python re module. We're doing import regex as re.",
        "tokens": [
            51548,
            321,
            434,
            406,
            445,
            884,
            974,
            319,
            11,
            597,
            307,
            264,
            15329,
            319,
            10088,
            13,
            492,
            434,
            884,
            974,
            319,
            432,
            87,
            382,
            319,
            13,
            51800
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1871279776096344,
        "compression_ratio": 1.7188612222671509,
        "no_speech_prob": 0.06008060649037361
    },
    {
        "id": 262,
        "seek": 155208,
        "start": 1552.0799560546875,
        "end": 1557.199951171875,
        "text": " And regex is a Python package that you can install, pip install regex, and it's basically",
        "tokens": [
            50364,
            400,
            319,
            432,
            87,
            307,
            257,
            15329,
            7372,
            300,
            291,
            393,
            3625,
            11,
            8489,
            3625,
            319,
            432,
            87,
            11,
            293,
            309,
            311,
            1936,
            50620
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17905914783477783,
        "compression_ratio": 1.7209302186965942,
        "no_speech_prob": 0.00022693320352118462
    },
    {
        "id": 263,
        "seek": 155208,
        "start": 1557.199951171875,
        "end": 1565.52001953125,
        "text": " an extension of re, so it's a bit more powerful re. So let's take a look at this pattern and what",
        "tokens": [
            50620,
            364,
            10320,
            295,
            319,
            11,
            370,
            309,
            311,
            257,
            857,
            544,
            4005,
            319,
            13,
            407,
            718,
            311,
            747,
            257,
            574,
            412,
            341,
            5102,
            293,
            437,
            51036
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17905914783477783,
        "compression_ratio": 1.7209302186965942,
        "no_speech_prob": 0.00022693320352118462
    },
    {
        "id": 264,
        "seek": 155208,
        "start": 1565.52001953125,
        "end": 1569.5999755859375,
        "text": " it's doing and why this is actually doing the separation that they are looking for.",
        "tokens": [
            51036,
            309,
            311,
            884,
            293,
            983,
            341,
            307,
            767,
            884,
            264,
            14634,
            300,
            436,
            366,
            1237,
            337,
            13,
            51240
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17905914783477783,
        "compression_ratio": 1.7209302186965942,
        "no_speech_prob": 0.00022693320352118462
    },
    {
        "id": 265,
        "seek": 155208,
        "start": 1570.239990234375,
        "end": 1574.719970703125,
        "text": " Okay, so I've copy-pasted the pattern here to our Jupyter Notebook where we left off,",
        "tokens": [
            51272,
            1033,
            11,
            370,
            286,
            600,
            5055,
            12,
            79,
            34440,
            264,
            5102,
            510,
            281,
            527,
            22125,
            88,
            391,
            11633,
            2939,
            689,
            321,
            1411,
            766,
            11,
            51496
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17905914783477783,
        "compression_ratio": 1.7209302186965942,
        "no_speech_prob": 0.00022693320352118462
    },
    {
        "id": 266,
        "seek": 155208,
        "start": 1574.719970703125,
        "end": 1579.760009765625,
        "text": " and let's take this pattern for a spin. So in the exact same way that their code does,",
        "tokens": [
            51496,
            293,
            718,
            311,
            747,
            341,
            5102,
            337,
            257,
            6060,
            13,
            407,
            294,
            264,
            1900,
            912,
            636,
            300,
            641,
            3089,
            775,
            11,
            51748
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17905914783477783,
        "compression_ratio": 1.7209302186965942,
        "no_speech_prob": 0.00022693320352118462
    },
    {
        "id": 267,
        "seek": 157976,
        "start": 1579.8399658203125,
        "end": 1584.9599609375,
        "text": " we're going to call an re.findall for this pattern on any arbitrary string that we are",
        "tokens": [
            50368,
            321,
            434,
            516,
            281,
            818,
            364,
            319,
            13,
            35072,
            336,
            337,
            341,
            5102,
            322,
            604,
            23211,
            6798,
            300,
            321,
            366,
            50624
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19181416928768158,
        "compression_ratio": 1.6725664138793945,
        "no_speech_prob": 1.4738979189132806e-05
    },
    {
        "id": 268,
        "seek": 157976,
        "start": 1584.9599609375,
        "end": 1591.760009765625,
        "text": " interested in. So this is the string that we want to encode into tokens to feed into an LLM,",
        "tokens": [
            50624,
            3102,
            294,
            13,
            407,
            341,
            307,
            264,
            6798,
            300,
            321,
            528,
            281,
            2058,
            1429,
            666,
            22667,
            281,
            3154,
            666,
            364,
            441,
            43,
            44,
            11,
            50964
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19181416928768158,
        "compression_ratio": 1.6725664138793945,
        "no_speech_prob": 1.4738979189132806e-05
    },
    {
        "id": 269,
        "seek": 157976,
        "start": 1591.760009765625,
        "end": 1598.1600341796875,
        "text": " like GPT-2. So what exactly is this doing? Well, re.findall will take this pattern and try to match",
        "tokens": [
            50964,
            411,
            26039,
            51,
            12,
            17,
            13,
            407,
            437,
            2293,
            307,
            341,
            884,
            30,
            1042,
            11,
            319,
            13,
            35072,
            336,
            486,
            747,
            341,
            5102,
            293,
            853,
            281,
            2995,
            51284
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19181416928768158,
        "compression_ratio": 1.6725664138793945,
        "no_speech_prob": 1.4738979189132806e-05
    },
    {
        "id": 270,
        "seek": 157976,
        "start": 1598.1600341796875,
        "end": 1605.0400390625,
        "text": " it against this string. The way this works is that you are going from left to right in the string,",
        "tokens": [
            51284,
            309,
            1970,
            341,
            6798,
            13,
            440,
            636,
            341,
            1985,
            307,
            300,
            291,
            366,
            516,
            490,
            1411,
            281,
            558,
            294,
            264,
            6798,
            11,
            51628
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19181416928768158,
        "compression_ratio": 1.6725664138793945,
        "no_speech_prob": 1.4738979189132806e-05
    },
    {
        "id": 271,
        "seek": 160504,
        "start": 1605.0400390625,
        "end": 1611.52001953125,
        "text": " and you're trying to match the pattern. And re.findall will get all the occurrences",
        "tokens": [
            50364,
            293,
            291,
            434,
            1382,
            281,
            2995,
            264,
            5102,
            13,
            400,
            319,
            13,
            35072,
            336,
            486,
            483,
            439,
            264,
            5160,
            38983,
            50688
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19287468492984772,
        "compression_ratio": 1.6759259700775146,
        "no_speech_prob": 4.0063685446511954e-05
    },
    {
        "id": 272,
        "seek": 160504,
        "start": 1611.52001953125,
        "end": 1617.43994140625,
        "text": " and organize them into a list. Now, when you look at this pattern, first of all,",
        "tokens": [
            50688,
            293,
            13859,
            552,
            666,
            257,
            1329,
            13,
            823,
            11,
            562,
            291,
            574,
            412,
            341,
            5102,
            11,
            700,
            295,
            439,
            11,
            50984
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19287468492984772,
        "compression_ratio": 1.6759259700775146,
        "no_speech_prob": 4.0063685446511954e-05
    },
    {
        "id": 273,
        "seek": 160504,
        "start": 1617.43994140625,
        "end": 1624.3199462890625,
        "text": " notice that this is a raw string, and then these are three double quotes just to start the string.",
        "tokens": [
            50984,
            3449,
            300,
            341,
            307,
            257,
            8936,
            6798,
            11,
            293,
            550,
            613,
            366,
            1045,
            3834,
            19963,
            445,
            281,
            722,
            264,
            6798,
            13,
            51328
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19287468492984772,
        "compression_ratio": 1.6759259700775146,
        "no_speech_prob": 4.0063685446511954e-05
    },
    {
        "id": 274,
        "seek": 160504,
        "start": 1624.3199462890625,
        "end": 1630.6400146484375,
        "text": " So really, the string itself, this is the pattern itself, right? And notice that it's made up of a",
        "tokens": [
            51328,
            407,
            534,
            11,
            264,
            6798,
            2564,
            11,
            341,
            307,
            264,
            5102,
            2564,
            11,
            558,
            30,
            400,
            3449,
            300,
            309,
            311,
            1027,
            493,
            295,
            257,
            51644
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19287468492984772,
        "compression_ratio": 1.6759259700775146,
        "no_speech_prob": 4.0063685446511954e-05
    },
    {
        "id": 275,
        "seek": 163064,
        "start": 1630.6400146484375,
        "end": 1637.43994140625,
        "text": " lot of ORs. So see these vertical bars? Those are ORs in regex. And so you go from left to right",
        "tokens": [
            50364,
            688,
            295,
            19654,
            82,
            13,
            407,
            536,
            613,
            9429,
            10228,
            30,
            3950,
            366,
            19654,
            82,
            294,
            319,
            432,
            87,
            13,
            400,
            370,
            291,
            352,
            490,
            1411,
            281,
            558,
            50704
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2005005180835724,
        "compression_ratio": 1.6255319118499756,
        "no_speech_prob": 0.004755106288939714
    },
    {
        "id": 276,
        "seek": 163064,
        "start": 1637.43994140625,
        "end": 1642.8800048828125,
        "text": " in this pattern and try to match it against the string wherever you are. So we have hello,",
        "tokens": [
            50704,
            294,
            341,
            5102,
            293,
            853,
            281,
            2995,
            309,
            1970,
            264,
            6798,
            8660,
            291,
            366,
            13,
            407,
            321,
            362,
            7751,
            11,
            50976
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2005005180835724,
        "compression_ratio": 1.6255319118499756,
        "no_speech_prob": 0.004755106288939714
    },
    {
        "id": 277,
        "seek": 163064,
        "start": 1642.8800048828125,
        "end": 1647.8399658203125,
        "text": " and we're going to try to match it. Well, it's not apostrophe s, it's not apostrophe t, or any of",
        "tokens": [
            50976,
            293,
            321,
            434,
            516,
            281,
            853,
            281,
            2995,
            309,
            13,
            1042,
            11,
            309,
            311,
            406,
            19484,
            27194,
            262,
            11,
            309,
            311,
            406,
            19484,
            27194,
            256,
            11,
            420,
            604,
            295,
            51224
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2005005180835724,
        "compression_ratio": 1.6255319118499756,
        "no_speech_prob": 0.004755106288939714
    },
    {
        "id": 278,
        "seek": 163064,
        "start": 1647.8399658203125,
        "end": 1656.800048828125,
        "text": " these, but it is an optional space followed by dash p of, sorry, slash p of L one or more times.",
        "tokens": [
            51224,
            613,
            11,
            457,
            309,
            307,
            364,
            17312,
            1901,
            6263,
            538,
            8240,
            280,
            295,
            11,
            2597,
            11,
            17330,
            280,
            295,
            441,
            472,
            420,
            544,
            1413,
            13,
            51672
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2005005180835724,
        "compression_ratio": 1.6255319118499756,
        "no_speech_prob": 0.004755106288939714
    },
    {
        "id": 279,
        "seek": 165680,
        "start": 1656.800048828125,
        "end": 1663.760009765625,
        "text": " What is slash p of L? It is coming to some documentation that I found. There might be",
        "tokens": [
            50364,
            708,
            307,
            17330,
            280,
            295,
            441,
            30,
            467,
            307,
            1348,
            281,
            512,
            14333,
            300,
            286,
            1352,
            13,
            821,
            1062,
            312,
            50712
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17885315418243408,
        "compression_ratio": 1.600000023841858,
        "no_speech_prob": 2.7264700293017086e-06
    },
    {
        "id": 280,
        "seek": 165680,
        "start": 1663.760009765625,
        "end": 1671.43994140625,
        "text": " other sources as well. Slash p of L is a letter, any kind of letter from any language. And hello",
        "tokens": [
            50712,
            661,
            7139,
            382,
            731,
            13,
            6187,
            1299,
            280,
            295,
            441,
            307,
            257,
            5063,
            11,
            604,
            733,
            295,
            5063,
            490,
            604,
            2856,
            13,
            400,
            7751,
            51096
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17885315418243408,
        "compression_ratio": 1.600000023841858,
        "no_speech_prob": 2.7264700293017086e-06
    },
    {
        "id": 281,
        "seek": 165680,
        "start": 1671.43994140625,
        "end": 1678.0,
        "text": " is made up of letters, H-E-L-L-O, et cetera. So optional space followed by a bunch of letters,",
        "tokens": [
            51096,
            307,
            1027,
            493,
            295,
            7825,
            11,
            389,
            12,
            36,
            12,
            43,
            12,
            43,
            12,
            46,
            11,
            1030,
            11458,
            13,
            407,
            17312,
            1901,
            6263,
            538,
            257,
            3840,
            295,
            7825,
            11,
            51424
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17885315418243408,
        "compression_ratio": 1.600000023841858,
        "no_speech_prob": 2.7264700293017086e-06
    },
    {
        "id": 282,
        "seek": 165680,
        "start": 1678.0,
        "end": 1684.0799560546875,
        "text": " one or more letters, is going to match hello, but then the match ends because a whitespace",
        "tokens": [
            51424,
            472,
            420,
            544,
            7825,
            11,
            307,
            516,
            281,
            2995,
            7751,
            11,
            457,
            550,
            264,
            2995,
            5314,
            570,
            257,
            21909,
            17940,
            51728
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17885315418243408,
        "compression_ratio": 1.600000023841858,
        "no_speech_prob": 2.7264700293017086e-06
    },
    {
        "id": 283,
        "seek": 168408,
        "start": 1684.0799560546875,
        "end": 1691.52001953125,
        "text": " is not a letter. So from there on begins a new sort of attempt to match against the string again.",
        "tokens": [
            50364,
            307,
            406,
            257,
            5063,
            13,
            407,
            490,
            456,
            322,
            7338,
            257,
            777,
            1333,
            295,
            5217,
            281,
            2995,
            1970,
            264,
            6798,
            797,
            13,
            50736
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1751244217157364,
        "compression_ratio": 1.6909091472625732,
        "no_speech_prob": 0.00020027333812322468
    },
    {
        "id": 284,
        "seek": 168408,
        "start": 1692.3199462890625,
        "end": 1696.719970703125,
        "text": " And starting in here, we're going to skip over all of these again until we get to the exact",
        "tokens": [
            50776,
            400,
            2891,
            294,
            510,
            11,
            321,
            434,
            516,
            281,
            10023,
            670,
            439,
            295,
            613,
            797,
            1826,
            321,
            483,
            281,
            264,
            1900,
            50996
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1751244217157364,
        "compression_ratio": 1.6909091472625732,
        "no_speech_prob": 0.00020027333812322468
    },
    {
        "id": 285,
        "seek": 168408,
        "start": 1696.719970703125,
        "end": 1701.52001953125,
        "text": " same point again. And we see that there's an optional space. This is the optional space",
        "tokens": [
            50996,
            912,
            935,
            797,
            13,
            400,
            321,
            536,
            300,
            456,
            311,
            364,
            17312,
            1901,
            13,
            639,
            307,
            264,
            17312,
            1901,
            51236
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1751244217157364,
        "compression_ratio": 1.6909091472625732,
        "no_speech_prob": 0.00020027333812322468
    },
    {
        "id": 286,
        "seek": 168408,
        "start": 1701.52001953125,
        "end": 1706.0,
        "text": " followed by a bunch of letters, one or more of them. And so that matches. So when we run this,",
        "tokens": [
            51236,
            6263,
            538,
            257,
            3840,
            295,
            7825,
            11,
            472,
            420,
            544,
            295,
            552,
            13,
            400,
            370,
            300,
            10676,
            13,
            407,
            562,
            321,
            1190,
            341,
            11,
            51460
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1751244217157364,
        "compression_ratio": 1.6909091472625732,
        "no_speech_prob": 0.00020027333812322468
    },
    {
        "id": 287,
        "seek": 170600,
        "start": 1706.6400146484375,
        "end": 1714.719970703125,
        "text": " we get a list of two elements, hello, and then space world. So how are you if we add more letters?",
        "tokens": [
            50396,
            321,
            483,
            257,
            1329,
            295,
            732,
            4959,
            11,
            7751,
            11,
            293,
            550,
            1901,
            1002,
            13,
            407,
            577,
            366,
            291,
            498,
            321,
            909,
            544,
            7825,
            30,
            50800
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1932193487882614,
        "compression_ratio": 1.5689655542373657,
        "no_speech_prob": 0.013222205452620983
    },
    {
        "id": 288,
        "seek": 170600,
        "start": 1715.280029296875,
        "end": 1719.5999755859375,
        "text": " We would just get them like this. Now, what is this doing and why is this important?",
        "tokens": [
            50828,
            492,
            576,
            445,
            483,
            552,
            411,
            341,
            13,
            823,
            11,
            437,
            307,
            341,
            884,
            293,
            983,
            307,
            341,
            1021,
            30,
            51044
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1932193487882614,
        "compression_ratio": 1.5689655542373657,
        "no_speech_prob": 0.013222205452620983
    },
    {
        "id": 289,
        "seek": 170600,
        "start": 1720.1600341796875,
        "end": 1727.43994140625,
        "text": " We are taking our string and instead of directly encoding it for tokenization, we are first",
        "tokens": [
            51072,
            492,
            366,
            1940,
            527,
            6798,
            293,
            2602,
            295,
            3838,
            43430,
            309,
            337,
            14862,
            2144,
            11,
            321,
            366,
            700,
            51436
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1932193487882614,
        "compression_ratio": 1.5689655542373657,
        "no_speech_prob": 0.013222205452620983
    },
    {
        "id": 290,
        "seek": 170600,
        "start": 1727.43994140625,
        "end": 1731.6800537109375,
        "text": " splitting it up. And when you actually step through the code, and we'll do that in a bit",
        "tokens": [
            51436,
            30348,
            309,
            493,
            13,
            400,
            562,
            291,
            767,
            1823,
            807,
            264,
            3089,
            11,
            293,
            321,
            603,
            360,
            300,
            294,
            257,
            857,
            51648
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1932193487882614,
        "compression_ratio": 1.5689655542373657,
        "no_speech_prob": 0.013222205452620983
    },
    {
        "id": 291,
        "seek": 173168,
        "start": 1731.6800537109375,
        "end": 1737.760009765625,
        "text": " more detail, what really it's doing on a high level is that it first splits your text into",
        "tokens": [
            50364,
            544,
            2607,
            11,
            437,
            534,
            309,
            311,
            884,
            322,
            257,
            1090,
            1496,
            307,
            300,
            309,
            700,
            37741,
            428,
            2487,
            666,
            50668
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19407908618450165,
        "compression_ratio": 1.6200873851776123,
        "no_speech_prob": 0.021285049617290497
    },
    {
        "id": 292,
        "seek": 173168,
        "start": 1738.4000244140625,
        "end": 1744.1600341796875,
        "text": " a list of texts, just like this one. And all these elements of this list are processed independently",
        "tokens": [
            50700,
            257,
            1329,
            295,
            15765,
            11,
            445,
            411,
            341,
            472,
            13,
            400,
            439,
            613,
            4959,
            295,
            341,
            1329,
            366,
            18846,
            21761,
            50988
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19407908618450165,
        "compression_ratio": 1.6200873851776123,
        "no_speech_prob": 0.021285049617290497
    },
    {
        "id": 293,
        "seek": 173168,
        "start": 1744.1600341796875,
        "end": 1749.0400390625,
        "text": " by the tokenizer. And all of the results of that processing are simply concatenated.",
        "tokens": [
            50988,
            538,
            264,
            14862,
            6545,
            13,
            400,
            439,
            295,
            264,
            3542,
            295,
            300,
            9007,
            366,
            2935,
            1588,
            7186,
            770,
            13,
            51232
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19407908618450165,
        "compression_ratio": 1.6200873851776123,
        "no_speech_prob": 0.021285049617290497
    },
    {
        "id": 294,
        "seek": 173168,
        "start": 1749.760009765625,
        "end": 1757.6800537109375,
        "text": " So hello, world. Oh, I missed how. Hello, world, how are you? We have five elements of a list.",
        "tokens": [
            51268,
            407,
            7751,
            11,
            1002,
            13,
            876,
            11,
            286,
            6721,
            577,
            13,
            2425,
            11,
            1002,
            11,
            577,
            366,
            291,
            30,
            492,
            362,
            1732,
            4959,
            295,
            257,
            1329,
            13,
            51664
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19407908618450165,
        "compression_ratio": 1.6200873851776123,
        "no_speech_prob": 0.021285049617290497
    },
    {
        "id": 295,
        "seek": 175768,
        "start": 1757.6800537109375,
        "end": 1765.760009765625,
        "text": " All of these will independently go from text to a token sequence. And then that token sequence is",
        "tokens": [
            50364,
            1057,
            295,
            613,
            486,
            21761,
            352,
            490,
            2487,
            281,
            257,
            14862,
            8310,
            13,
            400,
            550,
            300,
            14862,
            8310,
            307,
            50768
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18159402906894684,
        "compression_ratio": 1.7129629850387573,
        "no_speech_prob": 5.225224595051259e-05
    },
    {
        "id": 296,
        "seek": 175768,
        "start": 1765.760009765625,
        "end": 1771.52001953125,
        "text": " going to be concatenated. It's all going to be joined up. And roughly speaking, what that does",
        "tokens": [
            50768,
            516,
            281,
            312,
            1588,
            7186,
            770,
            13,
            467,
            311,
            439,
            516,
            281,
            312,
            6869,
            493,
            13,
            400,
            9810,
            4124,
            11,
            437,
            300,
            775,
            51056
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18159402906894684,
        "compression_ratio": 1.7129629850387573,
        "no_speech_prob": 5.225224595051259e-05
    },
    {
        "id": 297,
        "seek": 175768,
        "start": 1771.52001953125,
        "end": 1776.56005859375,
        "text": " is you're only ever finding merges between the elements of this list. So you can only ever",
        "tokens": [
            51056,
            307,
            291,
            434,
            787,
            1562,
            5006,
            3551,
            2880,
            1296,
            264,
            4959,
            295,
            341,
            1329,
            13,
            407,
            291,
            393,
            787,
            1562,
            51308
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18159402906894684,
        "compression_ratio": 1.7129629850387573,
        "no_speech_prob": 5.225224595051259e-05
    },
    {
        "id": 298,
        "seek": 175768,
        "start": 1776.56005859375,
        "end": 1782.8800048828125,
        "text": " consider merges within every one of these elements individually. And after you've done",
        "tokens": [
            51308,
            1949,
            3551,
            2880,
            1951,
            633,
            472,
            295,
            613,
            4959,
            16652,
            13,
            400,
            934,
            291,
            600,
            1096,
            51624
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18159402906894684,
        "compression_ratio": 1.7129629850387573,
        "no_speech_prob": 5.225224595051259e-05
    },
    {
        "id": 299,
        "seek": 178288,
        "start": 1782.8800048828125,
        "end": 1787.5999755859375,
        "text": " all the possible merging for all these elements individually, the results of all that will be",
        "tokens": [
            50364,
            439,
            264,
            1944,
            44559,
            337,
            439,
            613,
            4959,
            16652,
            11,
            264,
            3542,
            295,
            439,
            300,
            486,
            312,
            50600
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21611443161964417,
        "compression_ratio": 1.815533995628357,
        "no_speech_prob": 0.0036498820409178734
    },
    {
        "id": 300,
        "seek": 178288,
        "start": 1787.5999755859375,
        "end": 1795.52001953125,
        "text": " joined by concatenation. And so you are basically, what you're doing effectively is you are never",
        "tokens": [
            50600,
            6869,
            538,
            1588,
            7186,
            399,
            13,
            400,
            370,
            291,
            366,
            1936,
            11,
            437,
            291,
            434,
            884,
            8659,
            307,
            291,
            366,
            1128,
            50996
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21611443161964417,
        "compression_ratio": 1.815533995628357,
        "no_speech_prob": 0.0036498820409178734
    },
    {
        "id": 301,
        "seek": 178288,
        "start": 1795.52001953125,
        "end": 1800.8800048828125,
        "text": " going to be merging this E with this space, because they are now parts of the separate",
        "tokens": [
            50996,
            516,
            281,
            312,
            44559,
            341,
            462,
            365,
            341,
            1901,
            11,
            570,
            436,
            366,
            586,
            3166,
            295,
            264,
            4994,
            51264
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21611443161964417,
        "compression_ratio": 1.815533995628357,
        "no_speech_prob": 0.0036498820409178734
    },
    {
        "id": 302,
        "seek": 178288,
        "start": 1800.8800048828125,
        "end": 1808.0,
        "text": " elements of this list. And so you are saying we are never going to merge E space, because we're",
        "tokens": [
            51264,
            4959,
            295,
            341,
            1329,
            13,
            400,
            370,
            291,
            366,
            1566,
            321,
            366,
            1128,
            516,
            281,
            22183,
            462,
            1901,
            11,
            570,
            321,
            434,
            51620
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21611443161964417,
        "compression_ratio": 1.815533995628357,
        "no_speech_prob": 0.0036498820409178734
    },
    {
        "id": 303,
        "seek": 180800,
        "start": 1808.0,
        "end": 1814.239990234375,
        "text": " breaking it up in this way. So basically using this regex pattern to chunk up the text is just",
        "tokens": [
            50364,
            7697,
            309,
            493,
            294,
            341,
            636,
            13,
            407,
            1936,
            1228,
            341,
            319,
            432,
            87,
            5102,
            281,
            16635,
            493,
            264,
            2487,
            307,
            445,
            50676
        ],
        "temperature": 0.0,
        "avg_logprob": -0.15913067758083344,
        "compression_ratio": 1.7276785373687744,
        "no_speech_prob": 0.009125220589339733
    },
    {
        "id": 304,
        "seek": 180800,
        "start": 1814.239990234375,
        "end": 1820.800048828125,
        "text": " one way of enforcing that some merges are not to happen. And we're going to go into more of this",
        "tokens": [
            50676,
            472,
            636,
            295,
            25495,
            2175,
            300,
            512,
            3551,
            2880,
            366,
            406,
            281,
            1051,
            13,
            400,
            321,
            434,
            516,
            281,
            352,
            666,
            544,
            295,
            341,
            51004
        ],
        "temperature": 0.0,
        "avg_logprob": -0.15913067758083344,
        "compression_ratio": 1.7276785373687744,
        "no_speech_prob": 0.009125220589339733
    },
    {
        "id": 305,
        "seek": 180800,
        "start": 1820.800048828125,
        "end": 1824.56005859375,
        "text": " text. And we'll see that what this is trying to do on a high level is we're trying to not merge",
        "tokens": [
            51004,
            2487,
            13,
            400,
            321,
            603,
            536,
            300,
            437,
            341,
            307,
            1382,
            281,
            360,
            322,
            257,
            1090,
            1496,
            307,
            321,
            434,
            1382,
            281,
            406,
            22183,
            51192
        ],
        "temperature": 0.0,
        "avg_logprob": -0.15913067758083344,
        "compression_ratio": 1.7276785373687744,
        "no_speech_prob": 0.009125220589339733
    },
    {
        "id": 306,
        "seek": 180800,
        "start": 1824.56005859375,
        "end": 1830.47998046875,
        "text": " across letters, across numbers, across punctuation, and so on. So let's see in more detail how that",
        "tokens": [
            51192,
            2108,
            7825,
            11,
            2108,
            3547,
            11,
            2108,
            27006,
            16073,
            11,
            293,
            370,
            322,
            13,
            407,
            718,
            311,
            536,
            294,
            544,
            2607,
            577,
            300,
            51488
        ],
        "temperature": 0.0,
        "avg_logprob": -0.15913067758083344,
        "compression_ratio": 1.7276785373687744,
        "no_speech_prob": 0.009125220589339733
    },
    {
        "id": 307,
        "seek": 183048,
        "start": 1830.47998046875,
        "end": 1837.760009765625,
        "text": " works. So let's continue now. We have slash p of n. If you go to the documentation, slash p of n",
        "tokens": [
            50364,
            1985,
            13,
            407,
            718,
            311,
            2354,
            586,
            13,
            492,
            362,
            17330,
            280,
            295,
            297,
            13,
            759,
            291,
            352,
            281,
            264,
            14333,
            11,
            17330,
            280,
            295,
            297,
            50728
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20816929638385773,
        "compression_ratio": 1.7236363887786865,
        "no_speech_prob": 0.04084101691842079
    },
    {
        "id": 308,
        "seek": 183048,
        "start": 1837.760009765625,
        "end": 1843.43994140625,
        "text": " is any kind of numeric character in any script. So it's numbers. So we have an optional space",
        "tokens": [
            50728,
            307,
            604,
            733,
            295,
            7866,
            299,
            2517,
            294,
            604,
            5755,
            13,
            407,
            309,
            311,
            3547,
            13,
            407,
            321,
            362,
            364,
            17312,
            1901,
            51012
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20816929638385773,
        "compression_ratio": 1.7236363887786865,
        "no_speech_prob": 0.04084101691842079
    },
    {
        "id": 309,
        "seek": 183048,
        "start": 1843.43994140625,
        "end": 1848.0,
        "text": " followed by numbers, and those would be separated out. So letters and numbers are being separated.",
        "tokens": [
            51012,
            6263,
            538,
            3547,
            11,
            293,
            729,
            576,
            312,
            12005,
            484,
            13,
            407,
            7825,
            293,
            3547,
            366,
            885,
            12005,
            13,
            51240
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20816929638385773,
        "compression_ratio": 1.7236363887786865,
        "no_speech_prob": 0.04084101691842079
    },
    {
        "id": 310,
        "seek": 183048,
        "start": 1848.0,
        "end": 1853.43994140625,
        "text": " So if I do, hello, world, one, two, three, how are you? Then world will stop matching here,",
        "tokens": [
            51240,
            407,
            498,
            286,
            360,
            11,
            7751,
            11,
            1002,
            11,
            472,
            11,
            732,
            11,
            1045,
            11,
            577,
            366,
            291,
            30,
            1396,
            1002,
            486,
            1590,
            14324,
            510,
            11,
            51512
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20816929638385773,
        "compression_ratio": 1.7236363887786865,
        "no_speech_prob": 0.04084101691842079
    },
    {
        "id": 311,
        "seek": 183048,
        "start": 1853.43994140625,
        "end": 1858.9599609375,
        "text": " because one is not a letter anymore. But one is a number, so this group will match for that,",
        "tokens": [
            51512,
            570,
            472,
            307,
            406,
            257,
            5063,
            3602,
            13,
            583,
            472,
            307,
            257,
            1230,
            11,
            370,
            341,
            1594,
            486,
            2995,
            337,
            300,
            11,
            51788
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20816929638385773,
        "compression_ratio": 1.7236363887786865,
        "no_speech_prob": 0.04084101691842079
    },
    {
        "id": 312,
        "seek": 185896,
        "start": 1858.9599609375,
        "end": 1865.8399658203125,
        "text": " and we'll get it as a separate entity. Let's see how these apostrophes work. So here,",
        "tokens": [
            50364,
            293,
            321,
            603,
            483,
            309,
            382,
            257,
            4994,
            13977,
            13,
            961,
            311,
            536,
            577,
            613,
            19484,
            11741,
            279,
            589,
            13,
            407,
            510,
            11,
            50708
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20379213988780975,
        "compression_ratio": 1.6144578456878662,
        "no_speech_prob": 7.368584192590788e-05
    },
    {
        "id": 313,
        "seek": 185896,
        "start": 1865.8399658203125,
        "end": 1875.8399658203125,
        "text": " if we have apostrophe v as an example, then apostrophe here is not a letter or a number.",
        "tokens": [
            50708,
            498,
            321,
            362,
            19484,
            27194,
            371,
            382,
            364,
            1365,
            11,
            550,
            19484,
            27194,
            510,
            307,
            406,
            257,
            5063,
            420,
            257,
            1230,
            13,
            51208
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20379213988780975,
        "compression_ratio": 1.6144578456878662,
        "no_speech_prob": 7.368584192590788e-05
    },
    {
        "id": 314,
        "seek": 185896,
        "start": 1877.0400390625,
        "end": 1883.6800537109375,
        "text": " So hello will stop matching, and then we will exactly match this with that. So that will come",
        "tokens": [
            51268,
            407,
            7751,
            486,
            1590,
            14324,
            11,
            293,
            550,
            321,
            486,
            2293,
            2995,
            341,
            365,
            300,
            13,
            407,
            300,
            486,
            808,
            51600
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20379213988780975,
        "compression_ratio": 1.6144578456878662,
        "no_speech_prob": 7.368584192590788e-05
    },
    {
        "id": 315,
        "seek": 188368,
        "start": 1883.6800537109375,
        "end": 1888.9599609375,
        "text": " out as a separate thing. So why are they doing the apostrophes here? Honestly, I think that these",
        "tokens": [
            50364,
            484,
            382,
            257,
            4994,
            551,
            13,
            407,
            983,
            366,
            436,
            884,
            264,
            19484,
            11741,
            279,
            510,
            30,
            12348,
            11,
            286,
            519,
            300,
            613,
            50628
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1661737710237503,
        "compression_ratio": 1.6697674989700317,
        "no_speech_prob": 0.008711216039955616
    },
    {
        "id": 316,
        "seek": 188368,
        "start": 1888.9599609375,
        "end": 1896.1600341796875,
        "text": " are just very common apostrophes that are used typically. I don't love that they've done this,",
        "tokens": [
            50628,
            366,
            445,
            588,
            2689,
            19484,
            11741,
            279,
            300,
            366,
            1143,
            5850,
            13,
            286,
            500,
            380,
            959,
            300,
            436,
            600,
            1096,
            341,
            11,
            50988
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1661737710237503,
        "compression_ratio": 1.6697674989700317,
        "no_speech_prob": 0.008711216039955616
    },
    {
        "id": 317,
        "seek": 188368,
        "start": 1896.1600341796875,
        "end": 1903.43994140625,
        "text": " because let me show you what happens when you have some Unicode apostrophes. For example,",
        "tokens": [
            50988,
            570,
            718,
            385,
            855,
            291,
            437,
            2314,
            562,
            291,
            362,
            512,
            1156,
            299,
            1429,
            19484,
            11741,
            279,
            13,
            1171,
            1365,
            11,
            51352
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1661737710237503,
        "compression_ratio": 1.6697674989700317,
        "no_speech_prob": 0.008711216039955616
    },
    {
        "id": 318,
        "seek": 188368,
        "start": 1904.6400146484375,
        "end": 1908.6400146484375,
        "text": " if you have house, then this will be separated out because of this matching.",
        "tokens": [
            51412,
            498,
            291,
            362,
            1782,
            11,
            550,
            341,
            486,
            312,
            12005,
            484,
            570,
            295,
            341,
            14324,
            13,
            51612
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1661737710237503,
        "compression_ratio": 1.6697674989700317,
        "no_speech_prob": 0.008711216039955616
    },
    {
        "id": 319,
        "seek": 190864,
        "start": 1909.52001953125,
        "end": 1916.8800048828125,
        "text": " But if you use the Unicode apostrophe like this, then suddenly this does not work. And so this",
        "tokens": [
            50408,
            583,
            498,
            291,
            764,
            264,
            1156,
            299,
            1429,
            19484,
            27194,
            411,
            341,
            11,
            550,
            5800,
            341,
            775,
            406,
            589,
            13,
            400,
            370,
            341,
            50776
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19194233417510986,
        "compression_ratio": 1.62895929813385,
        "no_speech_prob": 5.9209134633420035e-05
    },
    {
        "id": 320,
        "seek": 190864,
        "start": 1916.8800048828125,
        "end": 1922.800048828125,
        "text": " apostrophe will actually become its own thing now. And so it's basically hard-coded for this",
        "tokens": [
            50776,
            19484,
            27194,
            486,
            767,
            1813,
            1080,
            1065,
            551,
            586,
            13,
            400,
            370,
            309,
            311,
            1936,
            1152,
            12,
            66,
            12340,
            337,
            341,
            51072
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19194233417510986,
        "compression_ratio": 1.62895929813385,
        "no_speech_prob": 5.9209134633420035e-05
    },
    {
        "id": 321,
        "seek": 190864,
        "start": 1922.800048828125,
        "end": 1928.56005859375,
        "text": " specific kind of apostrophe, and otherwise they become completely separate tokens.",
        "tokens": [
            51072,
            2685,
            733,
            295,
            19484,
            27194,
            11,
            293,
            5911,
            436,
            1813,
            2584,
            4994,
            22667,
            13,
            51360
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19194233417510986,
        "compression_ratio": 1.62895929813385,
        "no_speech_prob": 5.9209134633420035e-05
    },
    {
        "id": 322,
        "seek": 190864,
        "start": 1929.5999755859375,
        "end": 1935.9200439453125,
        "text": " In addition to this, you can go to the GPT-2 docs, and here when they define the pattern,",
        "tokens": [
            51412,
            682,
            4500,
            281,
            341,
            11,
            291,
            393,
            352,
            281,
            264,
            26039,
            51,
            12,
            17,
            45623,
            11,
            293,
            510,
            562,
            436,
            6964,
            264,
            5102,
            11,
            51728
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19194233417510986,
        "compression_ratio": 1.62895929813385,
        "no_speech_prob": 5.9209134633420035e-05
    },
    {
        "id": 323,
        "seek": 193592,
        "start": 1935.9200439453125,
        "end": 1940.8800048828125,
        "text": " they say, should have added re.ignore case, so BP mergers can happen for capitalized versions",
        "tokens": [
            50364,
            436,
            584,
            11,
            820,
            362,
            3869,
            319,
            13,
            788,
            418,
            1389,
            11,
            370,
            40533,
            3551,
            9458,
            393,
            1051,
            337,
            4238,
            1602,
            9606,
            50612
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2134374976158142,
        "compression_ratio": 1.5872340202331543,
        "no_speech_prob": 0.0003150374977849424
    },
    {
        "id": 324,
        "seek": 193592,
        "start": 1940.8800048828125,
        "end": 1945.760009765625,
        "text": " of contractions. So what they're pointing out is that you see how this is apostrophe and then",
        "tokens": [
            50612,
            295,
            4364,
            626,
            13,
            407,
            437,
            436,
            434,
            12166,
            484,
            307,
            300,
            291,
            536,
            577,
            341,
            307,
            19484,
            27194,
            293,
            550,
            50856
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2134374976158142,
        "compression_ratio": 1.5872340202331543,
        "no_speech_prob": 0.0003150374977849424
    },
    {
        "id": 325,
        "seek": 193592,
        "start": 1945.760009765625,
        "end": 1953.0400390625,
        "text": " lowercase letters? Well, because they didn't do re.ignore case, then these rules will not",
        "tokens": [
            50856,
            3126,
            9765,
            7825,
            30,
            1042,
            11,
            570,
            436,
            994,
            380,
            360,
            319,
            13,
            788,
            418,
            1389,
            11,
            550,
            613,
            4474,
            486,
            406,
            51220
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2134374976158142,
        "compression_ratio": 1.5872340202331543,
        "no_speech_prob": 0.0003150374977849424
    },
    {
        "id": 326,
        "seek": 193592,
        "start": 1953.0400390625,
        "end": 1963.3599853515625,
        "text": " separate out the apostrophes if it's uppercase. So house would be like this. But if I did house",
        "tokens": [
            51220,
            4994,
            484,
            264,
            19484,
            11741,
            279,
            498,
            309,
            311,
            11775,
            2869,
            651,
            13,
            407,
            1782,
            576,
            312,
            411,
            341,
            13,
            583,
            498,
            286,
            630,
            1782,
            51736
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2134374976158142,
        "compression_ratio": 1.5872340202331543,
        "no_speech_prob": 0.0003150374977849424
    },
    {
        "id": 327,
        "seek": 196336,
        "start": 1964.239990234375,
        "end": 1968.56005859375,
        "text": " from uppercase, then notice suddenly the apostrophe comes by itself.",
        "tokens": [
            50408,
            490,
            11775,
            2869,
            651,
            11,
            550,
            3449,
            5800,
            264,
            19484,
            27194,
            1487,
            538,
            2564,
            13,
            50624
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20686383545398712,
        "compression_ratio": 1.6679842472076416,
        "no_speech_prob": 7.843770436011255e-05
    },
    {
        "id": 328,
        "seek": 196336,
        "start": 1969.760009765625,
        "end": 1974.719970703125,
        "text": " So the tokenization will work differently in uppercase and lowercase, inconsistently",
        "tokens": [
            50684,
            407,
            264,
            14862,
            2144,
            486,
            589,
            7614,
            294,
            11775,
            2869,
            651,
            293,
            3126,
            9765,
            11,
            22039,
            468,
            2276,
            50932
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20686383545398712,
        "compression_ratio": 1.6679842472076416,
        "no_speech_prob": 7.843770436011255e-05
    },
    {
        "id": 329,
        "seek": 196336,
        "start": 1974.719970703125,
        "end": 1978.56005859375,
        "text": " separating out these apostrophes. So it feels extremely gnarly and slightly gross,",
        "tokens": [
            50932,
            29279,
            484,
            613,
            19484,
            11741,
            279,
            13,
            407,
            309,
            3417,
            4664,
            290,
            20062,
            356,
            293,
            4748,
            11367,
            11,
            51124
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20686383545398712,
        "compression_ratio": 1.6679842472076416,
        "no_speech_prob": 7.843770436011255e-05
    },
    {
        "id": 330,
        "seek": 196336,
        "start": 1980.3199462890625,
        "end": 1984.8800048828125,
        "text": " but that's how that works. Okay, so let's come back. After trying to match a bunch of",
        "tokens": [
            51212,
            457,
            300,
            311,
            577,
            300,
            1985,
            13,
            1033,
            11,
            370,
            718,
            311,
            808,
            646,
            13,
            2381,
            1382,
            281,
            2995,
            257,
            3840,
            295,
            51440
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20686383545398712,
        "compression_ratio": 1.6679842472076416,
        "no_speech_prob": 7.843770436011255e-05
    },
    {
        "id": 331,
        "seek": 196336,
        "start": 1984.8800048828125,
        "end": 1989.3599853515625,
        "text": " apostrophe expressions, by the way, the other issue here is that these are quite language-specific,",
        "tokens": [
            51440,
            19484,
            27194,
            15277,
            11,
            538,
            264,
            636,
            11,
            264,
            661,
            2734,
            510,
            307,
            300,
            613,
            366,
            1596,
            2856,
            12,
            29258,
            11,
            51664
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20686383545398712,
        "compression_ratio": 1.6679842472076416,
        "no_speech_prob": 7.843770436011255e-05
    },
    {
        "id": 332,
        "seek": 198936,
        "start": 1989.8399658203125,
        "end": 1993.5999755859375,
        "text": " so I don't know that all the languages, for example, use or don't use apostrophes,",
        "tokens": [
            50388,
            370,
            286,
            500,
            380,
            458,
            300,
            439,
            264,
            8650,
            11,
            337,
            1365,
            11,
            764,
            420,
            500,
            380,
            764,
            19484,
            11741,
            279,
            11,
            50576
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1729244440793991,
        "compression_ratio": 1.7804877758026123,
        "no_speech_prob": 0.0010649607283994555
    },
    {
        "id": 333,
        "seek": 198936,
        "start": 1993.5999755859375,
        "end": 1998.8800048828125,
        "text": " but that would be inconsistently tokenized as a result. Then we try to match letters,",
        "tokens": [
            50576,
            457,
            300,
            576,
            312,
            22039,
            468,
            2276,
            14862,
            1602,
            382,
            257,
            1874,
            13,
            1396,
            321,
            853,
            281,
            2995,
            7825,
            11,
            50840
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1729244440793991,
        "compression_ratio": 1.7804877758026123,
        "no_speech_prob": 0.0010649607283994555
    },
    {
        "id": 334,
        "seek": 198936,
        "start": 1998.8800048828125,
        "end": 2003.1199951171875,
        "text": " then we try to match numbers, and then if that doesn't work, we fall back to here.",
        "tokens": [
            50840,
            550,
            321,
            853,
            281,
            2995,
            3547,
            11,
            293,
            550,
            498,
            300,
            1177,
            380,
            589,
            11,
            321,
            2100,
            646,
            281,
            510,
            13,
            51052
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1729244440793991,
        "compression_ratio": 1.7804877758026123,
        "no_speech_prob": 0.0010649607283994555
    },
    {
        "id": 335,
        "seek": 198936,
        "start": 2003.9200439453125,
        "end": 2007.8399658203125,
        "text": " And what this is saying is, again, optional space followed by something that is not a letter,",
        "tokens": [
            51092,
            400,
            437,
            341,
            307,
            1566,
            307,
            11,
            797,
            11,
            17312,
            1901,
            6263,
            538,
            746,
            300,
            307,
            406,
            257,
            5063,
            11,
            51288
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1729244440793991,
        "compression_ratio": 1.7804877758026123,
        "no_speech_prob": 0.0010649607283994555
    },
    {
        "id": 336,
        "seek": 198936,
        "start": 2007.8399658203125,
        "end": 2013.1199951171875,
        "text": " number, or a space, and one or more of that. So what this is doing effectively is this is",
        "tokens": [
            51288,
            1230,
            11,
            420,
            257,
            1901,
            11,
            293,
            472,
            420,
            544,
            295,
            300,
            13,
            407,
            437,
            341,
            307,
            884,
            8659,
            307,
            341,
            307,
            51552
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1729244440793991,
        "compression_ratio": 1.7804877758026123,
        "no_speech_prob": 0.0010649607283994555
    },
    {
        "id": 337,
        "seek": 198936,
        "start": 2013.1199951171875,
        "end": 2016.719970703125,
        "text": " trying to match punctuation, roughly speaking, not letters and not numbers.",
        "tokens": [
            51552,
            1382,
            281,
            2995,
            27006,
            16073,
            11,
            9810,
            4124,
            11,
            406,
            7825,
            293,
            406,
            3547,
            13,
            51732
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1729244440793991,
        "compression_ratio": 1.7804877758026123,
        "no_speech_prob": 0.0010649607283994555
    },
    {
        "id": 338,
        "seek": 201672,
        "start": 2017.5999755859375,
        "end": 2021.0400390625,
        "text": " So this group will try to trigger for that. So if I do something like this,",
        "tokens": [
            50408,
            407,
            341,
            1594,
            486,
            853,
            281,
            7875,
            337,
            300,
            13,
            407,
            498,
            286,
            360,
            746,
            411,
            341,
            11,
            50580
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1940198391675949,
        "compression_ratio": 1.5735293626785278,
        "no_speech_prob": 0.00011591902148211375
    },
    {
        "id": 339,
        "seek": 201672,
        "start": 2022.0799560546875,
        "end": 2029.280029296875,
        "text": " then these parts here are not letters or numbers, but they will actually get caught here,",
        "tokens": [
            50632,
            550,
            613,
            3166,
            510,
            366,
            406,
            7825,
            420,
            3547,
            11,
            457,
            436,
            486,
            767,
            483,
            5415,
            510,
            11,
            50992
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1940198391675949,
        "compression_ratio": 1.5735293626785278,
        "no_speech_prob": 0.00011591902148211375
    },
    {
        "id": 340,
        "seek": 201672,
        "start": 2030.0,
        "end": 2033.6800537109375,
        "text": " and so they become its own group. So we've separated out the punctuation.",
        "tokens": [
            51028,
            293,
            370,
            436,
            1813,
            1080,
            1065,
            1594,
            13,
            407,
            321,
            600,
            12005,
            484,
            264,
            27006,
            16073,
            13,
            51212
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1940198391675949,
        "compression_ratio": 1.5735293626785278,
        "no_speech_prob": 0.00011591902148211375
    },
    {
        "id": 341,
        "seek": 201672,
        "start": 2034.8800048828125,
        "end": 2040.0,
        "text": " And finally, this is also a little bit confusing. So this is matching whitespace,",
        "tokens": [
            51272,
            400,
            2721,
            11,
            341,
            307,
            611,
            257,
            707,
            857,
            13181,
            13,
            407,
            341,
            307,
            14324,
            21909,
            17940,
            11,
            51528
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1940198391675949,
        "compression_ratio": 1.5735293626785278,
        "no_speech_prob": 0.00011591902148211375
    },
    {
        "id": 342,
        "seek": 204000,
        "start": 2040.56005859375,
        "end": 2047.43994140625,
        "text": " but this is using a negative lookahead assertion in regex. So what this is doing is it's matching",
        "tokens": [
            50392,
            457,
            341,
            307,
            1228,
            257,
            3671,
            574,
            545,
            2056,
            19810,
            313,
            294,
            319,
            432,
            87,
            13,
            407,
            437,
            341,
            307,
            884,
            307,
            309,
            311,
            14324,
            50736
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22253990173339844,
        "compression_ratio": 1.6089109182357788,
        "no_speech_prob": 0.0011335500748828053
    },
    {
        "id": 343,
        "seek": 204000,
        "start": 2047.43994140625,
        "end": 2053.360107421875,
        "text": " whitespace up to, but not including, the last whitespace character. Why is this important?",
        "tokens": [
            50736,
            21909,
            17940,
            493,
            281,
            11,
            457,
            406,
            3009,
            11,
            264,
            1036,
            21909,
            17940,
            2517,
            13,
            1545,
            307,
            341,
            1021,
            30,
            51032
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22253990173339844,
        "compression_ratio": 1.6089109182357788,
        "no_speech_prob": 0.0011335500748828053
    },
    {
        "id": 344,
        "seek": 204000,
        "start": 2054.320068359375,
        "end": 2057.760009765625,
        "text": " This is pretty subtle, I think. So you see how the whitespace is always",
        "tokens": [
            51080,
            639,
            307,
            1238,
            13743,
            11,
            286,
            519,
            13,
            407,
            291,
            536,
            577,
            264,
            21909,
            17940,
            307,
            1009,
            51252
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22253990173339844,
        "compression_ratio": 1.6089109182357788,
        "no_speech_prob": 0.0011335500748828053
    },
    {
        "id": 345,
        "seek": 204000,
        "start": 2057.760009765625,
        "end": 2063.280029296875,
        "text": " included at the beginning of the word, so space R, space U, etc.",
        "tokens": [
            51252,
            5556,
            412,
            264,
            2863,
            295,
            264,
            1349,
            11,
            370,
            1901,
            497,
            11,
            1901,
            624,
            11,
            5183,
            13,
            51528
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22253990173339844,
        "compression_ratio": 1.6089109182357788,
        "no_speech_prob": 0.0011335500748828053
    }
]