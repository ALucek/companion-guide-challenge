[
    {
        "id": 0,
        "seek": 0,
        "start": 0.0,
        "end": 4.320000171661377,
        "text": " Hi everyone. So in this video I'd like us to cover the process of tokenization in large",
        "tokens": [
            50364,
            2421,
            1518,
            13,
            407,
            294,
            341,
            960,
            286,
            1116,
            411,
            505,
            281,
            2060,
            264,
            1399,
            295,
            14862,
            2144,
            294,
            2416,
            50580
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20340920984745026,
        "compression_ratio": 1.7595419883728027,
        "no_speech_prob": 0.0043975552543997765
    },
    {
        "id": 1,
        "seek": 0,
        "start": 4.320000171661377,
        "end": 10.079999923706055,
        "text": " language models. Now you see here that I have a sad face and that's because tokenization is my",
        "tokens": [
            50580,
            2856,
            5245,
            13,
            823,
            291,
            536,
            510,
            300,
            286,
            362,
            257,
            4227,
            1851,
            293,
            300,
            311,
            570,
            14862,
            2144,
            307,
            452,
            50868
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20340920984745026,
        "compression_ratio": 1.7595419883728027,
        "no_speech_prob": 0.0043975552543997765
    },
    {
        "id": 2,
        "seek": 0,
        "start": 10.079999923706055,
        "end": 14.079999923706055,
        "text": " least favorite part of working with large language models but unfortunately it is necessary to",
        "tokens": [
            50868,
            1935,
            2954,
            644,
            295,
            1364,
            365,
            2416,
            2856,
            5245,
            457,
            7015,
            309,
            307,
            4818,
            281,
            51068
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20340920984745026,
        "compression_ratio": 1.7595419883728027,
        "no_speech_prob": 0.0043975552543997765
    },
    {
        "id": 3,
        "seek": 0,
        "start": 14.079999923706055,
        "end": 18.639999389648438,
        "text": " understand in some detail because it is fairly hairy, gnarly and there's a lot of hidden foot",
        "tokens": [
            51068,
            1223,
            294,
            512,
            2607,
            570,
            309,
            307,
            6457,
            42346,
            11,
            290,
            20062,
            356,
            293,
            456,
            311,
            257,
            688,
            295,
            7633,
            2671,
            51296
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20340920984745026,
        "compression_ratio": 1.7595419883728027,
        "no_speech_prob": 0.0043975552543997765
    },
    {
        "id": 4,
        "seek": 0,
        "start": 18.639999389648438,
        "end": 23.84000015258789,
        "text": " gums to be aware of and a lot of oddness with large language models typically traces back",
        "tokens": [
            51296,
            290,
            8099,
            281,
            312,
            3650,
            295,
            293,
            257,
            688,
            295,
            7401,
            1287,
            365,
            2416,
            2856,
            5245,
            5850,
            26076,
            646,
            51556
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20340920984745026,
        "compression_ratio": 1.7595419883728027,
        "no_speech_prob": 0.0043975552543997765
    },
    {
        "id": 5,
        "seek": 2384,
        "start": 23.84000015258789,
        "end": 30.399999618530273,
        "text": " to tokenization. So what is tokenization? Now in my previous video Let's Build GPT from Scratch",
        "tokens": [
            50364,
            281,
            14862,
            2144,
            13,
            407,
            437,
            307,
            14862,
            2144,
            30,
            823,
            294,
            452,
            3894,
            960,
            961,
            311,
            11875,
            26039,
            51,
            490,
            34944,
            852,
            50692
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20672139525413513,
        "compression_ratio": 1.7363636493682861,
        "no_speech_prob": 0.10229131579399109
    },
    {
        "id": 6,
        "seek": 2384,
        "start": 31.440000534057617,
        "end": 37.040000915527344,
        "text": " we actually already did tokenization but we did a very naive simple version of tokenization. So",
        "tokens": [
            50744,
            321,
            767,
            1217,
            630,
            14862,
            2144,
            457,
            321,
            630,
            257,
            588,
            29052,
            2199,
            3037,
            295,
            14862,
            2144,
            13,
            407,
            51024
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20672139525413513,
        "compression_ratio": 1.7363636493682861,
        "no_speech_prob": 0.10229131579399109
    },
    {
        "id": 7,
        "seek": 2384,
        "start": 37.040000915527344,
        "end": 42.560001373291016,
        "text": " when you go to the Google Colab for that video you see here that we loaded our training set",
        "tokens": [
            51024,
            562,
            291,
            352,
            281,
            264,
            3329,
            4004,
            455,
            337,
            300,
            960,
            291,
            536,
            510,
            300,
            321,
            13210,
            527,
            3097,
            992,
            51300
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20672139525413513,
        "compression_ratio": 1.7363636493682861,
        "no_speech_prob": 0.10229131579399109
    },
    {
        "id": 8,
        "seek": 2384,
        "start": 43.119998931884766,
        "end": 49.119998931884766,
        "text": " and our training set was this Shakespeare dataset. Now in the beginning the Shakespeare dataset is",
        "tokens": [
            51328,
            293,
            527,
            3097,
            992,
            390,
            341,
            22825,
            28872,
            13,
            823,
            294,
            264,
            2863,
            264,
            22825,
            28872,
            307,
            51628
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20672139525413513,
        "compression_ratio": 1.7363636493682861,
        "no_speech_prob": 0.10229131579399109
    },
    {
        "id": 9,
        "seek": 4912,
        "start": 49.119998931884766,
        "end": 54.959999084472656,
        "text": " just a large string in Python it's just text and so the question is how do we plug text into",
        "tokens": [
            50364,
            445,
            257,
            2416,
            6798,
            294,
            15329,
            309,
            311,
            445,
            2487,
            293,
            370,
            264,
            1168,
            307,
            577,
            360,
            321,
            5452,
            2487,
            666,
            50656
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2068072110414505,
        "compression_ratio": 1.7557603120803833,
        "no_speech_prob": 0.024422382935881615
    },
    {
        "id": 10,
        "seek": 4912,
        "start": 54.959999084472656,
        "end": 62.63999938964844,
        "text": " large language models and in this case here we created a vocabulary of 65 possible characters",
        "tokens": [
            50656,
            2416,
            2856,
            5245,
            293,
            294,
            341,
            1389,
            510,
            321,
            2942,
            257,
            19864,
            295,
            11624,
            1944,
            4342,
            51040
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2068072110414505,
        "compression_ratio": 1.7557603120803833,
        "no_speech_prob": 0.024422382935881615
    },
    {
        "id": 11,
        "seek": 4912,
        "start": 62.63999938964844,
        "end": 68.0,
        "text": " that we saw occur in this string. These were the possible characters and we saw that there are 65",
        "tokens": [
            51040,
            300,
            321,
            1866,
            5160,
            294,
            341,
            6798,
            13,
            1981,
            645,
            264,
            1944,
            4342,
            293,
            321,
            1866,
            300,
            456,
            366,
            11624,
            51308
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2068072110414505,
        "compression_ratio": 1.7557603120803833,
        "no_speech_prob": 0.024422382935881615
    },
    {
        "id": 12,
        "seek": 4912,
        "start": 68.0,
        "end": 74.31999969482422,
        "text": " of them and then we created a lookup table for converting from every possible character a little",
        "tokens": [
            51308,
            295,
            552,
            293,
            550,
            321,
            2942,
            257,
            574,
            1010,
            3199,
            337,
            29942,
            490,
            633,
            1944,
            2517,
            257,
            707,
            51624
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2068072110414505,
        "compression_ratio": 1.7557603120803833,
        "no_speech_prob": 0.024422382935881615
    },
    {
        "id": 13,
        "seek": 7432,
        "start": 74.31999969482422,
        "end": 82.72000122070312,
        "text": " string piece into a token an integer. So here for example we tokenized the string hi there and we",
        "tokens": [
            50364,
            6798,
            2522,
            666,
            257,
            14862,
            364,
            24922,
            13,
            407,
            510,
            337,
            1365,
            321,
            14862,
            1602,
            264,
            6798,
            4879,
            456,
            293,
            321,
            50784
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22843414545059204,
        "compression_ratio": 1.7696969509124756,
        "no_speech_prob": 0.00010071381984744221
    },
    {
        "id": 14,
        "seek": 7432,
        "start": 82.72000122070312,
        "end": 89.91999816894531,
        "text": " received this sequence of tokens and here we took the first 1000 characters of our dataset and we",
        "tokens": [
            50784,
            4613,
            341,
            8310,
            295,
            22667,
            293,
            510,
            321,
            1890,
            264,
            700,
            9714,
            4342,
            295,
            527,
            28872,
            293,
            321,
            51144
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22843414545059204,
        "compression_ratio": 1.7696969509124756,
        "no_speech_prob": 0.00010071381984744221
    },
    {
        "id": 15,
        "seek": 7432,
        "start": 89.91999816894531,
        "end": 96.87999725341797,
        "text": " encoded it into tokens and because this is character level we received 1000 tokens in a sequence",
        "tokens": [
            51144,
            2058,
            12340,
            309,
            666,
            22667,
            293,
            570,
            341,
            307,
            2517,
            1496,
            321,
            4613,
            9714,
            22667,
            294,
            257,
            8310,
            51492
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22843414545059204,
        "compression_ratio": 1.7696969509124756,
        "no_speech_prob": 0.00010071381984744221
    },
    {
        "id": 16,
        "seek": 9688,
        "start": 97.68000030517578,
        "end": 106.0,
        "text": " so token 18, 47, etc. Now later we saw that the way we plug these tokens into the language model",
        "tokens": [
            50404,
            370,
            14862,
            2443,
            11,
            16953,
            11,
            5183,
            13,
            823,
            1780,
            321,
            1866,
            300,
            264,
            636,
            321,
            5452,
            613,
            22667,
            666,
            264,
            2856,
            2316,
            50820
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19079861044883728,
        "compression_ratio": 1.707762598991394,
        "no_speech_prob": 7.722186273895204e-05
    },
    {
        "id": 17,
        "seek": 9688,
        "start": 106.55999755859375,
        "end": 113.44000244140625,
        "text": " is by using an embedding table and so basically if we have 65 possible tokens then this embedding",
        "tokens": [
            50848,
            307,
            538,
            1228,
            364,
            12240,
            3584,
            3199,
            293,
            370,
            1936,
            498,
            321,
            362,
            11624,
            1944,
            22667,
            550,
            341,
            12240,
            3584,
            51192
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19079861044883728,
        "compression_ratio": 1.707762598991394,
        "no_speech_prob": 7.722186273895204e-05
    },
    {
        "id": 18,
        "seek": 9688,
        "start": 113.44000244140625,
        "end": 119.36000061035156,
        "text": " table is going to have 65 rows and roughly speaking we're taking the integer associated",
        "tokens": [
            51192,
            3199,
            307,
            516,
            281,
            362,
            11624,
            13241,
            293,
            9810,
            4124,
            321,
            434,
            1940,
            264,
            24922,
            6615,
            51488
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19079861044883728,
        "compression_ratio": 1.707762598991394,
        "no_speech_prob": 7.722186273895204e-05
    },
    {
        "id": 19,
        "seek": 9688,
        "start": 119.36000061035156,
        "end": 124.63999938964844,
        "text": " with every single token we're using that as a lookup into this table and we're plucking out",
        "tokens": [
            51488,
            365,
            633,
            2167,
            14862,
            321,
            434,
            1228,
            300,
            382,
            257,
            574,
            1010,
            666,
            341,
            3199,
            293,
            321,
            434,
            499,
            33260,
            484,
            51752
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19079861044883728,
        "compression_ratio": 1.707762598991394,
        "no_speech_prob": 7.722186273895204e-05
    },
    {
        "id": 20,
        "seek": 12464,
        "start": 124.63999938964844,
        "end": 130.32000732421875,
        "text": " the corresponding row and this row is trainable parameters that we're going to train using",
        "tokens": [
            50364,
            264,
            11760,
            5386,
            293,
            341,
            5386,
            307,
            3847,
            712,
            9834,
            300,
            321,
            434,
            516,
            281,
            3847,
            1228,
            50648
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17744430899620056,
        "compression_ratio": 1.7348484992980957,
        "no_speech_prob": 4.75763481517788e-05
    },
    {
        "id": 21,
        "seek": 12464,
        "start": 130.32000732421875,
        "end": 135.83999633789062,
        "text": " backpropagation and this is the vector that then feeds into the transformer and that's how the",
        "tokens": [
            50648,
            646,
            79,
            1513,
            559,
            399,
            293,
            341,
            307,
            264,
            8062,
            300,
            550,
            23712,
            666,
            264,
            31782,
            293,
            300,
            311,
            577,
            264,
            50924
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17744430899620056,
        "compression_ratio": 1.7348484992980957,
        "no_speech_prob": 4.75763481517788e-05
    },
    {
        "id": 22,
        "seek": 12464,
        "start": 135.83999633789062,
        "end": 142.55999755859375,
        "text": " transformer sort of perceives every single token. So here we had a very naive tokenization process",
        "tokens": [
            50924,
            31782,
            1333,
            295,
            9016,
            1539,
            633,
            2167,
            14862,
            13,
            407,
            510,
            321,
            632,
            257,
            588,
            29052,
            14862,
            2144,
            1399,
            51260
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17744430899620056,
        "compression_ratio": 1.7348484992980957,
        "no_speech_prob": 4.75763481517788e-05
    },
    {
        "id": 23,
        "seek": 12464,
        "start": 142.55999755859375,
        "end": 147.83999633789062,
        "text": " that was a character level tokenizer but in practice in state-of-the-art language models",
        "tokens": [
            51260,
            300,
            390,
            257,
            2517,
            1496,
            14862,
            6545,
            457,
            294,
            3124,
            294,
            1785,
            12,
            2670,
            12,
            3322,
            12,
            446,
            2856,
            5245,
            51524
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17744430899620056,
        "compression_ratio": 1.7348484992980957,
        "no_speech_prob": 4.75763481517788e-05
    },
    {
        "id": 24,
        "seek": 12464,
        "start": 147.83999633789062,
        "end": 153.36000061035156,
        "text": " people use a lot more complicated schemes unfortunately for constructing these token",
        "tokens": [
            51524,
            561,
            764,
            257,
            688,
            544,
            6179,
            26954,
            7015,
            337,
            39969,
            613,
            14862,
            51800
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17744430899620056,
        "compression_ratio": 1.7348484992980957,
        "no_speech_prob": 4.75763481517788e-05
    },
    {
        "id": 25,
        "seek": 15336,
        "start": 153.36000061035156,
        "end": 159.9199981689453,
        "text": " vocabularies so we're not dealing on the character level we're dealing on chunk level and the way",
        "tokens": [
            50364,
            2329,
            455,
            1040,
            530,
            370,
            321,
            434,
            406,
            6260,
            322,
            264,
            2517,
            1496,
            321,
            434,
            6260,
            322,
            16635,
            1496,
            293,
            264,
            636,
            50692
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1771320104598999,
        "compression_ratio": 1.70652174949646,
        "no_speech_prob": 8.53031815495342e-06
    },
    {
        "id": 26,
        "seek": 15336,
        "start": 159.9199981689453,
        "end": 165.75999450683594,
        "text": " these character chunks are constructed is using algorithms such as for example the byte pair",
        "tokens": [
            50692,
            613,
            2517,
            24004,
            366,
            17083,
            307,
            1228,
            14642,
            1270,
            382,
            337,
            1365,
            264,
            40846,
            6119,
            50984
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1771320104598999,
        "compression_ratio": 1.70652174949646,
        "no_speech_prob": 8.53031815495342e-06
    },
    {
        "id": 27,
        "seek": 15336,
        "start": 165.75999450683594,
        "end": 172.47999572753906,
        "text": " encoding algorithm which we're going to go into in detail and cover in this video. I'd like to",
        "tokens": [
            50984,
            43430,
            9284,
            597,
            321,
            434,
            516,
            281,
            352,
            666,
            294,
            2607,
            293,
            2060,
            294,
            341,
            960,
            13,
            286,
            1116,
            411,
            281,
            51320
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1771320104598999,
        "compression_ratio": 1.70652174949646,
        "no_speech_prob": 8.53031815495342e-06
    },
    {
        "id": 28,
        "seek": 15336,
        "start": 172.47999572753906,
        "end": 177.60000610351562,
        "text": " briefly show you the paper that introduced byte level encoding as a mechanism for tokenization",
        "tokens": [
            51320,
            10515,
            855,
            291,
            264,
            3035,
            300,
            7268,
            40846,
            1496,
            43430,
            382,
            257,
            7513,
            337,
            14862,
            2144,
            51576
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1771320104598999,
        "compression_ratio": 1.70652174949646,
        "no_speech_prob": 8.53031815495342e-06
    },
    {
        "id": 29,
        "seek": 15336,
        "start": 177.60000610351562,
        "end": 182.47999572753906,
        "text": " in the context of large language models and I would say that that's probably a GPT-2 paper",
        "tokens": [
            51576,
            294,
            264,
            4319,
            295,
            2416,
            2856,
            5245,
            293,
            286,
            576,
            584,
            300,
            300,
            311,
            1391,
            257,
            26039,
            51,
            12,
            17,
            3035,
            51820
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1771320104598999,
        "compression_ratio": 1.70652174949646,
        "no_speech_prob": 8.53031815495342e-06
    },
    {
        "id": 30,
        "seek": 18248,
        "start": 182.47999572753906,
        "end": 188.0,
        "text": " and if you scroll down here to the section input representation this is where they cover",
        "tokens": [
            50364,
            293,
            498,
            291,
            11369,
            760,
            510,
            281,
            264,
            3541,
            4846,
            10290,
            341,
            307,
            689,
            436,
            2060,
            50640
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1966203898191452,
        "compression_ratio": 1.736842155456543,
        "no_speech_prob": 1.1843115316878539e-05
    },
    {
        "id": 31,
        "seek": 18248,
        "start": 188.0,
        "end": 192.9600067138672,
        "text": " tokenization the kinds of properties that you'd like the tokenization to have and they conclude",
        "tokens": [
            50640,
            14862,
            2144,
            264,
            3685,
            295,
            7221,
            300,
            291,
            1116,
            411,
            264,
            14862,
            2144,
            281,
            362,
            293,
            436,
            16886,
            50888
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1966203898191452,
        "compression_ratio": 1.736842155456543,
        "no_speech_prob": 1.1843115316878539e-05
    },
    {
        "id": 32,
        "seek": 18248,
        "start": 192.9600067138672,
        "end": 200.0800018310547,
        "text": " here that they're going to have a tokenizer where you have a vocabulary of 50,257 possible",
        "tokens": [
            50888,
            510,
            300,
            436,
            434,
            516,
            281,
            362,
            257,
            14862,
            6545,
            689,
            291,
            362,
            257,
            19864,
            295,
            2625,
            11,
            6074,
            22,
            1944,
            51244
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1966203898191452,
        "compression_ratio": 1.736842155456543,
        "no_speech_prob": 1.1843115316878539e-05
    },
    {
        "id": 33,
        "seek": 18248,
        "start": 200.0800018310547,
        "end": 208.8000030517578,
        "text": " tokens and the context size is going to be 1024 tokens so in the intention layer of the",
        "tokens": [
            51244,
            22667,
            293,
            264,
            4319,
            2744,
            307,
            516,
            281,
            312,
            1266,
            7911,
            22667,
            370,
            294,
            264,
            7789,
            4583,
            295,
            264,
            51680
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1966203898191452,
        "compression_ratio": 1.736842155456543,
        "no_speech_prob": 1.1843115316878539e-05
    },
    {
        "id": 34,
        "seek": 20880,
        "start": 208.8000030517578,
        "end": 213.75999450683594,
        "text": " transformer neural network every single token is attending to the previous tokens in the sequence",
        "tokens": [
            50364,
            31782,
            18161,
            3209,
            633,
            2167,
            14862,
            307,
            15862,
            281,
            264,
            3894,
            22667,
            294,
            264,
            8310,
            50612
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19734135270118713,
        "compression_ratio": 1.7706421613693237,
        "no_speech_prob": 0.00015118172450456768
    },
    {
        "id": 35,
        "seek": 20880,
        "start": 213.75999450683594,
        "end": 222.24000549316406,
        "text": " and it's going to see up to 1024 tokens so tokens are this like fundamental unit the atom of large",
        "tokens": [
            50612,
            293,
            309,
            311,
            516,
            281,
            536,
            493,
            281,
            1266,
            7911,
            22667,
            370,
            22667,
            366,
            341,
            411,
            8088,
            4985,
            264,
            12018,
            295,
            2416,
            51036
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19734135270118713,
        "compression_ratio": 1.7706421613693237,
        "no_speech_prob": 0.00015118172450456768
    },
    {
        "id": 36,
        "seek": 20880,
        "start": 222.24000549316406,
        "end": 226.8800048828125,
        "text": " language models if you will and everything is in units of tokens everything is about tokens",
        "tokens": [
            51036,
            2856,
            5245,
            498,
            291,
            486,
            293,
            1203,
            307,
            294,
            6815,
            295,
            22667,
            1203,
            307,
            466,
            22667,
            51268
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19734135270118713,
        "compression_ratio": 1.7706421613693237,
        "no_speech_prob": 0.00015118172450456768
    },
    {
        "id": 37,
        "seek": 20880,
        "start": 226.8800048828125,
        "end": 233.60000610351562,
        "text": " and tokenization is the process for translating strings or text into sequences of tokens and vice",
        "tokens": [
            51268,
            293,
            14862,
            2144,
            307,
            264,
            1399,
            337,
            35030,
            13985,
            420,
            2487,
            666,
            22978,
            295,
            22667,
            293,
            11964,
            51604
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19734135270118713,
        "compression_ratio": 1.7706421613693237,
        "no_speech_prob": 0.00015118172450456768
    },
    {
        "id": 38,
        "seek": 23360,
        "start": 233.60000610351562,
        "end": 239.1199951171875,
        "text": " versa. When you go into the LLAMA2 paper as well I can show you that when you search token you're",
        "tokens": [
            50364,
            25650,
            13,
            1133,
            291,
            352,
            666,
            264,
            441,
            43,
            38136,
            17,
            3035,
            382,
            731,
            286,
            393,
            855,
            291,
            300,
            562,
            291,
            3164,
            14862,
            291,
            434,
            50640
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1951778084039688,
        "compression_ratio": 1.6586207151412964,
        "no_speech_prob": 0.0009399314294569194
    },
    {
        "id": 39,
        "seek": 23360,
        "start": 239.1199951171875,
        "end": 245.1199951171875,
        "text": " going to get 63 hits and that's because tokens are again pervasive so here they mention that",
        "tokens": [
            50640,
            516,
            281,
            483,
            25082,
            8664,
            293,
            300,
            311,
            570,
            22667,
            366,
            797,
            680,
            39211,
            370,
            510,
            436,
            2152,
            300,
            50940
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1951778084039688,
        "compression_ratio": 1.6586207151412964,
        "no_speech_prob": 0.0009399314294569194
    },
    {
        "id": 40,
        "seek": 23360,
        "start": 245.1199951171875,
        "end": 251.9199981689453,
        "text": " they trained on two trillion tokens of data and so on. So we're going to build our own tokenizer",
        "tokens": [
            50940,
            436,
            8895,
            322,
            732,
            18723,
            22667,
            295,
            1412,
            293,
            370,
            322,
            13,
            407,
            321,
            434,
            516,
            281,
            1322,
            527,
            1065,
            14862,
            6545,
            51280
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1951778084039688,
        "compression_ratio": 1.6586207151412964,
        "no_speech_prob": 0.0009399314294569194
    },
    {
        "id": 41,
        "seek": 23360,
        "start": 251.9199981689453,
        "end": 256.9599914550781,
        "text": " luckily the byte pair encoding algorithm is not that super complicated and we can build it from",
        "tokens": [
            51280,
            22880,
            264,
            40846,
            6119,
            43430,
            9284,
            307,
            406,
            300,
            1687,
            6179,
            293,
            321,
            393,
            1322,
            309,
            490,
            51532
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1951778084039688,
        "compression_ratio": 1.6586207151412964,
        "no_speech_prob": 0.0009399314294569194
    },
    {
        "id": 42,
        "seek": 23360,
        "start": 256.9599914550781,
        "end": 261.3599853515625,
        "text": " scratch ourselves and we'll see exactly how this works. Before we dive into code I'd like to give",
        "tokens": [
            51532,
            8459,
            4175,
            293,
            321,
            603,
            536,
            2293,
            577,
            341,
            1985,
            13,
            4546,
            321,
            9192,
            666,
            3089,
            286,
            1116,
            411,
            281,
            976,
            51752
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1951778084039688,
        "compression_ratio": 1.6586207151412964,
        "no_speech_prob": 0.0009399314294569194
    },
    {
        "id": 43,
        "seek": 26136,
        "start": 261.3599853515625,
        "end": 265.9200134277344,
        "text": " you a brief taste of some of the complexities that come from the tokenization because I just",
        "tokens": [
            50364,
            291,
            257,
            5353,
            3939,
            295,
            512,
            295,
            264,
            48705,
            300,
            808,
            490,
            264,
            14862,
            2144,
            570,
            286,
            445,
            50592
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16591353714466095,
        "compression_ratio": 1.7814815044403076,
        "no_speech_prob": 0.00038594831130467355
    },
    {
        "id": 44,
        "seek": 26136,
        "start": 265.9200134277344,
        "end": 270.3999938964844,
        "text": " want to make sure that we motivate it sufficiently for why we are doing all this and why this is so",
        "tokens": [
            50592,
            528,
            281,
            652,
            988,
            300,
            321,
            28497,
            309,
            31868,
            337,
            983,
            321,
            366,
            884,
            439,
            341,
            293,
            983,
            341,
            307,
            370,
            50816
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16591353714466095,
        "compression_ratio": 1.7814815044403076,
        "no_speech_prob": 0.00038594831130467355
    },
    {
        "id": 45,
        "seek": 26136,
        "start": 270.3999938964844,
        "end": 276.32000732421875,
        "text": " gross. So tokenization is at the heart of a lot of weirdness in large language models and I would",
        "tokens": [
            50816,
            11367,
            13,
            407,
            14862,
            2144,
            307,
            412,
            264,
            1917,
            295,
            257,
            688,
            295,
            3657,
            1287,
            294,
            2416,
            2856,
            5245,
            293,
            286,
            576,
            51112
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16591353714466095,
        "compression_ratio": 1.7814815044403076,
        "no_speech_prob": 0.00038594831130467355
    },
    {
        "id": 46,
        "seek": 26136,
        "start": 276.32000732421875,
        "end": 281.760009765625,
        "text": " advise that you do not brush it off. A lot of the issues that may look like just issues with the",
        "tokens": [
            51112,
            18312,
            300,
            291,
            360,
            406,
            5287,
            309,
            766,
            13,
            316,
            688,
            295,
            264,
            2663,
            300,
            815,
            574,
            411,
            445,
            2663,
            365,
            264,
            51384
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16591353714466095,
        "compression_ratio": 1.7814815044403076,
        "no_speech_prob": 0.00038594831130467355
    },
    {
        "id": 47,
        "seek": 26136,
        "start": 281.760009765625,
        "end": 286.4800109863281,
        "text": " neural network architecture or a lot the large language model itself are actually issues with",
        "tokens": [
            51384,
            18161,
            3209,
            9482,
            420,
            257,
            688,
            264,
            2416,
            2856,
            2316,
            2564,
            366,
            767,
            2663,
            365,
            51620
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16591353714466095,
        "compression_ratio": 1.7814815044403076,
        "no_speech_prob": 0.00038594831130467355
    },
    {
        "id": 48,
        "seek": 28648,
        "start": 286.4800109863281,
        "end": 292.79998779296875,
        "text": " the tokenization and fundamentally trace back to it. So if you've noticed any issues with large",
        "tokens": [
            50364,
            264,
            14862,
            2144,
            293,
            17879,
            13508,
            646,
            281,
            309,
            13,
            407,
            498,
            291,
            600,
            5694,
            604,
            2663,
            365,
            2416,
            50680
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2063574492931366,
        "compression_ratio": 1.7984790802001953,
        "no_speech_prob": 0.00017674437549430877
    },
    {
        "id": 49,
        "seek": 28648,
        "start": 292.79998779296875,
        "end": 297.8399963378906,
        "text": " language models can you know not able to do spelling tasks very easily that's usually due to",
        "tokens": [
            50680,
            2856,
            5245,
            393,
            291,
            458,
            406,
            1075,
            281,
            360,
            22254,
            9608,
            588,
            3612,
            300,
            311,
            2673,
            3462,
            281,
            50932
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2063574492931366,
        "compression_ratio": 1.7984790802001953,
        "no_speech_prob": 0.00017674437549430877
    },
    {
        "id": 50,
        "seek": 28648,
        "start": 297.8399963378906,
        "end": 303.1199951171875,
        "text": " tokenization. Simple string processing can be difficult for the large language model to perform",
        "tokens": [
            50932,
            14862,
            2144,
            13,
            21532,
            6798,
            9007,
            393,
            312,
            2252,
            337,
            264,
            2416,
            2856,
            2316,
            281,
            2042,
            51196
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2063574492931366,
        "compression_ratio": 1.7984790802001953,
        "no_speech_prob": 0.00017674437549430877
    },
    {
        "id": 51,
        "seek": 28648,
        "start": 303.1199951171875,
        "end": 309.44000244140625,
        "text": " natively. Non-english languages can work much worse and to a large extent this is due to tokenization.",
        "tokens": [
            51196,
            8470,
            356,
            13,
            8774,
            12,
            1501,
            1933,
            8650,
            393,
            589,
            709,
            5324,
            293,
            281,
            257,
            2416,
            8396,
            341,
            307,
            3462,
            281,
            14862,
            2144,
            13,
            51512
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2063574492931366,
        "compression_ratio": 1.7984790802001953,
        "no_speech_prob": 0.00017674437549430877
    },
    {
        "id": 52,
        "seek": 28648,
        "start": 310.239990234375,
        "end": 315.44000244140625,
        "text": " Sometimes LLMs are bad at simple arithmetic also can trace be traced to tokenization.",
        "tokens": [
            51552,
            4803,
            441,
            43,
            26386,
            366,
            1578,
            412,
            2199,
            42973,
            611,
            393,
            13508,
            312,
            38141,
            281,
            14862,
            2144,
            13,
            51812
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2063574492931366,
        "compression_ratio": 1.7984790802001953,
        "no_speech_prob": 0.00017674437549430877
    },
    {
        "id": 53,
        "seek": 31648,
        "start": 316.6400146484375,
        "end": 322.1600036621094,
        "text": " GPT-2 specifically would have had quite a bit more issues with Python than future versions of it due",
        "tokens": [
            50372,
            26039,
            51,
            12,
            17,
            4682,
            576,
            362,
            632,
            1596,
            257,
            857,
            544,
            2663,
            365,
            15329,
            813,
            2027,
            9606,
            295,
            309,
            3462,
            50648
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20545391738414764,
        "compression_ratio": 1.5973597764968872,
        "no_speech_prob": 3.9442438719561324e-05
    },
    {
        "id": 54,
        "seek": 31648,
        "start": 322.1600036621094,
        "end": 326.4800109863281,
        "text": " to tokenization. There's a lot of other issues maybe you've seen weird warnings about a trail",
        "tokens": [
            50648,
            281,
            14862,
            2144,
            13,
            821,
            311,
            257,
            688,
            295,
            661,
            2663,
            1310,
            291,
            600,
            1612,
            3657,
            30009,
            466,
            257,
            9924,
            50864
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20545391738414764,
        "compression_ratio": 1.5973597764968872,
        "no_speech_prob": 3.9442438719561324e-05
    },
    {
        "id": 55,
        "seek": 31648,
        "start": 326.4800109863281,
        "end": 334.32000732421875,
        "text": " in whitespace this is a tokenization issue. If you had asked GPT earlier about solid gold magicarp",
        "tokens": [
            50864,
            294,
            21909,
            17940,
            341,
            307,
            257,
            14862,
            2144,
            2734,
            13,
            759,
            291,
            632,
            2351,
            26039,
            51,
            3071,
            466,
            5100,
            3821,
            5585,
            6529,
            51256
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20545391738414764,
        "compression_ratio": 1.5973597764968872,
        "no_speech_prob": 3.9442438719561324e-05
    },
    {
        "id": 56,
        "seek": 31648,
        "start": 334.32000732421875,
        "end": 339.1199951171875,
        "text": " and what it is you would see the LLM go totally crazy and it would start going off about a",
        "tokens": [
            51256,
            293,
            437,
            309,
            307,
            291,
            576,
            536,
            264,
            441,
            43,
            44,
            352,
            3879,
            3219,
            293,
            309,
            576,
            722,
            516,
            766,
            466,
            257,
            51496
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20545391738414764,
        "compression_ratio": 1.5973597764968872,
        "no_speech_prob": 3.9442438719561324e-05
    },
    {
        "id": 57,
        "seek": 31648,
        "start": 339.1199951171875,
        "end": 344.7200012207031,
        "text": " completely unrelated tangent topic. Maybe you've been told to use YAML over JSON in structured data",
        "tokens": [
            51496,
            2584,
            38967,
            27747,
            4829,
            13,
            2704,
            291,
            600,
            668,
            1907,
            281,
            764,
            398,
            2865,
            43,
            670,
            31828,
            294,
            18519,
            1412,
            51776
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20545391738414764,
        "compression_ratio": 1.5973597764968872,
        "no_speech_prob": 3.9442438719561324e-05
    },
    {
        "id": 58,
        "seek": 34472,
        "start": 344.7200012207031,
        "end": 349.2799987792969,
        "text": " all of that has to do with tokenization. So basically tokenization is at the heart of many",
        "tokens": [
            50364,
            439,
            295,
            300,
            575,
            281,
            360,
            365,
            14862,
            2144,
            13,
            407,
            1936,
            14862,
            2144,
            307,
            412,
            264,
            1917,
            295,
            867,
            50592
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2099875658750534,
        "compression_ratio": 1.7655677795410156,
        "no_speech_prob": 0.00024156604195013642
    },
    {
        "id": 59,
        "seek": 34472,
        "start": 349.2799987792969,
        "end": 355.9200134277344,
        "text": " issues. I will loop back around to these at the end of the video but for now let me just skip over",
        "tokens": [
            50592,
            2663,
            13,
            286,
            486,
            6367,
            646,
            926,
            281,
            613,
            412,
            264,
            917,
            295,
            264,
            960,
            457,
            337,
            586,
            718,
            385,
            445,
            10023,
            670,
            50924
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2099875658750534,
        "compression_ratio": 1.7655677795410156,
        "no_speech_prob": 0.00024156604195013642
    },
    {
        "id": 60,
        "seek": 34472,
        "start": 355.9200134277344,
        "end": 362.8800048828125,
        "text": " it a little bit and let's go to this web app the ticktokenizer.bristle.app. So I have it loaded",
        "tokens": [
            50924,
            309,
            257,
            707,
            857,
            293,
            718,
            311,
            352,
            281,
            341,
            3670,
            724,
            264,
            5204,
            83,
            8406,
            6545,
            13,
            1443,
            16088,
            13,
            1746,
            13,
            407,
            286,
            362,
            309,
            13210,
            51272
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2099875658750534,
        "compression_ratio": 1.7655677795410156,
        "no_speech_prob": 0.00024156604195013642
    },
    {
        "id": 61,
        "seek": 34472,
        "start": 362.8800048828125,
        "end": 367.760009765625,
        "text": " here and what I like about this web app is that tokenization is running a sort of live in your",
        "tokens": [
            51272,
            510,
            293,
            437,
            286,
            411,
            466,
            341,
            3670,
            724,
            307,
            300,
            14862,
            2144,
            307,
            2614,
            257,
            1333,
            295,
            1621,
            294,
            428,
            51516
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2099875658750534,
        "compression_ratio": 1.7655677795410156,
        "no_speech_prob": 0.00024156604195013642
    },
    {
        "id": 62,
        "seek": 34472,
        "start": 367.760009765625,
        "end": 373.9200134277344,
        "text": " browser in JavaScript. So you can just type here stuff hello world and the whole string re-tokenizes.",
        "tokens": [
            51516,
            11185,
            294,
            15778,
            13,
            407,
            291,
            393,
            445,
            2010,
            510,
            1507,
            7751,
            1002,
            293,
            264,
            1379,
            6798,
            319,
            12,
            83,
            8406,
            5660,
            13,
            51824
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2099875658750534,
        "compression_ratio": 1.7655677795410156,
        "no_speech_prob": 0.00024156604195013642
    },
    {
        "id": 63,
        "seek": 37472,
        "start": 375.20001220703125,
        "end": 381.44000244140625,
        "text": " So here what we see on the left is a string that you put in on the right we're currently using a",
        "tokens": [
            50388,
            407,
            510,
            437,
            321,
            536,
            322,
            264,
            1411,
            307,
            257,
            6798,
            300,
            291,
            829,
            294,
            322,
            264,
            558,
            321,
            434,
            4362,
            1228,
            257,
            50700
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21279296278953552,
        "compression_ratio": 1.6412556171417236,
        "no_speech_prob": 1.1843130778288469e-05
    },
    {
        "id": 64,
        "seek": 37472,
        "start": 381.44000244140625,
        "end": 388.0799865722656,
        "text": " GPT-2 tokenizer we see that this string that I pasted here is currently tokenizing into 300 tokens",
        "tokens": [
            50700,
            26039,
            51,
            12,
            17,
            14862,
            6545,
            321,
            536,
            300,
            341,
            6798,
            300,
            286,
            1791,
            292,
            510,
            307,
            4362,
            14862,
            3319,
            666,
            6641,
            22667,
            51032
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21279296278953552,
        "compression_ratio": 1.6412556171417236,
        "no_speech_prob": 1.1843130778288469e-05
    },
    {
        "id": 65,
        "seek": 37472,
        "start": 388.6400146484375,
        "end": 393.5199890136719,
        "text": " and here they are sort of shown explicitly in different colors for every single token.",
        "tokens": [
            51060,
            293,
            510,
            436,
            366,
            1333,
            295,
            4898,
            20803,
            294,
            819,
            4577,
            337,
            633,
            2167,
            14862,
            13,
            51304
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21279296278953552,
        "compression_ratio": 1.6412556171417236,
        "no_speech_prob": 1.1843130778288469e-05
    },
    {
        "id": 66,
        "seek": 37472,
        "start": 394.32000732421875,
        "end": 403.760009765625,
        "text": " So for example this word tokenization became two tokens the token 30,642 and 1,634.",
        "tokens": [
            51344,
            407,
            337,
            1365,
            341,
            1349,
            14862,
            2144,
            3062,
            732,
            22667,
            264,
            14862,
            2217,
            11,
            21,
            15628,
            293,
            502,
            11,
            21,
            12249,
            13,
            51816
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21279296278953552,
        "compression_ratio": 1.6412556171417236,
        "no_speech_prob": 1.1843130778288469e-05
    },
    {
        "id": 67,
        "seek": 40472,
        "start": 404.9599914550781,
        "end": 412.4800109863281,
        "text": " The token space is token 318. So be careful on the bottom you can show white space",
        "tokens": [
            50376,
            440,
            14862,
            1901,
            307,
            14862,
            805,
            6494,
            13,
            407,
            312,
            5026,
            322,
            264,
            2767,
            291,
            393,
            855,
            2418,
            1901,
            50752
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24281352758407593,
        "compression_ratio": 1.644736886024475,
        "no_speech_prob": 2.39234996115556e-05
    },
    {
        "id": 68,
        "seek": 40472,
        "start": 413.2799987792969,
        "end": 418.55999755859375,
        "text": " and keep in mind that there are spaces and slash and newline characters in here",
        "tokens": [
            50792,
            293,
            1066,
            294,
            1575,
            300,
            456,
            366,
            7673,
            293,
            17330,
            293,
            777,
            1889,
            4342,
            294,
            510,
            51056
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24281352758407593,
        "compression_ratio": 1.644736886024475,
        "no_speech_prob": 2.39234996115556e-05
    },
    {
        "id": 69,
        "seek": 40472,
        "start": 418.55999755859375,
        "end": 427.8399963378906,
        "text": " but you can hide them for clarity. The token space at is token 379. The token space the",
        "tokens": [
            51056,
            457,
            291,
            393,
            6479,
            552,
            337,
            16992,
            13,
            440,
            14862,
            1901,
            412,
            307,
            14862,
            13435,
            24,
            13,
            440,
            14862,
            1901,
            264,
            51520
        ],
        "temperature": 0.0,
        "avg_logprob": -0.24281352758407593,
        "compression_ratio": 1.644736886024475,
        "no_speech_prob": 2.39234996115556e-05
    },
    {
        "id": 70,
        "seek": 42784,
        "start": 428.4800109863281,
        "end": 435.0400085449219,
        "text": " is 262 etc. So you notice here that the space is part of that token chunk.",
        "tokens": [
            50396,
            307,
            7551,
            17,
            5183,
            13,
            407,
            291,
            3449,
            510,
            300,
            264,
            1901,
            307,
            644,
            295,
            300,
            14862,
            16635,
            13,
            50724
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20982711017131805,
        "compression_ratio": 1.5114942789077759,
        "no_speech_prob": 0.0016484770458191633
    },
    {
        "id": 71,
        "seek": 42784,
        "start": 436.8800048828125,
        "end": 442.0799865722656,
        "text": " Now so this is kind of like how our English sentence broke up and that seems all well and",
        "tokens": [
            50816,
            823,
            370,
            341,
            307,
            733,
            295,
            411,
            577,
            527,
            3669,
            8174,
            6902,
            493,
            293,
            300,
            2544,
            439,
            731,
            293,
            51076
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20982711017131805,
        "compression_ratio": 1.5114942789077759,
        "no_speech_prob": 0.0016484770458191633
    },
    {
        "id": 72,
        "seek": 42784,
        "start": 442.0799865722656,
        "end": 452.4800109863281,
        "text": " good. Now here I put in some arithmetic so we see that the token 127 plus and then token 6 space 6",
        "tokens": [
            51076,
            665,
            13,
            823,
            510,
            286,
            829,
            294,
            512,
            42973,
            370,
            321,
            536,
            300,
            264,
            14862,
            47561,
            1804,
            293,
            550,
            14862,
            1386,
            1901,
            1386,
            51596
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20982711017131805,
        "compression_ratio": 1.5114942789077759,
        "no_speech_prob": 0.0016484770458191633
    },
    {
        "id": 73,
        "seek": 45248,
        "start": 452.55999755859375,
        "end": 458.0,
        "text": " followed by 77. So what's happening here is that 127 is feeding in as a single token into the",
        "tokens": [
            50368,
            6263,
            538,
            25546,
            13,
            407,
            437,
            311,
            2737,
            510,
            307,
            300,
            47561,
            307,
            12919,
            294,
            382,
            257,
            2167,
            14862,
            666,
            264,
            50640
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2050095647573471,
        "compression_ratio": 1.6651785373687744,
        "no_speech_prob": 0.004829676356166601
    },
    {
        "id": 74,
        "seek": 45248,
        "start": 458.0,
        "end": 464.9599914550781,
        "text": " large language model but the number 677 will actually feed in as two separate tokens.",
        "tokens": [
            50640,
            2416,
            2856,
            2316,
            457,
            264,
            1230,
            1386,
            17512,
            486,
            767,
            3154,
            294,
            382,
            732,
            4994,
            22667,
            13,
            50988
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2050095647573471,
        "compression_ratio": 1.6651785373687744,
        "no_speech_prob": 0.004829676356166601
    },
    {
        "id": 75,
        "seek": 45248,
        "start": 465.8399963378906,
        "end": 472.239990234375,
        "text": " And so the large language model has to sort of take account of that and process it correctly",
        "tokens": [
            51032,
            400,
            370,
            264,
            2416,
            2856,
            2316,
            575,
            281,
            1333,
            295,
            747,
            2696,
            295,
            300,
            293,
            1399,
            309,
            8944,
            51352
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2050095647573471,
        "compression_ratio": 1.6651785373687744,
        "no_speech_prob": 0.004829676356166601
    },
    {
        "id": 76,
        "seek": 45248,
        "start": 472.239990234375,
        "end": 478.3999938964844,
        "text": " in its network and see here 804 will be broken up into two tokens and it's all completely arbitrary.",
        "tokens": [
            51352,
            294,
            1080,
            3209,
            293,
            536,
            510,
            4688,
            19,
            486,
            312,
            5463,
            493,
            666,
            732,
            22667,
            293,
            309,
            311,
            439,
            2584,
            23211,
            13,
            51660
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2050095647573471,
        "compression_ratio": 1.6651785373687744,
        "no_speech_prob": 0.004829676356166601
    },
    {
        "id": 77,
        "seek": 47840,
        "start": 478.9599914550781,
        "end": 483.3599853515625,
        "text": " And here I have another example of four digit numbers and they break up in a way that they",
        "tokens": [
            50392,
            400,
            510,
            286,
            362,
            1071,
            1365,
            295,
            1451,
            14293,
            3547,
            293,
            436,
            1821,
            493,
            294,
            257,
            636,
            300,
            436,
            50612
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1875,
        "compression_ratio": 1.8547717332839966,
        "no_speech_prob": 0.0002531559148337692
    },
    {
        "id": 78,
        "seek": 47840,
        "start": 483.3599853515625,
        "end": 488.55999755859375,
        "text": " break up and it's totally arbitrary. Sometimes you have multiple digits single token sometimes",
        "tokens": [
            50612,
            1821,
            493,
            293,
            309,
            311,
            3879,
            23211,
            13,
            4803,
            291,
            362,
            3866,
            27011,
            2167,
            14862,
            2171,
            50872
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1875,
        "compression_ratio": 1.8547717332839966,
        "no_speech_prob": 0.0002531559148337692
    },
    {
        "id": 79,
        "seek": 47840,
        "start": 488.55999755859375,
        "end": 492.8800048828125,
        "text": " you have individual digits as many tokens and it's all kind of pretty arbitrary and",
        "tokens": [
            50872,
            291,
            362,
            2609,
            27011,
            382,
            867,
            22667,
            293,
            309,
            311,
            439,
            733,
            295,
            1238,
            23211,
            293,
            51088
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1875,
        "compression_ratio": 1.8547717332839966,
        "no_speech_prob": 0.0002531559148337692
    },
    {
        "id": 80,
        "seek": 47840,
        "start": 492.8800048828125,
        "end": 500.55999755859375,
        "text": " coming out of the tokenizer. Here's another example we have the string egg and you see",
        "tokens": [
            51088,
            1348,
            484,
            295,
            264,
            14862,
            6545,
            13,
            1692,
            311,
            1071,
            1365,
            321,
            362,
            264,
            6798,
            3777,
            293,
            291,
            536,
            51472
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1875,
        "compression_ratio": 1.8547717332839966,
        "no_speech_prob": 0.0002531559148337692
    },
    {
        "id": 81,
        "seek": 47840,
        "start": 500.55999755859375,
        "end": 506.7200012207031,
        "text": " here that this became two tokens but for some reason when I say I have an egg you see when",
        "tokens": [
            51472,
            510,
            300,
            341,
            3062,
            732,
            22667,
            457,
            337,
            512,
            1778,
            562,
            286,
            584,
            286,
            362,
            364,
            3777,
            291,
            536,
            562,
            51780
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1875,
        "compression_ratio": 1.8547717332839966,
        "no_speech_prob": 0.0002531559148337692
    },
    {
        "id": 82,
        "seek": 50672,
        "start": 506.7200012207031,
        "end": 513.3599853515625,
        "text": " it's a space egg it's two token it's sorry it's a single token so just egg by itself in the",
        "tokens": [
            50364,
            309,
            311,
            257,
            1901,
            3777,
            309,
            311,
            732,
            14862,
            309,
            311,
            2597,
            309,
            311,
            257,
            2167,
            14862,
            370,
            445,
            3777,
            538,
            2564,
            294,
            264,
            50696
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2119937390089035,
        "compression_ratio": 1.8762376308441162,
        "no_speech_prob": 1.8058495697914623e-05
    },
    {
        "id": 83,
        "seek": 50672,
        "start": 513.3599853515625,
        "end": 520.1599731445312,
        "text": " beginning of a sentence is two tokens but here as a space egg is suddenly a single token for",
        "tokens": [
            50696,
            2863,
            295,
            257,
            8174,
            307,
            732,
            22667,
            457,
            510,
            382,
            257,
            1901,
            3777,
            307,
            5800,
            257,
            2167,
            14862,
            337,
            51036
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2119937390089035,
        "compression_ratio": 1.8762376308441162,
        "no_speech_prob": 1.8058495697914623e-05
    },
    {
        "id": 84,
        "seek": 50672,
        "start": 520.1599731445312,
        "end": 526.8800048828125,
        "text": " the exact same string. Here lowercase egg turns out to be a single token and in particular notice",
        "tokens": [
            51036,
            264,
            1900,
            912,
            6798,
            13,
            1692,
            3126,
            9765,
            3777,
            4523,
            484,
            281,
            312,
            257,
            2167,
            14862,
            293,
            294,
            1729,
            3449,
            51372
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2119937390089035,
        "compression_ratio": 1.8762376308441162,
        "no_speech_prob": 1.8058495697914623e-05
    },
    {
        "id": 85,
        "seek": 50672,
        "start": 526.8800048828125,
        "end": 532.0,
        "text": " that the color is different so this is a different token so this is case sensitive and of course",
        "tokens": [
            51372,
            300,
            264,
            2017,
            307,
            819,
            370,
            341,
            307,
            257,
            819,
            14862,
            370,
            341,
            307,
            1389,
            9477,
            293,
            295,
            1164,
            51628
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2119937390089035,
        "compression_ratio": 1.8762376308441162,
        "no_speech_prob": 1.8058495697914623e-05
    },
    {
        "id": 86,
        "seek": 53200,
        "start": 532.0,
        "end": 538.5599975585938,
        "text": " capital egg would also be different tokens and again this would be two tokens arbitrarily.",
        "tokens": [
            50364,
            4238,
            3777,
            576,
            611,
            312,
            819,
            22667,
            293,
            797,
            341,
            576,
            312,
            732,
            22667,
            19071,
            3289,
            13,
            50692
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20965908467769623,
        "compression_ratio": 1.9011857509613037,
        "no_speech_prob": 0.00010720852151280269
    },
    {
        "id": 87,
        "seek": 53200,
        "start": 539.1199951171875,
        "end": 543.8400268554688,
        "text": " So for the same concept egg depending on if it's in the beginning of a sentence at the end of a",
        "tokens": [
            50720,
            407,
            337,
            264,
            912,
            3410,
            3777,
            5413,
            322,
            498,
            309,
            311,
            294,
            264,
            2863,
            295,
            257,
            8174,
            412,
            264,
            917,
            295,
            257,
            50956
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20965908467769623,
        "compression_ratio": 1.9011857509613037,
        "no_speech_prob": 0.00010720852151280269
    },
    {
        "id": 88,
        "seek": 53200,
        "start": 543.8400268554688,
        "end": 549.4400024414062,
        "text": " sentence lowercase uppercase or mixed all this will be basically very different tokens and different",
        "tokens": [
            50956,
            8174,
            3126,
            9765,
            11775,
            2869,
            651,
            420,
            7467,
            439,
            341,
            486,
            312,
            1936,
            588,
            819,
            22667,
            293,
            819,
            51236
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20965908467769623,
        "compression_ratio": 1.9011857509613037,
        "no_speech_prob": 0.00010720852151280269
    },
    {
        "id": 89,
        "seek": 53200,
        "start": 549.4400024414062,
        "end": 554.0,
        "text": " ids and the language model has to learn from raw data from all the internet text that it's going to",
        "tokens": [
            51236,
            220,
            3742,
            293,
            264,
            2856,
            2316,
            575,
            281,
            1466,
            490,
            8936,
            1412,
            490,
            439,
            264,
            4705,
            2487,
            300,
            309,
            311,
            516,
            281,
            51464
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20965908467769623,
        "compression_ratio": 1.9011857509613037,
        "no_speech_prob": 0.00010720852151280269
    },
    {
        "id": 90,
        "seek": 53200,
        "start": 554.0,
        "end": 558.5599975585938,
        "text": " be training on that these are actually all the exact same concept and it has to sort of group",
        "tokens": [
            51464,
            312,
            3097,
            322,
            300,
            613,
            366,
            767,
            439,
            264,
            1900,
            912,
            3410,
            293,
            309,
            575,
            281,
            1333,
            295,
            1594,
            51692
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20965908467769623,
        "compression_ratio": 1.9011857509613037,
        "no_speech_prob": 0.00010720852151280269
    },
    {
        "id": 91,
        "seek": 55856,
        "start": 558.5599975585938,
        "end": 563.2000122070312,
        "text": " them in the parameters of the neural network and understand just based on the data patterns",
        "tokens": [
            50364,
            552,
            294,
            264,
            9834,
            295,
            264,
            18161,
            3209,
            293,
            1223,
            445,
            2361,
            322,
            264,
            1412,
            8294,
            50596
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2817307412624359,
        "compression_ratio": 1.6238532066345215,
        "no_speech_prob": 0.009705796837806702
    },
    {
        "id": 92,
        "seek": 55856,
        "start": 563.2000122070312,
        "end": 568.4000244140625,
        "text": " that these are all very similar but maybe not almost exactly similar but very very similar.",
        "tokens": [
            50596,
            300,
            613,
            366,
            439,
            588,
            2531,
            457,
            1310,
            406,
            1920,
            2293,
            2531,
            457,
            588,
            588,
            2531,
            13,
            50856
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2817307412624359,
        "compression_ratio": 1.6238532066345215,
        "no_speech_prob": 0.009705796837806702
    },
    {
        "id": 93,
        "seek": 55856,
        "start": 570.6400146484375,
        "end": 576.719970703125,
        "text": " After the demonstration here I have an introduction from OpenAI's ChaChabitty",
        "tokens": [
            50968,
            2381,
            264,
            16520,
            510,
            286,
            362,
            364,
            9339,
            490,
            7238,
            48698,
            311,
            12374,
            6546,
            455,
            10016,
            51272
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2817307412624359,
        "compression_ratio": 1.6238532066345215,
        "no_speech_prob": 0.009705796837806702
    },
    {
        "id": 94,
        "seek": 55856,
        "start": 576.719970703125,
        "end": 586.7999877929688,
        "text": " in Korean. So this is in Korean and the reason I put this here is because you'll notice that",
        "tokens": [
            51272,
            294,
            6933,
            13,
            407,
            341,
            307,
            294,
            6933,
            293,
            264,
            1778,
            286,
            829,
            341,
            510,
            307,
            570,
            291,
            603,
            3449,
            300,
            51776
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2817307412624359,
        "compression_ratio": 1.6238532066345215,
        "no_speech_prob": 0.009705796837806702
    },
    {
        "id": 95,
        "seek": 58856,
        "start": 589.3599853515625,
        "end": 594.7999877929688,
        "text": " non-English languages work slightly worse in ChaChabitty. Part of this is because of course",
        "tokens": [
            50404,
            2107,
            12,
            31254,
            1933,
            8650,
            589,
            4748,
            5324,
            294,
            12374,
            6546,
            455,
            10016,
            13,
            4100,
            295,
            341,
            307,
            570,
            295,
            1164,
            50676
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16672682762145996,
        "compression_ratio": 1.8658536672592163,
        "no_speech_prob": 0.00010554454638622701
    },
    {
        "id": 96,
        "seek": 58856,
        "start": 594.7999877929688,
        "end": 599.280029296875,
        "text": " the training data set for ChaChabitty is much larger for English than for everything else",
        "tokens": [
            50676,
            264,
            3097,
            1412,
            992,
            337,
            12374,
            6546,
            455,
            10016,
            307,
            709,
            4833,
            337,
            3669,
            813,
            337,
            1203,
            1646,
            50900
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16672682762145996,
        "compression_ratio": 1.8658536672592163,
        "no_speech_prob": 0.00010554454638622701
    },
    {
        "id": 97,
        "seek": 58856,
        "start": 599.280029296875,
        "end": 604.3200073242188,
        "text": " but the same is true not just for the large language model itself but also for the tokenizer.",
        "tokens": [
            50900,
            457,
            264,
            912,
            307,
            2074,
            406,
            445,
            337,
            264,
            2416,
            2856,
            2316,
            2564,
            457,
            611,
            337,
            264,
            14862,
            6545,
            13,
            51152
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16672682762145996,
        "compression_ratio": 1.8658536672592163,
        "no_speech_prob": 0.00010554454638622701
    },
    {
        "id": 98,
        "seek": 58856,
        "start": 604.3200073242188,
        "end": 608.0,
        "text": " So when we train the tokenizer we're going to see that there's a training set as well",
        "tokens": [
            51152,
            407,
            562,
            321,
            3847,
            264,
            14862,
            6545,
            321,
            434,
            516,
            281,
            536,
            300,
            456,
            311,
            257,
            3097,
            992,
            382,
            731,
            51336
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16672682762145996,
        "compression_ratio": 1.8658536672592163,
        "no_speech_prob": 0.00010554454638622701
    },
    {
        "id": 99,
        "seek": 58856,
        "start": 608.0,
        "end": 613.0399780273438,
        "text": " and there's a lot more English than non-English and what ends up happening is that we're going to",
        "tokens": [
            51336,
            293,
            456,
            311,
            257,
            688,
            544,
            3669,
            813,
            2107,
            12,
            31254,
            1933,
            293,
            437,
            5314,
            493,
            2737,
            307,
            300,
            321,
            434,
            516,
            281,
            51588
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16672682762145996,
        "compression_ratio": 1.8658536672592163,
        "no_speech_prob": 0.00010554454638622701
    },
    {
        "id": 100,
        "seek": 61304,
        "start": 613.1199951171875,
        "end": 620.4000244140625,
        "text": " have a lot more longer tokens for English. So how do I put this? If you have a single sentence",
        "tokens": [
            50368,
            362,
            257,
            688,
            544,
            2854,
            22667,
            337,
            3669,
            13,
            407,
            577,
            360,
            286,
            829,
            341,
            30,
            759,
            291,
            362,
            257,
            2167,
            8174,
            50732
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1782134473323822,
        "compression_ratio": 1.7658730745315552,
        "no_speech_prob": 0.0028894501738250256
    },
    {
        "id": 101,
        "seek": 61304,
        "start": 620.4000244140625,
        "end": 625.1199951171875,
        "text": " in English and you tokenize it you might see that it's 10 tokens or something like that",
        "tokens": [
            50732,
            294,
            3669,
            293,
            291,
            14862,
            1125,
            309,
            291,
            1062,
            536,
            300,
            309,
            311,
            1266,
            22667,
            420,
            746,
            411,
            300,
            50968
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1782134473323822,
        "compression_ratio": 1.7658730745315552,
        "no_speech_prob": 0.0028894501738250256
    },
    {
        "id": 102,
        "seek": 61304,
        "start": 625.1199951171875,
        "end": 628.6400146484375,
        "text": " but if you translate that sentence into say Korean or Japanese or something else",
        "tokens": [
            50968,
            457,
            498,
            291,
            13799,
            300,
            8174,
            666,
            584,
            6933,
            420,
            5433,
            420,
            746,
            1646,
            51144
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1782134473323822,
        "compression_ratio": 1.7658730745315552,
        "no_speech_prob": 0.0028894501738250256
    },
    {
        "id": 103,
        "seek": 61304,
        "start": 629.2000122070312,
        "end": 634.3200073242188,
        "text": " you'll typically see that number of tokens used is much larger and that's because the chunks here",
        "tokens": [
            51172,
            291,
            603,
            5850,
            536,
            300,
            1230,
            295,
            22667,
            1143,
            307,
            709,
            4833,
            293,
            300,
            311,
            570,
            264,
            24004,
            510,
            51428
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1782134473323822,
        "compression_ratio": 1.7658730745315552,
        "no_speech_prob": 0.0028894501738250256
    },
    {
        "id": 104,
        "seek": 61304,
        "start": 634.3200073242188,
        "end": 639.5999755859375,
        "text": " are a lot more broken up. So we're using a lot more tokens for the exact same thing",
        "tokens": [
            51428,
            366,
            257,
            688,
            544,
            5463,
            493,
            13,
            407,
            321,
            434,
            1228,
            257,
            688,
            544,
            22667,
            337,
            264,
            1900,
            912,
            551,
            51692
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1782134473323822,
        "compression_ratio": 1.7658730745315552,
        "no_speech_prob": 0.0028894501738250256
    },
    {
        "id": 105,
        "seek": 63960,
        "start": 640.4000244140625,
        "end": 644.9600219726562,
        "text": " and what this does is it bloats up the sequence length of all the documents.",
        "tokens": [
            50404,
            293,
            437,
            341,
            775,
            307,
            309,
            1749,
            1720,
            493,
            264,
            8310,
            4641,
            295,
            439,
            264,
            8512,
            13,
            50632
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1954752653837204,
        "compression_ratio": 1.86776864528656,
        "no_speech_prob": 0.00013765435141976923
    },
    {
        "id": 106,
        "seek": 63960,
        "start": 644.9600219726562,
        "end": 649.5999755859375,
        "text": " So you're using up more tokens and then in the attention of the transformer when these tokens",
        "tokens": [
            50632,
            407,
            291,
            434,
            1228,
            493,
            544,
            22667,
            293,
            550,
            294,
            264,
            3202,
            295,
            264,
            31782,
            562,
            613,
            22667,
            50864
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1954752653837204,
        "compression_ratio": 1.86776864528656,
        "no_speech_prob": 0.00013765435141976923
    },
    {
        "id": 107,
        "seek": 63960,
        "start": 649.5999755859375,
        "end": 655.4400024414062,
        "text": " try to attend each other you are running out of context in the maximum context length of that",
        "tokens": [
            50864,
            853,
            281,
            6888,
            1184,
            661,
            291,
            366,
            2614,
            484,
            295,
            4319,
            294,
            264,
            6674,
            4319,
            4641,
            295,
            300,
            51156
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1954752653837204,
        "compression_ratio": 1.86776864528656,
        "no_speech_prob": 0.00013765435141976923
    },
    {
        "id": 108,
        "seek": 63960,
        "start": 655.4400024414062,
        "end": 662.3200073242188,
        "text": " transformer and so basically all the non-English text is stretched out from the perspective of",
        "tokens": [
            51156,
            31782,
            293,
            370,
            1936,
            439,
            264,
            2107,
            12,
            31254,
            1933,
            2487,
            307,
            23563,
            484,
            490,
            264,
            4585,
            295,
            51500
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1954752653837204,
        "compression_ratio": 1.86776864528656,
        "no_speech_prob": 0.00013765435141976923
    },
    {
        "id": 109,
        "seek": 63960,
        "start": 662.3200073242188,
        "end": 667.6799926757812,
        "text": " the transformer and this just has to do with the training set used for the tokenizer and the",
        "tokens": [
            51500,
            264,
            31782,
            293,
            341,
            445,
            575,
            281,
            360,
            365,
            264,
            3097,
            992,
            1143,
            337,
            264,
            14862,
            6545,
            293,
            264,
            51768
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1954752653837204,
        "compression_ratio": 1.86776864528656,
        "no_speech_prob": 0.00013765435141976923
    },
    {
        "id": 110,
        "seek": 66768,
        "start": 667.6799926757812,
        "end": 673.5999755859375,
        "text": " tokenization itself. So it will create a lot bigger tokens and a lot larger groups in English",
        "tokens": [
            50364,
            14862,
            2144,
            2564,
            13,
            407,
            309,
            486,
            1884,
            257,
            688,
            3801,
            22667,
            293,
            257,
            688,
            4833,
            3935,
            294,
            3669,
            50660
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1726744920015335,
        "compression_ratio": 1.6653543710708618,
        "no_speech_prob": 1.7502912669442594e-05
    },
    {
        "id": 111,
        "seek": 66768,
        "start": 673.5999755859375,
        "end": 677.280029296875,
        "text": " and it will have a lot of little boundaries for all the other non-English text.",
        "tokens": [
            50660,
            293,
            309,
            486,
            362,
            257,
            688,
            295,
            707,
            13180,
            337,
            439,
            264,
            661,
            2107,
            12,
            31254,
            1933,
            2487,
            13,
            50844
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1726744920015335,
        "compression_ratio": 1.6653543710708618,
        "no_speech_prob": 1.7502912669442594e-05
    },
    {
        "id": 112,
        "seek": 66768,
        "start": 679.3599853515625,
        "end": 683.1199951171875,
        "text": " So if we translated this into English it would be significantly fewer tokens.",
        "tokens": [
            50948,
            407,
            498,
            321,
            16805,
            341,
            666,
            3669,
            309,
            576,
            312,
            10591,
            13366,
            22667,
            13,
            51136
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1726744920015335,
        "compression_ratio": 1.6653543710708618,
        "no_speech_prob": 1.7502912669442594e-05
    },
    {
        "id": 113,
        "seek": 66768,
        "start": 684.239990234375,
        "end": 688.3200073242188,
        "text": " The final example I have here is a little snippet of Python for doing fizzbuzz",
        "tokens": [
            51192,
            440,
            2572,
            1365,
            286,
            362,
            510,
            307,
            257,
            707,
            35623,
            302,
            295,
            15329,
            337,
            884,
            283,
            8072,
            65,
            16740,
            51396
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1726744920015335,
        "compression_ratio": 1.6653543710708618,
        "no_speech_prob": 1.7502912669442594e-05
    },
    {
        "id": 114,
        "seek": 66768,
        "start": 689.1199951171875,
        "end": 695.0399780273438,
        "text": " and what I'd like you to notice is look all these individual spaces are all separate tokens.",
        "tokens": [
            51436,
            293,
            437,
            286,
            1116,
            411,
            291,
            281,
            3449,
            307,
            574,
            439,
            613,
            2609,
            7673,
            366,
            439,
            4994,
            22667,
            13,
            51732
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1726744920015335,
        "compression_ratio": 1.6653543710708618,
        "no_speech_prob": 1.7502912669442594e-05
    },
    {
        "id": 115,
        "seek": 69504,
        "start": 695.5999755859375,
        "end": 705.4400024414062,
        "text": " They are token 220. So 220, 220, 220, 220 and then space if is a single token and so what's going on",
        "tokens": [
            50392,
            814,
            366,
            14862,
            29387,
            13,
            407,
            29387,
            11,
            29387,
            11,
            29387,
            11,
            29387,
            293,
            550,
            1901,
            498,
            307,
            257,
            2167,
            14862,
            293,
            370,
            437,
            311,
            516,
            322,
            50884
        ],
        "temperature": 0.0,
        "avg_logprob": -0.205285906791687,
        "compression_ratio": 1.7162162065505981,
        "no_speech_prob": 5.3075043979333714e-05
    },
    {
        "id": 116,
        "seek": 69504,
        "start": 705.4400024414062,
        "end": 711.280029296875,
        "text": " here is that when the transformer is going to consume or try to create this text it needs to",
        "tokens": [
            50884,
            510,
            307,
            300,
            562,
            264,
            31782,
            307,
            516,
            281,
            14732,
            420,
            853,
            281,
            1884,
            341,
            2487,
            309,
            2203,
            281,
            51176
        ],
        "temperature": 0.0,
        "avg_logprob": -0.205285906791687,
        "compression_ratio": 1.7162162065505981,
        "no_speech_prob": 5.3075043979333714e-05
    },
    {
        "id": 117,
        "seek": 69504,
        "start": 712.1599731445312,
        "end": 717.5999755859375,
        "text": " handle all these spaces individually. They all feed in one by one into the entire transformer",
        "tokens": [
            51220,
            4813,
            439,
            613,
            7673,
            16652,
            13,
            814,
            439,
            3154,
            294,
            472,
            538,
            472,
            666,
            264,
            2302,
            31782,
            51492
        ],
        "temperature": 0.0,
        "avg_logprob": -0.205285906791687,
        "compression_ratio": 1.7162162065505981,
        "no_speech_prob": 5.3075043979333714e-05
    },
    {
        "id": 118,
        "seek": 69504,
        "start": 717.5999755859375,
        "end": 724.0,
        "text": " in the sequence and so this is being extremely wasteful tokenizing it in this way and so as a",
        "tokens": [
            51492,
            294,
            264,
            8310,
            293,
            370,
            341,
            307,
            885,
            4664,
            5964,
            906,
            14862,
            3319,
            309,
            294,
            341,
            636,
            293,
            370,
            382,
            257,
            51812
        ],
        "temperature": 0.0,
        "avg_logprob": -0.205285906791687,
        "compression_ratio": 1.7162162065505981,
        "no_speech_prob": 5.3075043979333714e-05
    },
    {
        "id": 119,
        "seek": 72400,
        "start": 724.0,
        "end": 728.9600219726562,
        "text": " result of that GPT-2 is not very good with Python and it's not anything to do with coding or the",
        "tokens": [
            50364,
            1874,
            295,
            300,
            26039,
            51,
            12,
            17,
            307,
            406,
            588,
            665,
            365,
            15329,
            293,
            309,
            311,
            406,
            1340,
            281,
            360,
            365,
            17720,
            420,
            264,
            50612
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18311215937137604,
        "compression_ratio": 1.805031418800354,
        "no_speech_prob": 1.8342852854402736e-05
    },
    {
        "id": 120,
        "seek": 72400,
        "start": 728.9600219726562,
        "end": 734.1599731445312,
        "text": " language model itself it's just that if you use a lot of indentation using space in Python like",
        "tokens": [
            50612,
            2856,
            2316,
            2564,
            309,
            311,
            445,
            300,
            498,
            291,
            764,
            257,
            688,
            295,
            44494,
            399,
            1228,
            1901,
            294,
            15329,
            411,
            50872
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18311215937137604,
        "compression_ratio": 1.805031418800354,
        "no_speech_prob": 1.8342852854402736e-05
    },
    {
        "id": 121,
        "seek": 72400,
        "start": 734.1599731445312,
        "end": 739.52001953125,
        "text": " we usually do you just end up bloating out all the text and it's separated across way too much",
        "tokens": [
            50872,
            321,
            2673,
            360,
            291,
            445,
            917,
            493,
            1749,
            990,
            484,
            439,
            264,
            2487,
            293,
            309,
            311,
            12005,
            2108,
            636,
            886,
            709,
            51140
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18311215937137604,
        "compression_ratio": 1.805031418800354,
        "no_speech_prob": 1.8342852854402736e-05
    },
    {
        "id": 122,
        "seek": 72400,
        "start": 739.52001953125,
        "end": 744.0800170898438,
        "text": " of the sequence and we are running out of the context length in the sequence. That's roughly",
        "tokens": [
            51140,
            295,
            264,
            8310,
            293,
            321,
            366,
            2614,
            484,
            295,
            264,
            4319,
            4641,
            294,
            264,
            8310,
            13,
            663,
            311,
            9810,
            51368
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18311215937137604,
        "compression_ratio": 1.805031418800354,
        "no_speech_prob": 1.8342852854402736e-05
    },
    {
        "id": 123,
        "seek": 72400,
        "start": 744.0800170898438,
        "end": 747.9199829101562,
        "text": " speaking what's what's happening. We're being way too wasteful. We're taking up way too much token",
        "tokens": [
            51368,
            4124,
            437,
            311,
            437,
            311,
            2737,
            13,
            492,
            434,
            885,
            636,
            886,
            5964,
            906,
            13,
            492,
            434,
            1940,
            493,
            636,
            886,
            709,
            14862,
            51560
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18311215937137604,
        "compression_ratio": 1.805031418800354,
        "no_speech_prob": 1.8342852854402736e-05
    },
    {
        "id": 124,
        "seek": 72400,
        "start": 747.9199829101562,
        "end": 752.9600219726562,
        "text": " space. Now we can also scroll up here and we can change the tokenizer. So note here that GPT-2",
        "tokens": [
            51560,
            1901,
            13,
            823,
            321,
            393,
            611,
            11369,
            493,
            510,
            293,
            321,
            393,
            1319,
            264,
            14862,
            6545,
            13,
            407,
            3637,
            510,
            300,
            26039,
            51,
            12,
            17,
            51812
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18311215937137604,
        "compression_ratio": 1.805031418800354,
        "no_speech_prob": 1.8342852854402736e-05
    },
    {
        "id": 125,
        "seek": 75296,
        "start": 752.9600219726562,
        "end": 759.52001953125,
        "text": " tokenizer creates a token count of 300 for this string here. We can change it to CL 100k base",
        "tokens": [
            50364,
            14862,
            6545,
            7829,
            257,
            14862,
            1207,
            295,
            6641,
            337,
            341,
            6798,
            510,
            13,
            492,
            393,
            1319,
            309,
            281,
            12855,
            2319,
            74,
            3096,
            50692
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1771012544631958,
        "compression_ratio": 1.8122066259384155,
        "no_speech_prob": 0.0001334197586402297
    },
    {
        "id": 126,
        "seek": 75296,
        "start": 759.52001953125,
        "end": 765.5999755859375,
        "text": " which is the GPT-4 tokenizer and we see that the token count drops to 185. So for the exact same",
        "tokens": [
            50692,
            597,
            307,
            264,
            26039,
            51,
            12,
            19,
            14862,
            6545,
            293,
            321,
            536,
            300,
            264,
            14862,
            1207,
            11438,
            281,
            2443,
            20,
            13,
            407,
            337,
            264,
            1900,
            912,
            50996
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1771012544631958,
        "compression_ratio": 1.8122066259384155,
        "no_speech_prob": 0.0001334197586402297
    },
    {
        "id": 127,
        "seek": 75296,
        "start": 765.5999755859375,
        "end": 771.760009765625,
        "text": " string we are now roughly halving the number of tokens and roughly speaking this is because the",
        "tokens": [
            50996,
            6798,
            321,
            366,
            586,
            9810,
            7523,
            798,
            264,
            1230,
            295,
            22667,
            293,
            9810,
            4124,
            341,
            307,
            570,
            264,
            51304
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1771012544631958,
        "compression_ratio": 1.8122066259384155,
        "no_speech_prob": 0.0001334197586402297
    },
    {
        "id": 128,
        "seek": 75296,
        "start": 771.760009765625,
        "end": 777.6799926757812,
        "text": " number of tokens in the GPT-4 tokenizer is roughly double that of the number of tokens in the GPT-2",
        "tokens": [
            51304,
            1230,
            295,
            22667,
            294,
            264,
            26039,
            51,
            12,
            19,
            14862,
            6545,
            307,
            9810,
            3834,
            300,
            295,
            264,
            1230,
            295,
            22667,
            294,
            264,
            26039,
            51,
            12,
            17,
            51600
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1771012544631958,
        "compression_ratio": 1.8122066259384155,
        "no_speech_prob": 0.0001334197586402297
    },
    {
        "id": 129,
        "seek": 77768,
        "start": 777.6799926757812,
        "end": 782.9600219726562,
        "text": " tokenizer. So we went from roughly 50k to roughly 100k. Now you can imagine that this is a good",
        "tokens": [
            50364,
            14862,
            6545,
            13,
            407,
            321,
            1437,
            490,
            9810,
            2625,
            74,
            281,
            9810,
            2319,
            74,
            13,
            823,
            291,
            393,
            3811,
            300,
            341,
            307,
            257,
            665,
            50628
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17045317590236664,
        "compression_ratio": 1.6855895519256592,
        "no_speech_prob": 0.0001911040599225089
    },
    {
        "id": 130,
        "seek": 77768,
        "start": 782.9600219726562,
        "end": 791.760009765625,
        "text": " thing because the same text is now squished into half as many tokens so this is a lot denser input",
        "tokens": [
            50628,
            551,
            570,
            264,
            912,
            2487,
            307,
            586,
            2339,
            4729,
            666,
            1922,
            382,
            867,
            22667,
            370,
            341,
            307,
            257,
            688,
            24505,
            260,
            4846,
            51068
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17045317590236664,
        "compression_ratio": 1.6855895519256592,
        "no_speech_prob": 0.0001911040599225089
    },
    {
        "id": 131,
        "seek": 77768,
        "start": 791.760009765625,
        "end": 797.6799926757812,
        "text": " to the transformer and in the transformer every single token has a finite number of tokens before",
        "tokens": [
            51068,
            281,
            264,
            31782,
            293,
            294,
            264,
            31782,
            633,
            2167,
            14862,
            575,
            257,
            19362,
            1230,
            295,
            22667,
            949,
            51364
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17045317590236664,
        "compression_ratio": 1.6855895519256592,
        "no_speech_prob": 0.0001911040599225089
    },
    {
        "id": 132,
        "seek": 77768,
        "start": 797.6799926757812,
        "end": 802.0,
        "text": " it that it's going to pay attention to and so what this is doing is we're roughly able to see",
        "tokens": [
            51364,
            309,
            300,
            309,
            311,
            516,
            281,
            1689,
            3202,
            281,
            293,
            370,
            437,
            341,
            307,
            884,
            307,
            321,
            434,
            9810,
            1075,
            281,
            536,
            51580
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17045317590236664,
        "compression_ratio": 1.6855895519256592,
        "no_speech_prob": 0.0001911040599225089
    },
    {
        "id": 133,
        "seek": 80200,
        "start": 802.8800048828125,
        "end": 809.52001953125,
        "text": " twice as much text as a context for what token to predict next because of this change. But of",
        "tokens": [
            50408,
            6091,
            382,
            709,
            2487,
            382,
            257,
            4319,
            337,
            437,
            14862,
            281,
            6069,
            958,
            570,
            295,
            341,
            1319,
            13,
            583,
            295,
            50740
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16363635659217834,
        "compression_ratio": 1.7999999523162842,
        "no_speech_prob": 0.00013982178643345833
    },
    {
        "id": 134,
        "seek": 80200,
        "start": 809.52001953125,
        "end": 814.9600219726562,
        "text": " course just increasing the number of tokens is not strictly better infinitely because as you",
        "tokens": [
            50740,
            1164,
            445,
            5662,
            264,
            1230,
            295,
            22667,
            307,
            406,
            20792,
            1101,
            36227,
            570,
            382,
            291,
            51012
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16363635659217834,
        "compression_ratio": 1.7999999523162842,
        "no_speech_prob": 0.00013982178643345833
    },
    {
        "id": 135,
        "seek": 80200,
        "start": 814.9600219726562,
        "end": 820.3200073242188,
        "text": " increase the number of tokens now your embedding table is sort of getting a lot larger and also",
        "tokens": [
            51012,
            3488,
            264,
            1230,
            295,
            22667,
            586,
            428,
            12240,
            3584,
            3199,
            307,
            1333,
            295,
            1242,
            257,
            688,
            4833,
            293,
            611,
            51280
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16363635659217834,
        "compression_ratio": 1.7999999523162842,
        "no_speech_prob": 0.00013982178643345833
    },
    {
        "id": 136,
        "seek": 80200,
        "start": 820.3200073242188,
        "end": 824.6400146484375,
        "text": " at the output we are trying to predict the next token and there's the softmax there and that grows",
        "tokens": [
            51280,
            412,
            264,
            5598,
            321,
            366,
            1382,
            281,
            6069,
            264,
            958,
            14862,
            293,
            456,
            311,
            264,
            2787,
            41167,
            456,
            293,
            300,
            13156,
            51496
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16363635659217834,
        "compression_ratio": 1.7999999523162842,
        "no_speech_prob": 0.00013982178643345833
    },
    {
        "id": 137,
        "seek": 80200,
        "start": 824.6400146484375,
        "end": 828.719970703125,
        "text": " as well. We're going to go into more detail later on this but there's some kind of a sweet spot",
        "tokens": [
            51496,
            382,
            731,
            13,
            492,
            434,
            516,
            281,
            352,
            666,
            544,
            2607,
            1780,
            322,
            341,
            457,
            456,
            311,
            512,
            733,
            295,
            257,
            3844,
            4008,
            51700
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16363635659217834,
        "compression_ratio": 1.7999999523162842,
        "no_speech_prob": 0.00013982178643345833
    },
    {
        "id": 138,
        "seek": 82872,
        "start": 828.719970703125,
        "end": 833.8400268554688,
        "text": " somewhere where you have a just right number of tokens in your vocabulary where everything is",
        "tokens": [
            50364,
            4079,
            689,
            291,
            362,
            257,
            445,
            558,
            1230,
            295,
            22667,
            294,
            428,
            19864,
            689,
            1203,
            307,
            50620
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17162968218326569,
        "compression_ratio": 1.6317992210388184,
        "no_speech_prob": 0.00015843537403270602
    },
    {
        "id": 139,
        "seek": 82872,
        "start": 833.8400268554688,
        "end": 839.280029296875,
        "text": " appropriately dense and still fairly efficient. Now one thing I would like you to note specifically",
        "tokens": [
            50620,
            23505,
            18011,
            293,
            920,
            6457,
            7148,
            13,
            823,
            472,
            551,
            286,
            576,
            411,
            291,
            281,
            3637,
            4682,
            50892
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17162968218326569,
        "compression_ratio": 1.6317992210388184,
        "no_speech_prob": 0.00015843537403270602
    },
    {
        "id": 140,
        "seek": 82872,
        "start": 839.280029296875,
        "end": 846.0800170898438,
        "text": " for the GPT-4 tokenizer is that the handling of the white space for Python has improved a lot.",
        "tokens": [
            50892,
            337,
            264,
            26039,
            51,
            12,
            19,
            14862,
            6545,
            307,
            300,
            264,
            13175,
            295,
            264,
            2418,
            1901,
            337,
            15329,
            575,
            9689,
            257,
            688,
            13,
            51232
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17162968218326569,
        "compression_ratio": 1.6317992210388184,
        "no_speech_prob": 0.00015843537403270602
    },
    {
        "id": 141,
        "seek": 82872,
        "start": 846.0800170898438,
        "end": 852.4000244140625,
        "text": " You see that here these four spaces are represented as one single token for the three spaces here and",
        "tokens": [
            51232,
            509,
            536,
            300,
            510,
            613,
            1451,
            7673,
            366,
            10379,
            382,
            472,
            2167,
            14862,
            337,
            264,
            1045,
            7673,
            510,
            293,
            51548
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17162968218326569,
        "compression_ratio": 1.6317992210388184,
        "no_speech_prob": 0.00015843537403270602
    },
    {
        "id": 142,
        "seek": 85240,
        "start": 852.47998046875,
        "end": 859.1199951171875,
        "text": " then the token spaces and here seven spaces were all grouped into a single token so we're being a",
        "tokens": [
            50368,
            550,
            264,
            14862,
            7673,
            293,
            510,
            3407,
            7673,
            645,
            439,
            41877,
            666,
            257,
            2167,
            14862,
            370,
            321,
            434,
            885,
            257,
            50700
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19962991774082184,
        "compression_ratio": 1.7219730615615845,
        "no_speech_prob": 4.908658593194559e-05
    },
    {
        "id": 143,
        "seek": 85240,
        "start": 859.1199951171875,
        "end": 863.760009765625,
        "text": " lot more efficient in how we represent Python and this was a deliberate choice made by OpenAI when",
        "tokens": [
            50700,
            688,
            544,
            7148,
            294,
            577,
            321,
            2906,
            15329,
            293,
            341,
            390,
            257,
            30515,
            3922,
            1027,
            538,
            7238,
            48698,
            562,
            50932
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19962991774082184,
        "compression_ratio": 1.7219730615615845,
        "no_speech_prob": 4.908658593194559e-05
    },
    {
        "id": 144,
        "seek": 85240,
        "start": 863.760009765625,
        "end": 870.47998046875,
        "text": " they designed the GPT-4 tokenizer and they group a lot more white space into a single character.",
        "tokens": [
            50932,
            436,
            4761,
            264,
            26039,
            51,
            12,
            19,
            14862,
            6545,
            293,
            436,
            1594,
            257,
            688,
            544,
            2418,
            1901,
            666,
            257,
            2167,
            2517,
            13,
            51268
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19962991774082184,
        "compression_ratio": 1.7219730615615845,
        "no_speech_prob": 4.908658593194559e-05
    },
    {
        "id": 145,
        "seek": 85240,
        "start": 870.47998046875,
        "end": 877.8400268554688,
        "text": " What this does is this densifies Python and therefore we can attend to more code before it",
        "tokens": [
            51268,
            708,
            341,
            775,
            307,
            341,
            24505,
            11221,
            15329,
            293,
            4412,
            321,
            393,
            6888,
            281,
            544,
            3089,
            949,
            309,
            51636
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19962991774082184,
        "compression_ratio": 1.7219730615615845,
        "no_speech_prob": 4.908658593194559e-05
    },
    {
        "id": 146,
        "seek": 87784,
        "start": 877.8400268554688,
        "end": 882.719970703125,
        "text": " when we're trying to predict the next token in the sequence and so the improvement in the Python",
        "tokens": [
            50364,
            562,
            321,
            434,
            1382,
            281,
            6069,
            264,
            958,
            14862,
            294,
            264,
            8310,
            293,
            370,
            264,
            10444,
            294,
            264,
            15329,
            50608
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1909920871257782,
        "compression_ratio": 1.6572438478469849,
        "no_speech_prob": 0.00024536901037208736
    },
    {
        "id": 147,
        "seek": 87784,
        "start": 882.719970703125,
        "end": 888.3200073242188,
        "text": " coding ability from GPT-2 to GPT-4 is not just a matter of the language model and the architecture",
        "tokens": [
            50608,
            17720,
            3485,
            490,
            26039,
            51,
            12,
            17,
            281,
            26039,
            51,
            12,
            19,
            307,
            406,
            445,
            257,
            1871,
            295,
            264,
            2856,
            2316,
            293,
            264,
            9482,
            50888
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1909920871257782,
        "compression_ratio": 1.6572438478469849,
        "no_speech_prob": 0.00024536901037208736
    },
    {
        "id": 148,
        "seek": 87784,
        "start": 888.3200073242188,
        "end": 892.4000244140625,
        "text": " and details of the optimization but a lot of the improvement here is also coming from the",
        "tokens": [
            50888,
            293,
            4365,
            295,
            264,
            19618,
            457,
            257,
            688,
            295,
            264,
            10444,
            510,
            307,
            611,
            1348,
            490,
            264,
            51092
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1909920871257782,
        "compression_ratio": 1.6572438478469849,
        "no_speech_prob": 0.00024536901037208736
    },
    {
        "id": 149,
        "seek": 87784,
        "start": 892.4000244140625,
        "end": 897.52001953125,
        "text": " design of the tokenizer and how it groups characters into tokens. Okay so let's now start",
        "tokens": [
            51092,
            1715,
            295,
            264,
            14862,
            6545,
            293,
            577,
            309,
            3935,
            4342,
            666,
            22667,
            13,
            1033,
            370,
            718,
            311,
            586,
            722,
            51348
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1909920871257782,
        "compression_ratio": 1.6572438478469849,
        "no_speech_prob": 0.00024536901037208736
    },
    {
        "id": 150,
        "seek": 87784,
        "start": 897.52001953125,
        "end": 903.760009765625,
        "text": " writing some code. So remember what we want to do. We want to take strings and feed them into",
        "tokens": [
            51348,
            3579,
            512,
            3089,
            13,
            407,
            1604,
            437,
            321,
            528,
            281,
            360,
            13,
            492,
            528,
            281,
            747,
            13985,
            293,
            3154,
            552,
            666,
            51660
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1909920871257782,
        "compression_ratio": 1.6572438478469849,
        "no_speech_prob": 0.00024536901037208736
    },
    {
        "id": 151,
        "seek": 90376,
        "start": 903.760009765625,
        "end": 910.4000244140625,
        "text": " language models. For that we need to somehow tokenize strings into some integers in some",
        "tokens": [
            50364,
            2856,
            5245,
            13,
            1171,
            300,
            321,
            643,
            281,
            6063,
            14862,
            1125,
            13985,
            666,
            512,
            41674,
            294,
            512,
            50696
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1870727837085724,
        "compression_ratio": 1.7251908779144287,
        "no_speech_prob": 0.0011159424902871251
    },
    {
        "id": 152,
        "seek": 90376,
        "start": 910.4000244140625,
        "end": 915.8400268554688,
        "text": " fixed vocabulary and then we will use those integers to make a lookup into a lookup table",
        "tokens": [
            50696,
            6806,
            19864,
            293,
            550,
            321,
            486,
            764,
            729,
            41674,
            281,
            652,
            257,
            574,
            1010,
            666,
            257,
            574,
            1010,
            3199,
            50968
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1870727837085724,
        "compression_ratio": 1.7251908779144287,
        "no_speech_prob": 0.0011159424902871251
    },
    {
        "id": 153,
        "seek": 90376,
        "start": 915.8400268554688,
        "end": 921.52001953125,
        "text": " of vectors and feed those vectors into the transformer as an input. Now the reason this",
        "tokens": [
            50968,
            295,
            18875,
            293,
            3154,
            729,
            18875,
            666,
            264,
            31782,
            382,
            364,
            4846,
            13,
            823,
            264,
            1778,
            341,
            51252
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1870727837085724,
        "compression_ratio": 1.7251908779144287,
        "no_speech_prob": 0.0011159424902871251
    },
    {
        "id": 154,
        "seek": 90376,
        "start": 921.52001953125,
        "end": 924.9600219726562,
        "text": " gets a little bit tricky of course is that we don't just want to support the simple English",
        "tokens": [
            51252,
            2170,
            257,
            707,
            857,
            12414,
            295,
            1164,
            307,
            300,
            321,
            500,
            380,
            445,
            528,
            281,
            1406,
            264,
            2199,
            3669,
            51424
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1870727837085724,
        "compression_ratio": 1.7251908779144287,
        "no_speech_prob": 0.0011159424902871251
    },
    {
        "id": 155,
        "seek": 90376,
        "start": 924.9600219726562,
        "end": 930.4000244140625,
        "text": " alphabet. We want to support different kinds of languages so this is annyeonghaseyo in Korean",
        "tokens": [
            51424,
            23339,
            13,
            492,
            528,
            281,
            1406,
            819,
            3685,
            295,
            8650,
            370,
            341,
            307,
            2324,
            18122,
            71,
            651,
            8308,
            294,
            6933,
            51696
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1870727837085724,
        "compression_ratio": 1.7251908779144287,
        "no_speech_prob": 0.0011159424902871251
    },
    {
        "id": 156,
        "seek": 93040,
        "start": 930.4000244140625,
        "end": 934.7999877929688,
        "text": " which is hello and we also want to support many kinds of special characters that we might find on",
        "tokens": [
            50364,
            597,
            307,
            7751,
            293,
            321,
            611,
            528,
            281,
            1406,
            867,
            3685,
            295,
            2121,
            4342,
            300,
            321,
            1062,
            915,
            322,
            50584
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2016017735004425,
        "compression_ratio": 1.649350643157959,
        "no_speech_prob": 6.144172857602825e-06
    },
    {
        "id": 157,
        "seek": 93040,
        "start": 934.7999877929688,
        "end": 944.3200073242188,
        "text": " the internet for example emoji. So how do we feed this text into transformers? Well what is",
        "tokens": [
            50584,
            264,
            4705,
            337,
            1365,
            31595,
            13,
            407,
            577,
            360,
            321,
            3154,
            341,
            2487,
            666,
            4088,
            433,
            30,
            1042,
            437,
            307,
            51060
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2016017735004425,
        "compression_ratio": 1.649350643157959,
        "no_speech_prob": 6.144172857602825e-06
    },
    {
        "id": 158,
        "seek": 93040,
        "start": 944.3200073242188,
        "end": 950.0,
        "text": " this text anyway in Python? So if you go to the documentation of a string in Python you can see",
        "tokens": [
            51060,
            341,
            2487,
            4033,
            294,
            15329,
            30,
            407,
            498,
            291,
            352,
            281,
            264,
            14333,
            295,
            257,
            6798,
            294,
            15329,
            291,
            393,
            536,
            51344
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2016017735004425,
        "compression_ratio": 1.649350643157959,
        "no_speech_prob": 6.144172857602825e-06
    },
    {
        "id": 159,
        "seek": 93040,
        "start": 950.0,
        "end": 956.719970703125,
        "text": " that strings are immutable sequences of Unicode code points. Okay what are Unicode code points?",
        "tokens": [
            51344,
            300,
            13985,
            366,
            3397,
            32148,
            22978,
            295,
            1156,
            299,
            1429,
            3089,
            2793,
            13,
            1033,
            437,
            366,
            1156,
            299,
            1429,
            3089,
            2793,
            30,
            51680
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2016017735004425,
        "compression_ratio": 1.649350643157959,
        "no_speech_prob": 6.144172857602825e-06
    },
    {
        "id": 160,
        "seek": 95672,
        "start": 957.6799926757812,
        "end": 963.5999755859375,
        "text": " We can go to Wikipedia. So Unicode code points are defined by the Unicode consortium",
        "tokens": [
            50412,
            492,
            393,
            352,
            281,
            28999,
            13,
            407,
            1156,
            299,
            1429,
            3089,
            2793,
            366,
            7642,
            538,
            264,
            1156,
            299,
            1429,
            38343,
            2197,
            50708
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22968344390392303,
        "compression_ratio": 1.7476634979248047,
        "no_speech_prob": 0.00023413471353705972
    },
    {
        "id": 161,
        "seek": 95672,
        "start": 964.239990234375,
        "end": 970.0800170898438,
        "text": " as part of the Unicode standard and what this is really is that it's just a definition of roughly",
        "tokens": [
            50740,
            382,
            644,
            295,
            264,
            1156,
            299,
            1429,
            3832,
            293,
            437,
            341,
            307,
            534,
            307,
            300,
            309,
            311,
            445,
            257,
            7123,
            295,
            9810,
            51032
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22968344390392303,
        "compression_ratio": 1.7476634979248047,
        "no_speech_prob": 0.00023413471353705972
    },
    {
        "id": 162,
        "seek": 95672,
        "start": 970.0800170898438,
        "end": 977.3599853515625,
        "text": " 150,000 characters right now and roughly speaking what they look like and what integers represent",
        "tokens": [
            51032,
            8451,
            11,
            1360,
            4342,
            558,
            586,
            293,
            9810,
            4124,
            437,
            436,
            574,
            411,
            293,
            437,
            41674,
            2906,
            51396
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22968344390392303,
        "compression_ratio": 1.7476634979248047,
        "no_speech_prob": 0.00023413471353705972
    },
    {
        "id": 163,
        "seek": 95672,
        "start": 977.3599853515625,
        "end": 983.6799926757812,
        "text": " those characters. So this is 150,000 characters across 161 scripts as of right now. So if you",
        "tokens": [
            51396,
            729,
            4342,
            13,
            407,
            341,
            307,
            8451,
            11,
            1360,
            4342,
            2108,
            3165,
            16,
            23294,
            382,
            295,
            558,
            586,
            13,
            407,
            498,
            291,
            51712
        ],
        "temperature": 0.0,
        "avg_logprob": -0.22968344390392303,
        "compression_ratio": 1.7476634979248047,
        "no_speech_prob": 0.00023413471353705972
    },
    {
        "id": 164,
        "seek": 98368,
        "start": 983.8400268554688,
        "end": 988.719970703125,
        "text": " scroll down here you can see that the standard is very much alive. The latest standard 15.1 is",
        "tokens": [
            50372,
            11369,
            760,
            510,
            291,
            393,
            536,
            300,
            264,
            3832,
            307,
            588,
            709,
            5465,
            13,
            440,
            6792,
            3832,
            2119,
            13,
            16,
            307,
            50616
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20606040954589844,
        "compression_ratio": 1.5897436141967773,
        "no_speech_prob": 9.461237641517073e-05
    },
    {
        "id": 165,
        "seek": 98368,
        "start": 988.719970703125,
        "end": 996.4000244140625,
        "text": " September 2023 and basically this is just a way to define lots of types of characters",
        "tokens": [
            50616,
            7216,
            44377,
            293,
            1936,
            341,
            307,
            445,
            257,
            636,
            281,
            6964,
            3195,
            295,
            3467,
            295,
            4342,
            51000
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20606040954589844,
        "compression_ratio": 1.5897436141967773,
        "no_speech_prob": 9.461237641517073e-05
    },
    {
        "id": 166,
        "seek": 98368,
        "start": 997.9199829101562,
        "end": 1003.280029296875,
        "text": " like for example all these characters across different scripts. So the way we can access the",
        "tokens": [
            51076,
            411,
            337,
            1365,
            439,
            613,
            4342,
            2108,
            819,
            23294,
            13,
            407,
            264,
            636,
            321,
            393,
            2105,
            264,
            51344
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20606040954589844,
        "compression_ratio": 1.5897436141967773,
        "no_speech_prob": 9.461237641517073e-05
    },
    {
        "id": 167,
        "seek": 98368,
        "start": 1003.280029296875,
        "end": 1008.239990234375,
        "text": " Unicode code point given a single character is by using the ORD function in Python. So for example",
        "tokens": [
            51344,
            1156,
            299,
            1429,
            3089,
            935,
            2212,
            257,
            2167,
            2517,
            307,
            538,
            1228,
            264,
            19654,
            35,
            2445,
            294,
            15329,
            13,
            407,
            337,
            1365,
            51592
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20606040954589844,
        "compression_ratio": 1.5897436141967773,
        "no_speech_prob": 9.461237641517073e-05
    },
    {
        "id": 168,
        "seek": 100824,
        "start": 1008.239990234375,
        "end": 1016.4000244140625,
        "text": " I can pass in ORD of h and I can see that for the single character h the Unicode code point is 104.",
        "tokens": [
            50364,
            286,
            393,
            1320,
            294,
            19654,
            35,
            295,
            276,
            293,
            286,
            393,
            536,
            300,
            337,
            264,
            2167,
            2517,
            276,
            264,
            1156,
            299,
            1429,
            3089,
            935,
            307,
            47757,
            13,
            50772
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20361328125,
        "compression_ratio": 1.5714285373687744,
        "no_speech_prob": 0.00042388358269818127
    },
    {
        "id": 169,
        "seek": 100824,
        "start": 1019.2000122070312,
        "end": 1023.52001953125,
        "text": " But this can be arbitrarily complicated so we can take for example our emoji here",
        "tokens": [
            50912,
            583,
            341,
            393,
            312,
            19071,
            3289,
            6179,
            370,
            321,
            393,
            747,
            337,
            1365,
            527,
            31595,
            510,
            51128
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20361328125,
        "compression_ratio": 1.5714285373687744,
        "no_speech_prob": 0.00042388358269818127
    },
    {
        "id": 170,
        "seek": 100824,
        "start": 1023.52001953125,
        "end": 1028.800048828125,
        "text": " and we can see that the code point for this one is 128,000 or we can take UN",
        "tokens": [
            51128,
            293,
            321,
            393,
            536,
            300,
            264,
            3089,
            935,
            337,
            341,
            472,
            307,
            29810,
            11,
            1360,
            420,
            321,
            393,
            747,
            8229,
            51392
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20361328125,
        "compression_ratio": 1.5714285373687744,
        "no_speech_prob": 0.00042388358269818127
    },
    {
        "id": 171,
        "seek": 100824,
        "start": 1031.3599853515625,
        "end": 1037.760009765625,
        "text": " and this is 50,000. Now keep in mind you can't plug in strings here because this doesn't have",
        "tokens": [
            51520,
            293,
            341,
            307,
            2625,
            11,
            1360,
            13,
            823,
            1066,
            294,
            1575,
            291,
            393,
            380,
            5452,
            294,
            13985,
            510,
            570,
            341,
            1177,
            380,
            362,
            51840
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20361328125,
        "compression_ratio": 1.5714285373687744,
        "no_speech_prob": 0.00042388358269818127
    },
    {
        "id": 172,
        "seek": 103776,
        "start": 1037.760009765625,
        "end": 1043.199951171875,
        "text": " a single code point. It only takes a single Unicode code point character and tells you its",
        "tokens": [
            50364,
            257,
            2167,
            3089,
            935,
            13,
            467,
            787,
            2516,
            257,
            2167,
            1156,
            299,
            1429,
            3089,
            935,
            2517,
            293,
            5112,
            291,
            1080,
            50636
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19230769574642181,
        "compression_ratio": 1.7302325963974,
        "no_speech_prob": 9.915261034620926e-05
    },
    {
        "id": 173,
        "seek": 103776,
        "start": 1043.199951171875,
        "end": 1051.199951171875,
        "text": " integer. So in this way we can look up all the characters of this specific string and their",
        "tokens": [
            50636,
            24922,
            13,
            407,
            294,
            341,
            636,
            321,
            393,
            574,
            493,
            439,
            264,
            4342,
            295,
            341,
            2685,
            6798,
            293,
            641,
            51036
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19230769574642181,
        "compression_ratio": 1.7302325963974,
        "no_speech_prob": 9.915261034620926e-05
    },
    {
        "id": 174,
        "seek": 103776,
        "start": 1051.199951171875,
        "end": 1060.4000244140625,
        "text": " code points so ORD of x for x in this string and we get this encoding here. Now see here we've",
        "tokens": [
            51036,
            3089,
            2793,
            370,
            19654,
            35,
            295,
            2031,
            337,
            2031,
            294,
            341,
            6798,
            293,
            321,
            483,
            341,
            43430,
            510,
            13,
            823,
            536,
            510,
            321,
            600,
            51496
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19230769574642181,
        "compression_ratio": 1.7302325963974,
        "no_speech_prob": 9.915261034620926e-05
    },
    {
        "id": 175,
        "seek": 103776,
        "start": 1060.4000244140625,
        "end": 1065.3599853515625,
        "text": " already turned the raw code points already have integers so why can't we simply just use these",
        "tokens": [
            51496,
            1217,
            3574,
            264,
            8936,
            3089,
            2793,
            1217,
            362,
            41674,
            370,
            983,
            393,
            380,
            321,
            2935,
            445,
            764,
            613,
            51744
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19230769574642181,
        "compression_ratio": 1.7302325963974,
        "no_speech_prob": 9.915261034620926e-05
    },
    {
        "id": 176,
        "seek": 106536,
        "start": 1065.3599853515625,
        "end": 1070.719970703125,
        "text": " integers and not have any tokenization at all? Why can't we just use this natively as is and just",
        "tokens": [
            50364,
            41674,
            293,
            406,
            362,
            604,
            14862,
            2144,
            412,
            439,
            30,
            1545,
            393,
            380,
            321,
            445,
            764,
            341,
            8470,
            356,
            382,
            307,
            293,
            445,
            50632
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17423948645591736,
        "compression_ratio": 1.6875,
        "no_speech_prob": 3.763640779652633e-05
    },
    {
        "id": 177,
        "seek": 106536,
        "start": 1070.719970703125,
        "end": 1074.8800048828125,
        "text": " use the code point? Well one reason for that of course is that the vocabulary in that case would",
        "tokens": [
            50632,
            764,
            264,
            3089,
            935,
            30,
            1042,
            472,
            1778,
            337,
            300,
            295,
            1164,
            307,
            300,
            264,
            19864,
            294,
            300,
            1389,
            576,
            50840
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17423948645591736,
        "compression_ratio": 1.6875,
        "no_speech_prob": 3.763640779652633e-05
    },
    {
        "id": 178,
        "seek": 106536,
        "start": 1074.8800048828125,
        "end": 1081.6800537109375,
        "text": " be quite long. So in this case for Unicode this is a vocabulary of 150,000 different code points.",
        "tokens": [
            50840,
            312,
            1596,
            938,
            13,
            407,
            294,
            341,
            1389,
            337,
            1156,
            299,
            1429,
            341,
            307,
            257,
            19864,
            295,
            8451,
            11,
            1360,
            819,
            3089,
            2793,
            13,
            51180
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17423948645591736,
        "compression_ratio": 1.6875,
        "no_speech_prob": 3.763640779652633e-05
    },
    {
        "id": 179,
        "seek": 106536,
        "start": 1082.239990234375,
        "end": 1088.0,
        "text": " But more worryingly than that I think the Unicode standard is very much alive and it keeps changing",
        "tokens": [
            51208,
            583,
            544,
            3292,
            12163,
            813,
            300,
            286,
            519,
            264,
            1156,
            299,
            1429,
            3832,
            307,
            588,
            709,
            5465,
            293,
            309,
            5965,
            4473,
            51496
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17423948645591736,
        "compression_ratio": 1.6875,
        "no_speech_prob": 3.763640779652633e-05
    },
    {
        "id": 180,
        "seek": 106536,
        "start": 1088.0,
        "end": 1092.3199462890625,
        "text": " and so it's not kind of a stable representation necessarily that we may want to use directly.",
        "tokens": [
            51496,
            293,
            370,
            309,
            311,
            406,
            733,
            295,
            257,
            8351,
            10290,
            4725,
            300,
            321,
            815,
            528,
            281,
            764,
            3838,
            13,
            51712
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17423948645591736,
        "compression_ratio": 1.6875,
        "no_speech_prob": 3.763640779652633e-05
    },
    {
        "id": 181,
        "seek": 109232,
        "start": 1093.199951171875,
        "end": 1097.43994140625,
        "text": " So for those reasons we need something a bit better. So to find something better we turn to",
        "tokens": [
            50408,
            407,
            337,
            729,
            4112,
            321,
            643,
            746,
            257,
            857,
            1101,
            13,
            407,
            281,
            915,
            746,
            1101,
            321,
            1261,
            281,
            50620
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17793604731559753,
        "compression_ratio": 1.649350643157959,
        "no_speech_prob": 8.481086842948571e-05
    },
    {
        "id": 182,
        "seek": 109232,
        "start": 1097.43994140625,
        "end": 1102.8800048828125,
        "text": " encodings. So if you go to the Wikipedia page here we see that the Unicode consortium defines",
        "tokens": [
            50620,
            2058,
            378,
            1109,
            13,
            407,
            498,
            291,
            352,
            281,
            264,
            28999,
            3028,
            510,
            321,
            536,
            300,
            264,
            1156,
            299,
            1429,
            38343,
            2197,
            23122,
            50892
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17793604731559753,
        "compression_ratio": 1.649350643157959,
        "no_speech_prob": 8.481086842948571e-05
    },
    {
        "id": 183,
        "seek": 109232,
        "start": 1102.8800048828125,
        "end": 1111.0400390625,
        "text": " three types of encodings UTF-8, UTF-16 and UTF-32. These encodings are the way by which we can take",
        "tokens": [
            50892,
            1045,
            3467,
            295,
            2058,
            378,
            1109,
            624,
            20527,
            12,
            23,
            11,
            624,
            20527,
            12,
            6866,
            293,
            624,
            20527,
            12,
            11440,
            13,
            1981,
            2058,
            378,
            1109,
            366,
            264,
            636,
            538,
            597,
            321,
            393,
            747,
            51300
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17793604731559753,
        "compression_ratio": 1.649350643157959,
        "no_speech_prob": 8.481086842948571e-05
    },
    {
        "id": 184,
        "seek": 109232,
        "start": 1111.0400390625,
        "end": 1117.9200439453125,
        "text": " Unicode text and translate it into binary data or byte streams. UTF-8 is by far the most common",
        "tokens": [
            51300,
            1156,
            299,
            1429,
            2487,
            293,
            13799,
            309,
            666,
            17434,
            1412,
            420,
            40846,
            15842,
            13,
            624,
            20527,
            12,
            23,
            307,
            538,
            1400,
            264,
            881,
            2689,
            51644
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17793604731559753,
        "compression_ratio": 1.649350643157959,
        "no_speech_prob": 8.481086842948571e-05
    },
    {
        "id": 185,
        "seek": 111792,
        "start": 1118.8800048828125,
        "end": 1124.1600341796875,
        "text": " so this is the UTF-8 page. Now this Wikipedia page is actually quite long but what's important",
        "tokens": [
            50412,
            370,
            341,
            307,
            264,
            624,
            20527,
            12,
            23,
            3028,
            13,
            823,
            341,
            28999,
            3028,
            307,
            767,
            1596,
            938,
            457,
            437,
            311,
            1021,
            50676
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16012568771839142,
        "compression_ratio": 1.7035398483276367,
        "no_speech_prob": 0.00045120916911400855
    },
    {
        "id": 186,
        "seek": 111792,
        "start": 1124.1600341796875,
        "end": 1130.47998046875,
        "text": " for our purposes is that UTF-8 takes every single code point and it translates it to a byte stream",
        "tokens": [
            50676,
            337,
            527,
            9932,
            307,
            300,
            624,
            20527,
            12,
            23,
            2516,
            633,
            2167,
            3089,
            935,
            293,
            309,
            28468,
            309,
            281,
            257,
            40846,
            4309,
            50992
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16012568771839142,
        "compression_ratio": 1.7035398483276367,
        "no_speech_prob": 0.00045120916911400855
    },
    {
        "id": 187,
        "seek": 111792,
        "start": 1130.47998046875,
        "end": 1135.9200439453125,
        "text": " and this byte stream is between one to four bytes so it's a variable length encoding. So depending",
        "tokens": [
            50992,
            293,
            341,
            40846,
            4309,
            307,
            1296,
            472,
            281,
            1451,
            36088,
            370,
            309,
            311,
            257,
            7006,
            4641,
            43430,
            13,
            407,
            5413,
            51264
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16012568771839142,
        "compression_ratio": 1.7035398483276367,
        "no_speech_prob": 0.00045120916911400855
    },
    {
        "id": 188,
        "seek": 111792,
        "start": 1135.9200439453125,
        "end": 1139.8399658203125,
        "text": " on the Unicode point according to the schema you're going to end up with between one to four",
        "tokens": [
            51264,
            322,
            264,
            1156,
            299,
            1429,
            935,
            4650,
            281,
            264,
            34078,
            291,
            434,
            516,
            281,
            917,
            493,
            365,
            1296,
            472,
            281,
            1451,
            51460
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16012568771839142,
        "compression_ratio": 1.7035398483276367,
        "no_speech_prob": 0.00045120916911400855
    },
    {
        "id": 189,
        "seek": 113984,
        "start": 1139.8399658203125,
        "end": 1148.8800048828125,
        "text": " bytes for each code point. On top of that there's UTF-8, UTF-16 and UTF-32. UTF-32 is nice because",
        "tokens": [
            50364,
            36088,
            337,
            1184,
            3089,
            935,
            13,
            1282,
            1192,
            295,
            300,
            456,
            311,
            624,
            20527,
            12,
            23,
            11,
            624,
            20527,
            12,
            6866,
            293,
            624,
            20527,
            12,
            11440,
            13,
            624,
            20527,
            12,
            11440,
            307,
            1481,
            570,
            50816
        ],
        "temperature": 0.0,
        "avg_logprob": -0.15365980565547943,
        "compression_ratio": 1.5748988389968872,
        "no_speech_prob": 0.054194945842027664
    },
    {
        "id": 190,
        "seek": 113984,
        "start": 1148.8800048828125,
        "end": 1154.8800048828125,
        "text": " it is fixed length instead of variable length but it has many other downsides as well. So the full",
        "tokens": [
            50816,
            309,
            307,
            6806,
            4641,
            2602,
            295,
            7006,
            4641,
            457,
            309,
            575,
            867,
            661,
            21554,
            1875,
            382,
            731,
            13,
            407,
            264,
            1577,
            51116
        ],
        "temperature": 0.0,
        "avg_logprob": -0.15365980565547943,
        "compression_ratio": 1.5748988389968872,
        "no_speech_prob": 0.054194945842027664
    },
    {
        "id": 191,
        "seek": 113984,
        "start": 1154.8800048828125,
        "end": 1160.3199462890625,
        "text": " kind of spectrum of pros and cons of all these different three encodings are beyond the scope",
        "tokens": [
            51116,
            733,
            295,
            11143,
            295,
            6267,
            293,
            1014,
            295,
            439,
            613,
            819,
            1045,
            2058,
            378,
            1109,
            366,
            4399,
            264,
            11923,
            51388
        ],
        "temperature": 0.0,
        "avg_logprob": -0.15365980565547943,
        "compression_ratio": 1.5748988389968872,
        "no_speech_prob": 0.054194945842027664
    },
    {
        "id": 192,
        "seek": 113984,
        "start": 1160.3199462890625,
        "end": 1165.6800537109375,
        "text": " of this video. I'd just like to point out that I enjoyed this blog post and this blog post at the",
        "tokens": [
            51388,
            295,
            341,
            960,
            13,
            286,
            1116,
            445,
            411,
            281,
            935,
            484,
            300,
            286,
            4626,
            341,
            6968,
            2183,
            293,
            341,
            6968,
            2183,
            412,
            264,
            51656
        ],
        "temperature": 0.0,
        "avg_logprob": -0.15365980565547943,
        "compression_ratio": 1.5748988389968872,
        "no_speech_prob": 0.054194945842027664
    },
    {
        "id": 193,
        "seek": 116568,
        "start": 1165.6800537109375,
        "end": 1171.280029296875,
        "text": " end of it also has a number of references that can be quite useful. One of them is UTF-8",
        "tokens": [
            50364,
            917,
            295,
            309,
            611,
            575,
            257,
            1230,
            295,
            15400,
            300,
            393,
            312,
            1596,
            4420,
            13,
            1485,
            295,
            552,
            307,
            624,
            20527,
            12,
            23,
            50644
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18033023178577423,
        "compression_ratio": 1.6488889455795288,
        "no_speech_prob": 0.004069997929036617
    },
    {
        "id": 194,
        "seek": 116568,
        "start": 1171.280029296875,
        "end": 1178.0799560546875,
        "text": " Everywhere manifesto and this manifesto describes the reason why UTF-8 is significantly preferred",
        "tokens": [
            50644,
            37322,
            10067,
            78,
            293,
            341,
            10067,
            78,
            15626,
            264,
            1778,
            983,
            624,
            20527,
            12,
            23,
            307,
            10591,
            16494,
            50984
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18033023178577423,
        "compression_ratio": 1.6488889455795288,
        "no_speech_prob": 0.004069997929036617
    },
    {
        "id": 195,
        "seek": 116568,
        "start": 1178.0799560546875,
        "end": 1183.3599853515625,
        "text": " and a lot nicer than the other encodings and why it is used a lot more prominently",
        "tokens": [
            50984,
            293,
            257,
            688,
            22842,
            813,
            264,
            661,
            2058,
            378,
            1109,
            293,
            983,
            309,
            307,
            1143,
            257,
            688,
            544,
            39225,
            2276,
            51248
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18033023178577423,
        "compression_ratio": 1.6488889455795288,
        "no_speech_prob": 0.004069997929036617
    },
    {
        "id": 196,
        "seek": 116568,
        "start": 1184.719970703125,
        "end": 1190.9599609375,
        "text": " on the internet. One of the major advantages, just to give you a sense, is that UTF-8 is the only one",
        "tokens": [
            51316,
            322,
            264,
            4705,
            13,
            1485,
            295,
            264,
            2563,
            14906,
            11,
            445,
            281,
            976,
            291,
            257,
            2020,
            11,
            307,
            300,
            624,
            20527,
            12,
            23,
            307,
            264,
            787,
            472,
            51628
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18033023178577423,
        "compression_ratio": 1.6488889455795288,
        "no_speech_prob": 0.004069997929036617
    },
    {
        "id": 197,
        "seek": 119096,
        "start": 1190.9599609375,
        "end": 1197.199951171875,
        "text": " of these that is backwards compatible to the much simpler ASCII encoding of text but I'm not going",
        "tokens": [
            50364,
            295,
            613,
            300,
            307,
            12204,
            18218,
            281,
            264,
            709,
            18587,
            7469,
            34,
            9503,
            43430,
            295,
            2487,
            457,
            286,
            478,
            406,
            516,
            50676
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19426673650741577,
        "compression_ratio": 1.7121771574020386,
        "no_speech_prob": 0.001388505450449884
    },
    {
        "id": 198,
        "seek": 119096,
        "start": 1197.199951171875,
        "end": 1202.4000244140625,
        "text": " to go into the full detail in this video. So suffice to say that we like the UTF-8 encoding",
        "tokens": [
            50676,
            281,
            352,
            666,
            264,
            1577,
            2607,
            294,
            341,
            960,
            13,
            407,
            3889,
            573,
            281,
            584,
            300,
            321,
            411,
            264,
            624,
            20527,
            12,
            23,
            43430,
            50936
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19426673650741577,
        "compression_ratio": 1.7121771574020386,
        "no_speech_prob": 0.001388505450449884
    },
    {
        "id": 199,
        "seek": 119096,
        "start": 1202.4000244140625,
        "end": 1207.280029296875,
        "text": " and let's try to take this string and see what we get if we encode it into UTF-8.",
        "tokens": [
            50936,
            293,
            718,
            311,
            853,
            281,
            747,
            341,
            6798,
            293,
            536,
            437,
            321,
            483,
            498,
            321,
            2058,
            1429,
            309,
            666,
            624,
            20527,
            12,
            23,
            13,
            51180
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19426673650741577,
        "compression_ratio": 1.7121771574020386,
        "no_speech_prob": 0.001388505450449884
    },
    {
        "id": 200,
        "seek": 119096,
        "start": 1208.9599609375,
        "end": 1213.3599853515625,
        "text": " The string class in Python actually has dot encode and you can give it the encoding which is",
        "tokens": [
            51264,
            440,
            6798,
            1508,
            294,
            15329,
            767,
            575,
            5893,
            2058,
            1429,
            293,
            291,
            393,
            976,
            309,
            264,
            43430,
            597,
            307,
            51484
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19426673650741577,
        "compression_ratio": 1.7121771574020386,
        "no_speech_prob": 0.001388505450449884
    },
    {
        "id": 201,
        "seek": 119096,
        "start": 1213.3599853515625,
        "end": 1220.0799560546875,
        "text": " say UTF-8. Now what we get out of this is not very nice because this is the bytes, this is a bytes",
        "tokens": [
            51484,
            584,
            624,
            20527,
            12,
            23,
            13,
            823,
            437,
            321,
            483,
            484,
            295,
            341,
            307,
            406,
            588,
            1481,
            570,
            341,
            307,
            264,
            36088,
            11,
            341,
            307,
            257,
            36088,
            51820
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19426673650741577,
        "compression_ratio": 1.7121771574020386,
        "no_speech_prob": 0.001388505450449884
    },
    {
        "id": 202,
        "seek": 122008,
        "start": 1220.0799560546875,
        "end": 1225.43994140625,
        "text": " object and it's not very nice in the way that it's printed so I personally like to take it through a",
        "tokens": [
            50364,
            2657,
            293,
            309,
            311,
            406,
            588,
            1481,
            294,
            264,
            636,
            300,
            309,
            311,
            13567,
            370,
            286,
            5665,
            411,
            281,
            747,
            309,
            807,
            257,
            50632
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1724647581577301,
        "compression_ratio": 1.6581196784973145,
        "no_speech_prob": 2.6688441721489653e-05
    },
    {
        "id": 203,
        "seek": 122008,
        "start": 1225.43994140625,
        "end": 1233.3599853515625,
        "text": " list because then we actually get the raw bytes of this encoding. So this is the raw bytes that",
        "tokens": [
            50632,
            1329,
            570,
            550,
            321,
            767,
            483,
            264,
            8936,
            36088,
            295,
            341,
            43430,
            13,
            407,
            341,
            307,
            264,
            8936,
            36088,
            300,
            51028
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1724647581577301,
        "compression_ratio": 1.6581196784973145,
        "no_speech_prob": 2.6688441721489653e-05
    },
    {
        "id": 204,
        "seek": 122008,
        "start": 1233.3599853515625,
        "end": 1240.3199462890625,
        "text": " represent this string according to the UTF-8 encoding. We can also look at UTF-16. We get a",
        "tokens": [
            51028,
            2906,
            341,
            6798,
            4650,
            281,
            264,
            624,
            20527,
            12,
            23,
            43430,
            13,
            492,
            393,
            611,
            574,
            412,
            624,
            20527,
            12,
            6866,
            13,
            492,
            483,
            257,
            51376
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1724647581577301,
        "compression_ratio": 1.6581196784973145,
        "no_speech_prob": 2.6688441721489653e-05
    },
    {
        "id": 205,
        "seek": 122008,
        "start": 1240.3199462890625,
        "end": 1245.8399658203125,
        "text": " slightly different byte stream and here we start to see one of the disadvantages of UTF-16. You see",
        "tokens": [
            51376,
            4748,
            819,
            40846,
            4309,
            293,
            510,
            321,
            722,
            281,
            536,
            472,
            295,
            264,
            37431,
            295,
            624,
            20527,
            12,
            6866,
            13,
            509,
            536,
            51652
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1724647581577301,
        "compression_ratio": 1.6581196784973145,
        "no_speech_prob": 2.6688441721489653e-05
    },
    {
        "id": 206,
        "seek": 124584,
        "start": 1245.8399658203125,
        "end": 1250.239990234375,
        "text": " how we have zero something, zero something, zero something. We're starting to get a sense that",
        "tokens": [
            50364,
            577,
            321,
            362,
            4018,
            746,
            11,
            4018,
            746,
            11,
            4018,
            746,
            13,
            492,
            434,
            2891,
            281,
            483,
            257,
            2020,
            300,
            50584
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20249177515506744,
        "compression_ratio": 1.793578028678894,
        "no_speech_prob": 0.0012448146007955074
    },
    {
        "id": 207,
        "seek": 124584,
        "start": 1250.239990234375,
        "end": 1256.239990234375,
        "text": " this is a bit of a wasteful encoding and indeed for simple ASCII characters or English characters",
        "tokens": [
            50584,
            341,
            307,
            257,
            857,
            295,
            257,
            5964,
            906,
            43430,
            293,
            6451,
            337,
            2199,
            7469,
            34,
            9503,
            4342,
            420,
            3669,
            4342,
            50884
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20249177515506744,
        "compression_ratio": 1.793578028678894,
        "no_speech_prob": 0.0012448146007955074
    },
    {
        "id": 208,
        "seek": 124584,
        "start": 1256.239990234375,
        "end": 1262.56005859375,
        "text": " here we just have the structure of zero something, zero something and it's not exactly nice. Same for",
        "tokens": [
            50884,
            510,
            321,
            445,
            362,
            264,
            3877,
            295,
            4018,
            746,
            11,
            4018,
            746,
            293,
            309,
            311,
            406,
            2293,
            1481,
            13,
            10635,
            337,
            51200
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20249177515506744,
        "compression_ratio": 1.793578028678894,
        "no_speech_prob": 0.0012448146007955074
    },
    {
        "id": 209,
        "seek": 124584,
        "start": 1262.56005859375,
        "end": 1268.0799560546875,
        "text": " UTF-32. When we expand this we can start to get a sense of the wastefulness of this encoding for",
        "tokens": [
            51200,
            624,
            20527,
            12,
            11440,
            13,
            1133,
            321,
            5268,
            341,
            321,
            393,
            722,
            281,
            483,
            257,
            2020,
            295,
            264,
            5964,
            26872,
            295,
            341,
            43430,
            337,
            51476
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20249177515506744,
        "compression_ratio": 1.793578028678894,
        "no_speech_prob": 0.0012448146007955074
    },
    {
        "id": 210,
        "seek": 126808,
        "start": 1268.0799560546875,
        "end": 1274.4000244140625,
        "text": " our purposes. You see a lot of zeros followed by something and so this is not desirable.",
        "tokens": [
            50364,
            527,
            9932,
            13,
            509,
            536,
            257,
            688,
            295,
            35193,
            6263,
            538,
            746,
            293,
            370,
            341,
            307,
            406,
            30533,
            13,
            50680
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19490979611873627,
        "compression_ratio": 1.5958333015441895,
        "no_speech_prob": 0.003324308665469289
    },
    {
        "id": 211,
        "seek": 126808,
        "start": 1275.8399658203125,
        "end": 1283.199951171875,
        "text": " So suffice it to say that we would like to stick with UTF-8 for our purposes. However if we just",
        "tokens": [
            50752,
            407,
            3889,
            573,
            309,
            281,
            584,
            300,
            321,
            576,
            411,
            281,
            2897,
            365,
            624,
            20527,
            12,
            23,
            337,
            527,
            9932,
            13,
            2908,
            498,
            321,
            445,
            51120
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19490979611873627,
        "compression_ratio": 1.5958333015441895,
        "no_speech_prob": 0.003324308665469289
    },
    {
        "id": 212,
        "seek": 126808,
        "start": 1283.199951171875,
        "end": 1290.719970703125,
        "text": " use UTF-8 naively these are byte streams so that would imply a vocabulary length of only 256 possible",
        "tokens": [
            51120,
            764,
            624,
            20527,
            12,
            23,
            1667,
            3413,
            613,
            366,
            40846,
            15842,
            370,
            300,
            576,
            33616,
            257,
            19864,
            4641,
            295,
            787,
            38882,
            1944,
            51496
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19490979611873627,
        "compression_ratio": 1.5958333015441895,
        "no_speech_prob": 0.003324308665469289
    },
    {
        "id": 213,
        "seek": 126808,
        "start": 1290.719970703125,
        "end": 1296.719970703125,
        "text": " tokens but this vocabulary size is very very small. What this is going to do if we just were to",
        "tokens": [
            51496,
            22667,
            457,
            341,
            19864,
            2744,
            307,
            588,
            588,
            1359,
            13,
            708,
            341,
            307,
            516,
            281,
            360,
            498,
            321,
            445,
            645,
            281,
            51796
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19490979611873627,
        "compression_ratio": 1.5958333015441895,
        "no_speech_prob": 0.003324308665469289
    },
    {
        "id": 214,
        "seek": 129672,
        "start": 1296.719970703125,
        "end": 1303.1199951171875,
        "text": " use it naively is that all of our text would be stretched out over very very long sequences of",
        "tokens": [
            50364,
            764,
            309,
            1667,
            3413,
            307,
            300,
            439,
            295,
            527,
            2487,
            576,
            312,
            23563,
            484,
            670,
            588,
            588,
            938,
            22978,
            295,
            50684
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20536155998706818,
        "compression_ratio": 1.8275861740112305,
        "no_speech_prob": 1.9525848983903416e-05
    },
    {
        "id": 215,
        "seek": 129672,
        "start": 1303.1199951171875,
        "end": 1311.1199951171875,
        "text": " bytes and so what this does is that certainly the embedding table is going to be tiny and",
        "tokens": [
            50684,
            36088,
            293,
            370,
            437,
            341,
            775,
            307,
            300,
            3297,
            264,
            12240,
            3584,
            3199,
            307,
            516,
            281,
            312,
            5870,
            293,
            51084
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20536155998706818,
        "compression_ratio": 1.8275861740112305,
        "no_speech_prob": 1.9525848983903416e-05
    },
    {
        "id": 216,
        "seek": 129672,
        "start": 1311.1199951171875,
        "end": 1315.1199951171875,
        "text": " the prediction at the top at the final layer is going to be very tiny but our sequences are very",
        "tokens": [
            51084,
            264,
            17630,
            412,
            264,
            1192,
            412,
            264,
            2572,
            4583,
            307,
            516,
            281,
            312,
            588,
            5870,
            457,
            527,
            22978,
            366,
            588,
            51284
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20536155998706818,
        "compression_ratio": 1.8275861740112305,
        "no_speech_prob": 1.9525848983903416e-05
    },
    {
        "id": 217,
        "seek": 129672,
        "start": 1315.1199951171875,
        "end": 1321.0400390625,
        "text": " long and remember that we have pretty finite context lengths in the attention that we can",
        "tokens": [
            51284,
            938,
            293,
            1604,
            300,
            321,
            362,
            1238,
            19362,
            4319,
            26329,
            294,
            264,
            3202,
            300,
            321,
            393,
            51580
        ],
        "temperature": 0.0,
        "avg_logprob": -0.20536155998706818,
        "compression_ratio": 1.8275861740112305,
        "no_speech_prob": 1.9525848983903416e-05
    },
    {
        "id": 218,
        "seek": 132104,
        "start": 1321.0400390625,
        "end": 1327.3599853515625,
        "text": " support in a transformer for computational reasons and so we only have as much context length but now",
        "tokens": [
            50364,
            1406,
            294,
            257,
            31782,
            337,
            28270,
            4112,
            293,
            370,
            321,
            787,
            362,
            382,
            709,
            4319,
            4641,
            457,
            586,
            50680
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17031249403953552,
        "compression_ratio": 1.6843972206115723,
        "no_speech_prob": 0.003884353442117572
    },
    {
        "id": 219,
        "seek": 132104,
        "start": 1327.3599853515625,
        "end": 1331.52001953125,
        "text": " we have very very long sequences and this is just inefficient and it's not going to allow us to",
        "tokens": [
            50680,
            321,
            362,
            588,
            588,
            938,
            22978,
            293,
            341,
            307,
            445,
            43495,
            293,
            309,
            311,
            406,
            516,
            281,
            2089,
            505,
            281,
            50888
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17031249403953552,
        "compression_ratio": 1.6843972206115723,
        "no_speech_prob": 0.003884353442117572
    },
    {
        "id": 220,
        "seek": 132104,
        "start": 1331.52001953125,
        "end": 1337.1199951171875,
        "text": " attend to sufficiently long text before us for the purposes of the next token prediction task.",
        "tokens": [
            50888,
            6888,
            281,
            31868,
            938,
            2487,
            949,
            505,
            337,
            264,
            9932,
            295,
            264,
            958,
            14862,
            17630,
            5633,
            13,
            51168
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17031249403953552,
        "compression_ratio": 1.6843972206115723,
        "no_speech_prob": 0.003884353442117572
    },
    {
        "id": 221,
        "seek": 132104,
        "start": 1338.0,
        "end": 1345.0400390625,
        "text": " So we don't want to use the raw bytes of the UTF-8 encoding. We want to be able to support",
        "tokens": [
            51212,
            407,
            321,
            500,
            380,
            528,
            281,
            764,
            264,
            8936,
            36088,
            295,
            264,
            624,
            20527,
            12,
            23,
            43430,
            13,
            492,
            528,
            281,
            312,
            1075,
            281,
            1406,
            51564
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17031249403953552,
        "compression_ratio": 1.6843972206115723,
        "no_speech_prob": 0.003884353442117572
    },
    {
        "id": 222,
        "seek": 132104,
        "start": 1345.0400390625,
        "end": 1349.8399658203125,
        "text": " larger vocabulary size that we can tune as a height parameter but we want to stick with the",
        "tokens": [
            51564,
            4833,
            19864,
            2744,
            300,
            321,
            393,
            10864,
            382,
            257,
            6681,
            13075,
            457,
            321,
            528,
            281,
            2897,
            365,
            264,
            51804
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17031249403953552,
        "compression_ratio": 1.6843972206115723,
        "no_speech_prob": 0.003884353442117572
    },
    {
        "id": 223,
        "seek": 134984,
        "start": 1349.8399658203125,
        "end": 1355.760009765625,
        "text": " UTF-8 encoding of these strings so what do we do? Well the answer of course is we turn to the",
        "tokens": [
            50364,
            624,
            20527,
            12,
            23,
            43430,
            295,
            613,
            13985,
            370,
            437,
            360,
            321,
            360,
            30,
            1042,
            264,
            1867,
            295,
            1164,
            307,
            321,
            1261,
            281,
            264,
            50660
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17866761982440948,
        "compression_ratio": 1.631355881690979,
        "no_speech_prob": 9.314504859503359e-05
    },
    {
        "id": 224,
        "seek": 134984,
        "start": 1355.760009765625,
        "end": 1362.0,
        "text": " byte pair encoding algorithm which will allow us to compress these byte sequences to a variable",
        "tokens": [
            50660,
            40846,
            6119,
            43430,
            9284,
            597,
            486,
            2089,
            505,
            281,
            14778,
            613,
            40846,
            22978,
            281,
            257,
            7006,
            50972
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17866761982440948,
        "compression_ratio": 1.631355881690979,
        "no_speech_prob": 9.314504859503359e-05
    },
    {
        "id": 225,
        "seek": 134984,
        "start": 1362.0,
        "end": 1367.760009765625,
        "text": " amount so we'll get to that in a bit but I just want to briefly speak to the fact that I would",
        "tokens": [
            50972,
            2372,
            370,
            321,
            603,
            483,
            281,
            300,
            294,
            257,
            857,
            457,
            286,
            445,
            528,
            281,
            10515,
            1710,
            281,
            264,
            1186,
            300,
            286,
            576,
            51260
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17866761982440948,
        "compression_ratio": 1.631355881690979,
        "no_speech_prob": 9.314504859503359e-05
    },
    {
        "id": 226,
        "seek": 134984,
        "start": 1367.760009765625,
        "end": 1374.56005859375,
        "text": " love nothing more than to be able to feed raw byte sequences into language models. In fact there's a",
        "tokens": [
            51260,
            959,
            1825,
            544,
            813,
            281,
            312,
            1075,
            281,
            3154,
            8936,
            40846,
            22978,
            666,
            2856,
            5245,
            13,
            682,
            1186,
            456,
            311,
            257,
            51600
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17866761982440948,
        "compression_ratio": 1.631355881690979,
        "no_speech_prob": 9.314504859503359e-05
    },
    {
        "id": 227,
        "seek": 137456,
        "start": 1374.56005859375,
        "end": 1379.8399658203125,
        "text": " paper about how this could potentially be done from somewhere last year. Now the problem is you",
        "tokens": [
            50364,
            3035,
            466,
            577,
            341,
            727,
            7263,
            312,
            1096,
            490,
            4079,
            1036,
            1064,
            13,
            823,
            264,
            1154,
            307,
            291,
            50628
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16887426376342773,
        "compression_ratio": 1.7749077081680298,
        "no_speech_prob": 0.0005976501852273941
    },
    {
        "id": 228,
        "seek": 137456,
        "start": 1379.8399658203125,
        "end": 1384.56005859375,
        "text": " actually have to go in and you have to modify the transformer architecture because as I mentioned",
        "tokens": [
            50628,
            767,
            362,
            281,
            352,
            294,
            293,
            291,
            362,
            281,
            16927,
            264,
            31782,
            9482,
            570,
            382,
            286,
            2835,
            50864
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16887426376342773,
        "compression_ratio": 1.7749077081680298,
        "no_speech_prob": 0.0005976501852273941
    },
    {
        "id": 229,
        "seek": 137456,
        "start": 1384.56005859375,
        "end": 1388.9599609375,
        "text": " you're going to have a problem where the attention will start to become extremely expensive because",
        "tokens": [
            50864,
            291,
            434,
            516,
            281,
            362,
            257,
            1154,
            689,
            264,
            3202,
            486,
            722,
            281,
            1813,
            4664,
            5124,
            570,
            51084
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16887426376342773,
        "compression_ratio": 1.7749077081680298,
        "no_speech_prob": 0.0005976501852273941
    },
    {
        "id": 230,
        "seek": 137456,
        "start": 1388.9599609375,
        "end": 1395.6800537109375,
        "text": " the sequences are so long and so in this paper they propose kind of a hierarchical structuring",
        "tokens": [
            51084,
            264,
            22978,
            366,
            370,
            938,
            293,
            370,
            294,
            341,
            3035,
            436,
            17421,
            733,
            295,
            257,
            35250,
            804,
            6594,
            1345,
            51420
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16887426376342773,
        "compression_ratio": 1.7749077081680298,
        "no_speech_prob": 0.0005976501852273941
    },
    {
        "id": 231,
        "seek": 137456,
        "start": 1395.6800537109375,
        "end": 1400.9599609375,
        "text": " of the transformer that could allow you to just feed in raw bytes and so at the end they say",
        "tokens": [
            51420,
            295,
            264,
            31782,
            300,
            727,
            2089,
            291,
            281,
            445,
            3154,
            294,
            8936,
            36088,
            293,
            370,
            412,
            264,
            917,
            436,
            584,
            51684
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16887426376342773,
        "compression_ratio": 1.7749077081680298,
        "no_speech_prob": 0.0005976501852273941
    },
    {
        "id": 232,
        "seek": 140096,
        "start": 1400.9599609375,
        "end": 1405.0400390625,
        "text": " together these results establish the viability of tokenization-free autoregressive sequence",
        "tokens": [
            50364,
            1214,
            613,
            3542,
            8327,
            264,
            1932,
            2310,
            295,
            14862,
            2144,
            12,
            10792,
            1476,
            418,
            3091,
            488,
            8310,
            50568
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19947470724582672,
        "compression_ratio": 1.806249976158142,
        "no_speech_prob": 0.0002913637727033347
    },
    {
        "id": 233,
        "seek": 140096,
        "start": 1405.0400390625,
        "end": 1410.6400146484375,
        "text": " modeling at scale. So tokenization-free would indeed be amazing we would just feed byte streams",
        "tokens": [
            50568,
            15983,
            412,
            4373,
            13,
            407,
            14862,
            2144,
            12,
            10792,
            576,
            6451,
            312,
            2243,
            321,
            576,
            445,
            3154,
            40846,
            15842,
            50848
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19947470724582672,
        "compression_ratio": 1.806249976158142,
        "no_speech_prob": 0.0002913637727033347
    },
    {
        "id": 234,
        "seek": 140096,
        "start": 1410.6400146484375,
        "end": 1416.0799560546875,
        "text": " directly into our models but unfortunately I don't know that this has really been proven out yet by",
        "tokens": [
            50848,
            3838,
            666,
            527,
            5245,
            457,
            7015,
            286,
            500,
            380,
            458,
            300,
            341,
            575,
            534,
            668,
            12785,
            484,
            1939,
            538,
            51120
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19947470724582672,
        "compression_ratio": 1.806249976158142,
        "no_speech_prob": 0.0002913637727033347
    },
    {
        "id": 235,
        "seek": 140096,
        "start": 1416.0799560546875,
        "end": 1420.1600341796875,
        "text": " sufficiently many groups and at sufficient scale but something like this at one point would be",
        "tokens": [
            51120,
            31868,
            867,
            3935,
            293,
            412,
            11563,
            4373,
            457,
            746,
            411,
            341,
            412,
            472,
            935,
            576,
            312,
            51324
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19947470724582672,
        "compression_ratio": 1.806249976158142,
        "no_speech_prob": 0.0002913637727033347
    },
    {
        "id": 236,
        "seek": 140096,
        "start": 1420.1600341796875,
        "end": 1424.719970703125,
        "text": " amazing and I hope someone comes up with it but for now we have to come back and we can't feed",
        "tokens": [
            51324,
            2243,
            293,
            286,
            1454,
            1580,
            1487,
            493,
            365,
            309,
            457,
            337,
            586,
            321,
            362,
            281,
            808,
            646,
            293,
            321,
            393,
            380,
            3154,
            51552
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19947470724582672,
        "compression_ratio": 1.806249976158142,
        "no_speech_prob": 0.0002913637727033347
    },
    {
        "id": 237,
        "seek": 140096,
        "start": 1424.719970703125,
        "end": 1429.43994140625,
        "text": " this directly into language models and we have to compress it using the byte pair encoding algorithm",
        "tokens": [
            51552,
            341,
            3838,
            666,
            2856,
            5245,
            293,
            321,
            362,
            281,
            14778,
            309,
            1228,
            264,
            40846,
            6119,
            43430,
            9284,
            51788
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19947470724582672,
        "compression_ratio": 1.806249976158142,
        "no_speech_prob": 0.0002913637727033347
    },
    {
        "id": 238,
        "seek": 142944,
        "start": 1429.43994140625,
        "end": 1433.52001953125,
        "text": " so let's see how that works. So as I mentioned the byte pair encoding algorithm is not all that",
        "tokens": [
            50364,
            370,
            718,
            311,
            536,
            577,
            300,
            1985,
            13,
            407,
            382,
            286,
            2835,
            264,
            40846,
            6119,
            43430,
            9284,
            307,
            406,
            439,
            300,
            50568
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19885984063148499,
        "compression_ratio": 1.7408759593963623,
        "no_speech_prob": 0.0001007135069812648
    },
    {
        "id": 239,
        "seek": 142944,
        "start": 1433.52001953125,
        "end": 1438.1600341796875,
        "text": " complicated and the wikipedia page is actually quite instructive as far as the basic idea goes",
        "tokens": [
            50568,
            6179,
            293,
            264,
            261,
            1035,
            26633,
            3028,
            307,
            767,
            1596,
            7232,
            488,
            382,
            1400,
            382,
            264,
            3875,
            1558,
            1709,
            50800
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19885984063148499,
        "compression_ratio": 1.7408759593963623,
        "no_speech_prob": 0.0001007135069812648
    },
    {
        "id": 240,
        "seek": 142944,
        "start": 1438.8800048828125,
        "end": 1443.6800537109375,
        "text": " what we're doing is we have some kind of a input sequence like for example here we have only four",
        "tokens": [
            50836,
            437,
            321,
            434,
            884,
            307,
            321,
            362,
            512,
            733,
            295,
            257,
            4846,
            8310,
            411,
            337,
            1365,
            510,
            321,
            362,
            787,
            1451,
            51076
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19885984063148499,
        "compression_ratio": 1.7408759593963623,
        "no_speech_prob": 0.0001007135069812648
    },
    {
        "id": 241,
        "seek": 142944,
        "start": 1443.6800537109375,
        "end": 1449.280029296875,
        "text": " elements in our vocabulary a b c and d and we have a sequence of them so instead of bytes let's say",
        "tokens": [
            51076,
            4959,
            294,
            527,
            19864,
            257,
            272,
            269,
            293,
            274,
            293,
            321,
            362,
            257,
            8310,
            295,
            552,
            370,
            2602,
            295,
            36088,
            718,
            311,
            584,
            51356
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19885984063148499,
        "compression_ratio": 1.7408759593963623,
        "no_speech_prob": 0.0001007135069812648
    },
    {
        "id": 242,
        "seek": 142944,
        "start": 1449.280029296875,
        "end": 1454.800048828125,
        "text": " we just had four of a cap size of four the sequence is too long we'd like to compress it",
        "tokens": [
            51356,
            321,
            445,
            632,
            1451,
            295,
            257,
            1410,
            2744,
            295,
            1451,
            264,
            8310,
            307,
            886,
            938,
            321,
            1116,
            411,
            281,
            14778,
            309,
            51632
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19885984063148499,
        "compression_ratio": 1.7408759593963623,
        "no_speech_prob": 0.0001007135069812648
    },
    {
        "id": 243,
        "seek": 145480,
        "start": 1455.3599853515625,
        "end": 1464.800048828125,
        "text": " so we do is that we iteratively find the pair of tokens that occur the most frequently and then",
        "tokens": [
            50392,
            370,
            321,
            360,
            307,
            300,
            321,
            17138,
            19020,
            915,
            264,
            6119,
            295,
            22667,
            300,
            5160,
            264,
            881,
            10374,
            293,
            550,
            50864
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19292382895946503,
        "compression_ratio": 1.7000000476837158,
        "no_speech_prob": 2.1444922822411172e-05
    },
    {
        "id": 244,
        "seek": 145480,
        "start": 1464.800048828125,
        "end": 1471.8399658203125,
        "text": " once we've identified that pair we replace that pair with just a single new token that we append",
        "tokens": [
            50864,
            1564,
            321,
            600,
            9234,
            300,
            6119,
            321,
            7406,
            300,
            6119,
            365,
            445,
            257,
            2167,
            777,
            14862,
            300,
            321,
            34116,
            51216
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19292382895946503,
        "compression_ratio": 1.7000000476837158,
        "no_speech_prob": 2.1444922822411172e-05
    },
    {
        "id": 245,
        "seek": 145480,
        "start": 1471.8399658203125,
        "end": 1478.719970703125,
        "text": " to our vocabulary so for example here the byte pair a a occurs most often so we mint a new token",
        "tokens": [
            51216,
            281,
            527,
            19864,
            370,
            337,
            1365,
            510,
            264,
            40846,
            6119,
            257,
            257,
            11843,
            881,
            2049,
            370,
            321,
            18189,
            257,
            777,
            14862,
            51560
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19292382895946503,
        "compression_ratio": 1.7000000476837158,
        "no_speech_prob": 2.1444922822411172e-05
    },
    {
        "id": 246,
        "seek": 147872,
        "start": 1478.719970703125,
        "end": 1486.239990234375,
        "text": " let's call it capital z and we replace every single occurrence of a a by z so now we have",
        "tokens": [
            50364,
            718,
            311,
            818,
            309,
            4238,
            710,
            293,
            321,
            7406,
            633,
            2167,
            36122,
            295,
            257,
            257,
            538,
            710,
            370,
            586,
            321,
            362,
            50740
        ],
        "temperature": 0.0,
        "avg_logprob": -0.189546138048172,
        "compression_ratio": 1.7121951580047607,
        "no_speech_prob": 3.219224527128972e-05
    },
    {
        "id": 247,
        "seek": 147872,
        "start": 1486.239990234375,
        "end": 1492.9599609375,
        "text": " two z's here so here we took a sequence of 11 characters with vocabulary size four",
        "tokens": [
            50740,
            732,
            710,
            311,
            510,
            370,
            510,
            321,
            1890,
            257,
            8310,
            295,
            2975,
            4342,
            365,
            19864,
            2744,
            1451,
            51076
        ],
        "temperature": 0.0,
        "avg_logprob": -0.189546138048172,
        "compression_ratio": 1.7121951580047607,
        "no_speech_prob": 3.219224527128972e-05
    },
    {
        "id": 248,
        "seek": 147872,
        "start": 1493.6800537109375,
        "end": 1500.56005859375,
        "text": " and we've converted it to a sequence of only nine tokens but now with a vocabulary of five",
        "tokens": [
            51112,
            293,
            321,
            600,
            16424,
            309,
            281,
            257,
            8310,
            295,
            787,
            4949,
            22667,
            457,
            586,
            365,
            257,
            19864,
            295,
            1732,
            51456
        ],
        "temperature": 0.0,
        "avg_logprob": -0.189546138048172,
        "compression_ratio": 1.7121951580047607,
        "no_speech_prob": 3.219224527128972e-05
    },
    {
        "id": 249,
        "seek": 147872,
        "start": 1500.56005859375,
        "end": 1505.43994140625,
        "text": " because we have a fifth vocabulary element that we just created and it's z standing for",
        "tokens": [
            51456,
            570,
            321,
            362,
            257,
            9266,
            19864,
            4478,
            300,
            321,
            445,
            2942,
            293,
            309,
            311,
            710,
            4877,
            337,
            51700
        ],
        "temperature": 0.0,
        "avg_logprob": -0.189546138048172,
        "compression_ratio": 1.7121951580047607,
        "no_speech_prob": 3.219224527128972e-05
    },
    {
        "id": 250,
        "seek": 150544,
        "start": 1505.43994140625,
        "end": 1512.1600341796875,
        "text": " concatenation of a and we can again repeat this process so we again look at the sequence and",
        "tokens": [
            50364,
            1588,
            7186,
            399,
            295,
            257,
            293,
            321,
            393,
            797,
            7149,
            341,
            1399,
            370,
            321,
            797,
            574,
            412,
            264,
            8310,
            293,
            50700
        ],
        "temperature": 0.0,
        "avg_logprob": -0.15318943560123444,
        "compression_ratio": 1.8028168678283691,
        "no_speech_prob": 2.840955858118832e-05
    },
    {
        "id": 251,
        "seek": 150544,
        "start": 1512.1600341796875,
        "end": 1519.760009765625,
        "text": " identify the pair of tokens that are most frequent let's say that that is now a b well we are going",
        "tokens": [
            50700,
            5876,
            264,
            6119,
            295,
            22667,
            300,
            366,
            881,
            18004,
            718,
            311,
            584,
            300,
            300,
            307,
            586,
            257,
            272,
            731,
            321,
            366,
            516,
            51080
        ],
        "temperature": 0.0,
        "avg_logprob": -0.15318943560123444,
        "compression_ratio": 1.8028168678283691,
        "no_speech_prob": 2.840955858118832e-05
    },
    {
        "id": 252,
        "seek": 150544,
        "start": 1519.760009765625,
        "end": 1525.280029296875,
        "text": " to replace a b with a new token that we meant called y so y becomes a b and then every single",
        "tokens": [
            51080,
            281,
            7406,
            257,
            272,
            365,
            257,
            777,
            14862,
            300,
            321,
            4140,
            1219,
            288,
            370,
            288,
            3643,
            257,
            272,
            293,
            550,
            633,
            2167,
            51356
        ],
        "temperature": 0.0,
        "avg_logprob": -0.15318943560123444,
        "compression_ratio": 1.8028168678283691,
        "no_speech_prob": 2.840955858118832e-05
    },
    {
        "id": 253,
        "seek": 150544,
        "start": 1525.280029296875,
        "end": 1532.8800048828125,
        "text": " occurrence of a b is now replaced with y so we end up with this so now we only have one two three",
        "tokens": [
            51356,
            36122,
            295,
            257,
            272,
            307,
            586,
            10772,
            365,
            288,
            370,
            321,
            917,
            493,
            365,
            341,
            370,
            586,
            321,
            787,
            362,
            472,
            732,
            1045,
            51736
        ],
        "temperature": 0.0,
        "avg_logprob": -0.15318943560123444,
        "compression_ratio": 1.8028168678283691,
        "no_speech_prob": 2.840955858118832e-05
    },
    {
        "id": 254,
        "seek": 153288,
        "start": 1532.8800048828125,
        "end": 1541.3599853515625,
        "text": " four five six seven characters in our sequence but we have not just four vocabulary elements",
        "tokens": [
            50364,
            1451,
            1732,
            2309,
            3407,
            4342,
            294,
            527,
            8310,
            457,
            321,
            362,
            406,
            445,
            1451,
            19864,
            4959,
            50788
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17578125,
        "compression_ratio": 1.7441860437393188,
        "no_speech_prob": 0.00014653042308054864
    },
    {
        "id": 255,
        "seek": 153288,
        "start": 1541.3599853515625,
        "end": 1547.5999755859375,
        "text": " or five but now we have six and for the final round we again look through the sequence find",
        "tokens": [
            50788,
            420,
            1732,
            457,
            586,
            321,
            362,
            2309,
            293,
            337,
            264,
            2572,
            3098,
            321,
            797,
            574,
            807,
            264,
            8310,
            915,
            51100
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17578125,
        "compression_ratio": 1.7441860437393188,
        "no_speech_prob": 0.00014653042308054864
    },
    {
        "id": 256,
        "seek": 153288,
        "start": 1547.5999755859375,
        "end": 1554.239990234375,
        "text": " that the phrase z y or the pair z y is most common and replace it one more time with another",
        "tokens": [
            51100,
            300,
            264,
            9535,
            710,
            288,
            420,
            264,
            6119,
            710,
            288,
            307,
            881,
            2689,
            293,
            7406,
            309,
            472,
            544,
            565,
            365,
            1071,
            51432
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17578125,
        "compression_ratio": 1.7441860437393188,
        "no_speech_prob": 0.00014653042308054864
    },
    {
        "id": 257,
        "seek": 153288,
        "start": 1555.0400390625,
        "end": 1560.9599609375,
        "text": " character let's say x so x is z y and we replace all occurrences of z y and we get this following",
        "tokens": [
            51472,
            2517,
            718,
            311,
            584,
            2031,
            370,
            2031,
            307,
            710,
            288,
            293,
            321,
            7406,
            439,
            5160,
            38983,
            295,
            710,
            288,
            293,
            321,
            483,
            341,
            3480,
            51768
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17578125,
        "compression_ratio": 1.7441860437393188,
        "no_speech_prob": 0.00014653042308054864
    },
    {
        "id": 258,
        "seek": 156096,
        "start": 1560.9599609375,
        "end": 1566.239990234375,
        "text": " sequence so basically after we have gone through this process instead of having a",
        "tokens": [
            50364,
            8310,
            370,
            1936,
            934,
            321,
            362,
            2780,
            807,
            341,
            1399,
            2602,
            295,
            1419,
            257,
            50628
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1854248046875,
        "compression_ratio": 1.698113203048706,
        "no_speech_prob": 0.0001334180706180632
    },
    {
        "id": 259,
        "seek": 156096,
        "start": 1568.47998046875,
        "end": 1577.760009765625,
        "text": " sequence of 11 tokens with a vocabulary length of four we now have a sequence of one two three",
        "tokens": [
            50740,
            8310,
            295,
            2975,
            22667,
            365,
            257,
            19864,
            4641,
            295,
            1451,
            321,
            586,
            362,
            257,
            8310,
            295,
            472,
            732,
            1045,
            51204
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1854248046875,
        "compression_ratio": 1.698113203048706,
        "no_speech_prob": 0.0001334180706180632
    },
    {
        "id": 260,
        "seek": 156096,
        "start": 1578.3199462890625,
        "end": 1586.0,
        "text": " four five tokens but our vocabulary length now is seven and so in this way we can iteratively",
        "tokens": [
            51232,
            1451,
            1732,
            22667,
            457,
            527,
            19864,
            4641,
            586,
            307,
            3407,
            293,
            370,
            294,
            341,
            636,
            321,
            393,
            17138,
            19020,
            51616
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1854248046875,
        "compression_ratio": 1.698113203048706,
        "no_speech_prob": 0.0001334180706180632
    },
    {
        "id": 261,
        "seek": 158600,
        "start": 1586.0,
        "end": 1591.9200439453125,
        "text": " compress our sequence as we mint new tokens so in the in the exact same way we start we",
        "tokens": [
            50364,
            14778,
            527,
            8310,
            382,
            321,
            18189,
            777,
            22667,
            370,
            294,
            264,
            294,
            264,
            1900,
            912,
            636,
            321,
            722,
            321,
            50660
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1848466992378235,
        "compression_ratio": 1.8582676649093628,
        "no_speech_prob": 0.0003514315758366138
    },
    {
        "id": 262,
        "seek": 158600,
        "start": 1591.9200439453125,
        "end": 1598.6400146484375,
        "text": " start out with byte sequences so we have 256 vocabulary size but we're now going to go through",
        "tokens": [
            50660,
            722,
            484,
            365,
            40846,
            22978,
            370,
            321,
            362,
            38882,
            19864,
            2744,
            457,
            321,
            434,
            586,
            516,
            281,
            352,
            807,
            50996
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1848466992378235,
        "compression_ratio": 1.8582676649093628,
        "no_speech_prob": 0.0003514315758366138
    },
    {
        "id": 263,
        "seek": 158600,
        "start": 1598.6400146484375,
        "end": 1604.239990234375,
        "text": " these and find the byte pairs that occur the most and we're going to iteratively start minting new",
        "tokens": [
            50996,
            613,
            293,
            915,
            264,
            40846,
            15494,
            300,
            5160,
            264,
            881,
            293,
            321,
            434,
            516,
            281,
            17138,
            19020,
            722,
            18189,
            278,
            777,
            51276
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1848466992378235,
        "compression_ratio": 1.8582676649093628,
        "no_speech_prob": 0.0003514315758366138
    },
    {
        "id": 264,
        "seek": 158600,
        "start": 1604.239990234375,
        "end": 1609.0400390625,
        "text": " tokens appending them to our vocabulary and replacing things and in this way we're going",
        "tokens": [
            51276,
            22667,
            724,
            2029,
            552,
            281,
            527,
            19864,
            293,
            19139,
            721,
            293,
            294,
            341,
            636,
            321,
            434,
            516,
            51516
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1848466992378235,
        "compression_ratio": 1.8582676649093628,
        "no_speech_prob": 0.0003514315758366138
    },
    {
        "id": 265,
        "seek": 158600,
        "start": 1609.0400390625,
        "end": 1614.8800048828125,
        "text": " to end up with a compressed training data set and also an algorithm for taking any arbitrary sequence",
        "tokens": [
            51516,
            281,
            917,
            493,
            365,
            257,
            30353,
            3097,
            1412,
            992,
            293,
            611,
            364,
            9284,
            337,
            1940,
            604,
            23211,
            8310,
            51808
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1848466992378235,
        "compression_ratio": 1.8582676649093628,
        "no_speech_prob": 0.0003514315758366138
    },
    {
        "id": 266,
        "seek": 161488,
        "start": 1614.8800048828125,
        "end": 1621.6800537109375,
        "text": " and encoding it using this vocabulary and also decoding it back to strings so let's now implement",
        "tokens": [
            50364,
            293,
            43430,
            309,
            1228,
            341,
            19864,
            293,
            611,
            979,
            8616,
            309,
            646,
            281,
            13985,
            370,
            718,
            311,
            586,
            4445,
            50704
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16650883853435516,
        "compression_ratio": 1.730088472366333,
        "no_speech_prob": 0.000458303140476346
    },
    {
        "id": 267,
        "seek": 161488,
        "start": 1621.6800537109375,
        "end": 1627.8399658203125,
        "text": " all that so here's what i did i went to this blog post that i enjoyed and i took the first paragraph",
        "tokens": [
            50704,
            439,
            300,
            370,
            510,
            311,
            437,
            741,
            630,
            741,
            1437,
            281,
            341,
            6968,
            2183,
            300,
            741,
            4626,
            293,
            741,
            1890,
            264,
            700,
            18865,
            51012
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16650883853435516,
        "compression_ratio": 1.730088472366333,
        "no_speech_prob": 0.000458303140476346
    },
    {
        "id": 268,
        "seek": 161488,
        "start": 1627.8399658203125,
        "end": 1635.760009765625,
        "text": " and i copy pasted it here into text so this is one very long line here now to get the tokens",
        "tokens": [
            51012,
            293,
            741,
            5055,
            1791,
            292,
            309,
            510,
            666,
            2487,
            370,
            341,
            307,
            472,
            588,
            938,
            1622,
            510,
            586,
            281,
            483,
            264,
            22667,
            51408
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16650883853435516,
        "compression_ratio": 1.730088472366333,
        "no_speech_prob": 0.000458303140476346
    },
    {
        "id": 269,
        "seek": 161488,
        "start": 1635.760009765625,
        "end": 1640.719970703125,
        "text": " as i mentioned we just take our text and we encode it into utf-8 the tokens here at this point will",
        "tokens": [
            51408,
            382,
            741,
            2835,
            321,
            445,
            747,
            527,
            2487,
            293,
            321,
            2058,
            1429,
            309,
            666,
            2839,
            69,
            12,
            23,
            264,
            22667,
            510,
            412,
            341,
            935,
            486,
            51656
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16650883853435516,
        "compression_ratio": 1.730088472366333,
        "no_speech_prob": 0.000458303140476346
    },
    {
        "id": 270,
        "seek": 164072,
        "start": 1640.719970703125,
        "end": 1647.43994140625,
        "text": " be a raw bytes single stream of bytes and just so that it's easier to work with instead of just",
        "tokens": [
            50364,
            312,
            257,
            8936,
            36088,
            2167,
            4309,
            295,
            36088,
            293,
            445,
            370,
            300,
            309,
            311,
            3571,
            281,
            589,
            365,
            2602,
            295,
            445,
            50700
        ],
        "temperature": 0.0,
        "avg_logprob": -0.15814410150051117,
        "compression_ratio": 1.817307710647583,
        "no_speech_prob": 0.0002959560079034418
    },
    {
        "id": 271,
        "seek": 164072,
        "start": 1647.43994140625,
        "end": 1653.3599853515625,
        "text": " a bytes object i'm going to convert all those bytes to integers and then create a list of it",
        "tokens": [
            50700,
            257,
            36088,
            2657,
            741,
            478,
            516,
            281,
            7620,
            439,
            729,
            36088,
            281,
            41674,
            293,
            550,
            1884,
            257,
            1329,
            295,
            309,
            50996
        ],
        "temperature": 0.0,
        "avg_logprob": -0.15814410150051117,
        "compression_ratio": 1.817307710647583,
        "no_speech_prob": 0.0002959560079034418
    },
    {
        "id": 272,
        "seek": 164072,
        "start": 1653.3599853515625,
        "end": 1657.9200439453125,
        "text": " just so it's easier for us to manipulate and work with in python and visualize and here i'm printing",
        "tokens": [
            50996,
            445,
            370,
            309,
            311,
            3571,
            337,
            505,
            281,
            20459,
            293,
            589,
            365,
            294,
            38797,
            293,
            23273,
            293,
            510,
            741,
            478,
            14699,
            51224
        ],
        "temperature": 0.0,
        "avg_logprob": -0.15814410150051117,
        "compression_ratio": 1.817307710647583,
        "no_speech_prob": 0.0002959560079034418
    },
    {
        "id": 273,
        "seek": 164072,
        "start": 1657.9200439453125,
        "end": 1666.239990234375,
        "text": " all that so this is the original um this is the original paragraph and its length is 533",
        "tokens": [
            51224,
            439,
            300,
            370,
            341,
            307,
            264,
            3380,
            1105,
            341,
            307,
            264,
            3380,
            18865,
            293,
            1080,
            4641,
            307,
            1025,
            10191,
            51640
        ],
        "temperature": 0.0,
        "avg_logprob": -0.15814410150051117,
        "compression_ratio": 1.817307710647583,
        "no_speech_prob": 0.0002959560079034418
    },
    {
        "id": 274,
        "seek": 166624,
        "start": 1666.9599609375,
        "end": 1675.52001953125,
        "text": " code points and then here are the bytes encoded in utf-8 and we see that this has a length of 616",
        "tokens": [
            50400,
            3089,
            2793,
            293,
            550,
            510,
            366,
            264,
            36088,
            2058,
            12340,
            294,
            2839,
            69,
            12,
            23,
            293,
            321,
            536,
            300,
            341,
            575,
            257,
            4641,
            295,
            1386,
            6866,
            50828
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17496618628501892,
        "compression_ratio": 1.8047618865966797,
        "no_speech_prob": 0.0002492314961273223
    },
    {
        "id": 275,
        "seek": 166624,
        "start": 1675.52001953125,
        "end": 1681.9200439453125,
        "text": " bytes at this point or 616 tokens and the reason this is more is because a lot of these simple",
        "tokens": [
            50828,
            36088,
            412,
            341,
            935,
            420,
            1386,
            6866,
            22667,
            293,
            264,
            1778,
            341,
            307,
            544,
            307,
            570,
            257,
            688,
            295,
            613,
            2199,
            51148
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17496618628501892,
        "compression_ratio": 1.8047618865966797,
        "no_speech_prob": 0.0002492314961273223
    },
    {
        "id": 276,
        "seek": 166624,
        "start": 1681.9200439453125,
        "end": 1687.760009765625,
        "text": " ascii characters or simple characters they just become a single byte but a lot of these unicode",
        "tokens": [
            51148,
            382,
            537,
            72,
            4342,
            420,
            2199,
            4342,
            436,
            445,
            1813,
            257,
            2167,
            40846,
            457,
            257,
            688,
            295,
            613,
            517,
            299,
            1429,
            51440
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17496618628501892,
        "compression_ratio": 1.8047618865966797,
        "no_speech_prob": 0.0002492314961273223
    },
    {
        "id": 277,
        "seek": 166624,
        "start": 1687.760009765625,
        "end": 1692.47998046875,
        "text": " more complex characters become multiple bytes up to four and so we are expanding that size",
        "tokens": [
            51440,
            544,
            3997,
            4342,
            1813,
            3866,
            36088,
            493,
            281,
            1451,
            293,
            370,
            321,
            366,
            14702,
            300,
            2744,
            51676
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17496618628501892,
        "compression_ratio": 1.8047618865966797,
        "no_speech_prob": 0.0002492314961273223
    },
    {
        "id": 278,
        "seek": 169248,
        "start": 1692.800048828125,
        "end": 1696.47998046875,
        "text": " so now what we'd like to do as a first step of the algorithm is we'd like to iterate over here",
        "tokens": [
            50380,
            370,
            586,
            437,
            321,
            1116,
            411,
            281,
            360,
            382,
            257,
            700,
            1823,
            295,
            264,
            9284,
            307,
            321,
            1116,
            411,
            281,
            44497,
            670,
            510,
            50564
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2220650017261505,
        "compression_ratio": 1.891525387763977,
        "no_speech_prob": 0.00039203421329148114
    },
    {
        "id": 279,
        "seek": 169248,
        "start": 1696.47998046875,
        "end": 1702.3199462890625,
        "text": " and find the pair of bytes that occur most frequently because we're then going to merge it",
        "tokens": [
            50564,
            293,
            915,
            264,
            6119,
            295,
            36088,
            300,
            5160,
            881,
            10374,
            570,
            321,
            434,
            550,
            516,
            281,
            22183,
            309,
            50856
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2220650017261505,
        "compression_ratio": 1.891525387763977,
        "no_speech_prob": 0.00039203421329148114
    },
    {
        "id": 280,
        "seek": 169248,
        "start": 1702.8800048828125,
        "end": 1706.9599609375,
        "text": " so if you are working along on a notebook on a side then i encourage you to basically click",
        "tokens": [
            50884,
            370,
            498,
            291,
            366,
            1364,
            2051,
            322,
            257,
            21060,
            322,
            257,
            1252,
            550,
            741,
            5373,
            291,
            281,
            1936,
            2052,
            51088
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2220650017261505,
        "compression_ratio": 1.891525387763977,
        "no_speech_prob": 0.00039203421329148114
    },
    {
        "id": 281,
        "seek": 169248,
        "start": 1706.9599609375,
        "end": 1711.5999755859375,
        "text": " on the link find this notebook and try to write that function yourself otherwise i'm going to",
        "tokens": [
            51088,
            322,
            264,
            2113,
            915,
            341,
            21060,
            293,
            853,
            281,
            2464,
            300,
            2445,
            1803,
            5911,
            741,
            478,
            516,
            281,
            51320
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2220650017261505,
        "compression_ratio": 1.891525387763977,
        "no_speech_prob": 0.00039203421329148114
    },
    {
        "id": 282,
        "seek": 169248,
        "start": 1711.5999755859375,
        "end": 1716.0,
        "text": " come here and implement first the function that finds the most common pair okay so here's what",
        "tokens": [
            51320,
            808,
            510,
            293,
            4445,
            700,
            264,
            2445,
            300,
            10704,
            264,
            881,
            2689,
            6119,
            1392,
            370,
            510,
            311,
            437,
            51540
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2220650017261505,
        "compression_ratio": 1.891525387763977,
        "no_speech_prob": 0.00039203421329148114
    },
    {
        "id": 283,
        "seek": 169248,
        "start": 1716.0,
        "end": 1719.760009765625,
        "text": " i came up with there are many different ways to implement this but i'm calling the function",
        "tokens": [
            51540,
            741,
            1361,
            493,
            365,
            456,
            366,
            867,
            819,
            2098,
            281,
            4445,
            341,
            457,
            741,
            478,
            5141,
            264,
            2445,
            51728
        ],
        "temperature": 0.0,
        "avg_logprob": -0.2220650017261505,
        "compression_ratio": 1.891525387763977,
        "no_speech_prob": 0.00039203421329148114
    },
    {
        "id": 284,
        "seek": 171976,
        "start": 1719.8399658203125,
        "end": 1724.800048828125,
        "text": " but i'm calling the function get stats it expects a list of integers i'm using a dictionary to keep",
        "tokens": [
            50368,
            457,
            741,
            478,
            5141,
            264,
            2445,
            483,
            18152,
            309,
            33280,
            257,
            1329,
            295,
            41674,
            741,
            478,
            1228,
            257,
            25890,
            281,
            1066,
            50616
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1925172060728073,
        "compression_ratio": 1.9153225421905518,
        "no_speech_prob": 0.0005614717956632376
    },
    {
        "id": 285,
        "seek": 171976,
        "start": 1724.800048828125,
        "end": 1730.1600341796875,
        "text": " track of basically the counts and then this is a pythonic way to iterate consecutive elements",
        "tokens": [
            50616,
            2837,
            295,
            1936,
            264,
            14893,
            293,
            550,
            341,
            307,
            257,
            10664,
            392,
            11630,
            636,
            281,
            44497,
            30497,
            4959,
            50884
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1925172060728073,
        "compression_ratio": 1.9153225421905518,
        "no_speech_prob": 0.0005614717956632376
    },
    {
        "id": 286,
        "seek": 171976,
        "start": 1730.1600341796875,
        "end": 1736.0,
        "text": " of this list which we covered in the previous video and then here i'm just keeping track of",
        "tokens": [
            50884,
            295,
            341,
            1329,
            597,
            321,
            5343,
            294,
            264,
            3894,
            960,
            293,
            550,
            510,
            741,
            478,
            445,
            5145,
            2837,
            295,
            51176
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1925172060728073,
        "compression_ratio": 1.9153225421905518,
        "no_speech_prob": 0.0005614717956632376
    },
    {
        "id": 287,
        "seek": 171976,
        "start": 1736.0,
        "end": 1742.800048828125,
        "text": " just incrementing by one for all the pairs so if i call this on all the tokens here then the stats",
        "tokens": [
            51176,
            445,
            26200,
            278,
            538,
            472,
            337,
            439,
            264,
            15494,
            370,
            498,
            741,
            818,
            341,
            322,
            439,
            264,
            22667,
            510,
            550,
            264,
            18152,
            51516
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1925172060728073,
        "compression_ratio": 1.9153225421905518,
        "no_speech_prob": 0.0005614717956632376
    },
    {
        "id": 288,
        "seek": 171976,
        "start": 1742.800048828125,
        "end": 1749.52001953125,
        "text": " comes out here so this is the dictionary the keys are these tuples of consecutive elements",
        "tokens": [
            51516,
            1487,
            484,
            510,
            370,
            341,
            307,
            264,
            25890,
            264,
            9317,
            366,
            613,
            2604,
            2622,
            295,
            30497,
            4959,
            51852
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1925172060728073,
        "compression_ratio": 1.9153225421905518,
        "no_speech_prob": 0.0005614717956632376
    },
    {
        "id": 289,
        "seek": 174952,
        "start": 1749.52001953125,
        "end": 1756.239990234375,
        "text": " and this is the count so just to print it in a slightly better way this is one way that i like to",
        "tokens": [
            50364,
            293,
            341,
            307,
            264,
            1207,
            370,
            445,
            281,
            4482,
            309,
            294,
            257,
            4748,
            1101,
            636,
            341,
            307,
            472,
            636,
            300,
            741,
            411,
            281,
            50700
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16891446709632874,
        "compression_ratio": 1.7860465049743652,
        "no_speech_prob": 5.225189670454711e-05
    },
    {
        "id": 290,
        "seek": 174952,
        "start": 1756.239990234375,
        "end": 1763.1199951171875,
        "text": " do that where you it's a little bit compound here so you can pause if you like but we iterate all",
        "tokens": [
            50700,
            360,
            300,
            689,
            291,
            309,
            311,
            257,
            707,
            857,
            14154,
            510,
            370,
            291,
            393,
            10465,
            498,
            291,
            411,
            457,
            321,
            44497,
            439,
            51044
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16891446709632874,
        "compression_ratio": 1.7860465049743652,
        "no_speech_prob": 5.225189670454711e-05
    },
    {
        "id": 291,
        "seek": 174952,
        "start": 1763.1199951171875,
        "end": 1771.8399658203125,
        "text": " the items the items called on dictionary returns pairs of key value and instead i create a list",
        "tokens": [
            51044,
            264,
            4754,
            264,
            4754,
            1219,
            322,
            25890,
            11247,
            15494,
            295,
            2141,
            2158,
            293,
            2602,
            741,
            1884,
            257,
            1329,
            51480
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16891446709632874,
        "compression_ratio": 1.7860465049743652,
        "no_speech_prob": 5.225189670454711e-05
    },
    {
        "id": 292,
        "seek": 174952,
        "start": 1771.8399658203125,
        "end": 1779.1199951171875,
        "text": " here of value key because if it's a value key list then i can call sort on it and by default",
        "tokens": [
            51480,
            510,
            295,
            2158,
            2141,
            570,
            498,
            309,
            311,
            257,
            2158,
            2141,
            1329,
            550,
            741,
            393,
            818,
            1333,
            322,
            309,
            293,
            538,
            7576,
            51844
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16891446709632874,
        "compression_ratio": 1.7860465049743652,
        "no_speech_prob": 5.225189670454711e-05
    },
    {
        "id": 293,
        "seek": 177912,
        "start": 1779.1199951171875,
        "end": 1785.9200439453125,
        "text": " python will use the first element which in this case will be value to sort by if it's given tuples",
        "tokens": [
            50364,
            38797,
            486,
            764,
            264,
            700,
            4478,
            597,
            294,
            341,
            1389,
            486,
            312,
            2158,
            281,
            1333,
            538,
            498,
            309,
            311,
            2212,
            2604,
            2622,
            50704
        ],
        "temperature": 0.0,
        "avg_logprob": -0.210281103849411,
        "compression_ratio": 1.639830470085144,
        "no_speech_prob": 3.763626955333166e-05
    },
    {
        "id": 294,
        "seek": 177912,
        "start": 1786.56005859375,
        "end": 1792.9599609375,
        "text": " and then reverse so it's descending and print that so basically it looks like 101,32 was the",
        "tokens": [
            50736,
            293,
            550,
            9943,
            370,
            309,
            311,
            40182,
            293,
            4482,
            300,
            370,
            1936,
            309,
            1542,
            411,
            21055,
            11,
            11440,
            390,
            264,
            51056
        ],
        "temperature": 0.0,
        "avg_logprob": -0.210281103849411,
        "compression_ratio": 1.639830470085144,
        "no_speech_prob": 3.763626955333166e-05
    },
    {
        "id": 295,
        "seek": 177912,
        "start": 1792.9599609375,
        "end": 1798.8800048828125,
        "text": " most commonly occurring consecutive pair and it occurred 20 times we can double check that that",
        "tokens": [
            51056,
            881,
            12719,
            18386,
            30497,
            6119,
            293,
            309,
            11068,
            945,
            1413,
            321,
            393,
            3834,
            1520,
            300,
            300,
            51352
        ],
        "temperature": 0.0,
        "avg_logprob": -0.210281103849411,
        "compression_ratio": 1.639830470085144,
        "no_speech_prob": 3.763626955333166e-05
    },
    {
        "id": 296,
        "seek": 177912,
        "start": 1798.8800048828125,
        "end": 1806.3199462890625,
        "text": " makes reasonable sense so if i just search 101,32 then you see that these are the 20 occurrences of",
        "tokens": [
            51352,
            1669,
            10585,
            2020,
            370,
            498,
            741,
            445,
            3164,
            21055,
            11,
            11440,
            550,
            291,
            536,
            300,
            613,
            366,
            264,
            945,
            5160,
            38983,
            295,
            51724
        ],
        "temperature": 0.0,
        "avg_logprob": -0.210281103849411,
        "compression_ratio": 1.639830470085144,
        "no_speech_prob": 3.763626955333166e-05
    },
    {
        "id": 297,
        "seek": 180632,
        "start": 1806.3199462890625,
        "end": 1814.3199462890625,
        "text": " that pair and if we'd like to take a look at what exactly that pair is we can use char which is the",
        "tokens": [
            50364,
            300,
            6119,
            293,
            498,
            321,
            1116,
            411,
            281,
            747,
            257,
            574,
            412,
            437,
            2293,
            300,
            6119,
            307,
            321,
            393,
            764,
            1290,
            597,
            307,
            264,
            50764
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17010612785816193,
        "compression_ratio": 1.840375542640686,
        "no_speech_prob": 0.00010554632171988487
    },
    {
        "id": 298,
        "seek": 180632,
        "start": 1814.3199462890625,
        "end": 1823.0400390625,
        "text": " opposite of ord in python so we give it a unicode code point so 101 and of 32 and we see that this",
        "tokens": [
            50764,
            6182,
            295,
            4792,
            294,
            38797,
            370,
            321,
            976,
            309,
            257,
            517,
            299,
            1429,
            3089,
            935,
            370,
            21055,
            293,
            295,
            8858,
            293,
            321,
            536,
            300,
            341,
            51200
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17010612785816193,
        "compression_ratio": 1.840375542640686,
        "no_speech_prob": 0.00010554632171988487
    },
    {
        "id": 299,
        "seek": 180632,
        "start": 1823.0400390625,
        "end": 1829.52001953125,
        "text": " is e and space so basically there's a lot of e space here meaning that a lot of these words seem",
        "tokens": [
            51200,
            307,
            308,
            293,
            1901,
            370,
            1936,
            456,
            311,
            257,
            688,
            295,
            308,
            1901,
            510,
            3620,
            300,
            257,
            688,
            295,
            613,
            2283,
            1643,
            51524
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17010612785816193,
        "compression_ratio": 1.840375542640686,
        "no_speech_prob": 0.00010554632171988487
    },
    {
        "id": 300,
        "seek": 180632,
        "start": 1829.52001953125,
        "end": 1834.9599609375,
        "text": " to end with e so here's e space as an example so there's a lot of that going on here and this is",
        "tokens": [
            51524,
            281,
            917,
            365,
            308,
            370,
            510,
            311,
            308,
            1901,
            382,
            364,
            1365,
            370,
            456,
            311,
            257,
            688,
            295,
            300,
            516,
            322,
            510,
            293,
            341,
            307,
            51796
        ],
        "temperature": 0.0,
        "avg_logprob": -0.17010612785816193,
        "compression_ratio": 1.840375542640686,
        "no_speech_prob": 0.00010554632171988487
    },
    {
        "id": 301,
        "seek": 183496,
        "start": 1834.9599609375,
        "end": 1840.56005859375,
        "text": " the most common pair so now that we've identified the most common pair we would like to iterate over",
        "tokens": [
            50364,
            264,
            881,
            2689,
            6119,
            370,
            586,
            300,
            321,
            600,
            9234,
            264,
            881,
            2689,
            6119,
            321,
            576,
            411,
            281,
            44497,
            670,
            50644
        ],
        "temperature": 0.0,
        "avg_logprob": -0.203291118144989,
        "compression_ratio": 1.810185194015503,
        "no_speech_prob": 0.0002653015253599733
    },
    {
        "id": 302,
        "seek": 183496,
        "start": 1840.56005859375,
        "end": 1847.8399658203125,
        "text": " this sequence we're going to mint a new token with the id of 256 right because these tokens currently",
        "tokens": [
            50644,
            341,
            8310,
            321,
            434,
            516,
            281,
            18189,
            257,
            777,
            14862,
            365,
            264,
            4496,
            295,
            38882,
            558,
            570,
            613,
            22667,
            4362,
            51008
        ],
        "temperature": 0.0,
        "avg_logprob": -0.203291118144989,
        "compression_ratio": 1.810185194015503,
        "no_speech_prob": 0.0002653015253599733
    },
    {
        "id": 303,
        "seek": 183496,
        "start": 1847.8399658203125,
        "end": 1855.43994140625,
        "text": " go from 0 to 255 so when we create a new token it will have an id of 256 and we're going to iterate",
        "tokens": [
            51008,
            352,
            490,
            1958,
            281,
            3552,
            20,
            370,
            562,
            321,
            1884,
            257,
            777,
            14862,
            309,
            486,
            362,
            364,
            4496,
            295,
            38882,
            293,
            321,
            434,
            516,
            281,
            44497,
            51388
        ],
        "temperature": 0.0,
        "avg_logprob": -0.203291118144989,
        "compression_ratio": 1.810185194015503,
        "no_speech_prob": 0.0002653015253599733
    },
    {
        "id": 304,
        "seek": 183496,
        "start": 1855.43994140625,
        "end": 1863.199951171875,
        "text": " over this entire um list and every every time we see 101,32 we're going to swap that out",
        "tokens": [
            51388,
            670,
            341,
            2302,
            1105,
            1329,
            293,
            633,
            633,
            565,
            321,
            536,
            21055,
            11,
            11440,
            321,
            434,
            516,
            281,
            18135,
            300,
            484,
            51776
        ],
        "temperature": 0.0,
        "avg_logprob": -0.203291118144989,
        "compression_ratio": 1.810185194015503,
        "no_speech_prob": 0.0002653015253599733
    },
    {
        "id": 305,
        "seek": 186320,
        "start": 1863.199951171875,
        "end": 1871.199951171875,
        "text": " for 256 so let's implement that now and feel free to do that yourself as well so first i commented",
        "tokens": [
            50364,
            337,
            38882,
            370,
            718,
            311,
            4445,
            300,
            586,
            293,
            841,
            1737,
            281,
            360,
            300,
            1803,
            382,
            731,
            370,
            700,
            741,
            26940,
            50764
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19181033968925476,
        "compression_ratio": 1.6460176706314087,
        "no_speech_prob": 0.00023413446615450084
    },
    {
        "id": 306,
        "seek": 186320,
        "start": 1871.199951171875,
        "end": 1878.6400146484375,
        "text": " this just so we don't pollute the notebook too much this is a nice way of in python obtaining",
        "tokens": [
            50764,
            341,
            445,
            370,
            321,
            500,
            380,
            6418,
            1169,
            264,
            21060,
            886,
            709,
            341,
            307,
            257,
            1481,
            636,
            295,
            294,
            38797,
            36749,
            51136
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19181033968925476,
        "compression_ratio": 1.6460176706314087,
        "no_speech_prob": 0.00023413446615450084
    },
    {
        "id": 307,
        "seek": 186320,
        "start": 1878.6400146484375,
        "end": 1885.0400390625,
        "text": " the highest ranking pair so we're basically calling the max on this dictionary stats and",
        "tokens": [
            51136,
            264,
            6343,
            17833,
            6119,
            370,
            321,
            434,
            1936,
            5141,
            264,
            11469,
            322,
            341,
            25890,
            18152,
            293,
            51456
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19181033968925476,
        "compression_ratio": 1.6460176706314087,
        "no_speech_prob": 0.00023413446615450084
    },
    {
        "id": 308,
        "seek": 186320,
        "start": 1885.0400390625,
        "end": 1892.0799560546875,
        "text": " this will return the maximum key and then the question is how does it rank keys so you can",
        "tokens": [
            51456,
            341,
            486,
            2736,
            264,
            6674,
            2141,
            293,
            550,
            264,
            1168,
            307,
            577,
            775,
            309,
            6181,
            9317,
            370,
            291,
            393,
            51808
        ],
        "temperature": 0.0,
        "avg_logprob": -0.19181033968925476,
        "compression_ratio": 1.6460176706314087,
        "no_speech_prob": 0.00023413446615450084
    },
    {
        "id": 309,
        "seek": 189208,
        "start": 1892.0799560546875,
        "end": 1898.1600341796875,
        "text": " provide it with a function that ranks keys and that function is just stats that get stats that",
        "tokens": [
            50364,
            2893,
            309,
            365,
            257,
            2445,
            300,
            21406,
            9317,
            293,
            300,
            2445,
            307,
            445,
            18152,
            300,
            483,
            18152,
            300,
            50668
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1919974833726883,
        "compression_ratio": 1.8093023300170898,
        "no_speech_prob": 4.9086393119068816e-05
    },
    {
        "id": 310,
        "seek": 189208,
        "start": 1898.1600341796875,
        "end": 1904.47998046875,
        "text": " get would basically return the value and so we're ranking by the value and getting the maximum key",
        "tokens": [
            50668,
            483,
            576,
            1936,
            2736,
            264,
            2158,
            293,
            370,
            321,
            434,
            17833,
            538,
            264,
            2158,
            293,
            1242,
            264,
            6674,
            2141,
            50984
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1919974833726883,
        "compression_ratio": 1.8093023300170898,
        "no_speech_prob": 4.9086393119068816e-05
    },
    {
        "id": 311,
        "seek": 189208,
        "start": 1904.47998046875,
        "end": 1912.47998046875,
        "text": " so it's 101,32 as we saw now to actually merge 101,32 this is the function that i wrote but again",
        "tokens": [
            50984,
            370,
            309,
            311,
            21055,
            11,
            11440,
            382,
            321,
            1866,
            586,
            281,
            767,
            22183,
            21055,
            11,
            11440,
            341,
            307,
            264,
            2445,
            300,
            741,
            4114,
            457,
            797,
            51384
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1919974833726883,
        "compression_ratio": 1.8093023300170898,
        "no_speech_prob": 4.9086393119068816e-05
    },
    {
        "id": 312,
        "seek": 189208,
        "start": 1912.47998046875,
        "end": 1917.6800537109375,
        "text": " there are many different versions of it so we're going to take a list of ids and the pair that we",
        "tokens": [
            51384,
            456,
            366,
            867,
            819,
            9606,
            295,
            309,
            370,
            321,
            434,
            516,
            281,
            747,
            257,
            1329,
            295,
            220,
            3742,
            293,
            264,
            6119,
            300,
            321,
            51644
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1919974833726883,
        "compression_ratio": 1.8093023300170898,
        "no_speech_prob": 4.9086393119068816e-05
    },
    {
        "id": 313,
        "seek": 191768,
        "start": 1917.6800537109375,
        "end": 1924.9599609375,
        "text": " want to replace and that pair will be replaced with the new index idx so iterating through ids",
        "tokens": [
            50364,
            528,
            281,
            7406,
            293,
            300,
            6119,
            486,
            312,
            10772,
            365,
            264,
            777,
            8186,
            4496,
            87,
            370,
            17138,
            990,
            807,
            220,
            3742,
            50728
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1861979216337204,
        "compression_ratio": 1.8940677642822266,
        "no_speech_prob": 0.0029348921962082386
    },
    {
        "id": 314,
        "seek": 191768,
        "start": 1924.9599609375,
        "end": 1931.0400390625,
        "text": " if we find the pair swap it out for idx so we create this new list and then we start at zero",
        "tokens": [
            50728,
            498,
            321,
            915,
            264,
            6119,
            18135,
            309,
            484,
            337,
            4496,
            87,
            370,
            321,
            1884,
            341,
            777,
            1329,
            293,
            550,
            321,
            722,
            412,
            4018,
            51032
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1861979216337204,
        "compression_ratio": 1.8940677642822266,
        "no_speech_prob": 0.0029348921962082386
    },
    {
        "id": 315,
        "seek": 191768,
        "start": 1931.5999755859375,
        "end": 1934.56005859375,
        "text": " and then we go through this entire list sequentially from left to right",
        "tokens": [
            51060,
            293,
            550,
            321,
            352,
            807,
            341,
            2302,
            1329,
            5123,
            3137,
            490,
            1411,
            281,
            558,
            51208
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1861979216337204,
        "compression_ratio": 1.8940677642822266,
        "no_speech_prob": 0.0029348921962082386
    },
    {
        "id": 316,
        "seek": 191768,
        "start": 1935.760009765625,
        "end": 1942.800048828125,
        "text": " and here we are checking for equality at the current position with the pair so here we are",
        "tokens": [
            51268,
            293,
            510,
            321,
            366,
            8568,
            337,
            14949,
            412,
            264,
            2190,
            2535,
            365,
            264,
            6119,
            370,
            510,
            321,
            366,
            51620
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1861979216337204,
        "compression_ratio": 1.8940677642822266,
        "no_speech_prob": 0.0029348921962082386
    },
    {
        "id": 317,
        "seek": 191768,
        "start": 1942.800048828125,
        "end": 1947.3599853515625,
        "text": " checking that the pair matches now here's a bit of a tricky condition that you have to append if",
        "tokens": [
            51620,
            8568,
            300,
            264,
            6119,
            10676,
            586,
            510,
            311,
            257,
            857,
            295,
            257,
            12414,
            4188,
            300,
            291,
            362,
            281,
            34116,
            498,
            51848
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1861979216337204,
        "compression_ratio": 1.8940677642822266,
        "no_speech_prob": 0.0029348921962082386
    },
    {
        "id": 318,
        "seek": 194736,
        "start": 1947.3599853515625,
        "end": 1953.199951171875,
        "text": " you're trying to be careful and that is that you don't want this here to be out of bounds at the",
        "tokens": [
            50364,
            291,
            434,
            1382,
            281,
            312,
            5026,
            293,
            300,
            307,
            300,
            291,
            500,
            380,
            528,
            341,
            510,
            281,
            312,
            484,
            295,
            29905,
            412,
            264,
            50656
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16399456560611725,
        "compression_ratio": 1.9395161867141724,
        "no_speech_prob": 5.225185304880142e-05
    },
    {
        "id": 319,
        "seek": 194736,
        "start": 1953.199951171875,
        "end": 1957.9200439453125,
        "text": " very last position when you're on the rightmost element of this list otherwise this would give",
        "tokens": [
            50656,
            588,
            1036,
            2535,
            562,
            291,
            434,
            322,
            264,
            558,
            1761,
            4478,
            295,
            341,
            1329,
            5911,
            341,
            576,
            976,
            50892
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16399456560611725,
        "compression_ratio": 1.9395161867141724,
        "no_speech_prob": 5.225185304880142e-05
    },
    {
        "id": 320,
        "seek": 194736,
        "start": 1957.9200439453125,
        "end": 1962.800048828125,
        "text": " you an out of bounds error so we have to make sure that we're not at the very very last element",
        "tokens": [
            50892,
            291,
            364,
            484,
            295,
            29905,
            6713,
            370,
            321,
            362,
            281,
            652,
            988,
            300,
            321,
            434,
            406,
            412,
            264,
            588,
            588,
            1036,
            4478,
            51136
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16399456560611725,
        "compression_ratio": 1.9395161867141724,
        "no_speech_prob": 5.225185304880142e-05
    },
    {
        "id": 321,
        "seek": 194736,
        "start": 1962.800048828125,
        "end": 1971.6800537109375,
        "text": " so this would be false for that so if we find a match we append to this new list that replacement",
        "tokens": [
            51136,
            370,
            341,
            576,
            312,
            7908,
            337,
            300,
            370,
            498,
            321,
            915,
            257,
            2995,
            321,
            34116,
            281,
            341,
            777,
            1329,
            300,
            14419,
            51580
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16399456560611725,
        "compression_ratio": 1.9395161867141724,
        "no_speech_prob": 5.225185304880142e-05
    },
    {
        "id": 322,
        "seek": 194736,
        "start": 1971.6800537109375,
        "end": 1977.0400390625,
        "text": " index and we increment the position by two so we skip over that entire pair but otherwise if we",
        "tokens": [
            51580,
            8186,
            293,
            321,
            26200,
            264,
            2535,
            538,
            732,
            370,
            321,
            10023,
            670,
            300,
            2302,
            6119,
            457,
            5911,
            498,
            321,
            51848
        ],
        "temperature": 0.0,
        "avg_logprob": -0.16399456560611725,
        "compression_ratio": 1.9395161867141724,
        "no_speech_prob": 5.225185304880142e-05
    },
    {
        "id": 323,
        "seek": 197704,
        "start": 1977.52001953125,
        "end": 1984.8800048828125,
        "text": " found a matching pair we just sort of copy over the element at that position and increment by one",
        "tokens": [
            50388,
            1352,
            257,
            14324,
            6119,
            321,
            445,
            1333,
            295,
            5055,
            670,
            264,
            4478,
            412,
            300,
            2535,
            293,
            26200,
            538,
            472,
            50756
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21611842513084412,
        "compression_ratio": 1.6899563074111938,
        "no_speech_prob": 0.0002824057883117348
    },
    {
        "id": 324,
        "seek": 197704,
        "start": 1984.8800048828125,
        "end": 1989.6800537109375,
        "text": " and then return this so here's a very small toy example if we have a list five six six seven nine",
        "tokens": [
            50756,
            293,
            550,
            2736,
            341,
            370,
            510,
            311,
            257,
            588,
            1359,
            12058,
            1365,
            498,
            321,
            362,
            257,
            1329,
            1732,
            2309,
            2309,
            3407,
            4949,
            50996
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21611842513084412,
        "compression_ratio": 1.6899563074111938,
        "no_speech_prob": 0.0002824057883117348
    },
    {
        "id": 325,
        "seek": 197704,
        "start": 1989.6800537109375,
        "end": 1997.199951171875,
        "text": " one and we want to replace the occurrences of 67 with 99 then calling this on that will give us",
        "tokens": [
            50996,
            472,
            293,
            321,
            528,
            281,
            7406,
            264,
            5160,
            38983,
            295,
            23879,
            365,
            11803,
            550,
            5141,
            341,
            322,
            300,
            486,
            976,
            505,
            51372
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21611842513084412,
        "compression_ratio": 1.6899563074111938,
        "no_speech_prob": 0.0002824057883117348
    },
    {
        "id": 326,
        "seek": 197704,
        "start": 1997.199951171875,
        "end": 2004.0,
        "text": " what we're asking for so here the 67 is replaced with 99 so now i'm going to uncomment this for",
        "tokens": [
            51372,
            437,
            321,
            434,
            3365,
            337,
            370,
            510,
            264,
            23879,
            307,
            10772,
            365,
            11803,
            370,
            586,
            741,
            478,
            516,
            281,
            8585,
            518,
            341,
            337,
            51712
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21611842513084412,
        "compression_ratio": 1.6899563074111938,
        "no_speech_prob": 0.0002824057883117348
    },
    {
        "id": 327,
        "seek": 200400,
        "start": 2004.0,
        "end": 2011.1199951171875,
        "text": " our actual use case where we want to take our tokens we want to take the top pair here and",
        "tokens": [
            50364,
            527,
            3539,
            764,
            1389,
            689,
            321,
            528,
            281,
            747,
            527,
            22667,
            321,
            528,
            281,
            747,
            264,
            1192,
            6119,
            510,
            293,
            50720
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1746012419462204,
        "compression_ratio": 1.6271185874938965,
        "no_speech_prob": 9.915213013300672e-05
    },
    {
        "id": 328,
        "seek": 200400,
        "start": 2011.1199951171875,
        "end": 2020.0799560546875,
        "text": " replace it with 256 to get tokens too if we run this we get the following so recall that previously",
        "tokens": [
            50720,
            7406,
            309,
            365,
            38882,
            281,
            483,
            22667,
            886,
            498,
            321,
            1190,
            341,
            321,
            483,
            264,
            3480,
            370,
            9901,
            300,
            8046,
            51168
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1746012419462204,
        "compression_ratio": 1.6271185874938965,
        "no_speech_prob": 9.915213013300672e-05
    },
    {
        "id": 329,
        "seek": 200400,
        "start": 2020.719970703125,
        "end": 2029.760009765625,
        "text": " we had a length 616 in this list and now we have a length 596 right so this decreased by 20 which",
        "tokens": [
            51200,
            321,
            632,
            257,
            4641,
            1386,
            6866,
            294,
            341,
            1329,
            293,
            586,
            321,
            362,
            257,
            4641,
            1025,
            22962,
            558,
            370,
            341,
            24436,
            538,
            945,
            597,
            51652
        ],
        "temperature": 0.0,
        "avg_logprob": -0.1746012419462204,
        "compression_ratio": 1.6271185874938965,
        "no_speech_prob": 9.915213013300672e-05
    },
    {
        "id": 330,
        "seek": 202976,
        "start": 2029.760009765625,
        "end": 2035.6800537109375,
        "text": " makes sense because there are 20 occurrences moreover we can try to find 256 here and we",
        "tokens": [
            50364,
            1669,
            2020,
            570,
            456,
            366,
            945,
            5160,
            38983,
            544,
            3570,
            321,
            393,
            853,
            281,
            915,
            38882,
            510,
            293,
            321,
            50660
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18078398704528809,
        "compression_ratio": 1.8538461923599243,
        "no_speech_prob": 0.00020662584574893117
    },
    {
        "id": 331,
        "seek": 202976,
        "start": 2035.6800537109375,
        "end": 2040.800048828125,
        "text": " see plenty of occurrences of it and moreover just double check there should be no occurrence of",
        "tokens": [
            50660,
            536,
            7140,
            295,
            5160,
            38983,
            295,
            309,
            293,
            544,
            3570,
            445,
            3834,
            1520,
            456,
            820,
            312,
            572,
            36122,
            295,
            50916
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18078398704528809,
        "compression_ratio": 1.8538461923599243,
        "no_speech_prob": 0.00020662584574893117
    },
    {
        "id": 332,
        "seek": 202976,
        "start": 2040.800048828125,
        "end": 2046.719970703125,
        "text": " 10132 so this is the original array plenty of them and in the second array there are no occurrences",
        "tokens": [
            50916,
            21055,
            11440,
            370,
            341,
            307,
            264,
            3380,
            10225,
            7140,
            295,
            552,
            293,
            294,
            264,
            1150,
            10225,
            456,
            366,
            572,
            5160,
            38983,
            51212
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18078398704528809,
        "compression_ratio": 1.8538461923599243,
        "no_speech_prob": 0.00020662584574893117
    },
    {
        "id": 333,
        "seek": 202976,
        "start": 2046.719970703125,
        "end": 2054.080078125,
        "text": " of 1132 so we've successfully merged this single pair and now we just iterate this so we are going",
        "tokens": [
            51212,
            295,
            502,
            7668,
            17,
            370,
            321,
            600,
            10727,
            36427,
            341,
            2167,
            6119,
            293,
            586,
            321,
            445,
            44497,
            341,
            370,
            321,
            366,
            516,
            51580
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18078398704528809,
        "compression_ratio": 1.8538461923599243,
        "no_speech_prob": 0.00020662584574893117
    },
    {
        "id": 334,
        "seek": 202976,
        "start": 2054.080078125,
        "end": 2058.800048828125,
        "text": " to go over the sequence again find the most common pair and replace it so let me now write a while",
        "tokens": [
            51580,
            281,
            352,
            670,
            264,
            8310,
            797,
            915,
            264,
            881,
            2689,
            6119,
            293,
            7406,
            309,
            370,
            718,
            385,
            586,
            2464,
            257,
            1339,
            51816
        ],
        "temperature": 0.0,
        "avg_logprob": -0.18078398704528809,
        "compression_ratio": 1.8538461923599243,
        "no_speech_prob": 0.00020662584574893117
    },
    {
        "id": 335,
        "seek": 205880,
        "start": 2058.800048828125,
        "end": 2063.360107421875,
        "text": " loop that uses these functions to do this sort of iteratively and how many",
        "tokens": [
            50364,
            6367,
            300,
            4960,
            613,
            6828,
            281,
            360,
            341,
            1333,
            295,
            17138,
            19020,
            293,
            577,
            867,
            50592
        ],
        "temperature": 0.0,
        "avg_logprob": -0.21405713260173798,
        "compression_ratio": 1.0882352590560913,
        "no_speech_prob": 0.11418918520212173
    }
]