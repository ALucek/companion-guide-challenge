[
    {
        "id": 0,
        "start": 0.0,
        "end": 4.320000171661377,
        "text": " Hi everyone. So in this video I'd like us to cover the process of tokenization in large"
    },
    {
        "id": 1,
        "start": 4.320000171661377,
        "end": 10.079999923706055,
        "text": " language models. Now you see here that I have a sad face and that's because tokenization is my"
    },
    {
        "id": 2,
        "start": 10.079999923706055,
        "end": 14.079999923706055,
        "text": " least favorite part of working with large language models but unfortunately it is necessary to"
    },
    {
        "id": 3,
        "start": 14.079999923706055,
        "end": 18.639999389648438,
        "text": " understand in some detail because it is fairly hairy, gnarly and there's a lot of hidden foot"
    },
    {
        "id": 4,
        "start": 18.639999389648438,
        "end": 23.84000015258789,
        "text": " gums to be aware of and a lot of oddness with large language models typically traces back"
    },
    {
        "id": 5,
        "start": 23.84000015258789,
        "end": 30.399999618530273,
        "text": " to tokenization. So what is tokenization? Now in my previous video Let's Build GPT from Scratch"
    },
    {
        "id": 6,
        "start": 31.440000534057617,
        "end": 37.040000915527344,
        "text": " we actually already did tokenization but we did a very naive simple version of tokenization. So"
    },
    {
        "id": 7,
        "start": 37.040000915527344,
        "end": 42.560001373291016,
        "text": " when you go to the Google Colab for that video you see here that we loaded our training set"
    },
    {
        "id": 8,
        "start": 43.119998931884766,
        "end": 49.119998931884766,
        "text": " and our training set was this Shakespeare dataset. Now in the beginning the Shakespeare dataset is"
    },
    {
        "id": 9,
        "start": 49.119998931884766,
        "end": 54.959999084472656,
        "text": " just a large string in Python it's just text and so the question is how do we plug text into"
    },
    {
        "id": 10,
        "start": 54.959999084472656,
        "end": 62.63999938964844,
        "text": " large language models and in this case here we created a vocabulary of 65 possible characters"
    },
    {
        "id": 11,
        "start": 62.63999938964844,
        "end": 68.0,
        "text": " that we saw occur in this string. These were the possible characters and we saw that there are 65"
    },
    {
        "id": 12,
        "start": 68.0,
        "end": 74.31999969482422,
        "text": " of them and then we created a lookup table for converting from every possible character a little"
    },
    {
        "id": 13,
        "start": 74.31999969482422,
        "end": 82.72000122070312,
        "text": " string piece into a token an integer. So here for example we tokenized the string hi there and we"
    },
    {
        "id": 14,
        "start": 82.72000122070312,
        "end": 89.91999816894531,
        "text": " received this sequence of tokens and here we took the first 1000 characters of our dataset and we"
    },
    {
        "id": 15,
        "start": 89.91999816894531,
        "end": 96.87999725341797,
        "text": " encoded it into tokens and because this is character level we received 1000 tokens in a sequence"
    },
    {
        "id": 16,
        "start": 97.68000030517578,
        "end": 106.0,
        "text": " so token 18, 47, etc. Now later we saw that the way we plug these tokens into the language model"
    },
    {
        "id": 17,
        "start": 106.55999755859375,
        "end": 113.44000244140625,
        "text": " is by using an embedding table and so basically if we have 65 possible tokens then this embedding"
    },
    {
        "id": 18,
        "start": 113.44000244140625,
        "end": 119.36000061035156,
        "text": " table is going to have 65 rows and roughly speaking we're taking the integer associated"
    },
    {
        "id": 19,
        "start": 119.36000061035156,
        "end": 124.63999938964844,
        "text": " with every single token we're using that as a lookup into this table and we're plucking out"
    },
    {
        "id": 20,
        "start": 124.63999938964844,
        "end": 130.32000732421875,
        "text": " the corresponding row and this row is trainable parameters that we're going to train using"
    },
    {
        "id": 21,
        "start": 130.32000732421875,
        "end": 135.83999633789062,
        "text": " backpropagation and this is the vector that then feeds into the transformer and that's how the"
    },
    {
        "id": 22,
        "start": 135.83999633789062,
        "end": 142.55999755859375,
        "text": " transformer sort of perceives every single token. So here we had a very naive tokenization process"
    },
    {
        "id": 23,
        "start": 142.55999755859375,
        "end": 147.83999633789062,
        "text": " that was a character level tokenizer but in practice in state-of-the-art language models"
    },
    {
        "id": 24,
        "start": 147.83999633789062,
        "end": 153.36000061035156,
        "text": " people use a lot more complicated schemes unfortunately for constructing these token"
    },
    {
        "id": 25,
        "start": 153.36000061035156,
        "end": 159.9199981689453,
        "text": " vocabularies so we're not dealing on the character level we're dealing on chunk level and the way"
    },
    {
        "id": 26,
        "start": 159.9199981689453,
        "end": 165.75999450683594,
        "text": " these character chunks are constructed is using algorithms such as for example the byte pair"
    },
    {
        "id": 27,
        "start": 165.75999450683594,
        "end": 172.47999572753906,
        "text": " encoding algorithm which we're going to go into in detail and cover in this video. I'd like to"
    },
    {
        "id": 28,
        "start": 172.47999572753906,
        "end": 177.60000610351562,
        "text": " briefly show you the paper that introduced byte level encoding as a mechanism for tokenization"
    },
    {
        "id": 29,
        "start": 177.60000610351562,
        "end": 182.47999572753906,
        "text": " in the context of large language models and I would say that that's probably a GPT-2 paper"
    },
    {
        "id": 30,
        "start": 182.47999572753906,
        "end": 188.0,
        "text": " and if you scroll down here to the section input representation this is where they cover"
    },
    {
        "id": 31,
        "start": 188.0,
        "end": 192.9600067138672,
        "text": " tokenization the kinds of properties that you'd like the tokenization to have and they conclude"
    },
    {
        "id": 32,
        "start": 192.9600067138672,
        "end": 200.0800018310547,
        "text": " here that they're going to have a tokenizer where you have a vocabulary of 50,257 possible"
    },
    {
        "id": 33,
        "start": 200.0800018310547,
        "end": 208.8000030517578,
        "text": " tokens and the context size is going to be 1024 tokens so in the intention layer of the"
    },
    {
        "id": 34,
        "start": 208.8000030517578,
        "end": 213.75999450683594,
        "text": " transformer neural network every single token is attending to the previous tokens in the sequence"
    },
    {
        "id": 35,
        "start": 213.75999450683594,
        "end": 222.24000549316406,
        "text": " and it's going to see up to 1024 tokens so tokens are this like fundamental unit the atom of large"
    },
    {
        "id": 36,
        "start": 222.24000549316406,
        "end": 226.8800048828125,
        "text": " language models if you will and everything is in units of tokens everything is about tokens"
    },
    {
        "id": 37,
        "start": 226.8800048828125,
        "end": 233.60000610351562,
        "text": " and tokenization is the process for translating strings or text into sequences of tokens and vice"
    },
    {
        "id": 38,
        "start": 233.60000610351562,
        "end": 239.1199951171875,
        "text": " versa. When you go into the LLAMA2 paper as well I can show you that when you search token you're"
    },
    {
        "id": 39,
        "start": 239.1199951171875,
        "end": 245.1199951171875,
        "text": " going to get 63 hits and that's because tokens are again pervasive so here they mention that"
    },
    {
        "id": 40,
        "start": 245.1199951171875,
        "end": 251.9199981689453,
        "text": " they trained on two trillion tokens of data and so on. So we're going to build our own tokenizer"
    },
    {
        "id": 41,
        "start": 251.9199981689453,
        "end": 256.9599914550781,
        "text": " luckily the byte pair encoding algorithm is not that super complicated and we can build it from"
    },
    {
        "id": 42,
        "start": 256.9599914550781,
        "end": 261.3599853515625,
        "text": " scratch ourselves and we'll see exactly how this works. Before we dive into code I'd like to give"
    },
    {
        "id": 43,
        "start": 261.3599853515625,
        "end": 265.9200134277344,
        "text": " you a brief taste of some of the complexities that come from the tokenization because I just"
    },
    {
        "id": 44,
        "start": 265.9200134277344,
        "end": 270.3999938964844,
        "text": " want to make sure that we motivate it sufficiently for why we are doing all this and why this is so"
    },
    {
        "id": 45,
        "start": 270.3999938964844,
        "end": 276.32000732421875,
        "text": " gross. So tokenization is at the heart of a lot of weirdness in large language models and I would"
    },
    {
        "id": 46,
        "start": 276.32000732421875,
        "end": 281.760009765625,
        "text": " advise that you do not brush it off. A lot of the issues that may look like just issues with the"
    },
    {
        "id": 47,
        "start": 281.760009765625,
        "end": 286.4800109863281,
        "text": " neural network architecture or a lot the large language model itself are actually issues with"
    },
    {
        "id": 48,
        "start": 286.4800109863281,
        "end": 292.79998779296875,
        "text": " the tokenization and fundamentally trace back to it. So if you've noticed any issues with large"
    },
    {
        "id": 49,
        "start": 292.79998779296875,
        "end": 297.8399963378906,
        "text": " language models can you know not able to do spelling tasks very easily that's usually due to"
    },
    {
        "id": 50,
        "start": 297.8399963378906,
        "end": 303.1199951171875,
        "text": " tokenization. Simple string processing can be difficult for the large language model to perform"
    },
    {
        "id": 51,
        "start": 303.1199951171875,
        "end": 309.44000244140625,
        "text": " natively. Non-english languages can work much worse and to a large extent this is due to tokenization."
    },
    {
        "id": 52,
        "start": 310.239990234375,
        "end": 315.44000244140625,
        "text": " Sometimes LLMs are bad at simple arithmetic also can trace be traced to tokenization."
    },
    {
        "id": 53,
        "start": 316.6400146484375,
        "end": 322.1600036621094,
        "text": " GPT-2 specifically would have had quite a bit more issues with Python than future versions of it due"
    },
    {
        "id": 54,
        "start": 322.1600036621094,
        "end": 326.4800109863281,
        "text": " to tokenization. There's a lot of other issues maybe you've seen weird warnings about a trail"
    },
    {
        "id": 55,
        "start": 326.4800109863281,
        "end": 334.32000732421875,
        "text": " in whitespace this is a tokenization issue. If you had asked GPT earlier about solid gold magicarp"
    },
    {
        "id": 56,
        "start": 334.32000732421875,
        "end": 339.1199951171875,
        "text": " and what it is you would see the LLM go totally crazy and it would start going off about a"
    },
    {
        "id": 57,
        "start": 339.1199951171875,
        "end": 344.7200012207031,
        "text": " completely unrelated tangent topic. Maybe you've been told to use YAML over JSON in structured data"
    },
    {
        "id": 58,
        "start": 344.7200012207031,
        "end": 349.2799987792969,
        "text": " all of that has to do with tokenization. So basically tokenization is at the heart of many"
    },
    {
        "id": 59,
        "start": 349.2799987792969,
        "end": 355.9200134277344,
        "text": " issues. I will loop back around to these at the end of the video but for now let me just skip over"
    },
    {
        "id": 60,
        "start": 355.9200134277344,
        "end": 362.8800048828125,
        "text": " it a little bit and let's go to this web app the ticktokenizer.bristle.app. So I have it loaded"
    },
    {
        "id": 61,
        "start": 362.8800048828125,
        "end": 367.760009765625,
        "text": " here and what I like about this web app is that tokenization is running a sort of live in your"
    },
    {
        "id": 62,
        "start": 367.760009765625,
        "end": 373.9200134277344,
        "text": " browser in JavaScript. So you can just type here stuff hello world and the whole string re-tokenizes."
    },
    {
        "id": 63,
        "start": 375.20001220703125,
        "end": 381.44000244140625,
        "text": " So here what we see on the left is a string that you put in on the right we're currently using a"
    },
    {
        "id": 64,
        "start": 381.44000244140625,
        "end": 388.0799865722656,
        "text": " GPT-2 tokenizer we see that this string that I pasted here is currently tokenizing into 300 tokens"
    },
    {
        "id": 65,
        "start": 388.6400146484375,
        "end": 393.5199890136719,
        "text": " and here they are sort of shown explicitly in different colors for every single token."
    },
    {
        "id": 66,
        "start": 394.32000732421875,
        "end": 403.760009765625,
        "text": " So for example this word tokenization became two tokens the token 30,642 and 1,634."
    },
    {
        "id": 67,
        "start": 404.9599914550781,
        "end": 412.4800109863281,
        "text": " The token space is token 318. So be careful on the bottom you can show white space"
    },
    {
        "id": 68,
        "start": 413.2799987792969,
        "end": 418.55999755859375,
        "text": " and keep in mind that there are spaces and slash and newline characters in here"
    },
    {
        "id": 69,
        "start": 418.55999755859375,
        "end": 427.8399963378906,
        "text": " but you can hide them for clarity. The token space at is token 379. The token space the"
    },
    {
        "id": 70,
        "start": 428.4800109863281,
        "end": 435.0400085449219,
        "text": " is 262 etc. So you notice here that the space is part of that token chunk."
    },
    {
        "id": 71,
        "start": 436.8800048828125,
        "end": 442.0799865722656,
        "text": " Now so this is kind of like how our English sentence broke up and that seems all well and"
    },
    {
        "id": 72,
        "start": 442.0799865722656,
        "end": 452.4800109863281,
        "text": " good. Now here I put in some arithmetic so we see that the token 127 plus and then token 6 space 6"
    },
    {
        "id": 73,
        "start": 452.55999755859375,
        "end": 458.0,
        "text": " followed by 77. So what's happening here is that 127 is feeding in as a single token into the"
    },
    {
        "id": 74,
        "start": 458.0,
        "end": 464.9599914550781,
        "text": " large language model but the number 677 will actually feed in as two separate tokens."
    },
    {
        "id": 75,
        "start": 465.8399963378906,
        "end": 472.239990234375,
        "text": " And so the large language model has to sort of take account of that and process it correctly"
    },
    {
        "id": 76,
        "start": 472.239990234375,
        "end": 478.3999938964844,
        "text": " in its network and see here 804 will be broken up into two tokens and it's all completely arbitrary."
    },
    {
        "id": 77,
        "start": 478.9599914550781,
        "end": 483.3599853515625,
        "text": " And here I have another example of four digit numbers and they break up in a way that they"
    },
    {
        "id": 78,
        "start": 483.3599853515625,
        "end": 488.55999755859375,
        "text": " break up and it's totally arbitrary. Sometimes you have multiple digits single token sometimes"
    },
    {
        "id": 79,
        "start": 488.55999755859375,
        "end": 492.8800048828125,
        "text": " you have individual digits as many tokens and it's all kind of pretty arbitrary and"
    },
    {
        "id": 80,
        "start": 492.8800048828125,
        "end": 500.55999755859375,
        "text": " coming out of the tokenizer. Here's another example we have the string egg and you see"
    },
    {
        "id": 81,
        "start": 500.55999755859375,
        "end": 506.7200012207031,
        "text": " here that this became two tokens but for some reason when I say I have an egg you see when"
    },
    {
        "id": 82,
        "start": 506.7200012207031,
        "end": 513.3599853515625,
        "text": " it's a space egg it's two token it's sorry it's a single token so just egg by itself in the"
    },
    {
        "id": 83,
        "start": 513.3599853515625,
        "end": 520.1599731445312,
        "text": " beginning of a sentence is two tokens but here as a space egg is suddenly a single token for"
    },
    {
        "id": 84,
        "start": 520.1599731445312,
        "end": 526.8800048828125,
        "text": " the exact same string. Here lowercase egg turns out to be a single token and in particular notice"
    },
    {
        "id": 85,
        "start": 526.8800048828125,
        "end": 532.0,
        "text": " that the color is different so this is a different token so this is case sensitive and of course"
    },
    {
        "id": 86,
        "start": 532.0,
        "end": 538.5599975585938,
        "text": " capital egg would also be different tokens and again this would be two tokens arbitrarily."
    },
    {
        "id": 87,
        "start": 539.1199951171875,
        "end": 543.8400268554688,
        "text": " So for the same concept egg depending on if it's in the beginning of a sentence at the end of a"
    },
    {
        "id": 88,
        "start": 543.8400268554688,
        "end": 549.4400024414062,
        "text": " sentence lowercase uppercase or mixed all this will be basically very different tokens and different"
    },
    {
        "id": 89,
        "start": 549.4400024414062,
        "end": 554.0,
        "text": " ids and the language model has to learn from raw data from all the internet text that it's going to"
    },
    {
        "id": 90,
        "start": 554.0,
        "end": 558.5599975585938,
        "text": " be training on that these are actually all the exact same concept and it has to sort of group"
    },
    {
        "id": 91,
        "start": 558.5599975585938,
        "end": 563.2000122070312,
        "text": " them in the parameters of the neural network and understand just based on the data patterns"
    },
    {
        "id": 92,
        "start": 563.2000122070312,
        "end": 568.4000244140625,
        "text": " that these are all very similar but maybe not almost exactly similar but very very similar."
    },
    {
        "id": 93,
        "start": 570.6400146484375,
        "end": 576.719970703125,
        "text": " After the demonstration here I have an introduction from OpenAI's ChaChabitty"
    },
    {
        "id": 94,
        "start": 576.719970703125,
        "end": 586.7999877929688,
        "text": " in Korean. So this is in Korean and the reason I put this here is because you'll notice that"
    },
    {
        "id": 95,
        "start": 589.3599853515625,
        "end": 594.7999877929688,
        "text": " non-English languages work slightly worse in ChaChabitty. Part of this is because of course"
    },
    {
        "id": 96,
        "start": 594.7999877929688,
        "end": 599.280029296875,
        "text": " the training data set for ChaChabitty is much larger for English than for everything else"
    },
    {
        "id": 97,
        "start": 599.280029296875,
        "end": 604.3200073242188,
        "text": " but the same is true not just for the large language model itself but also for the tokenizer."
    },
    {
        "id": 98,
        "start": 604.3200073242188,
        "end": 608.0,
        "text": " So when we train the tokenizer we're going to see that there's a training set as well"
    },
    {
        "id": 99,
        "start": 608.0,
        "end": 613.0399780273438,
        "text": " and there's a lot more English than non-English and what ends up happening is that we're going to"
    },
    {
        "id": 100,
        "start": 613.1199951171875,
        "end": 620.4000244140625,
        "text": " have a lot more longer tokens for English. So how do I put this? If you have a single sentence"
    },
    {
        "id": 101,
        "start": 620.4000244140625,
        "end": 625.1199951171875,
        "text": " in English and you tokenize it you might see that it's 10 tokens or something like that"
    },
    {
        "id": 102,
        "start": 625.1199951171875,
        "end": 628.6400146484375,
        "text": " but if you translate that sentence into say Korean or Japanese or something else"
    },
    {
        "id": 103,
        "start": 629.2000122070312,
        "end": 634.3200073242188,
        "text": " you'll typically see that number of tokens used is much larger and that's because the chunks here"
    },
    {
        "id": 104,
        "start": 634.3200073242188,
        "end": 639.5999755859375,
        "text": " are a lot more broken up. So we're using a lot more tokens for the exact same thing"
    },
    {
        "id": 105,
        "start": 640.4000244140625,
        "end": 644.9600219726562,
        "text": " and what this does is it bloats up the sequence length of all the documents."
    },
    {
        "id": 106,
        "start": 644.9600219726562,
        "end": 649.5999755859375,
        "text": " So you're using up more tokens and then in the attention of the transformer when these tokens"
    },
    {
        "id": 107,
        "start": 649.5999755859375,
        "end": 655.4400024414062,
        "text": " try to attend each other you are running out of context in the maximum context length of that"
    },
    {
        "id": 108,
        "start": 655.4400024414062,
        "end": 662.3200073242188,
        "text": " transformer and so basically all the non-English text is stretched out from the perspective of"
    },
    {
        "id": 109,
        "start": 662.3200073242188,
        "end": 667.6799926757812,
        "text": " the transformer and this just has to do with the training set used for the tokenizer and the"
    },
    {
        "id": 110,
        "start": 667.6799926757812,
        "end": 673.5999755859375,
        "text": " tokenization itself. So it will create a lot bigger tokens and a lot larger groups in English"
    },
    {
        "id": 111,
        "start": 673.5999755859375,
        "end": 677.280029296875,
        "text": " and it will have a lot of little boundaries for all the other non-English text."
    },
    {
        "id": 112,
        "start": 679.3599853515625,
        "end": 683.1199951171875,
        "text": " So if we translated this into English it would be significantly fewer tokens."
    },
    {
        "id": 113,
        "start": 684.239990234375,
        "end": 688.3200073242188,
        "text": " The final example I have here is a little snippet of Python for doing fizzbuzz"
    },
    {
        "id": 114,
        "start": 689.1199951171875,
        "end": 695.0399780273438,
        "text": " and what I'd like you to notice is look all these individual spaces are all separate tokens."
    },
    {
        "id": 115,
        "start": 695.5999755859375,
        "end": 705.4400024414062,
        "text": " They are token 220. So 220, 220, 220, 220 and then space if is a single token and so what's going on"
    },
    {
        "id": 116,
        "start": 705.4400024414062,
        "end": 711.280029296875,
        "text": " here is that when the transformer is going to consume or try to create this text it needs to"
    },
    {
        "id": 117,
        "start": 712.1599731445312,
        "end": 717.5999755859375,
        "text": " handle all these spaces individually. They all feed in one by one into the entire transformer"
    },
    {
        "id": 118,
        "start": 717.5999755859375,
        "end": 724.0,
        "text": " in the sequence and so this is being extremely wasteful tokenizing it in this way and so as a"
    },
    {
        "id": 119,
        "start": 724.0,
        "end": 728.9600219726562,
        "text": " result of that GPT-2 is not very good with Python and it's not anything to do with coding or the"
    },
    {
        "id": 120,
        "start": 728.9600219726562,
        "end": 734.1599731445312,
        "text": " language model itself it's just that if you use a lot of indentation using space in Python like"
    },
    {
        "id": 121,
        "start": 734.1599731445312,
        "end": 739.52001953125,
        "text": " we usually do you just end up bloating out all the text and it's separated across way too much"
    },
    {
        "id": 122,
        "start": 739.52001953125,
        "end": 744.0800170898438,
        "text": " of the sequence and we are running out of the context length in the sequence. That's roughly"
    },
    {
        "id": 123,
        "start": 744.0800170898438,
        "end": 747.9199829101562,
        "text": " speaking what's what's happening. We're being way too wasteful. We're taking up way too much token"
    },
    {
        "id": 124,
        "start": 747.9199829101562,
        "end": 752.9600219726562,
        "text": " space. Now we can also scroll up here and we can change the tokenizer. So note here that GPT-2"
    },
    {
        "id": 125,
        "start": 752.9600219726562,
        "end": 759.52001953125,
        "text": " tokenizer creates a token count of 300 for this string here. We can change it to CL 100k base"
    },
    {
        "id": 126,
        "start": 759.52001953125,
        "end": 765.5999755859375,
        "text": " which is the GPT-4 tokenizer and we see that the token count drops to 185. So for the exact same"
    },
    {
        "id": 127,
        "start": 765.5999755859375,
        "end": 771.760009765625,
        "text": " string we are now roughly halving the number of tokens and roughly speaking this is because the"
    },
    {
        "id": 128,
        "start": 771.760009765625,
        "end": 777.6799926757812,
        "text": " number of tokens in the GPT-4 tokenizer is roughly double that of the number of tokens in the GPT-2"
    },
    {
        "id": 129,
        "start": 777.6799926757812,
        "end": 782.9600219726562,
        "text": " tokenizer. So we went from roughly 50k to roughly 100k. Now you can imagine that this is a good"
    },
    {
        "id": 130,
        "start": 782.9600219726562,
        "end": 791.760009765625,
        "text": " thing because the same text is now squished into half as many tokens so this is a lot denser input"
    },
    {
        "id": 131,
        "start": 791.760009765625,
        "end": 797.6799926757812,
        "text": " to the transformer and in the transformer every single token has a finite number of tokens before"
    },
    {
        "id": 132,
        "start": 797.6799926757812,
        "end": 802.0,
        "text": " it that it's going to pay attention to and so what this is doing is we're roughly able to see"
    },
    {
        "id": 133,
        "start": 802.8800048828125,
        "end": 809.52001953125,
        "text": " twice as much text as a context for what token to predict next because of this change. But of"
    },
    {
        "id": 134,
        "start": 809.52001953125,
        "end": 814.9600219726562,
        "text": " course just increasing the number of tokens is not strictly better infinitely because as you"
    },
    {
        "id": 135,
        "start": 814.9600219726562,
        "end": 820.3200073242188,
        "text": " increase the number of tokens now your embedding table is sort of getting a lot larger and also"
    },
    {
        "id": 136,
        "start": 820.3200073242188,
        "end": 824.6400146484375,
        "text": " at the output we are trying to predict the next token and there's the softmax there and that grows"
    },
    {
        "id": 137,
        "start": 824.6400146484375,
        "end": 828.719970703125,
        "text": " as well. We're going to go into more detail later on this but there's some kind of a sweet spot"
    },
    {
        "id": 138,
        "start": 828.719970703125,
        "end": 833.8400268554688,
        "text": " somewhere where you have a just right number of tokens in your vocabulary where everything is"
    },
    {
        "id": 139,
        "start": 833.8400268554688,
        "end": 839.280029296875,
        "text": " appropriately dense and still fairly efficient. Now one thing I would like you to note specifically"
    },
    {
        "id": 140,
        "start": 839.280029296875,
        "end": 846.0800170898438,
        "text": " for the GPT-4 tokenizer is that the handling of the white space for Python has improved a lot."
    },
    {
        "id": 141,
        "start": 846.0800170898438,
        "end": 852.4000244140625,
        "text": " You see that here these four spaces are represented as one single token for the three spaces here and"
    },
    {
        "id": 142,
        "start": 852.47998046875,
        "end": 859.1199951171875,
        "text": " then the token spaces and here seven spaces were all grouped into a single token so we're being a"
    },
    {
        "id": 143,
        "start": 859.1199951171875,
        "end": 863.760009765625,
        "text": " lot more efficient in how we represent Python and this was a deliberate choice made by OpenAI when"
    },
    {
        "id": 144,
        "start": 863.760009765625,
        "end": 870.47998046875,
        "text": " they designed the GPT-4 tokenizer and they group a lot more white space into a single character."
    },
    {
        "id": 145,
        "start": 870.47998046875,
        "end": 877.8400268554688,
        "text": " What this does is this densifies Python and therefore we can attend to more code before it"
    },
    {
        "id": 146,
        "start": 877.8400268554688,
        "end": 882.719970703125,
        "text": " when we're trying to predict the next token in the sequence and so the improvement in the Python"
    },
    {
        "id": 147,
        "start": 882.719970703125,
        "end": 888.3200073242188,
        "text": " coding ability from GPT-2 to GPT-4 is not just a matter of the language model and the architecture"
    },
    {
        "id": 148,
        "start": 888.3200073242188,
        "end": 892.4000244140625,
        "text": " and details of the optimization but a lot of the improvement here is also coming from the"
    },
    {
        "id": 149,
        "start": 892.4000244140625,
        "end": 897.52001953125,
        "text": " design of the tokenizer and how it groups characters into tokens. Okay so let's now start"
    },
    {
        "id": 150,
        "start": 897.52001953125,
        "end": 903.760009765625,
        "text": " writing some code. So remember what we want to do. We want to take strings and feed them into"
    },
    {
        "id": 151,
        "start": 903.760009765625,
        "end": 910.4000244140625,
        "text": " language models. For that we need to somehow tokenize strings into some integers in some"
    },
    {
        "id": 152,
        "start": 910.4000244140625,
        "end": 915.8400268554688,
        "text": " fixed vocabulary and then we will use those integers to make a lookup into a lookup table"
    },
    {
        "id": 153,
        "start": 915.8400268554688,
        "end": 921.52001953125,
        "text": " of vectors and feed those vectors into the transformer as an input. Now the reason this"
    },
    {
        "id": 154,
        "start": 921.52001953125,
        "end": 924.9600219726562,
        "text": " gets a little bit tricky of course is that we don't just want to support the simple English"
    },
    {
        "id": 155,
        "start": 924.9600219726562,
        "end": 930.4000244140625,
        "text": " alphabet. We want to support different kinds of languages so this is annyeonghaseyo in Korean"
    },
    {
        "id": 156,
        "start": 930.4000244140625,
        "end": 934.7999877929688,
        "text": " which is hello and we also want to support many kinds of special characters that we might find on"
    },
    {
        "id": 157,
        "start": 934.7999877929688,
        "end": 944.3200073242188,
        "text": " the internet for example emoji. So how do we feed this text into transformers? Well what is"
    },
    {
        "id": 158,
        "start": 944.3200073242188,
        "end": 950.0,
        "text": " this text anyway in Python? So if you go to the documentation of a string in Python you can see"
    },
    {
        "id": 159,
        "start": 950.0,
        "end": 956.719970703125,
        "text": " that strings are immutable sequences of Unicode code points. Okay what are Unicode code points?"
    },
    {
        "id": 160,
        "start": 957.6799926757812,
        "end": 963.5999755859375,
        "text": " We can go to Wikipedia. So Unicode code points are defined by the Unicode consortium"
    },
    {
        "id": 161,
        "start": 964.239990234375,
        "end": 970.0800170898438,
        "text": " as part of the Unicode standard and what this is really is that it's just a definition of roughly"
    },
    {
        "id": 162,
        "start": 970.0800170898438,
        "end": 977.3599853515625,
        "text": " 150,000 characters right now and roughly speaking what they look like and what integers represent"
    },
    {
        "id": 163,
        "start": 977.3599853515625,
        "end": 983.6799926757812,
        "text": " those characters. So this is 150,000 characters across 161 scripts as of right now. So if you"
    },
    {
        "id": 164,
        "start": 983.8400268554688,
        "end": 988.719970703125,
        "text": " scroll down here you can see that the standard is very much alive. The latest standard 15.1 is"
    },
    {
        "id": 165,
        "start": 988.719970703125,
        "end": 996.4000244140625,
        "text": " September 2023 and basically this is just a way to define lots of types of characters"
    },
    {
        "id": 166,
        "start": 997.9199829101562,
        "end": 1003.280029296875,
        "text": " like for example all these characters across different scripts. So the way we can access the"
    },
    {
        "id": 167,
        "start": 1003.280029296875,
        "end": 1008.239990234375,
        "text": " Unicode code point given a single character is by using the ORD function in Python. So for example"
    },
    {
        "id": 168,
        "start": 1008.239990234375,
        "end": 1016.4000244140625,
        "text": " I can pass in ORD of h and I can see that for the single character h the Unicode code point is 104."
    },
    {
        "id": 169,
        "start": 1019.2000122070312,
        "end": 1023.52001953125,
        "text": " But this can be arbitrarily complicated so we can take for example our emoji here"
    },
    {
        "id": 170,
        "start": 1023.52001953125,
        "end": 1028.800048828125,
        "text": " and we can see that the code point for this one is 128,000 or we can take UN"
    },
    {
        "id": 171,
        "start": 1031.3599853515625,
        "end": 1037.760009765625,
        "text": " and this is 50,000. Now keep in mind you can't plug in strings here because this doesn't have"
    },
    {
        "id": 172,
        "start": 1037.760009765625,
        "end": 1043.199951171875,
        "text": " a single code point. It only takes a single Unicode code point character and tells you its"
    },
    {
        "id": 173,
        "start": 1043.199951171875,
        "end": 1051.199951171875,
        "text": " integer. So in this way we can look up all the characters of this specific string and their"
    },
    {
        "id": 174,
        "start": 1051.199951171875,
        "end": 1060.4000244140625,
        "text": " code points so ORD of x for x in this string and we get this encoding here. Now see here we've"
    },
    {
        "id": 175,
        "start": 1060.4000244140625,
        "end": 1065.3599853515625,
        "text": " already turned the raw code points already have integers so why can't we simply just use these"
    },
    {
        "id": 176,
        "start": 1065.3599853515625,
        "end": 1070.719970703125,
        "text": " integers and not have any tokenization at all? Why can't we just use this natively as is and just"
    },
    {
        "id": 177,
        "start": 1070.719970703125,
        "end": 1074.8800048828125,
        "text": " use the code point? Well one reason for that of course is that the vocabulary in that case would"
    },
    {
        "id": 178,
        "start": 1074.8800048828125,
        "end": 1081.6800537109375,
        "text": " be quite long. So in this case for Unicode this is a vocabulary of 150,000 different code points."
    },
    {
        "id": 179,
        "start": 1082.239990234375,
        "end": 1088.0,
        "text": " But more worryingly than that I think the Unicode standard is very much alive and it keeps changing"
    },
    {
        "id": 180,
        "start": 1088.0,
        "end": 1092.3199462890625,
        "text": " and so it's not kind of a stable representation necessarily that we may want to use directly."
    },
    {
        "id": 181,
        "start": 1093.199951171875,
        "end": 1097.43994140625,
        "text": " So for those reasons we need something a bit better. So to find something better we turn to"
    },
    {
        "id": 182,
        "start": 1097.43994140625,
        "end": 1102.8800048828125,
        "text": " encodings. So if you go to the Wikipedia page here we see that the Unicode consortium defines"
    },
    {
        "id": 183,
        "start": 1102.8800048828125,
        "end": 1111.0400390625,
        "text": " three types of encodings UTF-8, UTF-16 and UTF-32. These encodings are the way by which we can take"
    },
    {
        "id": 184,
        "start": 1111.0400390625,
        "end": 1117.9200439453125,
        "text": " Unicode text and translate it into binary data or byte streams. UTF-8 is by far the most common"
    },
    {
        "id": 185,
        "start": 1118.8800048828125,
        "end": 1124.1600341796875,
        "text": " so this is the UTF-8 page. Now this Wikipedia page is actually quite long but what's important"
    },
    {
        "id": 186,
        "start": 1124.1600341796875,
        "end": 1130.47998046875,
        "text": " for our purposes is that UTF-8 takes every single code point and it translates it to a byte stream"
    },
    {
        "id": 187,
        "start": 1130.47998046875,
        "end": 1135.9200439453125,
        "text": " and this byte stream is between one to four bytes so it's a variable length encoding. So depending"
    },
    {
        "id": 188,
        "start": 1135.9200439453125,
        "end": 1139.8399658203125,
        "text": " on the Unicode point according to the schema you're going to end up with between one to four"
    },
    {
        "id": 189,
        "start": 1139.8399658203125,
        "end": 1148.8800048828125,
        "text": " bytes for each code point. On top of that there's UTF-8, UTF-16 and UTF-32. UTF-32 is nice because"
    },
    {
        "id": 190,
        "start": 1148.8800048828125,
        "end": 1154.8800048828125,
        "text": " it is fixed length instead of variable length but it has many other downsides as well. So the full"
    },
    {
        "id": 191,
        "start": 1154.8800048828125,
        "end": 1160.3199462890625,
        "text": " kind of spectrum of pros and cons of all these different three encodings are beyond the scope"
    },
    {
        "id": 192,
        "start": 1160.3199462890625,
        "end": 1165.6800537109375,
        "text": " of this video. I'd just like to point out that I enjoyed this blog post and this blog post at the"
    },
    {
        "id": 193,
        "start": 1165.6800537109375,
        "end": 1171.280029296875,
        "text": " end of it also has a number of references that can be quite useful. One of them is UTF-8"
    },
    {
        "id": 194,
        "start": 1171.280029296875,
        "end": 1178.0799560546875,
        "text": " Everywhere manifesto and this manifesto describes the reason why UTF-8 is significantly preferred"
    },
    {
        "id": 195,
        "start": 1178.0799560546875,
        "end": 1183.3599853515625,
        "text": " and a lot nicer than the other encodings and why it is used a lot more prominently"
    },
    {
        "id": 196,
        "start": 1184.719970703125,
        "end": 1190.9599609375,
        "text": " on the internet. One of the major advantages, just to give you a sense, is that UTF-8 is the only one"
    },
    {
        "id": 197,
        "start": 1190.9599609375,
        "end": 1197.199951171875,
        "text": " of these that is backwards compatible to the much simpler ASCII encoding of text but I'm not going"
    },
    {
        "id": 198,
        "start": 1197.199951171875,
        "end": 1202.4000244140625,
        "text": " to go into the full detail in this video. So suffice to say that we like the UTF-8 encoding"
    },
    {
        "id": 199,
        "start": 1202.4000244140625,
        "end": 1207.280029296875,
        "text": " and let's try to take this string and see what we get if we encode it into UTF-8."
    },
    {
        "id": 200,
        "start": 1208.9599609375,
        "end": 1213.3599853515625,
        "text": " The string class in Python actually has dot encode and you can give it the encoding which is"
    },
    {
        "id": 201,
        "start": 1213.3599853515625,
        "end": 1220.0799560546875,
        "text": " say UTF-8. Now what we get out of this is not very nice because this is the bytes, this is a bytes"
    },
    {
        "id": 202,
        "start": 1220.0799560546875,
        "end": 1225.43994140625,
        "text": " object and it's not very nice in the way that it's printed so I personally like to take it through a"
    },
    {
        "id": 203,
        "start": 1225.43994140625,
        "end": 1233.3599853515625,
        "text": " list because then we actually get the raw bytes of this encoding. So this is the raw bytes that"
    },
    {
        "id": 204,
        "start": 1233.3599853515625,
        "end": 1240.3199462890625,
        "text": " represent this string according to the UTF-8 encoding. We can also look at UTF-16. We get a"
    },
    {
        "id": 205,
        "start": 1240.3199462890625,
        "end": 1245.8399658203125,
        "text": " slightly different byte stream and here we start to see one of the disadvantages of UTF-16. You see"
    },
    {
        "id": 206,
        "start": 1245.8399658203125,
        "end": 1250.239990234375,
        "text": " how we have zero something, zero something, zero something. We're starting to get a sense that"
    },
    {
        "id": 207,
        "start": 1250.239990234375,
        "end": 1256.239990234375,
        "text": " this is a bit of a wasteful encoding and indeed for simple ASCII characters or English characters"
    },
    {
        "id": 208,
        "start": 1256.239990234375,
        "end": 1262.56005859375,
        "text": " here we just have the structure of zero something, zero something and it's not exactly nice. Same for"
    },
    {
        "id": 209,
        "start": 1262.56005859375,
        "end": 1268.0799560546875,
        "text": " UTF-32. When we expand this we can start to get a sense of the wastefulness of this encoding for"
    },
    {
        "id": 210,
        "start": 1268.0799560546875,
        "end": 1274.4000244140625,
        "text": " our purposes. You see a lot of zeros followed by something and so this is not desirable."
    },
    {
        "id": 211,
        "start": 1275.8399658203125,
        "end": 1283.199951171875,
        "text": " So suffice it to say that we would like to stick with UTF-8 for our purposes. However if we just"
    },
    {
        "id": 212,
        "start": 1283.199951171875,
        "end": 1290.719970703125,
        "text": " use UTF-8 naively these are byte streams so that would imply a vocabulary length of only 256 possible"
    },
    {
        "id": 213,
        "start": 1290.719970703125,
        "end": 1296.719970703125,
        "text": " tokens but this vocabulary size is very very small. What this is going to do if we just were to"
    },
    {
        "id": 214,
        "start": 1296.719970703125,
        "end": 1303.1199951171875,
        "text": " use it naively is that all of our text would be stretched out over very very long sequences of"
    },
    {
        "id": 215,
        "start": 1303.1199951171875,
        "end": 1311.1199951171875,
        "text": " bytes and so what this does is that certainly the embedding table is going to be tiny and"
    },
    {
        "id": 216,
        "start": 1311.1199951171875,
        "end": 1315.1199951171875,
        "text": " the prediction at the top at the final layer is going to be very tiny but our sequences are very"
    },
    {
        "id": 217,
        "start": 1315.1199951171875,
        "end": 1321.0400390625,
        "text": " long and remember that we have pretty finite context lengths in the attention that we can"
    },
    {
        "id": 218,
        "start": 1321.0400390625,
        "end": 1327.3599853515625,
        "text": " support in a transformer for computational reasons and so we only have as much context length but now"
    },
    {
        "id": 219,
        "start": 1327.3599853515625,
        "end": 1331.52001953125,
        "text": " we have very very long sequences and this is just inefficient and it's not going to allow us to"
    },
    {
        "id": 220,
        "start": 1331.52001953125,
        "end": 1337.1199951171875,
        "text": " attend to sufficiently long text before us for the purposes of the next token prediction task."
    },
    {
        "id": 221,
        "start": 1338.0,
        "end": 1345.0400390625,
        "text": " So we don't want to use the raw bytes of the UTF-8 encoding. We want to be able to support"
    },
    {
        "id": 222,
        "start": 1345.0400390625,
        "end": 1349.8399658203125,
        "text": " larger vocabulary size that we can tune as a height parameter but we want to stick with the"
    },
    {
        "id": 223,
        "start": 1349.8399658203125,
        "end": 1355.760009765625,
        "text": " UTF-8 encoding of these strings so what do we do? Well the answer of course is we turn to the"
    },
    {
        "id": 224,
        "start": 1355.760009765625,
        "end": 1362.0,
        "text": " byte pair encoding algorithm which will allow us to compress these byte sequences to a variable"
    },
    {
        "id": 225,
        "start": 1362.0,
        "end": 1367.760009765625,
        "text": " amount so we'll get to that in a bit but I just want to briefly speak to the fact that I would"
    },
    {
        "id": 226,
        "start": 1367.760009765625,
        "end": 1374.56005859375,
        "text": " love nothing more than to be able to feed raw byte sequences into language models. In fact there's a"
    },
    {
        "id": 227,
        "start": 1374.56005859375,
        "end": 1379.8399658203125,
        "text": " paper about how this could potentially be done from somewhere last year. Now the problem is you"
    },
    {
        "id": 228,
        "start": 1379.8399658203125,
        "end": 1384.56005859375,
        "text": " actually have to go in and you have to modify the transformer architecture because as I mentioned"
    },
    {
        "id": 229,
        "start": 1384.56005859375,
        "end": 1388.9599609375,
        "text": " you're going to have a problem where the attention will start to become extremely expensive because"
    },
    {
        "id": 230,
        "start": 1388.9599609375,
        "end": 1395.6800537109375,
        "text": " the sequences are so long and so in this paper they propose kind of a hierarchical structuring"
    },
    {
        "id": 231,
        "start": 1395.6800537109375,
        "end": 1400.9599609375,
        "text": " of the transformer that could allow you to just feed in raw bytes and so at the end they say"
    },
    {
        "id": 232,
        "start": 1400.9599609375,
        "end": 1405.0400390625,
        "text": " together these results establish the viability of tokenization-free autoregressive sequence"
    },
    {
        "id": 233,
        "start": 1405.0400390625,
        "end": 1410.6400146484375,
        "text": " modeling at scale. So tokenization-free would indeed be amazing we would just feed byte streams"
    },
    {
        "id": 234,
        "start": 1410.6400146484375,
        "end": 1416.0799560546875,
        "text": " directly into our models but unfortunately I don't know that this has really been proven out yet by"
    },
    {
        "id": 235,
        "start": 1416.0799560546875,
        "end": 1420.1600341796875,
        "text": " sufficiently many groups and at sufficient scale but something like this at one point would be"
    },
    {
        "id": 236,
        "start": 1420.1600341796875,
        "end": 1424.719970703125,
        "text": " amazing and I hope someone comes up with it but for now we have to come back and we can't feed"
    },
    {
        "id": 237,
        "start": 1424.719970703125,
        "end": 1429.43994140625,
        "text": " this directly into language models and we have to compress it using the byte pair encoding algorithm"
    },
    {
        "id": 238,
        "start": 1429.43994140625,
        "end": 1433.52001953125,
        "text": " so let's see how that works. So as I mentioned the byte pair encoding algorithm is not all that"
    },
    {
        "id": 239,
        "start": 1433.52001953125,
        "end": 1438.1600341796875,
        "text": " complicated and the wikipedia page is actually quite instructive as far as the basic idea goes"
    },
    {
        "id": 240,
        "start": 1438.8800048828125,
        "end": 1443.6800537109375,
        "text": " what we're doing is we have some kind of a input sequence like for example here we have only four"
    },
    {
        "id": 241,
        "start": 1443.6800537109375,
        "end": 1449.280029296875,
        "text": " elements in our vocabulary a b c and d and we have a sequence of them so instead of bytes let's say"
    },
    {
        "id": 242,
        "start": 1449.280029296875,
        "end": 1454.800048828125,
        "text": " we just had four of a cap size of four the sequence is too long we'd like to compress it"
    },
    {
        "id": 243,
        "start": 1455.3599853515625,
        "end": 1464.800048828125,
        "text": " so we do is that we iteratively find the pair of tokens that occur the most frequently and then"
    },
    {
        "id": 244,
        "start": 1464.800048828125,
        "end": 1471.8399658203125,
        "text": " once we've identified that pair we replace that pair with just a single new token that we append"
    },
    {
        "id": 245,
        "start": 1471.8399658203125,
        "end": 1478.719970703125,
        "text": " to our vocabulary so for example here the byte pair a a occurs most often so we mint a new token"
    },
    {
        "id": 246,
        "start": 1478.719970703125,
        "end": 1486.239990234375,
        "text": " let's call it capital z and we replace every single occurrence of a a by z so now we have"
    },
    {
        "id": 247,
        "start": 1486.239990234375,
        "end": 1492.9599609375,
        "text": " two z's here so here we took a sequence of 11 characters with vocabulary size four"
    },
    {
        "id": 248,
        "start": 1493.6800537109375,
        "end": 1500.56005859375,
        "text": " and we've converted it to a sequence of only nine tokens but now with a vocabulary of five"
    },
    {
        "id": 249,
        "start": 1500.56005859375,
        "end": 1505.43994140625,
        "text": " because we have a fifth vocabulary element that we just created and it's z standing for"
    },
    {
        "id": 250,
        "start": 1505.43994140625,
        "end": 1512.1600341796875,
        "text": " concatenation of a and we can again repeat this process so we again look at the sequence and"
    },
    {
        "id": 251,
        "start": 1512.1600341796875,
        "end": 1519.760009765625,
        "text": " identify the pair of tokens that are most frequent let's say that that is now a b well we are going"
    },
    {
        "id": 252,
        "start": 1519.760009765625,
        "end": 1525.280029296875,
        "text": " to replace a b with a new token that we meant called y so y becomes a b and then every single"
    },
    {
        "id": 253,
        "start": 1525.280029296875,
        "end": 1532.8800048828125,
        "text": " occurrence of a b is now replaced with y so we end up with this so now we only have one two three"
    },
    {
        "id": 254,
        "start": 1532.8800048828125,
        "end": 1541.3599853515625,
        "text": " four five six seven characters in our sequence but we have not just four vocabulary elements"
    },
    {
        "id": 255,
        "start": 1541.3599853515625,
        "end": 1547.5999755859375,
        "text": " or five but now we have six and for the final round we again look through the sequence find"
    },
    {
        "id": 256,
        "start": 1547.5999755859375,
        "end": 1554.239990234375,
        "text": " that the phrase z y or the pair z y is most common and replace it one more time with another"
    },
    {
        "id": 257,
        "start": 1555.0400390625,
        "end": 1560.9599609375,
        "text": " character let's say x so x is z y and we replace all occurrences of z y and we get this following"
    },
    {
        "id": 258,
        "start": 1560.9599609375,
        "end": 1566.239990234375,
        "text": " sequence so basically after we have gone through this process instead of having a"
    },
    {
        "id": 259,
        "start": 1568.47998046875,
        "end": 1577.760009765625,
        "text": " sequence of 11 tokens with a vocabulary length of four we now have a sequence of one two three"
    },
    {
        "id": 260,
        "start": 1578.3199462890625,
        "end": 1586.0,
        "text": " four five tokens but our vocabulary length now is seven and so in this way we can iteratively"
    },
    {
        "id": 261,
        "start": 1586.0,
        "end": 1591.9200439453125,
        "text": " compress our sequence as we mint new tokens so in the in the exact same way we start we"
    },
    {
        "id": 262,
        "start": 1591.9200439453125,
        "end": 1598.6400146484375,
        "text": " start out with byte sequences so we have 256 vocabulary size but we're now going to go through"
    },
    {
        "id": 263,
        "start": 1598.6400146484375,
        "end": 1604.239990234375,
        "text": " these and find the byte pairs that occur the most and we're going to iteratively start minting new"
    },
    {
        "id": 264,
        "start": 1604.239990234375,
        "end": 1609.0400390625,
        "text": " tokens appending them to our vocabulary and replacing things and in this way we're going"
    },
    {
        "id": 265,
        "start": 1609.0400390625,
        "end": 1614.8800048828125,
        "text": " to end up with a compressed training data set and also an algorithm for taking any arbitrary sequence"
    },
    {
        "id": 266,
        "start": 1614.8800048828125,
        "end": 1621.6800537109375,
        "text": " and encoding it using this vocabulary and also decoding it back to strings so let's now implement"
    },
    {
        "id": 267,
        "start": 1621.6800537109375,
        "end": 1627.8399658203125,
        "text": " all that so here's what i did i went to this blog post that i enjoyed and i took the first paragraph"
    },
    {
        "id": 268,
        "start": 1627.8399658203125,
        "end": 1635.760009765625,
        "text": " and i copy pasted it here into text so this is one very long line here now to get the tokens"
    },
    {
        "id": 269,
        "start": 1635.760009765625,
        "end": 1640.719970703125,
        "text": " as i mentioned we just take our text and we encode it into utf-8 the tokens here at this point will"
    },
    {
        "id": 270,
        "start": 1640.719970703125,
        "end": 1647.43994140625,
        "text": " be a raw bytes single stream of bytes and just so that it's easier to work with instead of just"
    },
    {
        "id": 271,
        "start": 1647.43994140625,
        "end": 1653.3599853515625,
        "text": " a bytes object i'm going to convert all those bytes to integers and then create a list of it"
    },
    {
        "id": 272,
        "start": 1653.3599853515625,
        "end": 1657.9200439453125,
        "text": " just so it's easier for us to manipulate and work with in python and visualize and here i'm printing"
    },
    {
        "id": 273,
        "start": 1657.9200439453125,
        "end": 1666.239990234375,
        "text": " all that so this is the original um this is the original paragraph and its length is 533"
    },
    {
        "id": 274,
        "start": 1666.9599609375,
        "end": 1675.52001953125,
        "text": " code points and then here are the bytes encoded in utf-8 and we see that this has a length of 616"
    },
    {
        "id": 275,
        "start": 1675.52001953125,
        "end": 1681.9200439453125,
        "text": " bytes at this point or 616 tokens and the reason this is more is because a lot of these simple"
    },
    {
        "id": 276,
        "start": 1681.9200439453125,
        "end": 1687.760009765625,
        "text": " ascii characters or simple characters they just become a single byte but a lot of these unicode"
    },
    {
        "id": 277,
        "start": 1687.760009765625,
        "end": 1692.47998046875,
        "text": " more complex characters become multiple bytes up to four and so we are expanding that size"
    },
    {
        "id": 278,
        "start": 1692.800048828125,
        "end": 1696.47998046875,
        "text": " so now what we'd like to do as a first step of the algorithm is we'd like to iterate over here"
    },
    {
        "id": 279,
        "start": 1696.47998046875,
        "end": 1702.3199462890625,
        "text": " and find the pair of bytes that occur most frequently because we're then going to merge it"
    },
    {
        "id": 280,
        "start": 1702.8800048828125,
        "end": 1706.9599609375,
        "text": " so if you are working along on a notebook on a side then i encourage you to basically click"
    },
    {
        "id": 281,
        "start": 1706.9599609375,
        "end": 1711.5999755859375,
        "text": " on the link find this notebook and try to write that function yourself otherwise i'm going to"
    },
    {
        "id": 282,
        "start": 1711.5999755859375,
        "end": 1716.0,
        "text": " come here and implement first the function that finds the most common pair okay so here's what"
    },
    {
        "id": 283,
        "start": 1716.0,
        "end": 1719.760009765625,
        "text": " i came up with there are many different ways to implement this but i'm calling the function"
    },
    {
        "id": 284,
        "start": 1719.8399658203125,
        "end": 1724.800048828125,
        "text": " but i'm calling the function get stats it expects a list of integers i'm using a dictionary to keep"
    },
    {
        "id": 285,
        "start": 1724.800048828125,
        "end": 1730.1600341796875,
        "text": " track of basically the counts and then this is a pythonic way to iterate consecutive elements"
    },
    {
        "id": 286,
        "start": 1730.1600341796875,
        "end": 1736.0,
        "text": " of this list which we covered in the previous video and then here i'm just keeping track of"
    },
    {
        "id": 287,
        "start": 1736.0,
        "end": 1742.800048828125,
        "text": " just incrementing by one for all the pairs so if i call this on all the tokens here then the stats"
    },
    {
        "id": 288,
        "start": 1742.800048828125,
        "end": 1749.52001953125,
        "text": " comes out here so this is the dictionary the keys are these tuples of consecutive elements"
    },
    {
        "id": 289,
        "start": 1749.52001953125,
        "end": 1756.239990234375,
        "text": " and this is the count so just to print it in a slightly better way this is one way that i like to"
    },
    {
        "id": 290,
        "start": 1756.239990234375,
        "end": 1763.1199951171875,
        "text": " do that where you it's a little bit compound here so you can pause if you like but we iterate all"
    },
    {
        "id": 291,
        "start": 1763.1199951171875,
        "end": 1771.8399658203125,
        "text": " the items the items called on dictionary returns pairs of key value and instead i create a list"
    },
    {
        "id": 292,
        "start": 1771.8399658203125,
        "end": 1779.1199951171875,
        "text": " here of value key because if it's a value key list then i can call sort on it and by default"
    },
    {
        "id": 293,
        "start": 1779.1199951171875,
        "end": 1785.9200439453125,
        "text": " python will use the first element which in this case will be value to sort by if it's given tuples"
    },
    {
        "id": 294,
        "start": 1786.56005859375,
        "end": 1792.9599609375,
        "text": " and then reverse so it's descending and print that so basically it looks like 101,32 was the"
    },
    {
        "id": 295,
        "start": 1792.9599609375,
        "end": 1798.8800048828125,
        "text": " most commonly occurring consecutive pair and it occurred 20 times we can double check that that"
    },
    {
        "id": 296,
        "start": 1798.8800048828125,
        "end": 1806.3199462890625,
        "text": " makes reasonable sense so if i just search 101,32 then you see that these are the 20 occurrences of"
    },
    {
        "id": 297,
        "start": 1806.3199462890625,
        "end": 1814.3199462890625,
        "text": " that pair and if we'd like to take a look at what exactly that pair is we can use char which is the"
    },
    {
        "id": 298,
        "start": 1814.3199462890625,
        "end": 1823.0400390625,
        "text": " opposite of ord in python so we give it a unicode code point so 101 and of 32 and we see that this"
    },
    {
        "id": 299,
        "start": 1823.0400390625,
        "end": 1829.52001953125,
        "text": " is e and space so basically there's a lot of e space here meaning that a lot of these words seem"
    },
    {
        "id": 300,
        "start": 1829.52001953125,
        "end": 1834.9599609375,
        "text": " to end with e so here's e space as an example so there's a lot of that going on here and this is"
    },
    {
        "id": 301,
        "start": 1834.9599609375,
        "end": 1840.56005859375,
        "text": " the most common pair so now that we've identified the most common pair we would like to iterate over"
    },
    {
        "id": 302,
        "start": 1840.56005859375,
        "end": 1847.8399658203125,
        "text": " this sequence we're going to mint a new token with the id of 256 right because these tokens currently"
    },
    {
        "id": 303,
        "start": 1847.8399658203125,
        "end": 1855.43994140625,
        "text": " go from 0 to 255 so when we create a new token it will have an id of 256 and we're going to iterate"
    },
    {
        "id": 304,
        "start": 1855.43994140625,
        "end": 1863.199951171875,
        "text": " over this entire um list and every every time we see 101,32 we're going to swap that out"
    },
    {
        "id": 305,
        "start": 1863.199951171875,
        "end": 1871.199951171875,
        "text": " for 256 so let's implement that now and feel free to do that yourself as well so first i commented"
    },
    {
        "id": 306,
        "start": 1871.199951171875,
        "end": 1878.6400146484375,
        "text": " this just so we don't pollute the notebook too much this is a nice way of in python obtaining"
    },
    {
        "id": 307,
        "start": 1878.6400146484375,
        "end": 1885.0400390625,
        "text": " the highest ranking pair so we're basically calling the max on this dictionary stats and"
    },
    {
        "id": 308,
        "start": 1885.0400390625,
        "end": 1892.0799560546875,
        "text": " this will return the maximum key and then the question is how does it rank keys so you can"
    },
    {
        "id": 309,
        "start": 1892.0799560546875,
        "end": 1898.1600341796875,
        "text": " provide it with a function that ranks keys and that function is just stats that get stats that"
    },
    {
        "id": 310,
        "start": 1898.1600341796875,
        "end": 1904.47998046875,
        "text": " get would basically return the value and so we're ranking by the value and getting the maximum key"
    },
    {
        "id": 311,
        "start": 1904.47998046875,
        "end": 1912.47998046875,
        "text": " so it's 101,32 as we saw now to actually merge 101,32 this is the function that i wrote but again"
    },
    {
        "id": 312,
        "start": 1912.47998046875,
        "end": 1917.6800537109375,
        "text": " there are many different versions of it so we're going to take a list of ids and the pair that we"
    },
    {
        "id": 313,
        "start": 1917.6800537109375,
        "end": 1924.9599609375,
        "text": " want to replace and that pair will be replaced with the new index idx so iterating through ids"
    },
    {
        "id": 314,
        "start": 1924.9599609375,
        "end": 1931.0400390625,
        "text": " if we find the pair swap it out for idx so we create this new list and then we start at zero"
    },
    {
        "id": 315,
        "start": 1931.5999755859375,
        "end": 1934.56005859375,
        "text": " and then we go through this entire list sequentially from left to right"
    },
    {
        "id": 316,
        "start": 1935.760009765625,
        "end": 1942.800048828125,
        "text": " and here we are checking for equality at the current position with the pair so here we are"
    },
    {
        "id": 317,
        "start": 1942.800048828125,
        "end": 1947.3599853515625,
        "text": " checking that the pair matches now here's a bit of a tricky condition that you have to append if"
    },
    {
        "id": 318,
        "start": 1947.3599853515625,
        "end": 1953.199951171875,
        "text": " you're trying to be careful and that is that you don't want this here to be out of bounds at the"
    },
    {
        "id": 319,
        "start": 1953.199951171875,
        "end": 1957.9200439453125,
        "text": " very last position when you're on the rightmost element of this list otherwise this would give"
    },
    {
        "id": 320,
        "start": 1957.9200439453125,
        "end": 1962.800048828125,
        "text": " you an out of bounds error so we have to make sure that we're not at the very very last element"
    },
    {
        "id": 321,
        "start": 1962.800048828125,
        "end": 1971.6800537109375,
        "text": " so this would be false for that so if we find a match we append to this new list that replacement"
    },
    {
        "id": 322,
        "start": 1971.6800537109375,
        "end": 1977.0400390625,
        "text": " index and we increment the position by two so we skip over that entire pair but otherwise if we"
    },
    {
        "id": 323,
        "start": 1977.52001953125,
        "end": 1984.8800048828125,
        "text": " found a matching pair we just sort of copy over the element at that position and increment by one"
    },
    {
        "id": 324,
        "start": 1984.8800048828125,
        "end": 1989.6800537109375,
        "text": " and then return this so here's a very small toy example if we have a list five six six seven nine"
    },
    {
        "id": 325,
        "start": 1989.6800537109375,
        "end": 1997.199951171875,
        "text": " one and we want to replace the occurrences of 67 with 99 then calling this on that will give us"
    },
    {
        "id": 326,
        "start": 1997.199951171875,
        "end": 2004.0,
        "text": " what we're asking for so here the 67 is replaced with 99 so now i'm going to uncomment this for"
    },
    {
        "id": 327,
        "start": 2004.0,
        "end": 2011.1199951171875,
        "text": " our actual use case where we want to take our tokens we want to take the top pair here and"
    },
    {
        "id": 328,
        "start": 2011.1199951171875,
        "end": 2020.0799560546875,
        "text": " replace it with 256 to get tokens too if we run this we get the following so recall that previously"
    },
    {
        "id": 329,
        "start": 2020.719970703125,
        "end": 2029.760009765625,
        "text": " we had a length 616 in this list and now we have a length 596 right so this decreased by 20 which"
    },
    {
        "id": 330,
        "start": 2029.760009765625,
        "end": 2035.6800537109375,
        "text": " makes sense because there are 20 occurrences moreover we can try to find 256 here and we"
    },
    {
        "id": 331,
        "start": 2035.6800537109375,
        "end": 2040.800048828125,
        "text": " see plenty of occurrences of it and moreover just double check there should be no occurrence of"
    },
    {
        "id": 332,
        "start": 2040.800048828125,
        "end": 2046.719970703125,
        "text": " 10132 so this is the original array plenty of them and in the second array there are no occurrences"
    },
    {
        "id": 333,
        "start": 2046.719970703125,
        "end": 2054.080078125,
        "text": " of 1132 so we've successfully merged this single pair and now we just iterate this so we are going"
    },
    {
        "id": 334,
        "start": 2054.080078125,
        "end": 2058.800048828125,
        "text": " to go over the sequence again find the most common pair and replace it so let me now write a while"
    },
    {
        "id": 335,
        "start": 2058.800048828125,
        "end": 2063.360107421875,
        "text": " loop that uses these functions to do this sort of iteratively and how many"
    },
    {
        "id": 336,
        "start": 2063.360107421875,
        "end": 2070.16010761261,
        "text": " times do we do it for? Well, that's totally up to us as a hyperparameter. The more steps we take,"
    },
    {
        "id": 337,
        "start": 2070.16010761261,
        "end": 2075.680107116699,
        "text": " the larger will be our vocabulary, and the shorter will be our sequence. And there is some sweet spot"
    },
    {
        "id": 338,
        "start": 2075.680107116699,
        "end": 2080.9601078033447,
        "text": " that we usually find works the best in practice. And so this is kind of a hyperparameter, and we"
    },
    {
        "id": 339,
        "start": 2080.9601078033447,
        "end": 2086.7201080322266,
        "text": " tune it, and we find good vocabulary sizes. As an example, GPT-4 currently uses roughly 100,000"
    },
    {
        "id": 340,
        "start": 2086.7201080322266,
        "end": 2092.4001083374023,
        "text": " tokens. And ballpark, those are reasonable numbers currently instead of the archaeological"
    },
    {
        "id": 341,
        "start": 2092.4001083374023,
        "end": 2098.4801063537598,
        "text": " models. So let me now write, putting it all together and iterating these steps."
    },
    {
        "id": 342,
        "start": 2098.4801063537598,
        "end": 2102.960105895996,
        "text": " Okay, now before we dive into the while loop, I wanted to add one more cell here,"
    },
    {
        "id": 343,
        "start": 2102.960105895996,
        "end": 2106.8801078796387,
        "text": " where I went to the blog post, and instead of grabbing just the first paragraph or two,"
    },
    {
        "id": 344,
        "start": 2106.8801078796387,
        "end": 2111.680107116699,
        "text": " I took the entire blog post, and I stretched it out in a single line. And basically just using"
    },
    {
        "id": 345,
        "start": 2111.680107116699,
        "end": 2115.760108947754,
        "text": " longer text will allow us to have more representative statistics for the byte pairs,"
    },
    {
        "id": 346,
        "start": 2115.760108947754,
        "end": 2119.8401069641113,
        "text": " and we'll just get more sensible results out of it because it's longer text."
    },
    {
        "id": 347,
        "start": 2121.2801055908203,
        "end": 2128.3201065063477,
        "text": " So here we have the raw text. We encode it into bytes using the UTF-8 encoding. And then here,"
    },
    {
        "id": 348,
        "start": 2128.3201065063477,
        "end": 2133.2801055908203,
        "text": " as before, we are just changing it into a list of integers in Python, just so it's easier to work"
    },
    {
        "id": 349,
        "start": 2133.2801055908203,
        "end": 2141.9201049804688,
        "text": " with instead of the raw bytes objects. And then this is the code that I came up with to actually"
    },
    {
        "id": 350,
        "start": 2141.9201049804688,
        "end": 2146.960105895996,
        "text": " do the merging and loop. These two functions here are identical to what we had above. I only"
    },
    {
        "id": 351,
        "start": 2146.960105895996,
        "end": 2153.8401107788086,
        "text": " included them here just so that you have the point of reference here. So these two are identical,"
    },
    {
        "id": 352,
        "start": 2153.8401107788086,
        "end": 2157.680107116699,
        "text": " and then this is the new code that I added. So the first thing we want to do is we want to"
    },
    {
        "id": 353,
        "start": 2157.680107116699,
        "end": 2162.960105895996,
        "text": " decide on a final vocabulary size that we want our tokenizer to have. And as I mentioned, this"
    },
    {
        "id": 354,
        "start": 2162.960105895996,
        "end": 2167.8401107788086,
        "text": " is a hyperparameter, and you set it in some way depending on your best performance. So let's say"
    },
    {
        "id": 355,
        "start": 2167.8401107788086,
        "end": 2172.7201080322266,
        "text": " for us, we're going to use 276 because that way we're going to be doing exactly 20 merges."
    },
    {
        "id": 356,
        "start": 2173.8401107788086,
        "end": 2181.6001052856445,
        "text": " And 20 merges because we already have 256 tokens for the raw bytes. And to reach 276,"
    },
    {
        "id": 357,
        "start": 2181.6001052856445,
        "end": 2189.2801055908203,
        "text": " we have to do 20 merges to add 20 new tokens. Here, this is one way in Python to just create"
    },
    {
        "id": 358,
        "start": 2189.2801055908203,
        "end": 2195.7601013183594,
        "text": " a copy of a list. So I'm taking the tokens list, and by wrapping it in the list, Python will"
    },
    {
        "id": 359,
        "start": 2195.7601013183594,
        "end": 2199.2801055908203,
        "text": " construct a new list of all the individual elements. So this is just a copy operation."
    },
    {
        "id": 360,
        "start": 2200.8001098632812,
        "end": 2206.0001068115234,
        "text": " Then here, I'm creating a merges dictionary. So this merges dictionary is going to maintain"
    },
    {
        "id": 361,
        "start": 2206.0001068115234,
        "end": 2213.2001037597656,
        "text": " basically the child one, child two mapping to a new token. And so what we're going to be building"
    },
    {
        "id": 362,
        "start": 2213.2001037597656,
        "end": 2219.360107421875,
        "text": " up here is a binary tree of merges. But actually, it's not exactly a tree because a tree would have"
    },
    {
        "id": 363,
        "start": 2219.360107421875,
        "end": 2224.4801025390625,
        "text": " a single root node with a bunch of leaves. For us, we're starting with the leaves on the bottom,"
    },
    {
        "id": 364,
        "start": 2224.4801025390625,
        "end": 2229.600112915039,
        "text": " which are the individual bytes. Those are the starting 256 tokens. And then we're starting to"
    },
    {
        "id": 365,
        "start": 2229.600112915039,
        "end": 2237.0401000976562,
        "text": " merge two of them at a time. And so it's not a tree. It's more like a forest as we merge these"
    },
    {
        "id": 366,
        "start": 2237.0401000976562,
        "end": 2245.2001037597656,
        "text": " elements. So for 20 merges, we're going to find the most commonly occurring pair. We're going to"
    },
    {
        "id": 367,
        "start": 2245.2001037597656,
        "end": 2251.0401000976562,
        "text": " mint a new token integer for it. So I here will start at zero. So we're going to start at 256."
    },
    {
        "id": 368,
        "start": 2251.6801147460938,
        "end": 2256.080108642578,
        "text": " We're going to print that we're merging it. And we're going to replace all the occurrences of that"
    },
    {
        "id": 369,
        "start": 2256.080108642578,
        "end": 2262.880111694336,
        "text": " pair with the new newly minted token. And we're going to record that this pair of integers"
    },
    {
        "id": 370,
        "start": 2262.880111694336,
        "end": 2269.600112915039,
        "text": " merged into this new integer. So running this gives us the following output."
    },
    {
        "id": 371,
        "start": 2272.0001068115234,
        "end": 2277.360107421875,
        "text": " So we did 20 merges. And for example, the first merge was exactly as before,"
    },
    {
        "id": 372,
        "start": 2277.360107421875,
        "end": 2284.320114135742,
        "text": " the 101, 32 tokens merging into a new token 256. Now keep in mind that the individual"
    },
    {
        "id": 373,
        "start": 2284.9601135253906,
        "end": 2290.7201080322266,
        "text": " tokens 101 and 32 can still occur in the sequence after merging. It's only when they occur exactly"
    },
    {
        "id": 374,
        "start": 2290.7201080322266,
        "end": 2297.600112915039,
        "text": " consecutively that that becomes 256 now. And in particular, the other thing to notice here is"
    },
    {
        "id": 375,
        "start": 2297.600112915039,
        "end": 2303.2001037597656,
        "text": " that the token 256, which is the newly minted token, is also eligible for merging. So here on"
    },
    {
        "id": 376,
        "start": 2303.2001037597656,
        "end": 2311.6801147460938,
        "text": " the bottom, the 20th merge was a merge of 256 and 259 becoming 275. So every time we replace these"
    },
    {
        "id": 377,
        "start": 2311.6801147460938,
        "end": 2316.2401123046875,
        "text": " tokens, they become eligible for merging in the next round of the iteration. So that's why we're"
    },
    {
        "id": 378,
        "start": 2316.2401123046875,
        "end": 2321.7601013183594,
        "text": " building up a small sort of binary forest instead of a single individual tree. One thing we can take"
    },
    {
        "id": 379,
        "start": 2321.7601013183594,
        "end": 2326.1600952148438,
        "text": " a look at as well is we can take a look at the compression ratio that we've achieved. So in"
    },
    {
        "id": 380,
        "start": 2326.1600952148438,
        "end": 2332.640106201172,
        "text": " particular, we started off with this tokens list. So we started off with 24,000 bytes,"
    },
    {
        "id": 381,
        "start": 2333.2801208496094,
        "end": 2342.1600952148438,
        "text": " and after merging 20 times, we now have only 19,000 tokens. And so therefore, the compression"
    },
    {
        "id": 382,
        "start": 2342.1600952148438,
        "end": 2347.5201110839844,
        "text": " ratio of simply just dividing the two is roughly 1.27. So that's the amount of compression we're"
    },
    {
        "id": 383,
        "start": 2347.5201110839844,
        "end": 2354.2401123046875,
        "text": " able to achieve of this text with only 20 merges. And of course, the more vocabulary elements you"
    },
    {
        "id": 384,
        "start": 2354.2401123046875,
        "end": 2361.6801147460938,
        "text": " add, the greater the compression ratio here would be. Finally, so that's kind of like"
    },
    {
        "id": 385,
        "start": 2363.5201110839844,
        "end": 2368.080108642578,
        "text": " the training of the tokenizer, if you will. Now, one point that I wanted to make is that,"
    },
    {
        "id": 386,
        "start": 2368.080108642578,
        "end": 2373.60009765625,
        "text": " and maybe this is a diagram that can help kind of illustrate, is that tokenizer is a completely"
    },
    {
        "id": 387,
        "start": 2373.60009765625,
        "end": 2377.840118408203,
        "text": " separate object from the large language model itself. So everything in this lecture, we're"
    },
    {
        "id": 388,
        "start": 2377.840118408203,
        "end": 2382.2401123046875,
        "text": " not really touching the LLM itself. We're just training the tokenizer. This is a completely"
    },
    {
        "id": 389,
        "start": 2382.2401123046875,
        "end": 2387.2001037597656,
        "text": " separate preprocessing stage, usually. So the tokenizer will have its own training set,"
    },
    {
        "id": 390,
        "start": 2387.2001037597656,
        "end": 2392.1600952148438,
        "text": " just like a large language model has a potentially different training set. So the tokenizer has a"
    },
    {
        "id": 391,
        "start": 2392.1600952148438,
        "end": 2398.0001220703125,
        "text": " training set of documents on which you're going to train the tokenizer. And then we're performing"
    },
    {
        "id": 392,
        "start": 2398.0001220703125,
        "end": 2402.5601196289062,
        "text": " the byte-pair encoding algorithm, as we saw above, to train the vocabulary of this tokenizer."
    },
    {
        "id": 393,
        "start": 2403.4400939941406,
        "end": 2407.0401000976562,
        "text": " So it has its own training set. It is a preprocessing stage that you would run"
    },
    {
        "id": 394,
        "start": 2407.0401000976562,
        "end": 2412.9601135253906,
        "text": " a single time in the beginning. And the tokenizer is trained using byte-pair encoding algorithm."
    },
    {
        "id": 395,
        "start": 2413.60009765625,
        "end": 2417.5201110839844,
        "text": " Once you have the tokenizer, once it's trained, and you have the vocabulary, and you have the"
    },
    {
        "id": 396,
        "start": 2417.5201110839844,
        "end": 2425.2801208496094,
        "text": " merges, we can do both encoding and decoding. So these two arrows here. So the tokenizer is a"
    },
    {
        "id": 397,
        "start": 2425.2801208496094,
        "end": 2431.60009765625,
        "text": " translation layer between raw text, which is, as we saw, the sequence of Unicode code points."
    },
    {
        "id": 398,
        "start": 2431.60009765625,
        "end": 2436.7200927734375,
        "text": " It can take raw text and turn it into a token sequence. And vice versa, it can take a token"
    },
    {
        "id": 399,
        "start": 2436.7200927734375,
        "end": 2443.9201049804688,
        "text": " sequence and translate it back into raw text. So now that we have trained the tokenizer,"
    },
    {
        "id": 400,
        "start": 2443.9201049804688,
        "end": 2449.2001037597656,
        "text": " and we have these merges, we are going to turn to how we can do the encoding and the decoding step."
    },
    {
        "id": 401,
        "start": 2449.2001037597656,
        "end": 2453.360107421875,
        "text": " If you give me text, here are the tokens. And vice versa, if you give me tokens, here's the text."
    },
    {
        "id": 402,
        "start": 2454.0001220703125,
        "end": 2458.4801025390625,
        "text": " Once we have that, we can translate between these two realms. And then the language model is going"
    },
    {
        "id": 403,
        "start": 2458.4801025390625,
        "end": 2465.0401000976562,
        "text": " to be trained as a step two afterwards. And typically, in a state-of-the-art application,"
    },
    {
        "id": 404,
        "start": 2465.0401000976562,
        "end": 2468.640106201172,
        "text": " you might stake all of your training data for the language model, and you might run it through the"
    },
    {
        "id": 405,
        "start": 2468.640106201172,
        "end": 2473.840118408203,
        "text": " tokenizer and translate everything into a massive token sequence. And then you can throw away the"
    },
    {
        "id": 406,
        "start": 2473.840118408203,
        "end": 2478.880096435547,
        "text": " raw text. You're just left with the tokens themselves. And those are stored on disk."
    },
    {
        "id": 407,
        "start": 2478.880096435547,
        "end": 2482.640106201172,
        "text": " And that is what the large language model is actually reading when it's training on them."
    },
    {
        "id": 408,
        "start": 2482.640106201172,
        "end": 2486.400115966797,
        "text": " So that's one approach that you can take as a single massive pre-processing stage."
    },
    {
        "id": 409,
        "start": 2488.8001098632812,
        "end": 2492.1600952148438,
        "text": " So yeah, basically, I think the most important thing I want to get across is that this is a"
    },
    {
        "id": 410,
        "start": 2492.1600952148438,
        "end": 2497.0401000976562,
        "text": " completely separate stage. It usually has its own entire training set. You may want to have those"
    },
    {
        "id": 411,
        "start": 2497.0401000976562,
        "end": 2501.0401000976562,
        "text": " training sets be different between the tokenizer and the large language model. So for example,"
    },
    {
        "id": 412,
        "start": 2501.0401000976562,
        "end": 2505.360107421875,
        "text": " when you're training the tokenizer, as I mentioned, we don't just care about the performance"
    },
    {
        "id": 413,
        "start": 2505.360107421875,
        "end": 2510.8001098632812,
        "text": " of English text. We care about many different languages. And we also care about code or not"
    },
    {
        "id": 414,
        "start": 2510.8001098632812,
        "end": 2516.080108642578,
        "text": " code. So you may want to look into different kinds of mixtures of different kinds of languages and"
    },
    {
        "id": 415,
        "start": 2516.080108642578,
        "end": 2521.4400939941406,
        "text": " different amounts of code and things like that, because the amount of different language that you"
    },
    {
        "id": 416,
        "start": 2521.4400939941406,
        "end": 2527.60009765625,
        "text": " have in your tokenizer training set will determine how many merges of it there will be. And therefore,"
    },
    {
        "id": 417,
        "start": 2527.60009765625,
        "end": 2535.2801208496094,
        "text": " that determines the density with which this type of data sort of has in the token space."
    },
    {
        "id": 418,
        "start": 2536.080108642578,
        "end": 2540.8001098632812,
        "text": " And so roughly speaking intuitively, if you add some amount of data, like say you have a ton of"
    },
    {
        "id": 419,
        "start": 2540.8001098632812,
        "end": 2545.7601013183594,
        "text": " Japanese data in your tokenizer training set, then that means that more Japanese tokens will"
    },
    {
        "id": 420,
        "start": 2545.7601013183594,
        "end": 2550.9601135253906,
        "text": " get merged. And therefore, Japanese will have shorter sequences. And that's going to be beneficial"
    },
    {
        "id": 421,
        "start": 2550.9601135253906,
        "end": 2556.080108642578,
        "text": " for the large language model, which has a finite context length on which it can work on in the"
    },
    {
        "id": 422,
        "start": 2556.080108642578,
        "end": 2561.9201049804688,
        "text": " token space. So hopefully that makes sense. So we're now going to turn to encoding and decoding"
    },
    {
        "id": 423,
        "start": 2561.9201049804688,
        "end": 2567.1201171875,
        "text": " now that we have trained a tokenizer. So we have our merges. And now how do we do encoding and"
    },
    {
        "id": 424,
        "start": 2567.1201171875,
        "end": 2573.0401000976562,
        "text": " decoding? Okay, so let's begin with decoding, which is this arrow over here. So given a token sequence,"
    },
    {
        "id": 425,
        "start": 2573.60009765625,
        "end": 2579.360107421875,
        "text": " let's go through the tokenizer to get back a Python string object, so the raw text. So this"
    },
    {
        "id": 426,
        "start": 2579.360107421875,
        "end": 2583.840087890625,
        "text": " is the function that we'd like to implement. We're given a list of integers, and we want to return a"
    },
    {
        "id": 427,
        "start": 2583.840087890625,
        "end": 2588.4000854492188,
        "text": " Python string. If you'd like, try to implement this function yourself. It's a fun exercise."
    },
    {
        "id": 428,
        "start": 2588.4000854492188,
        "end": 2594.0001220703125,
        "text": " Otherwise, I'm going to start pasting in my own solution. So there are many different ways to do"
    },
    {
        "id": 429,
        "start": 2594.0001220703125,
        "end": 2600.4000854492188,
        "text": " it. Here's one way. I will create a kind of pre-processing variable that I will call vocab."
    },
    {
        "id": 430,
        "start": 2601.2800903320312,
        "end": 2609.7601318359375,
        "text": " And vocab is a mapping or dictionary in Python from the token ID to the bytes object for that"
    },
    {
        "id": 431,
        "start": 2609.7601318359375,
        "end": 2616.9600830078125,
        "text": " token. So we begin with the raw bytes for tokens from 0 to 255. And then we go in order of all the"
    },
    {
        "id": 432,
        "start": 2616.9600830078125,
        "end": 2625.0401000976562,
        "text": " merges, and we sort of populate this vocab list by doing an addition here. So this is basically"
    },
    {
        "id": 433,
        "start": 2625.60009765625,
        "end": 2630.2401123046875,
        "text": " the bytes representation of the first child followed by the second one. And remember,"
    },
    {
        "id": 434,
        "start": 2630.2401123046875,
        "end": 2635.0401000976562,
        "text": " these are bytes objects. So this addition here is an addition of two bytes objects,"
    },
    {
        "id": 435,
        "start": 2635.0401000976562,
        "end": 2641.0401000976562,
        "text": " just concatenation. So that's what we get here. One tricky thing to be careful with,"
    },
    {
        "id": 436,
        "start": 2641.0401000976562,
        "end": 2647.2800903320312,
        "text": " by the way, is that I'm iterating a dictionary in Python using a dot items. And it really matters"
    },
    {
        "id": 437,
        "start": 2647.2800903320312,
        "end": 2653.4401245117188,
        "text": " that this runs in the order in which we inserted items into the merges dictionary. Luckily,"
    },
    {
        "id": 438,
        "start": 2653.4401245117188,
        "end": 2658.7200927734375,
        "text": " starting with Python 3.7, this is guaranteed to be the case. But before Python 3.7, this iteration"
    },
    {
        "id": 439,
        "start": 2658.7200927734375,
        "end": 2663.2001342773438,
        "text": " may have been out of order with respect to how we inserted elements into merges, and this may"
    },
    {
        "id": 440,
        "start": 2663.2001342773438,
        "end": 2671.0401000976562,
        "text": " not have worked. But we are using modern Python, so we're okay. And then here, given the IDs,"
    },
    {
        "id": 441,
        "start": 2671.0401000976562,
        "end": 2677.5200805664062,
        "text": " the first thing we're going to do is get the tokens. So the way I implemented this here is"
    },
    {
        "id": 442,
        "start": 2678.080078125,
        "end": 2683.4401245117188,
        "text": " I'm iterating over all the IDs. I'm using vocab to look up their bytes. And then here,"
    },
    {
        "id": 443,
        "start": 2683.4401245117188,
        "end": 2688.880126953125,
        "text": " this is one way in Python to concatenate all these bytes together to create our tokens."
    },
    {
        "id": 444,
        "start": 2689.5200805664062,
        "end": 2696.7200927734375,
        "text": " And then these tokens here at this point are raw bytes. So I have to decode using UTF-8 now"
    },
    {
        "id": 445,
        "start": 2696.7200927734375,
        "end": 2702.880126953125,
        "text": " back into Python strings. So previously, we called that encode on a string object to get the bytes,"
    },
    {
        "id": 446,
        "start": 2702.880126953125,
        "end": 2708.3201293945312,
        "text": " and now we're doing it opposite. We're taking the bytes and calling a decode on the bytes object"
    },
    {
        "id": 447,
        "start": 2708.3201293945312,
        "end": 2716.64013671875,
        "text": " to get a string in Python. And then we can return text. So this is how we can do it."
    },
    {
        "id": 448,
        "start": 2716.64013671875,
        "end": 2722.4801025390625,
        "text": " Now, this actually has an issue in the way I implemented it, and this could actually throw"
    },
    {
        "id": 449,
        "start": 2722.4801025390625,
        "end": 2728.3201293945312,
        "text": " an error. So try to figure out why this code could actually result in an error if we plug in"
    },
    {
        "id": 450,
        "start": 2728.8001098632812,
        "end": 2736.1600952148438,
        "text": " some sequence of IDs that is unlucky. So let me demonstrate the issue. When I try to decode"
    },
    {
        "id": 451,
        "start": 2736.1600952148438,
        "end": 2742.4801025390625,
        "text": " just something like 97, I'm going to get a letter A here back. So nothing too crazy happening."
    },
    {
        "id": 452,
        "start": 2743.4401245117188,
        "end": 2750.64013671875,
        "text": " But when I try to decode 128 as a single element, the token 128 is what in string"
    },
    {
        "id": 453,
        "start": 2750.7200927734375,
        "end": 2759.9201049804688,
        "text": " or in Python object, Unicode decoder. UTF-8 can't decode byte 0x80, which is this in hex,"
    },
    {
        "id": 454,
        "start": 2759.9201049804688,
        "end": 2763.9201049804688,
        "text": " in position zero, invalid start byte. What does that mean? Well, to understand what this means,"
    },
    {
        "id": 455,
        "start": 2763.9201049804688,
        "end": 2769.360107421875,
        "text": " we have to go back to our UTF-8 page that I briefly showed earlier. And this is Wikipedia"
    },
    {
        "id": 456,
        "start": 2769.360107421875,
        "end": 2776.880126953125,
        "text": " UTF-8. And basically, there's a specific schema that UTF-8 bytes take. So in particular, if you"
    },
    {
        "id": 457,
        "start": 2776.880126953125,
        "end": 2782.2401123046875,
        "text": " have a multi-byte object for some of the Unicode characters, they have to have this special sort"
    },
    {
        "id": 458,
        "start": 2782.2401123046875,
        "end": 2786.9600830078125,
        "text": " of envelope in how the encoding works. And so what's happening here is that"
    },
    {
        "id": 459,
        "start": 2788.2401123046875,
        "end": 2795.360107421875,
        "text": " invalid start byte, that's because 128, the binary representation of it is one followed by all zeros."
    },
    {
        "id": 460,
        "start": 2796.2401123046875,
        "end": 2800.9600830078125,
        "text": " So we have one and then all zero. And we see here that that doesn't conform to the format"
    },
    {
        "id": 461,
        "start": 2800.9600830078125,
        "end": 2806.0001220703125,
        "text": " because one followed by all zero just doesn't fit any of these rules, so to speak. So it's"
    },
    {
        "id": 462,
        "start": 2806.0001220703125,
        "end": 2812.64013671875,
        "text": " an invalid start byte, which is byte one. This one must have a one following it, and then a zero"
    },
    {
        "id": 463,
        "start": 2812.64013671875,
        "end": 2819.0401000976562,
        "text": " following it, and then the content of your Unicode in hexes here. So basically, we don't exactly"
    },
    {
        "id": 464,
        "start": 2819.0401000976562,
        "end": 2825.840087890625,
        "text": " follow the UTF-8 standard, and this cannot be decoded. And so the way to fix this is to use"
    },
    {
        "id": 465,
        "start": 2827.1201171875,
        "end": 2834.64013671875,
        "text": " this errors equals in bytes.decode function of Python. And by default, errors is strict,"
    },
    {
        "id": 466,
        "start": 2834.64013671875,
        "end": 2841.2001342773438,
        "text": " so we will throw an error if it's not valid UTF-8 bytes encoding. But there are many different"
    },
    {
        "id": 467,
        "start": 2841.2001342773438,
        "end": 2845.5200805664062,
        "text": " things that you could put here on error handling. This is the full list of all the errors that you"
    },
    {
        "id": 468,
        "start": 2845.5200805664062,
        "end": 2851.6801147460938,
        "text": " can use. And in particular, instead of strict, let's change it to replace. And that will replace"
    },
    {
        "id": 469,
        "start": 2851.6801147460938,
        "end": 2859.2001342773438,
        "text": " with this special marker, this replacement character. So errors equals replace,"
    },
    {
        "id": 470,
        "start": 2860.1600952148438,
        "end": 2867.4401245117188,
        "text": " and now we just get that character back. So basically, not every single byte sequence is"
    },
    {
        "id": 471,
        "start": 2867.4401245117188,
        "end": 2874.2401123046875,
        "text": " valid UTF-8. And if it happens that your large language model, for example, predicts your tokens"
    },
    {
        "id": 472,
        "start": 2874.2401123046875,
        "end": 2881.1201171875,
        "text": " in a bad manner, then they might not fall into valid UTF-8, and then we won't be able to decode"
    },
    {
        "id": 473,
        "start": 2881.1201171875,
        "end": 2887.5200805664062,
        "text": " them. So the standard practice is to basically use errors equals replace, and this is what you"
    },
    {
        "id": 474,
        "start": 2887.5200805664062,
        "end": 2893.0401000976562,
        "text": " will also find in the OpenAI code that they released as well. But basically, whenever you"
    },
    {
        "id": 475,
        "start": 2893.0401000976562,
        "end": 2898.2401123046875,
        "text": " see this kind of a character in your output, in that case, something went wrong, and the LM output"
    },
    {
        "id": 476,
        "start": 2898.2401123046875,
        "end": 2905.1201171875,
        "text": " was not valid sequence of tokens. Okay, and now we're going to go the other way. So we are going"
    },
    {
        "id": 477,
        "start": 2905.1201171875,
        "end": 2909.840087890625,
        "text": " to implement this error right here, where we are going to be given a string, and we want to encode"
    },
    {
        "id": 478,
        "start": 2909.840087890625,
        "end": 2917.2001342773438,
        "text": " it into tokens. So this is the signature of the function that we're interested in. And this should"
    },
    {
        "id": 479,
        "start": 2917.4401245117188,
        "end": 2923.2800903320312,
        "text": " print a list of integers of the tokens. So again, try to maybe implement this yourself if you'd like"
    },
    {
        "id": 480,
        "start": 2923.2800903320312,
        "end": 2927.2800903320312,
        "text": " a fun exercise, and pause here. Otherwise, I'm going to start putting in my solution."
    },
    {
        "id": 481,
        "start": 2928.880126953125,
        "end": 2935.840087890625,
        "text": " So again, there are many ways to do this. So this is one of the ways that I came up with."
    },
    {
        "id": 482,
        "start": 2937.2800903320312,
        "end": 2943.4401245117188,
        "text": " So the first thing we're going to do is we are going to take our text, encode it into UTF-8 to"
    },
    {
        "id": 483,
        "start": 2943.4401245117188,
        "end": 2948.080078125,
        "text": " get the raw bytes. And then as before, we're going to call list on the bytes object to get"
    },
    {
        "id": 484,
        "start": 2948.080078125,
        "end": 2954.1600952148438,
        "text": " a list of integers of those bytes. So those are the starting tokens. Those are the raw bytes of"
    },
    {
        "id": 485,
        "start": 2954.1600952148438,
        "end": 2959.4401245117188,
        "text": " our sequence. But now, of course, according to the merges dictionary above, and recall,"
    },
    {
        "id": 486,
        "start": 2959.4401245117188,
        "end": 2966.3201293945312,
        "text": " this was the merges, some of the bytes may be merged, according to this lookup. In addition"
    },
    {
        "id": 487,
        "start": 2966.3201293945312,
        "end": 2970.2401123046875,
        "text": " to that, remember that the merges was built from top to bottom. And this is sort of the order in"
    },
    {
        "id": 488,
        "start": 2970.3201293945312,
        "end": 2975.60009765625,
        "text": " which we inserted stuff into merges. And so we prefer to do all these merges in the beginning"
    },
    {
        "id": 489,
        "start": 2975.60009765625,
        "end": 2981.2800903320312,
        "text": " before we do these merges later. Because, for example, this merge over here relies on the"
    },
    {
        "id": 490,
        "start": 2981.2800903320312,
        "end": 2987.5200805664062,
        "text": " 256, which got merged here. So we have to go in the order from top to bottom, sort of, if we are"
    },
    {
        "id": 491,
        "start": 2987.5200805664062,
        "end": 2993.0401000976562,
        "text": " going to be merging anything. Now, we expect to be doing a few merges. So we're going to be doing"
    },
    {
        "id": 492,
        "start": 2993.0401000976562,
        "end": 3000.880126953125,
        "text": " while true. And now we want to find a pair of bytes that is consecutive that we are allowed"
    },
    {
        "id": 493,
        "start": 3000.880126953125,
        "end": 3006.0001220703125,
        "text": " to merge according to this. In order to reuse some of the functionality that we've already written,"
    },
    {
        "id": 494,
        "start": 3006.0001220703125,
        "end": 3013.0401000976562,
        "text": " I'm going to reuse the function getStats. So recall that getStats will give us the, will"
    },
    {
        "id": 495,
        "start": 3013.0401000976562,
        "end": 3018.7200927734375,
        "text": " basically count up how many times every single pair occurs in our sequence of tokens and return"
    },
    {
        "id": 496,
        "start": 3018.7200927734375,
        "end": 3026.4801025390625,
        "text": " that as a dictionary. And the dictionary was a mapping from all the different byte pairs to the"
    },
    {
        "id": 497,
        "start": 3026.4801025390625,
        "end": 3031.6801147460938,
        "text": " number of times that they occur, right? At this point, we don't actually care how many times"
    },
    {
        "id": 498,
        "start": 3031.6801147460938,
        "end": 3037.0401000976562,
        "text": " they occur in the sequence. We only care what the raw pairs are in that sequence. And so I'm only"
    },
    {
        "id": 499,
        "start": 3037.0401000976562,
        "end": 3041.6801147460938,
        "text": " going to be using basically the keys of this dictionary. I only care about the set of possible"
    },
    {
        "id": 500,
        "start": 3041.6801147460938,
        "end": 3046.7200927734375,
        "text": " merge candidates, if that makes sense. Now we want to identify the pair that we're going to be"
    },
    {
        "id": 501,
        "start": 3046.7200927734375,
        "end": 3053.2001342773438,
        "text": " merging at this stage of the loop. So what do we want? We want to find the pair or like a key"
    },
    {
        "id": 502,
        "start": 3053.2001342773438,
        "end": 3060.2401123046875,
        "text": " inside stats that has the lowest index in the merges dictionary, because we want to do all the"
    },
    {
        "id": 503,
        "start": 3060.2401123046875,
        "end": 3065.4401245117188,
        "text": " early merges before we work our way to the late merges. So again, there are many different ways"
    },
    {
        "id": 504,
        "start": 3065.4401245117188,
        "end": 3073.2800903320312,
        "text": " to implement this, but I'm going to do something a little bit fancy here. So I'm going to be using"
    },
    {
        "id": 505,
        "start": 3073.2800903320312,
        "end": 3078.880126953125,
        "text": " the min over an iterator. In Python, when you call min on an iterator, and stats here is a"
    },
    {
        "id": 506,
        "start": 3078.880126953125,
        "end": 3084.880126953125,
        "text": " dictionary, we're going to be iterating the keys of this dictionary in Python. So we're looking at"
    },
    {
        "id": 507,
        "start": 3084.880126953125,
        "end": 3091.4400634765625,
        "text": " all the pairs inside stats, which are all the consecutive pairs. And we're going to be taking"
    },
    {
        "id": 508,
        "start": 3091.4400634765625,
        "end": 3099.2801513671875,
        "text": " the consecutive pair inside tokens that has the minimum what the min takes a key, which gives us"
    },
    {
        "id": 509,
        "start": 3099.2801513671875,
        "end": 3104.7200927734375,
        "text": " the function that is going to return a value over which we're going to do the min. And the one we"
    },
    {
        "id": 510,
        "start": 3104.7200927734375,
        "end": 3115.4400634765625,
        "text": " care about is we care about taking merges, and basically getting that pair's index. So basically,"
    },
    {
        "id": 511,
        "start": 3115.4400634765625,
        "end": 3122.7200927734375,
        "text": " for any pair inside stats, we are going to be looking into merges at what index it has. And"
    },
    {
        "id": 512,
        "start": 3122.7200927734375,
        "end": 3128.16015625,
        "text": " we want to get the pair with the min number. So as an example, if there's a pair 101 and 32,"
    },
    {
        "id": 513,
        "start": 3128.16015625,
        "end": 3133.0401611328125,
        "text": " we definitely want to get that pair. We want to identify it here and return it. And pair would"
    },
    {
        "id": 514,
        "start": 3133.0401611328125,
        "end": 3141.2000732421875,
        "text": " become 101 32 if it occurs. And the reason that I'm putting a float inf here as a fallback is that"
    },
    {
        "id": 515,
        "start": 3141.2000732421875,
        "end": 3147.5201416015625,
        "text": " in the get function, when we basically consider a pair that doesn't occur in the merges,"
    },
    {
        "id": 516,
        "start": 3148.2401123046875,
        "end": 3153.360107421875,
        "text": " then that pair is not eligible to be merged. So if in the token sequence, there's some pair"
    },
    {
        "id": 517,
        "start": 3153.360107421875,
        "end": 3158.400146484375,
        "text": " that is not a merging pair, it cannot be merged, then it doesn't actually occur here,"
    },
    {
        "id": 518,
        "start": 3158.400146484375,
        "end": 3163.2801513671875,
        "text": " and it doesn't have an index, and it cannot be merged, which we will denote as float inf."
    },
    {
        "id": 519,
        "start": 3163.920166015625,
        "end": 3167.920166015625,
        "text": " And the reason infinity is nice here is because for sure we're guaranteed that it's not going to"
    },
    {
        "id": 520,
        "start": 3167.920166015625,
        "end": 3173.840087890625,
        "text": " participate in the list of candidates when we do the min. So this is one way to do it."
    },
    {
        "id": 521,
        "start": 3174.880126953125,
        "end": 3180.9600830078125,
        "text": " So basically, long story short, this returns the most eligible emerging candidate pair that occurs"
    },
    {
        "id": 522,
        "start": 3180.9600830078125,
        "end": 3188.320068359375,
        "text": " in the tokens. Now, one thing to be careful with here is this function here might fail in the"
    },
    {
        "id": 523,
        "start": 3188.320068359375,
        "end": 3197.5201416015625,
        "text": " following way. If there's nothing to merge, then there's nothing in merges that is satisfied"
    },
    {
        "id": 524,
        "start": 3197.5201416015625,
        "end": 3202.880126953125,
        "text": " anymore. There's nothing to merge. Everything just returns float infs. And then the pair,"
    },
    {
        "id": 525,
        "start": 3202.880126953125,
        "end": 3208.7200927734375,
        "text": " I think, will just become the very first element of stats. But this pair is not actually a mergeable"
    },
    {
        "id": 526,
        "start": 3208.7200927734375,
        "end": 3214.56005859375,
        "text": " pair. It just becomes the first pair inside stats arbitrarily because all these pairs evaluate to"
    },
    {
        "id": 527,
        "start": 3214.56005859375,
        "end": 3220.800048828125,
        "text": " float inf for the merging criterion. So basically, it could be that this doesn't succeed because"
    },
    {
        "id": 528,
        "start": 3220.800048828125,
        "end": 3226.320068359375,
        "text": " there's no more merging pairs. So if this pair is not in merges that was returned, then this is a"
    },
    {
        "id": 529,
        "start": 3226.320068359375,
        "end": 3231.2801513671875,
        "text": " signal for us that actually there was nothing to merge. No single pair can be merged anymore."
    },
    {
        "id": 530,
        "start": 3231.2801513671875,
        "end": 3236.080078125,
        "text": " In that case, we will break out. Nothing else can be merged."
    },
    {
        "id": 531,
        "start": 3238.800048828125,
        "end": 3242.56005859375,
        "text": " You may come up with a different implementation, by the way. This is kind of like really trying"
    },
    {
        "id": 532,
        "start": 3242.56005859375,
        "end": 3248.4801025390625,
        "text": " hard in Python. But really, we're just trying to find a pair that can be merged with a lowest"
    },
    {
        "id": 533,
        "start": 3248.4801025390625,
        "end": 3256.400146484375,
        "text": " index here. Now, if we did find a pair that is inside merges with the lowest index, then we can"
    },
    {
        "id": 534,
        "start": 3256.400146484375,
        "end": 3265.0401611328125,
        "text": " merge it. So we're going to look into the mergers dictionary for that pair to look up the index."
    },
    {
        "id": 535,
        "start": 3265.0401611328125,
        "end": 3270.0001220703125,
        "text": " And we're going to now merge that into that index. So we're going to do tokens equals,"
    },
    {
        "id": 536,
        "start": 3270.0001220703125,
        "end": 3276.400146484375,
        "text": " and we're going to replace the original tokens. We're going to be replacing the pair pair,"
    },
    {
        "id": 537,
        "start": 3276.400146484375,
        "end": 3281.4400634765625,
        "text": " and we're going to be replacing it with index idx. And this returns a new list of tokens"
    },
    {
        "id": 538,
        "start": 3281.4400634765625,
        "end": 3285.1201171875,
        "text": " where every occurrence of pair is replaced with idx. So we're doing a merge."
    },
    {
        "id": 539,
        "start": 3286.080078125,
        "end": 3289.5201416015625,
        "text": " And we're going to be continuing this until eventually nothing can be merged. We'll come"
    },
    {
        "id": 540,
        "start": 3289.5201416015625,
        "end": 3295.7601318359375,
        "text": " out here, and we'll break out. And here we just return tokens. And so that's the"
    },
    {
        "id": 541,
        "start": 3295.7601318359375,
        "end": 3302.880126953125,
        "text": " implementation, I think. So hopefully this runs. Okay, cool. Yeah, and this looks reasonable. So"
    },
    {
        "id": 542,
        "start": 3302.880126953125,
        "end": 3310.4801025390625,
        "text": " for example, 32 is a space in ASCII, so that's here. So this looks like it worked. Great."
    },
    {
        "id": 543,
        "start": 3310.4801025390625,
        "end": 3314.800048828125,
        "text": " Okay, so let's wrap up this section of the video, at least. I wanted to point out that this is not"
    },
    {
        "id": 544,
        "start": 3314.800048828125,
        "end": 3319.360107421875,
        "text": " quite the right implementation just yet, because we are leaving out a special case. So in particular,"
    },
    {
        "id": 545,
        "start": 3319.360107421875,
        "end": 3325.840087890625,
        "text": " if we try to do this, this would give us an error. And the issue is that if we only have a single"
    },
    {
        "id": 546,
        "start": 3325.840087890625,
        "end": 3330.320068359375,
        "text": " character or an empty string, then stats is empty, and that causes an issue inside min."
    },
    {
        "id": 547,
        "start": 3330.880126953125,
        "end": 3337.1201171875,
        "text": " So one way to fight this is if len of tokens is at least two, because if it's less than two,"
    },
    {
        "id": 548,
        "start": 3337.1201171875,
        "end": 3341.60009765625,
        "text": " it's just a single token or no tokens, then there's nothing to merge, so we just return."
    },
    {
        "id": 549,
        "start": 3342.16015625,
        "end": 3349.2801513671875,
        "text": " So that would fix that case. Okay, and then second, I have a few test cases here for us as well."
    },
    {
        "id": 550,
        "start": 3349.840087890625,
        "end": 3356.4801025390625,
        "text": " So first, let's make sure about, or let's note the following. If we take a string, and we try"
    },
    {
        "id": 551,
        "start": 3356.4801025390625,
        "end": 3360.2401123046875,
        "text": " to encode it, and then decode it back, you'd expect to get the same string back, right?"
    },
    {
        "id": 552,
        "start": 3360.880126953125,
        "end": 3368.2401123046875,
        "text": " Is that true for all strings? So I think, so here it is the case, and I think in general,"
    },
    {
        "id": 553,
        "start": 3368.2401123046875,
        "end": 3375.1201171875,
        "text": " this is probably the case. But notice that going backwards is not, you're not going to have an"
    },
    {
        "id": 554,
        "start": 3375.1201171875,
        "end": 3383.360107421875,
        "text": " identity going backwards, because as I mentioned, not all token sequences are valid UTF-8 sort of"
    },
    {
        "id": 555,
        "start": 3383.360107421875,
        "end": 3389.840087890625,
        "text": " byte streams. And so therefore, some of them can't even be decodable. So this only goes in"
    },
    {
        "id": 556,
        "start": 3389.840087890625,
        "end": 3395.4400634765625,
        "text": " one direction. But for that one direction, we can check here, if we take the training text,"
    },
    {
        "id": 557,
        "start": 3395.4400634765625,
        "end": 3399.2000732421875,
        "text": " which is the text that we trained the tokenizer on, we can make sure that when we encode and decode,"
    },
    {
        "id": 558,
        "start": 3399.2000732421875,
        "end": 3404.320068359375,
        "text": " we get the same thing back, which is true. And here, I took some validation data. So I went to,"
    },
    {
        "id": 559,
        "start": 3404.320068359375,
        "end": 3409.2000732421875,
        "text": " I think, this web page, and I grabbed some text. So this is text that the tokenizer has not seen,"
    },
    {
        "id": 560,
        "start": 3409.2000732421875,
        "end": 3414.080078125,
        "text": " and we can make sure that this also works. Okay, so that gives us some confidence that this was"
    },
    {
        "id": 561,
        "start": 3414.080078125,
        "end": 3419.840087890625,
        "text": " correctly implemented. So those are the basics of the byte pair encoding algorithm. We saw how we"
    },
    {
        "id": 562,
        "start": 3419.840087890625,
        "end": 3425.840087890625,
        "text": " can take some training set, train a tokenizer. The parameters of this tokenizer really are just"
    },
    {
        "id": 563,
        "start": 3425.840087890625,
        "end": 3430.64013671875,
        "text": " this dictionary of merges. And that basically creates the little binary forest on top of raw"
    },
    {
        "id": 564,
        "start": 3430.64013671875,
        "end": 3437.360107421875,
        "text": " bytes. Once we have this, the merges table, we can both encode and decode between raw text and"
    },
    {
        "id": 565,
        "start": 3437.360107421875,
        "end": 3443.0401611328125,
        "text": " token sequences. So that's the simplest setting of the tokenizer. What we're going to do now,"
    },
    {
        "id": 566,
        "start": 3443.0401611328125,
        "end": 3446.56005859375,
        "text": " though, is we're going to look at some of the state-of-the-art large language models and the"
    },
    {
        "id": 567,
        "start": 3446.56005859375,
        "end": 3450.320068359375,
        "text": " kinds of tokenizers that they use. And we're going to see that this picture complexifies very"
    },
    {
        "id": 568,
        "start": 3450.320068359375,
        "end": 3456.16015625,
        "text": " quickly. So we're going to go through the details of this complexification one at a time."
    },
    {
        "id": 569,
        "start": 3457.1201171875,
        "end": 3461.840087890625,
        "text": " So let's kick things off by looking at the GPT series. So in particular, I have the GPT-2 paper"
    },
    {
        "id": 570,
        "start": 3461.840087890625,
        "end": 3469.1201171875,
        "text": " here. And this paper is from 2019 or so, so five years ago. And let's scroll down to"
    },
    {
        "id": 571,
        "start": 3469.6800537109375,
        "end": 3474.16015625,
        "text": " input representation. This is where they talk about the tokenizer that they're using for GPT-2."
    },
    {
        "id": 572,
        "start": 3474.9600830078125,
        "end": 3480.080078125,
        "text": " Now, this is all fairly readable, so I encourage you to pause and read this yourself. But this is"
    },
    {
        "id": 573,
        "start": 3480.080078125,
        "end": 3486.320068359375,
        "text": " where they motivate the use of the byte-pair encoding algorithm on the byte-level representation"
    },
    {
        "id": 574,
        "start": 3486.320068359375,
        "end": 3491.6800537109375,
        "text": " of UTF-8 encoding. So this is where they motivate it, and they talk about the vocabulary sizes"
    },
    {
        "id": 575,
        "start": 3491.6800537109375,
        "end": 3496.4801025390625,
        "text": " and everything. Now, everything here is exactly as we've covered it so far, but things start to"
    },
    {
        "id": 576,
        "start": 3496.4801025390625,
        "end": 3502.16015625,
        "text": " depart around here. So what they mention is that they don't just apply the Naive algorithm as we"
    },
    {
        "id": 577,
        "start": 3502.16015625,
        "end": 3506.880126953125,
        "text": " have done it. And in particular, here's a motivating example. Suppose that you have common"
    },
    {
        "id": 578,
        "start": 3506.880126953125,
        "end": 3512.400146484375,
        "text": " words like dog. What will happen is that dog, of course, occurs very frequently in the text,"
    },
    {
        "id": 579,
        "start": 3512.400146484375,
        "end": 3517.360107421875,
        "text": " and it occurs right next to all kinds of punctuation, as an example. So dog dot,"
    },
    {
        "id": 580,
        "start": 3517.360107421875,
        "end": 3522.9600830078125,
        "text": " dog exclamation mark, dog question mark, et cetera. And naively, you might imagine that the BP"
    },
    {
        "id": 581,
        "start": 3522.9600830078125,
        "end": 3527.360107421875,
        "text": " algorithm could merge these to be single tokens. And then you end up with lots of tokens that are"
    },
    {
        "id": 582,
        "start": 3527.360107421875,
        "end": 3531.2801513671875,
        "text": " just like dog with a slightly different punctuation. And so it feels like you're clustering"
    },
    {
        "id": 583,
        "start": 3531.2801513671875,
        "end": 3535.1201171875,
        "text": " things that shouldn't be clustered. You're combining semantics with punctuation."
    },
    {
        "id": 584,
        "start": 3536.400146484375,
        "end": 3542.0001220703125,
        "text": " And this feels suboptimal. And indeed, they also say that this is suboptimal according to some of"
    },
    {
        "id": 585,
        "start": 3542.0001220703125,
        "end": 3547.360107421875,
        "text": " the experiments. So what they want to do is they want to top down in a manual way, enforce that"
    },
    {
        "id": 586,
        "start": 3547.360107421875,
        "end": 3554.16015625,
        "text": " some types of characters should never be merged together. So they want to enforce these merging"
    },
    {
        "id": 587,
        "start": 3554.16015625,
        "end": 3560.320068359375,
        "text": " rules on top of the byte-pair encoding algorithm. So let's take a look at their code and see how"
    },
    {
        "id": 588,
        "start": 3560.320068359375,
        "end": 3564.9600830078125,
        "text": " they actually enforce this and what kinds of mergers they actually do perform. So I have the"
    },
    {
        "id": 589,
        "start": 3564.9600830078125,
        "end": 3572.880126953125,
        "text": " tab open here for GPT-2 under OpenAI on GitHub. And when we go to source, there is an encoder.py."
    },
    {
        "id": 590,
        "start": 3573.840087890625,
        "end": 3577.920166015625,
        "text": " Now, I don't personally love that they called encoder.py because this is the tokenizer."
    },
    {
        "id": 591,
        "start": 3577.920166015625,
        "end": 3582.400146484375,
        "text": " And the tokenizer can do both encode and decode. So it feels kind of awkward to me that it's"
    },
    {
        "id": 592,
        "start": 3582.400146484375,
        "end": 3586.7200927734375,
        "text": " called encoder, but that is the tokenizer. And there's a lot going on here, and we're"
    },
    {
        "id": 593,
        "start": 3586.7200927734375,
        "end": 3592.0001220703125,
        "text": " going to step through it in detail at one point. For now, I just want to focus on this part here."
    },
    {
        "id": 594,
        "start": 3592.9600830078125,
        "end": 3596.880126953125,
        "text": " They create a regex pattern here that looks very complicated, and we're going to go through it"
    },
    {
        "id": 595,
        "start": 3596.880126953125,
        "end": 3604.400146484375,
        "text": " in a bit. But this is the core part that allows them to enforce rules for what parts of the text"
    },
    {
        "id": 596,
        "start": 3604.400146484375,
        "end": 3610.400146484375,
        "text": " will never be merged for sure. Now, notice that re.compile here is a little bit misleading because"
    },
    {
        "id": 597,
        "start": 3610.400146484375,
        "end": 3615.4400634765625,
        "text": " we're not just doing import re, which is the Python re module. We're doing import regex as re."
    },
    {
        "id": 598,
        "start": 3615.4400634765625,
        "end": 3620.56005859375,
        "text": " And regex is a Python package that you can install, pip install regex, and it's basically"
    },
    {
        "id": 599,
        "start": 3620.56005859375,
        "end": 3628.880126953125,
        "text": " an extension of re, so it's a bit more powerful re. So let's take a look at this pattern and what"
    },
    {
        "id": 600,
        "start": 3628.880126953125,
        "end": 3632.9600830078125,
        "text": " it's doing and why this is actually doing the separation that they are looking for."
    },
    {
        "id": 601,
        "start": 3633.60009765625,
        "end": 3638.080078125,
        "text": " Okay, so I've copy-pasted the pattern here to our Jupyter Notebook where we left off,"
    },
    {
        "id": 602,
        "start": 3638.080078125,
        "end": 3643.1201171875,
        "text": " and let's take this pattern for a spin. So in the exact same way that their code does,"
    },
    {
        "id": 603,
        "start": 3643.2000732421875,
        "end": 3648.320068359375,
        "text": " we're going to call an re.findall for this pattern on any arbitrary string that we are"
    },
    {
        "id": 604,
        "start": 3648.320068359375,
        "end": 3655.1201171875,
        "text": " interested in. So this is the string that we want to encode into tokens to feed into an LLM,"
    },
    {
        "id": 605,
        "start": 3655.1201171875,
        "end": 3661.5201416015625,
        "text": " like GPT-2. So what exactly is this doing? Well, re.findall will take this pattern and try to match"
    },
    {
        "id": 606,
        "start": 3661.5201416015625,
        "end": 3668.400146484375,
        "text": " it against this string. The way this works is that you are going from left to right in the string,"
    },
    {
        "id": 607,
        "start": 3668.400146484375,
        "end": 3674.880126953125,
        "text": " and you're trying to match the pattern. And re.findall will get all the occurrences"
    },
    {
        "id": 608,
        "start": 3674.880126953125,
        "end": 3680.800048828125,
        "text": " and organize them into a list. Now, when you look at this pattern, first of all,"
    },
    {
        "id": 609,
        "start": 3680.800048828125,
        "end": 3687.6800537109375,
        "text": " notice that this is a raw string, and then these are three double quotes just to start the string."
    },
    {
        "id": 610,
        "start": 3687.6800537109375,
        "end": 3694.0001220703125,
        "text": " So really, the string itself, this is the pattern itself, right? And notice that it's made up of a"
    },
    {
        "id": 611,
        "start": 3694.0001220703125,
        "end": 3700.800048828125,
        "text": " lot of ORs. So see these vertical bars? Those are ORs in regex. And so you go from left to right"
    },
    {
        "id": 612,
        "start": 3700.800048828125,
        "end": 3706.2401123046875,
        "text": " in this pattern and try to match it against the string wherever you are. So we have hello,"
    },
    {
        "id": 613,
        "start": 3706.2401123046875,
        "end": 3711.2000732421875,
        "text": " and we're going to try to match it. Well, it's not apostrophe s, it's not apostrophe t, or any of"
    },
    {
        "id": 614,
        "start": 3711.2000732421875,
        "end": 3720.16015625,
        "text": " these, but it is an optional space followed by dash p of, sorry, slash p of L one or more times."
    },
    {
        "id": 615,
        "start": 3720.16015625,
        "end": 3727.1201171875,
        "text": " What is slash p of L? It is coming to some documentation that I found. There might be"
    },
    {
        "id": 616,
        "start": 3727.1201171875,
        "end": 3734.800048828125,
        "text": " other sources as well. Slash p of L is a letter, any kind of letter from any language. And hello"
    },
    {
        "id": 617,
        "start": 3734.800048828125,
        "end": 3741.360107421875,
        "text": " is made up of letters, H-E-L-L-O, et cetera. So optional space followed by a bunch of letters,"
    },
    {
        "id": 618,
        "start": 3741.360107421875,
        "end": 3747.4400634765625,
        "text": " one or more letters, is going to match hello, but then the match ends because a whitespace"
    },
    {
        "id": 619,
        "start": 3747.4400634765625,
        "end": 3754.880126953125,
        "text": " is not a letter. So from there on begins a new sort of attempt to match against the string again."
    },
    {
        "id": 620,
        "start": 3755.6800537109375,
        "end": 3760.080078125,
        "text": " And starting in here, we're going to skip over all of these again until we get to the exact"
    },
    {
        "id": 621,
        "start": 3760.080078125,
        "end": 3764.880126953125,
        "text": " same point again. And we see that there's an optional space. This is the optional space"
    },
    {
        "id": 622,
        "start": 3764.880126953125,
        "end": 3769.360107421875,
        "text": " followed by a bunch of letters, one or more of them. And so that matches. So when we run this,"
    },
    {
        "id": 623,
        "start": 3770.0001220703125,
        "end": 3778.080078125,
        "text": " we get a list of two elements, hello, and then space world. So how are you if we add more letters?"
    },
    {
        "id": 624,
        "start": 3778.64013671875,
        "end": 3782.9600830078125,
        "text": " We would just get them like this. Now, what is this doing and why is this important?"
    },
    {
        "id": 625,
        "start": 3783.5201416015625,
        "end": 3790.800048828125,
        "text": " We are taking our string and instead of directly encoding it for tokenization, we are first"
    },
    {
        "id": 626,
        "start": 3790.800048828125,
        "end": 3795.0401611328125,
        "text": " splitting it up. And when you actually step through the code, and we'll do that in a bit"
    },
    {
        "id": 627,
        "start": 3795.0401611328125,
        "end": 3801.1201171875,
        "text": " more detail, what really it's doing on a high level is that it first splits your text into"
    },
    {
        "id": 628,
        "start": 3801.7601318359375,
        "end": 3807.5201416015625,
        "text": " a list of texts, just like this one. And all these elements of this list are processed independently"
    },
    {
        "id": 629,
        "start": 3807.5201416015625,
        "end": 3812.400146484375,
        "text": " by the tokenizer. And all of the results of that processing are simply concatenated."
    },
    {
        "id": 630,
        "start": 3813.1201171875,
        "end": 3821.0401611328125,
        "text": " So hello, world. Oh, I missed how. Hello, world, how are you? We have five elements of a list."
    },
    {
        "id": 631,
        "start": 3821.0401611328125,
        "end": 3829.1201171875,
        "text": " All of these will independently go from text to a token sequence. And then that token sequence is"
    },
    {
        "id": 632,
        "start": 3829.1201171875,
        "end": 3834.880126953125,
        "text": " going to be concatenated. It's all going to be joined up. And roughly speaking, what that does"
    },
    {
        "id": 633,
        "start": 3834.880126953125,
        "end": 3839.920166015625,
        "text": " is you're only ever finding merges between the elements of this list. So you can only ever"
    },
    {
        "id": 634,
        "start": 3839.920166015625,
        "end": 3846.2401123046875,
        "text": " consider merges within every one of these elements individually. And after you've done"
    },
    {
        "id": 635,
        "start": 3846.2401123046875,
        "end": 3850.9600830078125,
        "text": " all the possible merging for all these elements individually, the results of all that will be"
    },
    {
        "id": 636,
        "start": 3850.9600830078125,
        "end": 3858.880126953125,
        "text": " joined by concatenation. And so you are basically, what you're doing effectively is you are never"
    },
    {
        "id": 637,
        "start": 3858.880126953125,
        "end": 3864.2401123046875,
        "text": " going to be merging this E with this space, because they are now parts of the separate"
    },
    {
        "id": 638,
        "start": 3864.2401123046875,
        "end": 3871.360107421875,
        "text": " elements of this list. And so you are saying we are never going to merge E space, because we're"
    },
    {
        "id": 639,
        "start": 3871.360107421875,
        "end": 3877.60009765625,
        "text": " breaking it up in this way. So basically using this regex pattern to chunk up the text is just"
    },
    {
        "id": 640,
        "start": 3877.60009765625,
        "end": 3884.16015625,
        "text": " one way of enforcing that some merges are not to happen. And we're going to go into more of this"
    },
    {
        "id": 641,
        "start": 3884.16015625,
        "end": 3887.920166015625,
        "text": " text. And we'll see that what this is trying to do on a high level is we're trying to not merge"
    },
    {
        "id": 642,
        "start": 3887.920166015625,
        "end": 3893.840087890625,
        "text": " across letters, across numbers, across punctuation, and so on. So let's see in more detail how that"
    },
    {
        "id": 643,
        "start": 3893.840087890625,
        "end": 3901.1201171875,
        "text": " works. So let's continue now. We have slash p of n. If you go to the documentation, slash p of n"
    },
    {
        "id": 644,
        "start": 3901.1201171875,
        "end": 3906.800048828125,
        "text": " is any kind of numeric character in any script. So it's numbers. So we have an optional space"
    },
    {
        "id": 645,
        "start": 3906.800048828125,
        "end": 3911.360107421875,
        "text": " followed by numbers, and those would be separated out. So letters and numbers are being separated."
    },
    {
        "id": 646,
        "start": 3911.360107421875,
        "end": 3916.800048828125,
        "text": " So if I do, hello, world, one, two, three, how are you? Then world will stop matching here,"
    },
    {
        "id": 647,
        "start": 3916.800048828125,
        "end": 3922.320068359375,
        "text": " because one is not a letter anymore. But one is a number, so this group will match for that,"
    },
    {
        "id": 648,
        "start": 3922.320068359375,
        "end": 3929.2000732421875,
        "text": " and we'll get it as a separate entity. Let's see how these apostrophes work. So here,"
    },
    {
        "id": 649,
        "start": 3929.2000732421875,
        "end": 3939.2000732421875,
        "text": " if we have apostrophe v as an example, then apostrophe here is not a letter or a number."
    },
    {
        "id": 650,
        "start": 3940.400146484375,
        "end": 3947.0401611328125,
        "text": " So hello will stop matching, and then we will exactly match this with that. So that will come"
    },
    {
        "id": 651,
        "start": 3947.0401611328125,
        "end": 3952.320068359375,
        "text": " out as a separate thing. So why are they doing the apostrophes here? Honestly, I think that these"
    },
    {
        "id": 652,
        "start": 3952.320068359375,
        "end": 3959.5201416015625,
        "text": " are just very common apostrophes that are used typically. I don't love that they've done this,"
    },
    {
        "id": 653,
        "start": 3959.5201416015625,
        "end": 3966.800048828125,
        "text": " because let me show you what happens when you have some Unicode apostrophes. For example,"
    },
    {
        "id": 654,
        "start": 3968.0001220703125,
        "end": 3972.0001220703125,
        "text": " if you have house, then this will be separated out because of this matching."
    },
    {
        "id": 655,
        "start": 3972.880126953125,
        "end": 3980.2401123046875,
        "text": " But if you use the Unicode apostrophe like this, then suddenly this does not work. And so this"
    },
    {
        "id": 656,
        "start": 3980.2401123046875,
        "end": 3986.16015625,
        "text": " apostrophe will actually become its own thing now. And so it's basically hard-coded for this"
    },
    {
        "id": 657,
        "start": 3986.16015625,
        "end": 3991.920166015625,
        "text": " specific kind of apostrophe, and otherwise they become completely separate tokens."
    },
    {
        "id": 658,
        "start": 3992.9600830078125,
        "end": 3999.2801513671875,
        "text": " In addition to this, you can go to the GPT-2 docs, and here when they define the pattern,"
    },
    {
        "id": 659,
        "start": 3999.2801513671875,
        "end": 4004.2401123046875,
        "text": " they say, should have added re.ignore case, so BP mergers can happen for capitalized versions"
    },
    {
        "id": 660,
        "start": 4004.2401123046875,
        "end": 4009.1201171875,
        "text": " of contractions. So what they're pointing out is that you see how this is apostrophe and then"
    },
    {
        "id": 661,
        "start": 4009.1201171875,
        "end": 4016.400146484375,
        "text": " lowercase letters? Well, because they didn't do re.ignore case, then these rules will not"
    },
    {
        "id": 662,
        "start": 4016.400146484375,
        "end": 4026.7200927734375,
        "text": " separate out the apostrophes if it's uppercase. So house would be like this. But if I did house"
    },
    {
        "id": 663,
        "start": 4027.60009765625,
        "end": 4031.920166015625,
        "text": " from uppercase, then notice suddenly the apostrophe comes by itself."
    },
    {
        "id": 664,
        "start": 4033.1201171875,
        "end": 4038.080078125,
        "text": " So the tokenization will work differently in uppercase and lowercase, inconsistently"
    },
    {
        "id": 665,
        "start": 4038.080078125,
        "end": 4041.920166015625,
        "text": " separating out these apostrophes. So it feels extremely gnarly and slightly gross,"
    },
    {
        "id": 666,
        "start": 4043.6800537109375,
        "end": 4048.2401123046875,
        "text": " but that's how that works. Okay, so let's come back. After trying to match a bunch of"
    },
    {
        "id": 667,
        "start": 4048.2401123046875,
        "end": 4052.7200927734375,
        "text": " apostrophe expressions, by the way, the other issue here is that these are quite language-specific,"
    },
    {
        "id": 668,
        "start": 4053.2000732421875,
        "end": 4056.9600830078125,
        "text": " so I don't know that all the languages, for example, use or don't use apostrophes,"
    },
    {
        "id": 669,
        "start": 4056.9600830078125,
        "end": 4062.2401123046875,
        "text": " but that would be inconsistently tokenized as a result. Then we try to match letters,"
    },
    {
        "id": 670,
        "start": 4062.2401123046875,
        "end": 4066.4801025390625,
        "text": " then we try to match numbers, and then if that doesn't work, we fall back to here."
    },
    {
        "id": 671,
        "start": 4067.2801513671875,
        "end": 4071.2000732421875,
        "text": " And what this is saying is, again, optional space followed by something that is not a letter,"
    },
    {
        "id": 672,
        "start": 4071.2000732421875,
        "end": 4076.4801025390625,
        "text": " number, or a space, and one or more of that. So what this is doing effectively is this is"
    },
    {
        "id": 673,
        "start": 4076.4801025390625,
        "end": 4080.080078125,
        "text": " trying to match punctuation, roughly speaking, not letters and not numbers."
    },
    {
        "id": 674,
        "start": 4080.9600830078125,
        "end": 4084.400146484375,
        "text": " So this group will try to trigger for that. So if I do something like this,"
    },
    {
        "id": 675,
        "start": 4085.4400634765625,
        "end": 4092.64013671875,
        "text": " then these parts here are not letters or numbers, but they will actually get caught here,"
    },
    {
        "id": 676,
        "start": 4093.360107421875,
        "end": 4097.0401611328125,
        "text": " and so they become its own group. So we've separated out the punctuation."
    },
    {
        "id": 677,
        "start": 4098.2401123046875,
        "end": 4103.360107421875,
        "text": " And finally, this is also a little bit confusing. So this is matching whitespace,"
    },
    {
        "id": 678,
        "start": 4103.920166015625,
        "end": 4110.800048828125,
        "text": " but this is using a negative lookahead assertion in regex. So what this is doing is it's matching"
    },
    {
        "id": 679,
        "start": 4110.800048828125,
        "end": 4116.72021484375,
        "text": " whitespace up to, but not including, the last whitespace character. Why is this important?"
    },
    {
        "id": 680,
        "start": 4117.68017578125,
        "end": 4121.1201171875,
        "text": " This is pretty subtle, I think. So you see how the whitespace is always"
    },
    {
        "id": 681,
        "start": 4121.1201171875,
        "end": 4126.64013671875,
        "text": " included at the beginning of the word, so space R, space U, etc."
    },
    {
        "id": 682,
        "start": 4126.64013671875,
        "end": 4133.000136852264,
        "text": " Suppose we have a lot of spaces here, what's going to happen here is that these spaces"
    },
    {
        "id": 683,
        "start": 4133.000136852264,
        "end": 4137.880136489868,
        "text": " up to and not including the last character will get caught by this."
    },
    {
        "id": 684,
        "start": 4137.880136489868,
        "end": 4142.160137176514,
        "text": " And what that will do is it will separate out the spaces up to but not including the"
    },
    {
        "id": 685,
        "start": 4142.160137176514,
        "end": 4148.320137023926,
        "text": " last character, so that the last character can come here and join with the space U."
    },
    {
        "id": 686,
        "start": 4148.320137023926,
        "end": 4152.440135955811,
        "text": " And the reason that's nice is because space U is the common token."
    },
    {
        "id": 687,
        "start": 4152.440135955811,
        "end": 4156.220136642456,
        "text": " So if I didn't have these extra spaces here, we would just have space U."
    },
    {
        "id": 688,
        "start": 4156.300136566162,
        "end": 4161.540138244629,
        "text": " And if I add tokens, if I add spaces, we still have a space U, but now we have all this extra"
    },
    {
        "id": 689,
        "start": 4161.540138244629,
        "end": 4162.540138244629,
        "text": " white space."
    },
    {
        "id": 690,
        "start": 4162.540138244629,
        "end": 4168.460136413574,
        "text": " So basically, the GPT-2 tokenizer really likes to have a space letter for numbers, and it"
    },
    {
        "id": 691,
        "start": 4168.460136413574,
        "end": 4172.860137939453,
        "text": " prepends these spaces, and this is just something that it is consistent about."
    },
    {
        "id": 692,
        "start": 4172.860137939453,
        "end": 4174.340137481689,
        "text": " So that's what that is for."
    },
    {
        "id": 693,
        "start": 4174.340137481689,
        "end": 4179.660137176514,
        "text": " And then finally, we have all the last fallback is white space characters."
    },
    {
        "id": 694,
        "start": 4179.660137176514,
        "end": 4188.300136566162,
        "text": " So that would be just, if that doesn't get caught, then this thing will catch any trailing"
    },
    {
        "id": 695,
        "start": 4188.300136566162,
        "end": 4189.820137023926,
        "text": " spaces and so on."
    },
    {
        "id": 696,
        "start": 4189.820137023926,
        "end": 4192.64013671875,
        "text": " I wanted to show one more real world example here."
    },
    {
        "id": 697,
        "start": 4192.64013671875,
        "end": 4196.060134887695,
        "text": " So if we have this string, which is a piece of Python code, and then we try to split it"
    },
    {
        "id": 698,
        "start": 4196.060134887695,
        "end": 4199.3801345825195,
        "text": " up, then this is the kind of output we get."
    },
    {
        "id": 699,
        "start": 4199.3801345825195,
        "end": 4202.9001388549805,
        "text": " So you'll notice that the list has many elements here, and that's because we are splitting"
    },
    {
        "id": 700,
        "start": 4202.9001388549805,
        "end": 4208.440139770508,
        "text": " up fairly often, every time sort of a category changes."
    },
    {
        "id": 701,
        "start": 4208.440139770508,
        "end": 4213.480133056641,
        "text": " So there will never be any mergers within these elements, and that's what you are seeing"
    },
    {
        "id": 702,
        "start": 4213.480133056641,
        "end": 4214.480133056641,
        "text": " here."
    },
    {
        "id": 703,
        "start": 4214.480133056641,
        "end": 4221.240135192871,
        "text": " Now, you might think that in order to train the tokenizer, OpenAI has used this to split"
    },
    {
        "id": 704,
        "start": 4221.240135192871,
        "end": 4226.740135192871,
        "text": " up text into chunks, and then run just a BP algorithm within all the chunks."
    },
    {
        "id": 705,
        "start": 4226.740135192871,
        "end": 4230.320137023926,
        "text": " But that is not exactly what happened, and the reason is the following."
    },
    {
        "id": 706,
        "start": 4230.320137023926,
        "end": 4233.200134277344,
        "text": " Notice that we have the spaces here."
    },
    {
        "id": 707,
        "start": 4233.200134277344,
        "end": 4238.64013671875,
        "text": " Those spaces end up being entire elements, but these spaces never actually end up being"
    },
    {
        "id": 708,
        "start": 4238.64013671875,
        "end": 4243.240135192871,
        "text": " merged by OpenAI, and the way you can tell is that if you copy-paste the exact same chunk"
    },
    {
        "id": 709,
        "start": 4243.240135192871,
        "end": 4249.200134277344,
        "text": " here into TickTokenizer, you see that all the spaces are kept independent, and they're"
    },
    {
        "id": 710,
        "start": 4249.200134277344,
        "end": 4251.840133666992,
        "text": " all token 220."
    },
    {
        "id": 711,
        "start": 4251.840133666992,
        "end": 4257.480133056641,
        "text": " So I think OpenAI at some point enforced some rule that these spaces would never be merged,"
    },
    {
        "id": 712,
        "start": 4257.480133056641,
        "end": 4263.800140380859,
        "text": " and so there's some additional rules on top of just chunking and BP that OpenAI is not"
    },
    {
        "id": 713,
        "start": 4263.800140380859,
        "end": 4264.800140380859,
        "text": " clear about."
    },
    {
        "id": 714,
        "start": 4264.800140380859,
        "end": 4269.920135498047,
        "text": " Now, the training code for the GPT-2 tokenizer was never released, so all we have is the"
    },
    {
        "id": 715,
        "start": 4269.920135498047,
        "end": 4274.200134277344,
        "text": " code that I've already shown you, but this code here that they've released is only the"
    },
    {
        "id": 716,
        "start": 4274.200134277344,
        "end": 4277.14013671875,
        "text": " inference code for the tokens."
    },
    {
        "id": 717,
        "start": 4277.14013671875,
        "end": 4278.400131225586,
        "text": " So this is not the training code."
    },
    {
        "id": 718,
        "start": 4278.400131225586,
        "end": 4281.440139770508,
        "text": " You can't give it a piece of text and train the tokenizer."
    },
    {
        "id": 719,
        "start": 4281.440139770508,
        "end": 4286.560134887695,
        "text": " This is just the inference code, which takes the merges that we have up above and applies"
    },
    {
        "id": 720,
        "start": 4286.560134887695,
        "end": 4289.160140991211,
        "text": " them to a new piece of text."
    },
    {
        "id": 721,
        "start": 4289.160140991211,
        "end": 4294.5201416015625,
        "text": " And so we don't know exactly how OpenAI trained the tokenizer, but it wasn't as simple as"
    },
    {
        "id": 722,
        "start": 4294.5201416015625,
        "end": 4297.840133666992,
        "text": " chunk it up and BP it, whatever it was."
    },
    {
        "id": 723,
        "start": 4297.840133666992,
        "end": 4303.280136108398,
        "text": " Next, I wanted to introduce you to the TickToken library from OpenAI, which is the official"
    },
    {
        "id": 724,
        "start": 4303.280136108398,
        "end": 4306.080139160156,
        "text": " library for tokenization from OpenAI."
    },
    {
        "id": 725,
        "start": 4306.080139160156,
        "end": 4314.220138549805,
        "text": " So this is TickToken, pip install TickToken, and then you can do the tokenization inference."
    },
    {
        "id": 726,
        "start": 4314.220138549805,
        "end": 4315.540130615234,
        "text": " This is, again, not training code."
    },
    {
        "id": 727,
        "start": 4315.540130615234,
        "end": 4318.900131225586,
        "text": " This is only inference code for tokenization."
    },
    {
        "id": 728,
        "start": 4318.900131225586,
        "end": 4323.180130004883,
        "text": " I wanted to show you how you would use it, quite simple, and running this just gives"
    },
    {
        "id": 729,
        "start": 4323.180130004883,
        "end": 4326.460144042969,
        "text": " us the GPT-2 tokens or the GPT-4 tokens."
    },
    {
        "id": 730,
        "start": 4326.460144042969,
        "end": 4329.300140380859,
        "text": " So this is the tokenizer used for GPT-4."
    },
    {
        "id": 731,
        "start": 4329.300140380859,
        "end": 4334.580139160156,
        "text": " And so in particular, we see that the whitespace in GPT-2 remains unmerged, but in GPT-4, these"
    },
    {
        "id": 732,
        "start": 4334.580139160156,
        "end": 4340.0201416015625,
        "text": " whitespaces merge, as we also saw in this one, where here they're all unmerged, but"
    },
    {
        "id": 733,
        "start": 4340.0201416015625,
        "end": 4346.0201416015625,
        "text": " if we go down to GPT-4, they become merged."
    },
    {
        "id": 734,
        "start": 4346.0201416015625,
        "end": 4353.180130004883,
        "text": " Now in the GPT-4 tokenizer, they changed the regular expression that they use to chunk"
    },
    {
        "id": 735,
        "start": 4353.180130004883,
        "end": 4354.5201416015625,
        "text": " up text."
    },
    {
        "id": 736,
        "start": 4354.5201416015625,
        "end": 4359.940139770508,
        "text": " So the way to see this is that if you come to the TickToken library, and then you go"
    },
    {
        "id": 737,
        "start": 4359.940139770508,
        "end": 4365.0201416015625,
        "text": " to this file, TickToken.ext.openaipublic, this is where sort of like the definition"
    },
    {
        "id": 738,
        "start": 4365.0201416015625,
        "end": 4368.600143432617,
        "text": " of all these different tokenizers that OpenAI maintains is."
    },
    {
        "id": 739,
        "start": 4368.600143432617,
        "end": 4372.360137939453,
        "text": " And so necessarily to do the inference, they had to publish some of the details about the"
    },
    {
        "id": 740,
        "start": 4372.360137939453,
        "end": 4373.880142211914,
        "text": " strings."
    },
    {
        "id": 741,
        "start": 4373.880142211914,
        "end": 4376.940139770508,
        "text": " So this is the string that we already saw for GPT-2."
    },
    {
        "id": 742,
        "start": 4376.940139770508,
        "end": 4381.900131225586,
        "text": " It is slightly different, but it is actually equivalent to what we discussed here."
    },
    {
        "id": 743,
        "start": 4381.900131225586,
        "end": 4386.600128173828,
        "text": " So this pattern that we discussed is equivalent to this pattern, and this one just executes"
    },
    {
        "id": 744,
        "start": 4386.600128173828,
        "end": 4388.200134277344,
        "text": " a little bit faster."
    },
    {
        "id": 745,
        "start": 4388.200134277344,
        "end": 4391.040130615234,
        "text": " So here you see a little bit of a slightly different definition, but otherwise it's the"
    },
    {
        "id": 746,
        "start": 4391.040130615234,
        "end": 4392.040130615234,
        "text": " same."
    },
    {
        "id": 747,
        "start": 4392.040130615234,
        "end": 4395.040130615234,
        "text": " We're going to go into special tokens in a bit."
    },
    {
        "id": 748,
        "start": 4395.040130615234,
        "end": 4400.800140380859,
        "text": " And then if you scroll down to CL100K, this is the GPT-4 tokenizer, you see that the pattern"
    },
    {
        "id": 749,
        "start": 4400.800140380859,
        "end": 4403.360137939453,
        "text": " has changed."
    },
    {
        "id": 750,
        "start": 4403.360137939453,
        "end": 4407.64013671875,
        "text": " And this is kind of like the major change in addition to a bunch of other special tokens,"
    },
    {
        "id": 751,
        "start": 4407.64013671875,
        "end": 4410.2801513671875,
        "text": " which we'll go into in a bit again."
    },
    {
        "id": 752,
        "start": 4410.2801513671875,
        "end": 4414.320129394531,
        "text": " Now I'm not going to actually go into the full detail of the pattern change, because"
    },
    {
        "id": 753,
        "start": 4414.320129394531,
        "end": 4415.680145263672,
        "text": " honestly this is mind-numbing."
    },
    {
        "id": 754,
        "start": 4415.680145263672,
        "end": 4421.0001220703125,
        "text": " I would just advise that you pull out ChatGPT and the RegEx documentation and just step"
    },
    {
        "id": 755,
        "start": 4421.0001220703125,
        "end": 4422.080139160156,
        "text": " through it."
    },
    {
        "id": 756,
        "start": 4422.120147705078,
        "end": 4427.240142822266,
        "text": " But really the major changes are, number one, you see this I here?"
    },
    {
        "id": 757,
        "start": 4427.240142822266,
        "end": 4432.5201416015625,
        "text": " That means that the case sensitivity, this is case insensitive match."
    },
    {
        "id": 758,
        "start": 4432.5201416015625,
        "end": 4439.0001220703125,
        "text": " And so the comment that we saw earlier on, oh, we should have used re.uppercase, basically"
    },
    {
        "id": 759,
        "start": 4439.0001220703125,
        "end": 4446.480133056641,
        "text": " we're now going to be matching these, apostrophe s, apostrophe d, apostrophe m, etc."
    },
    {
        "id": 760,
        "start": 4446.480133056641,
        "end": 4449.5201416015625,
        "text": " We're going to be matching them both in lowercase and in uppercase."
    },
    {
        "id": 761,
        "start": 4449.5201416015625,
        "end": 4451.0001220703125,
        "text": " So that's fixed."
    },
    {
        "id": 762,
        "start": 4451.0001220703125,
        "end": 4454.040130615234,
        "text": " There's a bunch of different handling of the whitespace that I'm not going to go into"
    },
    {
        "id": 763,
        "start": 4454.040130615234,
        "end": 4455.720123291016,
        "text": " the full details of."
    },
    {
        "id": 764,
        "start": 4455.720123291016,
        "end": 4460.360137939453,
        "text": " And then one more thing here is you will notice that when they match the numbers, they only"
    },
    {
        "id": 765,
        "start": 4460.360137939453,
        "end": 4463.120147705078,
        "text": " match one to three numbers."
    },
    {
        "id": 766,
        "start": 4463.120147705078,
        "end": 4469.840148925781,
        "text": " So they will never merge numbers that are in more than three digits."
    },
    {
        "id": 767,
        "start": 4469.840148925781,
        "end": 4473.980133056641,
        "text": " Only up to three digits of numbers will ever be merged."
    },
    {
        "id": 768,
        "start": 4473.980133056641,
        "end": 4478.840148925781,
        "text": " And that's one change that they made as well, to prevent tokens that are very, very long"
    },
    {
        "id": 769,
        "start": 4478.840148925781,
        "end": 4481.040130615234,
        "text": " number sequences."
    },
    {
        "id": 770,
        "start": 4481.040130615234,
        "end": 4484.64013671875,
        "text": " But again, we don't really know why they do any of this stuff, because none of this is"
    },
    {
        "id": 771,
        "start": 4484.64013671875,
        "end": 4488.2801513671875,
        "text": " documented and it's just, we just get the pattern."
    },
    {
        "id": 772,
        "start": 4488.2801513671875,
        "end": 4491.440124511719,
        "text": " So yeah, it is what it is."
    },
    {
        "id": 773,
        "start": 4491.440124511719,
        "end": 4494.300140380859,
        "text": " Those are some of the changes that GPT-4 has made."
    },
    {
        "id": 774,
        "start": 4494.300140380859,
        "end": 4499.320129394531,
        "text": " And of course, the vocabulary size went from roughly 50k to roughly 100k."
    },
    {
        "id": 775,
        "start": 4499.320129394531,
        "end": 4503.7601318359375,
        "text": " The next thing I would like to do very briefly is to take you through the GPT-2 encoder.py"
    },
    {
        "id": 776,
        "start": 4503.7601318359375,
        "end": 4506.440124511719,
        "text": " that OpenAI has released."
    },
    {
        "id": 777,
        "start": 4506.440124511719,
        "end": 4509.160125732422,
        "text": " This is the file that I already mentioned to you briefly."
    },
    {
        "id": 778,
        "start": 4509.160125732422,
        "end": 4517.200134277344,
        "text": " Now this file is fairly short and should be relatively understandable to you at this point."
    },
    {
        "id": 779,
        "start": 4517.200134277344,
        "end": 4523.600128173828,
        "text": " Starting at the bottom here, they are loading two files, encoder.json and vocab.bpe."
    },
    {
        "id": 780,
        "start": 4523.600128173828,
        "end": 4524.960144042969,
        "text": " And they do some light processing on it."
    },
    {
        "id": 781,
        "start": 4524.960144042969,
        "end": 4528.5201416015625,
        "text": " And then they call this encoder object, which is the tokenizer."
    },
    {
        "id": 782,
        "start": 4528.5201416015625,
        "end": 4534.360137939453,
        "text": " Now if you'd like to inspect these two files, which together constitute their saved tokenizer,"
    },
    {
        "id": 783,
        "start": 4534.360137939453,
        "end": 4538.320129394531,
        "text": " then you can do that with a piece of code like this."
    },
    {
        "id": 784,
        "start": 4538.320129394531,
        "end": 4541.7601318359375,
        "text": " This is where you can download these two files and you can inspect them if you'd like."
    },
    {
        "id": 785,
        "start": 4541.7601318359375,
        "end": 4546.240142822266,
        "text": " And what you will find is that this encoder, as they call it in their code, is exactly"
    },
    {
        "id": 786,
        "start": 4546.240142822266,
        "end": 4548.64013671875,
        "text": " equivalent to our vocab."
    },
    {
        "id": 787,
        "start": 4548.64013671875,
        "end": 4555.040130615234,
        "text": " So remember here, where we have this vocab object, which allowed us to decode very efficiently."
    },
    {
        "id": 788,
        "start": 4555.040130615234,
        "end": 4561.5201416015625,
        "text": " And basically it took us from the integer to the bytes for that integer."
    },
    {
        "id": 789,
        "start": 4561.5201416015625,
        "end": 4564.960144042969,
        "text": " So our vocab is exactly their encoder."
    },
    {
        "id": 790,
        "start": 4564.960144042969,
        "end": 4570.920135498047,
        "text": " And then their vocab.bpe, confusingly, is actually our merges."
    },
    {
        "id": 791,
        "start": 4570.920135498047,
        "end": 4576.880126953125,
        "text": " So their bpe.merges, which is based on the data inside vocab.bpe, ends up being equivalent"
    },
    {
        "id": 792,
        "start": 4576.880126953125,
        "end": 4578.840148925781,
        "text": " to our merges."
    },
    {
        "id": 793,
        "start": 4578.840148925781,
        "end": 4586.200134277344,
        "text": " So basically they are saving and loading the two variables that for us are also critical,"
    },
    {
        "id": 794,
        "start": 4586.200134277344,
        "end": 4589.040130615234,
        "text": " the merges variable and the vocab variable."
    },
    {
        "id": 795,
        "start": 4589.040130615234,
        "end": 4593.480133056641,
        "text": " Using just these two variables, you can represent a tokenizer and you can both do encoding and"
    },
    {
        "id": 796,
        "start": 4593.480133056641,
        "end": 4596.960144042969,
        "text": " decoding once you've trained this tokenizer."
    },
    {
        "id": 797,
        "start": 4596.960144042969,
        "end": 4603.7601318359375,
        "text": " Now the only thing that is actually slightly confusing inside what OpenAI does here is"
    },
    {
        "id": 798,
        "start": 4603.7601318359375,
        "end": 4608.200134277344,
        "text": " that in addition to this encoder and the decoder, they also have something called a byte encoder"
    },
    {
        "id": 799,
        "start": 4608.200134277344,
        "end": 4610.400146484375,
        "text": " and a byte decoder."
    },
    {
        "id": 800,
        "start": 4610.400146484375,
        "end": 4615.680145263672,
        "text": " And this is actually unfortunately just kind of a spurious implementation detail."
    },
    {
        "id": 801,
        "start": 4615.680145263672,
        "end": 4619.200134277344,
        "text": " It isn't actually deep or interesting in any way, so I'm going to skip the discussion"
    },
    {
        "id": 802,
        "start": 4619.200134277344,
        "end": 4620.200134277344,
        "text": " of it."
    },
    {
        "id": 803,
        "start": 4620.200134277344,
        "end": 4624.120147705078,
        "text": " But what OpenAI does here, for reasons that I don't fully understand, is that not only"
    },
    {
        "id": 804,
        "start": 4624.120147705078,
        "end": 4627.920135498047,
        "text": " have they this tokenizer, which can encode and decode, but they have a whole separate"
    },
    {
        "id": 805,
        "start": 4627.920135498047,
        "end": 4631.920135498047,
        "text": " layer here in addition that is used serially with the tokenizer."
    },
    {
        "id": 806,
        "start": 4631.920135498047,
        "end": 4638.5201416015625,
        "text": " And so you first do byte encode and then encode, and then you do decode and then byte decode."
    },
    {
        "id": 807,
        "start": 4638.5201416015625,
        "end": 4642.960144042969,
        "text": " So that's the loop, and they are just stacked serial on top of each other."
    },
    {
        "id": 808,
        "start": 4642.960144042969,
        "end": 4646.0401611328125,
        "text": " And it's not that interesting, so I won't cover it, and you can step through it if you'd"
    },
    {
        "id": 809,
        "start": 4646.0401611328125,
        "end": 4647.0401611328125,
        "text": " like."
    },
    {
        "id": 810,
        "start": 4647.0401611328125,
        "end": 4651.400146484375,
        "text": " Otherwise, this file, if you ignore the byte encoder and the byte decoder, will be algorithmically"
    },
    {
        "id": 811,
        "start": 4651.400146484375,
        "end": 4653.060119628906,
        "text": " very familiar with you."
    },
    {
        "id": 812,
        "start": 4653.060119628906,
        "end": 4657.0001220703125,
        "text": " And the meat of it here is what they call the BPE function."
    },
    {
        "id": 813,
        "start": 4657.0001220703125,
        "end": 4661.960144042969,
        "text": " And you should recognize this loop here, which is very similar to our own while loop, where"
    },
    {
        "id": 814,
        "start": 4661.960144042969,
        "end": 4667.920166015625,
        "text": " they're trying to identify the bigram, a pair, that they should be merging next."
    },
    {
        "id": 815,
        "start": 4667.920166015625,
        "end": 4672.2401123046875,
        "text": " And then here, just like we had, they have a for loop trying to merge this pair."
    },
    {
        "id": 816,
        "start": 4672.2401123046875,
        "end": 4675.5201416015625,
        "text": " So they will go over all of the sequence, and they will merge the pair whenever they"
    },
    {
        "id": 817,
        "start": 4675.5201416015625,
        "end": 4677.14013671875,
        "text": " find it."
    },
    {
        "id": 818,
        "start": 4677.14013671875,
        "end": 4682.1201171875,
        "text": " And they keep repeating that until they run out of possible merges in the text."
    },
    {
        "id": 819,
        "start": 4682.1201171875,
        "end": 4686.200134277344,
        "text": " So that's the meat of this file, and there's an encode and decode function, just like we"
    },
    {
        "id": 820,
        "start": 4686.200134277344,
        "end": 4687.7601318359375,
        "text": " have implemented it."
    },
    {
        "id": 821,
        "start": 4687.7601318359375,
        "end": 4691.440124511719,
        "text": " So long story short, what I want you to take away at this point is that, unfortunately,"
    },
    {
        "id": 822,
        "start": 4691.440124511719,
        "end": 4694.800109863281,
        "text": " it's a little bit of a messy code that they have, but algorithmically, it is identical"
    },
    {
        "id": 823,
        "start": 4694.800109863281,
        "end": 4697.200134277344,
        "text": " to what we've built up above."
    },
    {
        "id": 824,
        "start": 4697.200134277344,
        "end": 4701.2401123046875,
        "text": " And what we've built up above, if you understand it, is algorithmically what is necessary to"
    },
    {
        "id": 825,
        "start": 4701.2401123046875,
        "end": 4706.680114746094,
        "text": " actually build a BPE tokenizer, train it, and then both encode and decode."
    },
    {
        "id": 826,
        "start": 4706.680114746094,
        "end": 4710.200134277344,
        "text": " The next topic I would like to turn to is that of special tokens."
    },
    {
        "id": 827,
        "start": 4710.200134277344,
        "end": 4715.300109863281,
        "text": " So in addition to tokens that are coming from raw bytes and the BPE merges, we can insert"
    },
    {
        "id": 828,
        "start": 4715.300109863281,
        "end": 4719.7601318359375,
        "text": " all kinds of tokens that we are going to use to delimit different parts of the data or"
    },
    {
        "id": 829,
        "start": 4719.7601318359375,
        "end": 4724.600158691406,
        "text": " introduce to create a special structure of the token streams."
    },
    {
        "id": 830,
        "start": 4724.600158691406,
        "end": 4731.0001220703125,
        "text": " So if you look at this encoder object from OpenAI's GPT-2 right here, we mentioned this"
    },
    {
        "id": 831,
        "start": 4731.0001220703125,
        "end": 4733.1201171875,
        "text": " is very similar to our vocab."
    },
    {
        "id": 832,
        "start": 4733.1201171875,
        "end": 4739.880126953125,
        "text": " You'll notice that the length of this is 50,257."
    },
    {
        "id": 833,
        "start": 4739.880126953125,
        "end": 4744.080139160156,
        "text": " As I mentioned, it's mapping, and it's inverted from the mapping of our vocab."
    },
    {
        "id": 834,
        "start": 4744.080139160156,
        "end": 4749.600158691406,
        "text": " Our vocab goes from integer to string, and they go the other way around for no amazing"
    },
    {
        "id": 835,
        "start": 4749.600158691406,
        "end": 4750.600158691406,
        "text": " reason."
    },
    {
        "id": 836,
        "start": 4750.600158691406,
        "end": 4755.960144042969,
        "text": " But the thing to note here is that the mapping table here is 50,257."
    },
    {
        "id": 837,
        "start": 4755.960144042969,
        "end": 4759.560119628906,
        "text": " Where does that number come from?"
    },
    {
        "id": 838,
        "start": 4759.560119628906,
        "end": 4760.560119628906,
        "text": " Where are the tokens?"
    },
    {
        "id": 839,
        "start": 4761.1201171875,
        "end": 4769.560119628906,
        "text": " As I mentioned, there are 256 raw byte tokens, and then OpenAI actually did 50,000 merges."
    },
    {
        "id": 840,
        "start": 4769.560119628906,
        "end": 4774.920166015625,
        "text": " So those become the other tokens, but this would have been 50,256."
    },
    {
        "id": 841,
        "start": 4774.920166015625,
        "end": 4777.480163574219,
        "text": " So what is the 57th token?"
    },
    {
        "id": 842,
        "start": 4777.480163574219,
        "end": 4781.480163574219,
        "text": " And there is basically one special token."
    },
    {
        "id": 843,
        "start": 4781.480163574219,
        "end": 4786.440124511719,
        "text": " And that one special token, you can see, is called end-of-text."
    },
    {
        "id": 844,
        "start": 4786.440124511719,
        "end": 4790.5201416015625,
        "text": " So this is a special token, and it's the very last token."
    },
    {
        "id": 845,
        "start": 4790.5201416015625,
        "end": 4794.960144042969,
        "text": " And this token is used to delimit documents in the training set."
    },
    {
        "id": 846,
        "start": 4794.960144042969,
        "end": 4798.880126953125,
        "text": " So when we're creating the training data, we have all these documents, and we tokenize"
    },
    {
        "id": 847,
        "start": 4798.880126953125,
        "end": 4801.560119628906,
        "text": " them and get a stream of tokens."
    },
    {
        "id": 848,
        "start": 4801.560119628906,
        "end": 4806.200134277344,
        "text": " Those tokens only range from zero to 50,256."
    },
    {
        "id": 849,
        "start": 4806.200134277344,
        "end": 4812.0001220703125,
        "text": " And then in between those documents, we put special end-of-text token, and we insert that"
    },
    {
        "id": 850,
        "start": 4812.0001220703125,
        "end": 4814.440124511719,
        "text": " token in between documents."
    },
    {
        "id": 851,
        "start": 4814.440124511719,
        "end": 4819.840148925781,
        "text": " And we are using this as a signal to the language model that the document has ended, and what"
    },
    {
        "id": 852,
        "start": 4819.840148925781,
        "end": 4824.200134277344,
        "text": " follows is going to be unrelated to the document previously."
    },
    {
        "id": 853,
        "start": 4824.200134277344,
        "end": 4826.800109863281,
        "text": " That said, the language model has to learn this from data."
    },
    {
        "id": 854,
        "start": 4826.800109863281,
        "end": 4832.0001220703125,
        "text": " It needs to learn that this token usually means that it should wipe its memory of what"
    },
    {
        "id": 855,
        "start": 4832.0001220703125,
        "end": 4835.960144042969,
        "text": " came before, and what came before this token is not actually informative to what comes"
    },
    {
        "id": 856,
        "start": 4835.960144042969,
        "end": 4836.960144042969,
        "text": " next."
    },
    {
        "id": 857,
        "start": 4836.960144042969,
        "end": 4841.200134277344,
        "text": " But we are expecting the language model to just learn this, but we're giving it the special"
    },
    {
        "id": 858,
        "start": 4841.200134277344,
        "end": 4844.0001220703125,
        "text": " delimiter of these documents."
    },
    {
        "id": 859,
        "start": 4844.0001220703125,
        "end": 4849.400146484375,
        "text": " We can go here to TickTokenizer, and that's the GPT to Tokenizer, our code that we've"
    },
    {
        "id": 860,
        "start": 4849.400146484375,
        "end": 4850.960144042969,
        "text": " been playing with before."
    },
    {
        "id": 861,
        "start": 4850.960144042969,
        "end": 4855.7601318359375,
        "text": " So we can add here, right, hello world, how are you, and we're getting different tokens."
    },
    {
        "id": 862,
        "start": 4855.7601318359375,
        "end": 4860.400146484375,
        "text": " But now you can see what happens if I put end-of-text."
    },
    {
        "id": 863,
        "start": 4860.400146484375,
        "end": 4867.600158691406,
        "text": " You see how until I finished it, these are all different tokens, end-of-text, still set"
    },
    {
        "id": 864,
        "start": 4867.600158691406,
        "end": 4874.200134277344,
        "text": " for tokens, and now when I finish it, suddenly we get token 50,256."
    },
    {
        "id": 865,
        "start": 4874.200134277344,
        "end": 4879.920166015625,
        "text": " And the reason this works is because this didn't actually go through the BPE merges."
    },
    {
        "id": 866,
        "start": 4879.920166015625,
        "end": 4886.960144042969,
        "text": " Instead, the code that actually outputs the tokens has special case instructions for handling"
    },
    {
        "id": 867,
        "start": 4886.960144042969,
        "end": 4889.2801513671875,
        "text": " special tokens."
    },
    {
        "id": 868,
        "start": 4889.2801513671875,
        "end": 4894.720153808594,
        "text": " We did not see these special instructions for handling special tokens in the encoder.py."
    },
    {
        "id": 869,
        "start": 4894.720153808594,
        "end": 4896.2801513671875,
        "text": " It's absent there."
    },
    {
        "id": 870,
        "start": 4896.2801513671875,
        "end": 4901.1201171875,
        "text": " But if you go to TickToken library, which is implemented in Rust, you will find all"
    },
    {
        "id": 871,
        "start": 4901.1201171875,
        "end": 4906.560119628906,
        "text": " kinds of special case handling for these special tokens that you can register, create,"
    },
    {
        "id": 872,
        "start": 4906.560119628906,
        "end": 4911.560119628906,
        "text": " add to the vocabulary, and then it looks for them, and whenever it sees these special tokens"
    },
    {
        "id": 873,
        "start": 4911.560119628906,
        "end": 4915.800109863281,
        "text": " like this, it will actually come in and swap in that special token."
    },
    {
        "id": 874,
        "start": 4915.800109863281,
        "end": 4921.440124511719,
        "text": " So these things are outside of the typical algorithm of byte-bearing coding."
    },
    {
        "id": 875,
        "start": 4921.440124511719,
        "end": 4926.560119628906,
        "text": " So these special tokens are used pervasively, not just in basically base language modeling"
    },
    {
        "id": 876,
        "start": 4926.560119628906,
        "end": 4930.480163574219,
        "text": " of predicting the next token in the sequence, but especially when it gets to later, to the"
    },
    {
        "id": 877,
        "start": 4930.840148925781,
        "end": 4935.400146484375,
        "text": " fine-tuning stage, and all of the chat GPT sort of aspects of it."
    },
    {
        "id": 878,
        "start": 4935.400146484375,
        "end": 4938.960144042969,
        "text": " Because we don't just want to delimit documents, we want to delimit entire conversations between"
    },
    {
        "id": 879,
        "start": 4938.960144042969,
        "end": 4941.0201416015625,
        "text": " an assistant and a user."
    },
    {
        "id": 880,
        "start": 4941.0201416015625,
        "end": 4947.080139160156,
        "text": " So if I refresh this TickTokenizer page, the default example that they have here is using"
    },
    {
        "id": 881,
        "start": 4947.080139160156,
        "end": 4954.0401611328125,
        "text": " not sort of base model encoders, but fine-tuned model sort of tokenizers."
    },
    {
        "id": 882,
        "start": 4954.0401611328125,
        "end": 4960.2801513671875,
        "text": " So for example, using the GPT 3.5 Turbo scheme, these here are all special tokens, imstart,"
    },
    {
        "id": 883,
        "start": 4960.2801513671875,
        "end": 4962.960144042969,
        "text": " imend, et cetera."
    },
    {
        "id": 884,
        "start": 4962.960144042969,
        "end": 4967.800109863281,
        "text": " This is short for imaginary malloc underscore start, by the way."
    },
    {
        "id": 885,
        "start": 4967.800109863281,
        "end": 4971.7601318359375,
        "text": " But you can see here that there's a sort of start and end of every single message, and"
    },
    {
        "id": 886,
        "start": 4971.7601318359375,
        "end": 4978.200134277344,
        "text": " there can be many other tokens, lots of tokens, in use to delimit these conversations and"
    },
    {
        "id": 887,
        "start": 4978.200134277344,
        "end": 4981.7601318359375,
        "text": " kind of keep track of the flow of the messages here."
    },
    {
        "id": 888,
        "start": 4981.7601318359375,
        "end": 4984.7601318359375,
        "text": " Now we can go back to the TickToken library."
    },
    {
        "id": 889,
        "start": 4984.7601318359375,
        "end": 4989.7601318359375,
        "text": " And here, when you scroll to the bottom, they talk about how you can extend TickToken, and"
    },
    {
        "id": 890,
        "start": 4989.7601318359375,
        "end": 4996.960144042969,
        "text": " you can create, basically, you can fork the CL 100K base tokenizers in GPT 4."
    },
    {
        "id": 891,
        "start": 4996.960144042969,
        "end": 5000.0001220703125,
        "text": " And for example, you can extend it by adding more special tokens, and these are totally"
    },
    {
        "id": 892,
        "start": 5000.0001220703125,
        "end": 5001.0001220703125,
        "text": " up to you."
    },
    {
        "id": 893,
        "start": 5001.0001220703125,
        "end": 5005.360107421875,
        "text": " You can come up with any arbitrary tokens and add them with a new ID afterwards."
    },
    {
        "id": 894,
        "start": 5005.360107421875,
        "end": 5012.560119628906,
        "text": " And the TickToken library will correctly swap them out when it sees this in the strings."
    },
    {
        "id": 895,
        "start": 5012.560119628906,
        "end": 5017.0401611328125,
        "text": " Now we can also go back to this file, which we looked at previously."
    },
    {
        "id": 896,
        "start": 5017.0401611328125,
        "end": 5023.480163574219,
        "text": " And I mentioned that the GPT 2 in TickToken OpenAI public.py, we have the vocabulary,"
    },
    {
        "id": 897,
        "start": 5023.480163574219,
        "end": 5027.560119628906,
        "text": " we have the pattern for splitting, and then here we are registering the single special"
    },
    {
        "id": 898,
        "start": 5027.560119628906,
        "end": 5033.0001220703125,
        "text": " token in GPT 2, which was the end of text token, and we saw that it has this ID."
    },
    {
        "id": 899,
        "start": 5033.0001220703125,
        "end": 5038.5201416015625,
        "text": " In GPT 4, when they define this here, you see that the pattern has changed, as we discussed,"
    },
    {
        "id": 900,
        "start": 5038.5201416015625,
        "end": 5041.200134277344,
        "text": " but also the special tokens have changed in this tokenizer."
    },
    {
        "id": 901,
        "start": 5041.200134277344,
        "end": 5046.960144042969,
        "text": " So we, of course, have the end of text, just like in GPT 2, but we also see three, sorry,"
    },
    {
        "id": 902,
        "start": 5046.960144042969,
        "end": 5050.960144042969,
        "text": " four additional tokens here, FIM prefix, middle, and suffix."
    },
    {
        "id": 903,
        "start": 5050.960144042969,
        "end": 5051.960144042969,
        "text": " What is FIM?"
    },
    {
        "id": 904,
        "start": 5051.960144042969,
        "end": 5054.600158691406,
        "text": " FIM is short for fill in the middle."
    },
    {
        "id": 905,
        "start": 5054.600158691406,
        "end": 5059.680114746094,
        "text": " And if you'd like to learn more about this idea, it comes from this paper."
    },
    {
        "id": 906,
        "start": 5059.680114746094,
        "end": 5062.600158691406,
        "text": " And I'm not going to go into detail in this video, it's beyond this video."
    },
    {
        "id": 907,
        "start": 5062.600158691406,
        "end": 5066.700134277344,
        "text": " And then there's one additional SERP token here."
    },
    {
        "id": 908,
        "start": 5066.700134277344,
        "end": 5069.440124511719,
        "text": " So that's that encoding as well."
    },
    {
        "id": 909,
        "start": 5069.440124511719,
        "end": 5072.840148925781,
        "text": " So it's very common, basically, to train a language model."
    },
    {
        "id": 910,
        "start": 5072.840148925781,
        "end": 5076.800109863281,
        "text": " And then if you'd like, you can add special tokens."
    },
    {
        "id": 911,
        "start": 5076.800109863281,
        "end": 5082.320129394531,
        "text": " Now when you add special tokens, you, of course, have to do some model surgery to the transformer"
    },
    {
        "id": 912,
        "start": 5082.320129394531,
        "end": 5085.600158691406,
        "text": " and all of the parameters involved in that transformer, because you are basically adding"
    },
    {
        "id": 913,
        "start": 5085.600158691406,
        "end": 5086.600158691406,
        "text": " an integer."
    },
    {
        "id": 914,
        "start": 5086.600158691406,
        "end": 5091.16015625,
        "text": " And you want to make sure that, for example, your embedding matrix for the vocabulary tokens"
    },
    {
        "id": 915,
        "start": 5091.16015625,
        "end": 5093.720153808594,
        "text": " has to be extended by adding a row."
    },
    {
        "id": 916,
        "start": 5093.720153808594,
        "end": 5097.200134277344,
        "text": " And typically this row would be initialized with small random numbers for something like"
    },
    {
        "id": 917,
        "start": 5097.200134277344,
        "end": 5102.360107421875,
        "text": " that, because we need to have a vector that now stands for that token."
    },
    {
        "id": 918,
        "start": 5102.360107421875,
        "end": 5104.960144042969,
        "text": " In addition to that, you have to go to the final layer of the transformer, and you have"
    },
    {
        "id": 919,
        "start": 5104.960144042969,
        "end": 5109.820129394531,
        "text": " to make sure that that projection at the very end into the classifier is extended by one"
    },
    {
        "id": 920,
        "start": 5109.820129394531,
        "end": 5110.820129394531,
        "text": " as well."
    },
    {
        "id": 921,
        "start": 5110.820129394531,
        "end": 5115.360107421875,
        "text": " So basically, there's some model surgery involved that you have to couple with the tokenization"
    },
    {
        "id": 922,
        "start": 5115.360107421875,
        "end": 5118.720153808594,
        "text": " changes if you are going to add special tokens."
    },
    {
        "id": 923,
        "start": 5118.720153808594,
        "end": 5122.1201171875,
        "text": " But this is a very common operation that people do, especially if they'd like to fine tune"
    },
    {
        "id": 924,
        "start": 5122.1201171875,
        "end": 5123.1201171875,
        "text": " the model."
    },
    {
        "id": 925,
        "start": 5123.1201171875,
        "end": 5127.400146484375,
        "text": " For example, taking it from a base model to a chat model, like ChatGPT."
    },
    {
        "id": 926,
        "start": 5127.400146484375,
        "end": 5132.2801513671875,
        "text": " Okay, so at this point, you should have everything you need in order to build your own GPT-4"
    },
    {
        "id": 927,
        "start": 5132.2801513671875,
        "end": 5133.2801513671875,
        "text": " tokenizer."
    },
    {
        "id": 928,
        "start": 5133.2801513671875,
        "end": 5136.7601318359375,
        "text": " Now, in the process of developing this lecture, I've done that, and I've published the code"
    },
    {
        "id": 929,
        "start": 5136.7601318359375,
        "end": 5139.800109863281,
        "text": " under this repository, MinBPE."
    },
    {
        "id": 930,
        "start": 5139.800109863281,
        "end": 5145.200134277344,
        "text": " So MinBPE looks like this right now as I'm recording, but the MinBPE repository will"
    },
    {
        "id": 931,
        "start": 5145.200134277344,
        "end": 5149.64013671875,
        "text": " probably change quite a bit because I intend to continue working on it."
    },
    {
        "id": 932,
        "start": 5149.64013671875,
        "end": 5153.920166015625,
        "text": " In addition to the MinBPE repository, I've published this exercise progression that you"
    },
    {
        "id": 933,
        "start": 5153.920166015625,
        "end": 5154.920166015625,
        "text": " can follow."
    },
    {
        "id": 934,
        "start": 5154.920166015625,
        "end": 5161.0401611328125,
        "text": " So if you go to exercise.md here, this is sort of me breaking up the task ahead of you"
    },
    {
        "id": 935,
        "start": 5161.0401611328125,
        "end": 5166.5201416015625,
        "text": " into four steps that sort of build up to what can be a GPT-4 tokenizer."
    },
    {
        "id": 936,
        "start": 5166.5201416015625,
        "end": 5170.60009765625,
        "text": " And so feel free to follow these steps exactly, and follow a little bit of the guidance that"
    },
    {
        "id": 937,
        "start": 5170.60009765625,
        "end": 5172.2001953125,
        "text": " I've laid out here."
    },
    {
        "id": 938,
        "start": 5172.2001953125,
        "end": 5176.920166015625,
        "text": " And anytime you feel stuck, just reference the MinBPE repository here."
    },
    {
        "id": 939,
        "start": 5176.920166015625,
        "end": 5181.2401123046875,
        "text": " So either the tests could be useful, or the MinBPE repository itself."
    },
    {
        "id": 940,
        "start": 5181.2401123046875,
        "end": 5188.880126953125,
        "text": " I try to keep the code fairly clean and understandable, and so feel free to reference it whenever"
    },
    {
        "id": 941,
        "start": 5188.880126953125,
        "end": 5191.1201171875,
        "text": " you get stuck."
    },
    {
        "id": 942,
        "start": 5191.1201171875,
        "end": 5195.3201904296875,
        "text": " In addition to that, basically, once you write it, you should be able to reproduce this behavior"
    },
    {
        "id": 943,
        "start": 5195.3201904296875,
        "end": 5196.7200927734375,
        "text": " from the token."
    },
    {
        "id": 944,
        "start": 5196.7200927734375,
        "end": 5202.8001708984375,
        "text": " So getting the GPT-4 tokenizer, you can encode this string, and you should get these tokens."
    },
    {
        "id": 945,
        "start": 5202.8001708984375,
        "end": 5206.360107421875,
        "text": " And then you can encode and decode the exact same string to recover it."
    },
    {
        "id": 946,
        "start": 5206.8001708984375,
        "end": 5210.0001220703125,
        "text": " And in addition to all that, you should be able to implement your own train function,"
    },
    {
        "id": 947,
        "start": 5210.0001220703125,
        "end": 5211.920166015625,
        "text": " which TickToken library does not provide."
    },
    {
        "id": 948,
        "start": 5211.920166015625,
        "end": 5213.880126953125,
        "text": " It's, again, only inference code."
    },
    {
        "id": 949,
        "start": 5213.880126953125,
        "end": 5217.8001708984375,
        "text": " But you should write your own train, MinBPE does it as well."
    },
    {
        "id": 950,
        "start": 5217.8001708984375,
        "end": 5221.64013671875,
        "text": " And that will allow you to train your own token vocabularies."
    },
    {
        "id": 951,
        "start": 5221.64013671875,
        "end": 5227.360107421875,
        "text": " So here's some of the code inside MinBPE, shows the token vocabularies that you might"
    },
    {
        "id": 952,
        "start": 5227.360107421875,
        "end": 5228.60009765625,
        "text": " obtain."
    },
    {
        "id": 953,
        "start": 5228.60009765625,
        "end": 5233.400146484375,
        "text": " So on the left here, we have the GPT-4 merges."
    },
    {
        "id": 954,
        "start": 5233.440185546875,
        "end": 5237.0001220703125,
        "text": " So the first 256 are raw individual bytes."
    },
    {
        "id": 955,
        "start": 5237.0001220703125,
        "end": 5241.4801025390625,
        "text": " And then here I am visualizing the merges that GPT-4 performed during its training."
    },
    {
        "id": 956,
        "start": 5241.4801025390625,
        "end": 5247.68017578125,
        "text": " So the very first merge that GPT-4 did was merge two spaces into a single token for two"
    },
    {
        "id": 957,
        "start": 5247.68017578125,
        "end": 5248.68017578125,
        "text": " spaces."
    },
    {
        "id": 958,
        "start": 5248.68017578125,
        "end": 5250.7401123046875,
        "text": " And that is a token 256."
    },
    {
        "id": 959,
        "start": 5250.7401123046875,
        "end": 5254.1201171875,
        "text": " And so this is the order in which things merged during GPT-4 training."
    },
    {
        "id": 960,
        "start": 5254.1201171875,
        "end": 5260.60009765625,
        "text": " And this is the merge order that we obtained in MinBPE by training a tokenizer."
    },
    {
        "id": 961,
        "start": 5260.60009765625,
        "end": 5264.400146484375,
        "text": " And in this case, I trained it on a Wikipedia page of Taylor Swift."
    },
    {
        "id": 962,
        "start": 5264.400146484375,
        "end": 5269.16015625,
        "text": " Not because I'm a Swifty, but because that is one of the longest Wikipedia pages apparently"
    },
    {
        "id": 963,
        "start": 5269.16015625,
        "end": 5270.400146484375,
        "text": " that's available."
    },
    {
        "id": 964,
        "start": 5270.400146484375,
        "end": 5272.400146484375,
        "text": " But she is pretty cool."
    },
    {
        "id": 965,
        "start": 5272.400146484375,
        "end": 5275.840087890625,
        "text": " And what was I going to say?"
    },
    {
        "id": 966,
        "start": 5275.840087890625,
        "end": 5276.840087890625,
        "text": " Yeah."
    },
    {
        "id": 967,
        "start": 5276.840087890625,
        "end": 5279.0401611328125,
        "text": " So you can compare these two vocabularies."
    },
    {
        "id": 968,
        "start": 5279.0401611328125,
        "end": 5285.840087890625,
        "text": " And so as an example, here GPT-4 merged IN to become IN."
    },
    {
        "id": 969,
        "start": 5285.840087890625,
        "end": 5289.880126953125,
        "text": " And we've done the exact same thing on this token, 259."
    },
    {
        "id": 970,
        "start": 5289.880126953125,
        "end": 5295.0401611328125,
        "text": " Here, space T becomes space T. And that happened for us a little bit later as well."
    },
    {
        "id": 971,
        "start": 5295.0401611328125,
        "end": 5299.360107421875,
        "text": " So the difference here is, again, to my understanding, only a difference of the training set."
    },
    {
        "id": 972,
        "start": 5299.360107421875,
        "end": 5303.2801513671875,
        "text": " So as an example, because I see a lot of white space, I expect that GPT-4 probably had a"
    },
    {
        "id": 973,
        "start": 5303.2801513671875,
        "end": 5308.60009765625,
        "text": " lot of Python code in its training set, I'm not sure, for the tokenizer."
    },
    {
        "id": 974,
        "start": 5308.60009765625,
        "end": 5312.880126953125,
        "text": " And here we see much less of that, of course, in the Wikipedia page."
    },
    {
        "id": 975,
        "start": 5312.880126953125,
        "end": 5314.68017578125,
        "text": " So roughly speaking, they look the same."
    },
    {
        "id": 976,
        "start": 5314.68017578125,
        "end": 5317.360107421875,
        "text": " And they look the same because they're running the same algorithm."
    },
    {
        "id": 977,
        "start": 5317.360107421875,
        "end": 5320.8001708984375,
        "text": " And when you train your own, you're probably going to get something similar, depending"
    },
    {
        "id": 978,
        "start": 5320.8001708984375,
        "end": 5321.9600830078125,
        "text": " on what you train it on."
    },
    {
        "id": 979,
        "start": 5321.9600830078125,
        "end": 5326.2801513671875,
        "text": " Okay, so we are now going to move on from TICToken and the way that OpenAI tokenizes"
    },
    {
        "id": 980,
        "start": 5326.2801513671875,
        "end": 5327.2801513671875,
        "text": " its strings."
    },
    {
        "id": 981,
        "start": 5327.2801513671875,
        "end": 5331.5601806640625,
        "text": " And we're going to discuss one more very commonly used library for working with tokenization"
    },
    {
        "id": 982,
        "start": 5331.5601806640625,
        "end": 5334.68017578125,
        "text": " in LLMs, and that is SentencePiece."
    },
    {
        "id": 983,
        "start": 5334.68017578125,
        "end": 5339.7200927734375,
        "text": " So SentencePiece is very commonly used in language models, because unlike TICToken,"
    },
    {
        "id": 984,
        "start": 5339.7200927734375,
        "end": 5344.2001953125,
        "text": " it can do both training and inference, and is quite efficient at both."
    },
    {
        "id": 985,
        "start": 5344.2001953125,
        "end": 5349.2401123046875,
        "text": " It supports a number of algorithms for training vocabularies, but one of them is the byte-pairing"
    },
    {
        "id": 986,
        "start": 5349.2401123046875,
        "end": 5351.1201171875,
        "text": " coding algorithm that we've been looking at."
    },
    {
        "id": 987,
        "start": 5351.1201171875,
        "end": 5353.080078125,
        "text": " So it supports it."
    },
    {
        "id": 988,
        "start": 5353.080078125,
        "end": 5358.9600830078125,
        "text": " Now, SentencePiece is used both by LLAMA and Mistral Series, and many other models as well."
    },
    {
        "id": 989,
        "start": 5358.9600830078125,
        "end": 5362.68017578125,
        "text": " It is on GitHub under google slash SentencePiece."
    },
    {
        "id": 990,
        "start": 5362.68017578125,
        "end": 5366.2401123046875,
        "text": " And the big difference with SentencePiece, and we're going to look at example, because"
    },
    {
        "id": 991,
        "start": 5366.2401123046875,
        "end": 5372.1201171875,
        "text": " this is kind of hard and subtle to explain, is that they think different about the order"
    },
    {
        "id": 992,
        "start": 5372.16015625,
        "end": 5374.400146484375,
        "text": " of operations here."
    },
    {
        "id": 993,
        "start": 5374.400146484375,
        "end": 5380.7601318359375,
        "text": " So in the case of TICToken, we first take our code points in the string, we encode them"
    },
    {
        "id": 994,
        "start": 5380.7601318359375,
        "end": 5383.7200927734375,
        "text": " using UTF-8 to bytes, and then we're merging bytes."
    },
    {
        "id": 995,
        "start": 5383.7200927734375,
        "end": 5385.9600830078125,
        "text": " It's fairly straightforward."
    },
    {
        "id": 996,
        "start": 5385.9600830078125,
        "end": 5391.64013671875,
        "text": " For SentencePiece, it works directly on the level of the code points themselves."
    },
    {
        "id": 997,
        "start": 5391.64013671875,
        "end": 5396.0001220703125,
        "text": " So it looks at whatever code points are available in your training set, and then it starts merging"
    },
    {
        "id": 998,
        "start": 5396.0001220703125,
        "end": 5397.7200927734375,
        "text": " those code points."
    },
    {
        "id": 999,
        "start": 5397.7200927734375,
        "end": 5402.4801025390625,
        "text": " And the BPE is running on the level of code points."
    },
    {
        "id": 1000,
        "start": 5402.4801025390625,
        "end": 5407.18017578125,
        "text": " And if you happen to run out of code points, so there are maybe some rare code points that"
    },
    {
        "id": 1001,
        "start": 5407.18017578125,
        "end": 5412.1201171875,
        "text": " just don't come up too often, and the rarity is determined by this character coverage hyperparameter,"
    },
    {
        "id": 1002,
        "start": 5412.1201171875,
        "end": 5418.8001708984375,
        "text": " then these code points will either get mapped to a special unknown token, like UNK, or if"
    },
    {
        "id": 1003,
        "start": 5418.8001708984375,
        "end": 5423.440185546875,
        "text": " you have the byte fallback option turned on, then it will take those rare code points,"
    },
    {
        "id": 1004,
        "start": 5423.440185546875,
        "end": 5427.4801025390625,
        "text": " it will encode them using UTF-8, and then the individual bytes of that encoding will"
    },
    {
        "id": 1005,
        "start": 5427.4801025390625,
        "end": 5432.360107421875,
        "text": " be translated into tokens, and there are these special byte tokens that basically get added"
    },
    {
        "id": 1006,
        "start": 5432.360107421875,
        "end": 5433.880126953125,
        "text": " to the vocabulary."
    },
    {
        "id": 1007,
        "start": 5433.880126953125,
        "end": 5442.60009765625,
        "text": " So it uses BPE on the code points, and then it falls back to bytes for rare code points."
    },
    {
        "id": 1008,
        "start": 5442.60009765625,
        "end": 5444.0001220703125,
        "text": " And so that's kind of like the difference."
    },
    {
        "id": 1009,
        "start": 5444.0001220703125,
        "end": 5448.0401611328125,
        "text": " Personally, I find the TICToken way significantly cleaner, but it's kind of like a subtle but"
    },
    {
        "id": 1010,
        "start": 5448.0401611328125,
        "end": 5451.3201904296875,
        "text": " pretty major difference between the way they approach tokenization."
    },
    {
        "id": 1011,
        "start": 5451.3201904296875,
        "end": 5456.2001953125,
        "text": " Let's work with a concrete example, because otherwise this is kind of hard to get your"
    },
    {
        "id": 1012,
        "start": 5456.2001953125,
        "end": 5457.68017578125,
        "text": " head around."
    },
    {
        "id": 1013,
        "start": 5457.68017578125,
        "end": 5460.0001220703125,
        "text": " So let's work with a concrete example."
    },
    {
        "id": 1014,
        "start": 5460.0001220703125,
        "end": 5462.64013671875,
        "text": " This is how we can import SentencePiece."
    },
    {
        "id": 1015,
        "start": 5462.64013671875,
        "end": 5466.2801513671875,
        "text": " And then here we're going to take, I think I took the description of SentencePiece, and"
    },
    {
        "id": 1016,
        "start": 5466.2801513671875,
        "end": 5468.4801025390625,
        "text": " I just created a little toy dataset."
    },
    {
        "id": 1017,
        "start": 5468.4801025390625,
        "end": 5473.880126953125,
        "text": " It really likes to have a file, so I created a toy.txt file with this content."
    },
    {
        "id": 1018,
        "start": 5473.880126953125,
        "end": 5477.920166015625,
        "text": " Now what's kind of a little bit crazy about SentencePiece is that there's a ton of options"
    },
    {
        "id": 1019,
        "start": 5477.920166015625,
        "end": 5479.60009765625,
        "text": " and configurations."
    },
    {
        "id": 1020,
        "start": 5479.60009765625,
        "end": 5483.400146484375,
        "text": " And the reason this is so is because SentencePiece has been around, I think, for a while, and"
    },
    {
        "id": 1021,
        "start": 5483.400146484375,
        "end": 5486.64013671875,
        "text": " it really tries to handle a large diversity of things."
    },
    {
        "id": 1022,
        "start": 5486.64013671875,
        "end": 5491.3201904296875,
        "text": " And because it's been around, I think it has quite a bit of accumulated historical baggage"
    },
    {
        "id": 1023,
        "start": 5491.3201904296875,
        "end": 5492.920166015625,
        "text": " as well."
    },
    {
        "id": 1024,
        "start": 5492.920166015625,
        "end": 5496.2401123046875,
        "text": " And so in particular, there's like a ton of configuration arguments."
    },
    {
        "id": 1025,
        "start": 5496.2401123046875,
        "end": 5498.080078125,
        "text": " This is not even all of it."
    },
    {
        "id": 1026,
        "start": 5498.080078125,
        "end": 5505.1201171875,
        "text": " You can go to here to see all the training options, and there's also quite useful documentation"
    },
    {
        "id": 1027,
        "start": 5505.1201171875,
        "end": 5510.0401611328125,
        "text": " when you look at the raw protobuf that is used to represent the trainer spec, and so"
    },
    {
        "id": 1028,
        "start": 5510.0401611328125,
        "end": 5512.400146484375,
        "text": " on."
    },
    {
        "id": 1029,
        "start": 5512.400146484375,
        "end": 5514.64013671875,
        "text": " Many of these options are irrelevant to us."
    },
    {
        "id": 1030,
        "start": 5514.64013671875,
        "end": 5519.920166015625,
        "text": " So maybe to point out one example, dash-dash shrinking factor, this shrinking factor is"
    },
    {
        "id": 1031,
        "start": 5519.920166015625,
        "end": 5522.080078125,
        "text": " not used in the byte-pair encoding algorithm."
    },
    {
        "id": 1032,
        "start": 5522.080078125,
        "end": 5525.68017578125,
        "text": " So this is just an argument that is irrelevant to us."
    },
    {
        "id": 1033,
        "start": 5525.68017578125,
        "end": 5530.3201904296875,
        "text": " It applies to a different training algorithm."
    },
    {
        "id": 1034,
        "start": 5530.3201904296875,
        "end": 5534.60009765625,
        "text": " Now what I tried to do here is I tried to set up SentencePiece in a way that is very,"
    },
    {
        "id": 1035,
        "start": 5534.60009765625,
        "end": 5540.4801025390625,
        "text": " very similar, as far as I can tell, to maybe identical, hopefully, to the way that Llama2"
    },
    {
        "id": 1036,
        "start": 5540.4801025390625,
        "end": 5546.16015625,
        "text": " was trained, so the way they trained their own tokenizer."
    },
    {
        "id": 1037,
        "start": 5546.16015625,
        "end": 5551.0401611328125,
        "text": " And the way I did this was basically you can take the tokenizer.model file that Meta released,"
    },
    {
        "id": 1038,
        "start": 5551.0401611328125,
        "end": 5559.0001220703125,
        "text": " and you can open it using the protobuf file that you can generate, and then you can inspect"
    },
    {
        "id": 1039,
        "start": 5559.0001220703125,
        "end": 5562.8001708984375,
        "text": " all the options, and I tried to copy over all the options that looked relevant."
    },
    {
        "id": 1040,
        "start": 5562.8001708984375,
        "end": 5564.4801025390625,
        "text": " So here we set up the input."
    },
    {
        "id": 1041,
        "start": 5564.4801025390625,
        "end": 5566.64013671875,
        "text": " It's raw text in this file."
    },
    {
        "id": 1042,
        "start": 5566.64013671875,
        "end": 5572.4801025390625,
        "text": " Here's going to be the output, so it's going to be protoc400.model and .vocap."
    },
    {
        "id": 1043,
        "start": 5572.4801025390625,
        "end": 5577.0401611328125,
        "text": " We're saying that we're going to use the BP algorithm, and we want to vocap size of 400."
    },
    {
        "id": 1044,
        "start": 5577.0401611328125,
        "end": 5585.60009765625,
        "text": " And there's a ton of configurations here for basically preprocessing and normalization"
    },
    {
        "id": 1045,
        "start": 5585.60009765625,
        "end": 5587.60009765625,
        "text": " rules, as they're called."
    },
    {
        "id": 1046,
        "start": 5587.60009765625,
        "end": 5591.2001953125,
        "text": " Normalization used to be very prevalent, I would say, before LLMs in natural language"
    },
    {
        "id": 1047,
        "start": 5591.2001953125,
        "end": 5592.2001953125,
        "text": " processing."
    },
    {
        "id": 1048,
        "start": 5592.2001953125,
        "end": 5596.880126953125,
        "text": " So in machine translation and text classification and so on, you want to normalize and simplify"
    },
    {
        "id": 1049,
        "start": 5596.880126953125,
        "end": 5600.9600830078125,
        "text": " the text, and you want to turn it all lowercase, and you want to remove all double whitespace,"
    },
    {
        "id": 1050,
        "start": 5600.9600830078125,
        "end": 5602.16015625,
        "text": " et cetera."
    },
    {
        "id": 1051,
        "start": 5602.16015625,
        "end": 5605.400146484375,
        "text": " And in language models, we prefer not to do any of it, or at least that is my preference"
    },
    {
        "id": 1052,
        "start": 5605.400146484375,
        "end": 5606.7601318359375,
        "text": " as a deep learning person."
    },
    {
        "id": 1053,
        "start": 5606.7601318359375,
        "end": 5608.2001953125,
        "text": " You want to not touch your data."
    },
    {
        "id": 1054,
        "start": 5608.2001953125,
        "end": 5614.080078125,
        "text": " You want to keep the raw data as much as possible in a raw form."
    },
    {
        "id": 1055,
        "start": 5614.080078125,
        "end": 5617.60009765625,
        "text": " So you're basically trying to turn off a lot of this, if you can."
    },
    {
        "id": 1056,
        "start": 5617.60009765625,
        "end": 5621.5601806640625,
        "text": " The other thing that sentence piece does is that it has this concept of sentences."
    },
    {
        "id": 1057,
        "start": 5621.5601806640625,
        "end": 5629.400146484375,
        "text": " So sentence piece, it kind of was developed, I think, early in the days where there was"
    },
    {
        "id": 1058,
        "start": 5629.400146484375,
        "end": 5633.6201171875,
        "text": " an idea that you're training a tokenizer on a bunch of independent sentences."
    },
    {
        "id": 1059,
        "start": 5633.6201171875,
        "end": 5638.2801513671875,
        "text": " So it has a lot of how many sentences you're going to train on, what is the maximum sentence"
    },
    {
        "id": 1060,
        "start": 5638.2801513671875,
        "end": 5644.9600830078125,
        "text": " length, shuffling sentences, and so for it, sentences are kind of like the individual"
    },
    {
        "id": 1061,
        "start": 5644.9600830078125,
        "end": 5645.9600830078125,
        "text": " training examples."
    },
    {
        "id": 1062,
        "start": 5645.9600830078125,
        "end": 5651.16015625,
        "text": " But again, in the context of LLMs, I find that this is a very spurious and weird distinction."
    },
    {
        "id": 1063,
        "start": 5652.16015625,
        "end": 5655.2401123046875,
        "text": " Sentences are just like, don't touch the raw data."
    },
    {
        "id": 1064,
        "start": 5655.2401123046875,
        "end": 5656.7601318359375,
        "text": " Sentences happen to exist."
    },
    {
        "id": 1065,
        "start": 5656.7601318359375,
        "end": 5661.68017578125,
        "text": " But in the raw datasets, there are a lot of in-betweens, like what exactly is a sentence?"
    },
    {
        "id": 1066,
        "start": 5661.68017578125,
        "end": 5664.080078125,
        "text": " What isn't a sentence?"
    },
    {
        "id": 1067,
        "start": 5664.080078125,
        "end": 5668.080078125,
        "text": " And so I think it's really hard to define what an actual sentence is if you really dig"
    },
    {
        "id": 1068,
        "start": 5668.080078125,
        "end": 5672.1201171875,
        "text": " into it, and there could be different concepts of it in different languages or something"
    },
    {
        "id": 1069,
        "start": 5672.1201171875,
        "end": 5673.1201171875,
        "text": " like that."
    },
    {
        "id": 1070,
        "start": 5673.1201171875,
        "end": 5674.5201416015625,
        "text": " So why even introduce the concept?"
    },
    {
        "id": 1071,
        "start": 5674.5201416015625,
        "end": 5676.3201904296875,
        "text": " It doesn't honestly make sense to me."
    },
    {
        "id": 1072,
        "start": 5676.3201904296875,
        "end": 5681.2401123046875,
        "text": " I would just prefer to treat a file as a giant stream of bytes."
    },
    {
        "id": 1073,
        "start": 5681.2401123046875,
        "end": 5685.360107421875,
        "text": " It has a lot of treatment around rare word characters, and when I say word, I mean code"
    },
    {
        "id": 1074,
        "start": 5685.360107421875,
        "end": 5686.360107421875,
        "text": " points."
    },
    {
        "id": 1075,
        "start": 5686.360107421875,
        "end": 5688.18017578125,
        "text": " We're going to come back to this in a second."
    },
    {
        "id": 1076,
        "start": 5688.18017578125,
        "end": 5694.440185546875,
        "text": " And it has a lot of other rules for basically splitting digits, splitting white space and"
    },
    {
        "id": 1077,
        "start": 5694.440185546875,
        "end": 5696.400146484375,
        "text": " numbers and how you deal with that."
    },
    {
        "id": 1078,
        "start": 5696.400146484375,
        "end": 5698.9600830078125,
        "text": " So these are some kind of like merge rules."
    },
    {
        "id": 1079,
        "start": 5698.9600830078125,
        "end": 5703.4801025390625,
        "text": " So I think this is a little bit equivalent to TickToken using the regular expression"
    },
    {
        "id": 1080,
        "start": 5703.4801025390625,
        "end": 5705.4801025390625,
        "text": " to split up categories."
    },
    {
        "id": 1081,
        "start": 5705.64013671875,
        "end": 5709.920166015625,
        "text": " There's kind of an equivalence of it, if you squint at it, in SentenceBees, where you can"
    },
    {
        "id": 1082,
        "start": 5709.920166015625,
        "end": 5716.8001708984375,
        "text": " also, for example, split up the digits and so on."
    },
    {
        "id": 1083,
        "start": 5716.8001708984375,
        "end": 5719.440185546875,
        "text": " There's a few more things here that I'll come back to in a bit, and then there are some"
    },
    {
        "id": 1084,
        "start": 5719.440185546875,
        "end": 5721.60009765625,
        "text": " special tokens that you can indicate."
    },
    {
        "id": 1085,
        "start": 5721.60009765625,
        "end": 5728.68017578125,
        "text": " And it hardcodes the UNCTOKEN, the beginning of sentence, end of sentence, and the PADTOKEN."
    },
    {
        "id": 1086,
        "start": 5728.68017578125,
        "end": 5732.440185546875,
        "text": " And the UNCTOKEN must exist, from my understanding."
    },
    {
        "id": 1087,
        "start": 5733.440185546875,
        "end": 5740.3201904296875,
        "text": " So we can train, and when I press train, it's going to create this file, TOC400.model and"
    },
    {
        "id": 1088,
        "start": 5740.3201904296875,
        "end": 5742.16015625,
        "text": " TOC400.vocab."
    },
    {
        "id": 1089,
        "start": 5742.16015625,
        "end": 5747.3201904296875,
        "text": " I can then load the model file, and I can inspect the vocabulary of it."
    },
    {
        "id": 1090,
        "start": 5747.3201904296875,
        "end": 5753.1201171875,
        "text": " And so we trained vocab size 400 on this text here."
    },
    {
        "id": 1091,
        "start": 5753.1201171875,
        "end": 5757.880126953125,
        "text": " And these are the individual pieces, the individual tokens that SentenceBees will create."
    },
    {
        "id": 1092,
        "start": 5757.880126953125,
        "end": 5762.920166015625,
        "text": " So in the beginning, we see that we have the UNCTOKEN with the ID 0."
    },
    {
        "id": 1093,
        "start": 5762.920166015625,
        "end": 5767.64013671875,
        "text": " Then we have the beginning of sequence, end of sequence, 1 and 2."
    },
    {
        "id": 1094,
        "start": 5767.64013671875,
        "end": 5771.68017578125,
        "text": " And then we said that the PADID is negative 1, so we chose not to use it."
    },
    {
        "id": 1095,
        "start": 5771.68017578125,
        "end": 5774.400146484375,
        "text": " So there's no PADID here."
    },
    {
        "id": 1096,
        "start": 5774.400146484375,
        "end": 5777.7401123046875,
        "text": " Then these are individual BYTE tokens."
    },
    {
        "id": 1097,
        "start": 5777.7401123046875,
        "end": 5783.340087890625,
        "text": " So here we saw that BYTE fallback in Llama was turned on, so it's true."
    },
    {
        "id": 1098,
        "start": 5783.340087890625,
        "end": 5792.7401123046875,
        "text": " So what follows are going to be the 256 BYTE tokens, and these are their IDs."
    },
    {
        "id": 1099,
        "start": 5792.7401123046875,
        "end": 5798.66015625,
        "text": " And then at the bottom, after the BYTE tokens, come the merges."
    },
    {
        "id": 1100,
        "start": 5798.66015625,
        "end": 5801.420166015625,
        "text": " And these are the parent nodes in the merges."
    },
    {
        "id": 1101,
        "start": 5801.420166015625,
        "end": 5805.5401611328125,
        "text": " So we're not seeing the children, we're just seeing the parents and their ID."
    },
    {
        "id": 1102,
        "start": 5805.5401611328125,
        "end": 5812.9801025390625,
        "text": " And then after the merges comes, eventually, the individual tokens and their IDs."
    },
    {
        "id": 1103,
        "start": 5812.9801025390625,
        "end": 5817.2200927734375,
        "text": " And so these are the individual tokens, so these are the individual code point tokens,"
    },
    {
        "id": 1104,
        "start": 5817.2200927734375,
        "end": 5818.2200927734375,
        "text": " if you will."
    },
    {
        "id": 1105,
        "start": 5818.2200927734375,
        "end": 5819.2200927734375,
        "text": " And they come at the end."
    },
    {
        "id": 1106,
        "start": 5819.2200927734375,
        "end": 5823.7001953125,
        "text": " So that is the ordering with which SentenceBees represents its vocabularies."
    },
    {
        "id": 1107,
        "start": 5823.7001953125,
        "end": 5828.10009765625,
        "text": " It starts with special tokens, then the BYTE tokens, then the merge tokens, and then the"
    },
    {
        "id": 1108,
        "start": 5828.10009765625,
        "end": 5830.7001953125,
        "text": " individual code point tokens."
    },
    {
        "id": 1109,
        "start": 5830.7001953125,
        "end": 5837.0201416015625,
        "text": " And all these raw code point tokens are the ones that it encountered in the training set."
    },
    {
        "id": 1110,
        "start": 5837.0201416015625,
        "end": 5845.360107421875,
        "text": " So those individual code points are the entire set of code points that occurred here."
    },
    {
        "id": 1111,
        "start": 5845.360107421875,
        "end": 5847.340087890625,
        "text": " So those all get put in there."
    },
    {
        "id": 1112,
        "start": 5847.340087890625,
        "end": 5850.7601318359375,
        "text": " And then those are extremely rare, as determined by character coverage."
    },
    {
        "id": 1113,
        "start": 5850.7601318359375,
        "end": 5855.2200927734375,
        "text": " So if a code point occurred only a single time out of like a million sentences or something"
    },
    {
        "id": 1114,
        "start": 5855.2200927734375,
        "end": 5862.0601806640625,
        "text": " like that, then it would be ignored, and it would not be added to our vocabulary."
    },
    {
        "id": 1115,
        "start": 5862.0601806640625,
        "end": 5868.340087890625,
        "text": " Once we have a vocabulary, we can encode into IDs, and we can sort of get a list."
    },
    {
        "id": 1116,
        "start": 5868.340087890625,
        "end": 5876.4600830078125,
        "text": " And then here, I am also decoding the individual tokens back into little pieces, as they call it."
    },
    {
        "id": 1117,
        "start": 5876.4600830078125,
        "end": 5878.7401123046875,
        "text": " So let's take a look at what happened here."
    },
    {
        "id": 1118,
        "start": 5878.7401123046875,
        "end": 5881.940185546875,
        "text": " Hello, space, annyeonghaseyo."
    },
    {
        "id": 1119,
        "start": 5881.940185546875,
        "end": 5885.580078125,
        "text": " So these are the token IDs we got back."
    },
    {
        "id": 1120,
        "start": 5885.580078125,
        "end": 5891.2401123046875,
        "text": " And when we look here, a few things sort of jump to mind."
    },
    {
        "id": 1121,
        "start": 5891.2401123046875,
        "end": 5894.0001220703125,
        "text": " For one, take a look at these characters."
    },
    {
        "id": 1122,
        "start": 5894.0001220703125,
        "end": 5897.0001220703125,
        "text": " The Korean characters, of course, were not part of the training set."
    },
    {
        "id": 1123,
        "start": 5897.0001220703125,
        "end": 5902.0401611328125,
        "text": " So SentenceBees is encountering code points that it has not seen during training time."
    },
    {
        "id": 1124,
        "start": 5902.0401611328125,
        "end": 5905.7200927734375,
        "text": " And those code points do not have a token associated with them."
    },
    {
        "id": 1125,
        "start": 5905.7200927734375,
        "end": 5909.68017578125,
        "text": " So suddenly, these are unk tokens, unknown tokens."
    },
    {
        "id": 1126,
        "start": 5909.68017578125,
        "end": 5915.64013671875,
        "text": " But because byte fallback is true, instead, SentenceBees falls back to bytes."
    },
    {
        "id": 1127,
        "start": 5915.64013671875,
        "end": 5922.920166015625,
        "text": " And so it takes this, it encodes it with UTF-8, and then it uses these tokens to represent"
    },
    {
        "id": 1128,
        "start": 5922.920166015625,
        "end": 5924.9600830078125,
        "text": " those bytes."
    },
    {
        "id": 1129,
        "start": 5924.9600830078125,
        "end": 5927.8001708984375,
        "text": " And that's what we are getting sort of here."
    },
    {
        "id": 1130,
        "start": 5927.8001708984375,
        "end": 5935.840087890625,
        "text": " This is the UTF-8 encoding, and it is shifted by three, because of these special tokens"
    },
    {
        "id": 1131,
        "start": 5935.840087890625,
        "end": 5938.2801513671875,
        "text": " here that have IDs earlier on."
    },
    {
        "id": 1132,
        "start": 5938.2801513671875,
        "end": 5939.880126953125,
        "text": " So that's what happened here."
    },
    {
        "id": 1133,
        "start": 5940.2001953125,
        "end": 5946.8001708984375,
        "text": " Now, one more thing that, well, first, before I go on, with respect to the byte fallback,"
    },
    {
        "id": 1134,
        "start": 5946.8001708984375,
        "end": 5948.64013671875,
        "text": " let me remove byte fallback."
    },
    {
        "id": 1135,
        "start": 5948.64013671875,
        "end": 5951.60009765625,
        "text": " If this is false, what's going to happen?"
    },
    {
        "id": 1136,
        "start": 5951.60009765625,
        "end": 5953.5201416015625,
        "text": " Let's retrain."
    },
    {
        "id": 1137,
        "start": 5953.5201416015625,
        "end": 5957.2401123046875,
        "text": " So the first thing that happened is all the byte tokens disappeared, right?"
    },
    {
        "id": 1138,
        "start": 5957.2401123046875,
        "end": 5960.5601806640625,
        "text": " And now we just have the merges, and we have a lot more merges now, because we have a lot"
    },
    {
        "id": 1139,
        "start": 5960.5601806640625,
        "end": 5966.880126953125,
        "text": " more space, because we're not taking up space in the vocab size with all the bytes."
    },
    {
        "id": 1140,
        "start": 5966.880126953125,
        "end": 5971.880126953125,
        "text": " And now if we encode this, we get a zero."
    },
    {
        "id": 1141,
        "start": 5971.880126953125,
        "end": 5976.60009765625,
        "text": " So this entire string here, suddenly, there's no byte fallback, so this is unknown, and"
    },
    {
        "id": 1142,
        "start": 5976.60009765625,
        "end": 5978.9600830078125,
        "text": " unknown is UNK."
    },
    {
        "id": 1143,
        "start": 5978.9600830078125,
        "end": 5984.080078125,
        "text": " And so this is zero, because the UNK token is token zero."
    },
    {
        "id": 1144,
        "start": 5984.080078125,
        "end": 5987.64013671875,
        "text": " And you have to keep in mind that this would feed into your language model."
    },
    {
        "id": 1145,
        "start": 5987.64013671875,
        "end": 5991.2001953125,
        "text": " So what is a language model supposed to do when all kinds of different things that are"
    },
    {
        "id": 1146,
        "start": 5991.2001953125,
        "end": 5994.8001708984375,
        "text": " unrecognized because they're rare just end up mapping into UNK?"
    },
    {
        "id": 1147,
        "start": 5994.8001708984375,
        "end": 5996.9600830078125,
        "text": " It's not exactly the property that you want."
    },
    {
        "id": 1148,
        "start": 5996.9600830078125,
        "end": 6002.7200927734375,
        "text": " So that's why I think Llama correctly used byte fallback true, because we definitely"
    },
    {
        "id": 1149,
        "start": 6002.7200927734375,
        "end": 6008.4801025390625,
        "text": " want to feed these unknown or rare code points into the model in some manner."
    },
    {
        "id": 1150,
        "start": 6008.4801025390625,
        "end": 6011.7200927734375,
        "text": " The next thing I want to show you is the following."
    },
    {
        "id": 1151,
        "start": 6011.7200927734375,
        "end": 6017.7200927734375,
        "text": " Notice here when we are decoding all the individual tokens, you see how spaces, space here, ends"
    },
    {
        "id": 1152,
        "start": 6017.7200927734375,
        "end": 6020.68017578125,
        "text": " up being this bold underline."
    },
    {
        "id": 1153,
        "start": 6020.68017578125,
        "end": 6025.68017578125,
        "text": " I'm not 100% sure, by the way, why SentenceBees switches whitespace into these bold underscore"
    },
    {
        "id": 1154,
        "start": 6025.68017578125,
        "end": 6026.68017578125,
        "text": " characters."
    },
    {
        "id": 1155,
        "start": 6026.68017578125,
        "end": 6027.68017578125,
        "text": " Maybe it's for visualization."
    },
    {
        "id": 1156,
        "start": 6027.68017578125,
        "end": 6030.920166015625,
        "text": " I'm not 100% sure why that happens."
    },
    {
        "id": 1157,
        "start": 6030.920166015625,
        "end": 6031.920166015625,
        "text": " But notice this."
    },
    {
        "id": 1158,
        "start": 6031.920166015625,
        "end": 6037.3201904296875,
        "text": " Why do we have an extra space in the front of hello?"
    },
    {
        "id": 1159,
        "start": 6037.3201904296875,
        "end": 6040.0001220703125,
        "text": " Where is this coming from?"
    },
    {
        "id": 1160,
        "start": 6040.0001220703125,
        "end": 6046.080078125,
        "text": " Well, it's coming from this option here."
    },
    {
        "id": 1161,
        "start": 6046.080078125,
        "end": 6048.0001220703125,
        "text": " Add dummy prefix is true."
    },
    {
        "id": 1162,
        "start": 6048.0001220703125,
        "end": 6052.400146484375,
        "text": " And when you go to the documentation, add dummy whitespace at the beginning of text"
    },
    {
        "id": 1163,
        "start": 6052.400146484375,
        "end": 6057.16015625,
        "text": " in order to treat world in world and hello world in the exact same way."
    },
    {
        "id": 1164,
        "start": 6057.16015625,
        "end": 6060.2001953125,
        "text": " So what this is trying to do is the following."
    },
    {
        "id": 1165,
        "start": 6060.2001953125,
        "end": 6068.3201904296875,
        "text": " If we go back to our TickTokenizer, world as a token by itself has a different ID than"
    },
    {
        "id": 1166,
        "start": 6068.3201904296875,
        "end": 6070.0001220703125,
        "text": " space world."
    },
    {
        "id": 1167,
        "start": 6070.0001220703125,
        "end": 6074.64013671875,
        "text": " So this is 1917, but this is 14, et cetera."
    },
    {
        "id": 1168,
        "start": 6074.64013671875,
        "end": 6076.920166015625,
        "text": " So these are two different tokens for the language model."
    },
    {
        "id": 1169,
        "start": 6076.920166015625,
        "end": 6079.5201416015625,
        "text": " And the language model has to learn from data that they are actually kind of like a"
    },
    {
        "id": 1170,
        "start": 6079.5201416015625,
        "end": 6081.080078125,
        "text": " very similar concept."
    },
    {
        "id": 1171,
        "start": 6081.080078125,
        "end": 6086.2401123046875,
        "text": " So to the language model in the TickToken world, basically words in the beginning of"
    },
    {
        "id": 1172,
        "start": 6086.2401123046875,
        "end": 6091.4801025390625,
        "text": " sentences and words in the middle of sentences actually look completely different."
    },
    {
        "id": 1173,
        "start": 6091.4801025390625,
        "end": 6094.3201904296875,
        "text": " And it has learned that they are roughly the same."
    },
    {
        "id": 1174,
        "start": 6094.3201904296875,
        "end": 6098.360107421875,
        "text": " So this add dummy prefix is trying to fight that a little bit."
    },
    {
        "id": 1175,
        "start": 6098.360107421875,
        "end": 6103.840087890625,
        "text": " And the way that works is that it basically adds a dummy prefix."
    },
    {
        "id": 1176,
        "start": 6103.840087890625,
        "end": 6111.0001220703125,
        "text": " So as a part of preprocessing, it will take the string and it will add a space."
    },
    {
        "id": 1177,
        "start": 6111.0001220703125,
        "end": 6112.920166015625,
        "text": " It will do this."
    },
    {
        "id": 1178,
        "start": 6112.920166015625,
        "end": 6117.440185546875,
        "text": " And that's done in an effort to make this world and that world the same."
    },
    {
        "id": 1179,
        "start": 6117.440185546875,
        "end": 6119.880126953125,
        "text": " They will both be space world."
    },
    {
        "id": 1180,
        "start": 6119.880126953125,
        "end": 6123.360107421875,
        "text": " So that's one other kind of preprocessing option that is turned on."
    },
    {
        "id": 1181,
        "start": 6123.360107421875,
        "end": 6126.7200927734375,
        "text": " And Lama 2 also uses this option."
    },
    {
        "id": 1182,
        "start": 6126.7200927734375,
        "end": 6129.64013671875,
        "text": " And that's, I think, everything that I want to say from my preview of sentence piece and"
    },
    {
        "id": 1183,
        "start": 6129.64013671875,
        "end": 6131.920166015625,
        "text": " how it is different."
    },
    {
        "id": 1184,
        "start": 6131.920166015625,
        "end": 6138.2001953125,
        "text": " Basically here, what I've done is I just put in the raw protocol buffer representation"
    },
    {
        "id": 1185,
        "start": 6138.2001953125,
        "end": 6142.2001953125,
        "text": " basically of the tokenizer, the Lama 2 trained."
    },
    {
        "id": 1186,
        "start": 6142.2001953125,
        "end": 6144.400146484375,
        "text": " So feel free to sort of step through this."
    },
    {
        "id": 1187,
        "start": 6144.400146484375,
        "end": 6150.2401123046875,
        "text": " And if you would like your tokenization to look identical to that of the meta Lama 2,"
    },
    {
        "id": 1188,
        "start": 6150.2401123046875,
        "end": 6154.2401123046875,
        "text": " then you would be copy pasting these settings as I've tried to do up above."
    },
    {
        "id": 1189,
        "start": 6154.2401123046875,
        "end": 6157.5201416015625,
        "text": " And yeah, I think that's it for this section."
    },
    {
        "id": 1190,
        "start": 6157.5201416015625,
        "end": 6161.900146484375,
        "text": " I think my summary for sentence piece from all this is, number one, I think that there's"
    },
    {
        "id": 1191,
        "start": 6161.900146484375,
        "end": 6165.8201904296875,
        "text": " a lot of historical baggage in sentence piece, a lot of concepts that I think are slightly"
    },
    {
        "id": 1192,
        "start": 6165.8201904296875,
        "end": 6170.2601318359375,
        "text": " confusing and I think potentially contain foot guns, like this concept of a sentence"
    },
    {
        "id": 1193,
        "start": 6170.2601318359375,
        "end": 6172.8201904296875,
        "text": " and its maximum length and stuff like that."
    },
    {
        "id": 1194,
        "start": 6172.8201904296875,
        "end": 6179.06005859375,
        "text": " Otherwise, it is fairly commonly used in the industry because it is efficient and can do"
    },
    {
        "id": 1195,
        "start": 6179.06005859375,
        "end": 6181.02001953125,
        "text": " both training and inference."
    },
    {
        "id": 1196,
        "start": 6181.02001953125,
        "end": 6185.340087890625,
        "text": " It has a few quirks, like for example, UNCTOKEN must exist and the way the byte fallbacks"
    },
    {
        "id": 1197,
        "start": 6185.340087890625,
        "end": 6188.26025390625,
        "text": " are done and so on, I don't find particularly elegant."
    },
    {
        "id": 1198,
        "start": 6188.26025390625,
        "end": 6190.22021484375,
        "text": " And unfortunately, I have to say it's not very well documented."
    },
    {
        "id": 1199,
        "start": 6190.22021484375,
        "end": 6195.7402148246765,
        "text": " so it took me a lot of time working with this myself and just visualizing things and trying"
    },
    {
        "id": 1200,
        "start": 6195.7402148246765,
        "end": 6200.22021484375,
        "text": " to really understand what is happening here because the documentation unfortunately is,"
    },
    {
        "id": 1201,
        "start": 6200.22021484375,
        "end": 6205.82021522522,
        "text": " in my opinion, not super amazing. But it is a very nice repo that is available to you if you'd"
    },
    {
        "id": 1202,
        "start": 6205.82021522522,
        "end": 6209.580215454102,
        "text": " like to train your own tokenizer right now. Okay, let me now switch gears again as we're"
    },
    {
        "id": 1203,
        "start": 6209.580215454102,
        "end": 6213.740215301514,
        "text": " starting to slowly wrap up here. I want to revisit this issue in a bit more detail of"
    },
    {
        "id": 1204,
        "start": 6213.740215301514,
        "end": 6217.100214004517,
        "text": " how we should set the vocab size and what are some of the considerations around it."
    },
    {
        "id": 1205,
        "start": 6217.900215148926,
        "end": 6222.780216217041,
        "text": " So for this, I'd like to go back to the model architecture that we developed in the last video"
    },
    {
        "id": 1206,
        "start": 6222.780216217041,
        "end": 6228.700214385986,
        "text": " when we built the GPT from scratch. So this here was the file that we built in the previous video"
    },
    {
        "id": 1207,
        "start": 6228.700214385986,
        "end": 6232.620216369629,
        "text": " and we defined the transformer model. And let's specifically look at vocab size and"
    },
    {
        "id": 1208,
        "start": 6232.620216369629,
        "end": 6237.820213317871,
        "text": " where it appears in this file. So here we define the vocab size. At this time,"
    },
    {
        "id": 1209,
        "start": 6237.820213317871,
        "end": 6241.980213165283,
        "text": " it was 65 or something like that, extremely small number. So this will grow much larger."
    },
    {
        "id": 1210,
        "start": 6242.860214233398,
        "end": 6245.900215148926,
        "text": " You'll see that vocab size doesn't come up too much in most of these layers."
    },
    {
        "id": 1211,
        "start": 6245.900215148926,
        "end": 6249.820213317871,
        "text": " The only place that it comes up to is in exactly these two places here."
    },
    {
        "id": 1212,
        "start": 6250.700214385986,
        "end": 6254.380218505859,
        "text": " So when we define the language model, there's the token embedding table,"
    },
    {
        "id": 1213,
        "start": 6255.020217895508,
        "end": 6259.420211791992,
        "text": " which is this two-dimensional array where the vocab size is basically the number of rows."
    },
    {
        "id": 1214,
        "start": 6260.380218505859,
        "end": 6266.140213012695,
        "text": " And each vocabulary element, each token has a vector that we're going to train using back"
    },
    {
        "id": 1215,
        "start": 6266.140213012695,
        "end": 6270.380218505859,
        "text": " propagation. That vector is of size and embed, which is number of channels in the transformer."
    },
    {
        "id": 1216,
        "start": 6271.180213928223,
        "end": 6275.500213623047,
        "text": " And basically, as vocab size increases, this embedding table, as I mentioned earlier,"
    },
    {
        "id": 1217,
        "start": 6275.500213623047,
        "end": 6277.420211791992,
        "text": " is going to also grow. We're going to be adding rows."
    },
    {
        "id": 1218,
        "start": 6278.380218505859,
        "end": 6282.22021484375,
        "text": " In addition to that, at the end of the transformer, there's this LM head layer,"
    },
    {
        "id": 1219,
        "start": 6282.22021484375,
        "end": 6286.860214233398,
        "text": " which is a linear layer. And you'll notice that that layer is used at the very end to"
    },
    {
        "id": 1220,
        "start": 6286.860214233398,
        "end": 6291.260215759277,
        "text": " produce the logits, which become the probabilities for the next token in a sequence."
    },
    {
        "id": 1221,
        "start": 6291.260215759277,
        "end": 6296.300216674805,
        "text": " And so intuitively, we're trying to produce a probability for every single token that might"
    },
    {
        "id": 1222,
        "start": 6296.300216674805,
        "end": 6301.180213928223,
        "text": " come next at every point in time of that transformer. And if we have more and more"
    },
    {
        "id": 1223,
        "start": 6301.180213928223,
        "end": 6305.420211791992,
        "text": " tokens, we need to produce more and more probabilities. So every single token is going"
    },
    {
        "id": 1224,
        "start": 6305.420211791992,
        "end": 6310.060211181641,
        "text": " to introduce an additional dot product that we have to do here in this linear layer for this"
    },
    {
        "id": 1225,
        "start": 6310.060211181641,
        "end": 6315.900215148926,
        "text": " final layer in the transformer. So why can't vocab size be infinite? Why can't we grow to"
    },
    {
        "id": 1226,
        "start": 6315.900215148926,
        "end": 6321.900207519531,
        "text": " infinity? Well, number one, your token embedding table is going to grow. Your linear layer is going"
    },
    {
        "id": 1227,
        "start": 6321.900207519531,
        "end": 6326.140213012695,
        "text": " to grow. So we're going to be doing a lot more computation here because this LM head layer will"
    },
    {
        "id": 1228,
        "start": 6326.140213012695,
        "end": 6330.620208740234,
        "text": " become more computationally expensive. Number two, because we have more parameters, we could"
    },
    {
        "id": 1229,
        "start": 6330.620208740234,
        "end": 6337.020217895508,
        "text": " be worried that we are going to be under-training some of these parameters. So intuitively, if you"
    },
    {
        "id": 1230,
        "start": 6337.020217895508,
        "end": 6341.740219116211,
        "text": " have a very large vocabulary size, say we have a million tokens, then every one of these tokens is"
    },
    {
        "id": 1231,
        "start": 6341.740219116211,
        "end": 6345.980209350586,
        "text": " going to come up more and more rarely in the training data because there's a lot more other"
    },
    {
        "id": 1232,
        "start": 6345.980209350586,
        "end": 6351.260208129883,
        "text": " tokens all over the place. And so we're going to be seeing fewer and fewer examples for each"
    },
    {
        "id": 1233,
        "start": 6351.260208129883,
        "end": 6355.820220947266,
        "text": " individual token. And you might be worried that basically the vectors associated with every token"
    },
    {
        "id": 1234,
        "start": 6355.820220947266,
        "end": 6359.820220947266,
        "text": " will be under-trained as a result because they just don't come up too often and they don't"
    },
    {
        "id": 1235,
        "start": 6359.820220947266,
        "end": 6364.700210571289,
        "text": " participate in the forward-backward pass. In addition to that, as your vocab size grows, you're going to"
    },
    {
        "id": 1236,
        "start": 6364.700210571289,
        "end": 6369.660217285156,
        "text": " start shrinking your sequences a lot, right? And that's really nice because that means that we're"
    },
    {
        "id": 1237,
        "start": 6369.660217285156,
        "end": 6373.980209350586,
        "text": " going to be attending to more and more text. So that's nice. But also you might be worrying that"
    },
    {
        "id": 1238,
        "start": 6373.980209350586,
        "end": 6380.140213012695,
        "text": " too large of chunks are being squished into single tokens. And so the model just doesn't have as much"
    },
    {
        "id": 1239,
        "start": 6380.140213012695,
        "end": 6386.780212402344,
        "text": " sort of time to think per sort of some number of characters in the text. Or you can think about"
    },
    {
        "id": 1240,
        "start": 6386.780212402344,
        "end": 6391.420211791992,
        "text": " it that way, right? So basically we're squishing too much information into a single token and then"
    },
    {
        "id": 1241,
        "start": 6391.420211791992,
        "end": 6395.180221557617,
        "text": " the forward pass of the transformer is not enough to actually process that information appropriately."
    },
    {
        "id": 1242,
        "start": 6395.740219116211,
        "end": 6399.420211791992,
        "text": " And so these are some of the considerations you're thinking about when you're designing the vocab size."
    },
    {
        "id": 1243,
        "start": 6399.420211791992,
        "end": 6403.500213623047,
        "text": " As I mentioned, this is mostly an empirical hyperparameter and it seems like in state-of-the-art"
    },
    {
        "id": 1244,
        "start": 6403.500213623047,
        "end": 6408.940216064453,
        "text": " architectures today, this is usually in the high 10,000s or somewhere around 100,000 today."
    },
    {
        "id": 1245,
        "start": 6408.940216064453,
        "end": 6413.260208129883,
        "text": " And the next consideration I want to briefly talk about is what if we want to take a pre-trained"
    },
    {
        "id": 1246,
        "start": 6413.260208129883,
        "end": 6418.22021484375,
        "text": " model and we want to extend the vocab size. And this is done fairly commonly actually. So for"
    },
    {
        "id": 1247,
        "start": 6418.22021484375,
        "end": 6423.980209350586,
        "text": " example, when you're doing fine tuning for chat GPT, a lot more new special tokens get introduced"
    },
    {
        "id": 1248,
        "start": 6423.980209350586,
        "end": 6429.660217285156,
        "text": " on top of the base model to maintain the metadata and all the structure of conversation objects"
    },
    {
        "id": 1249,
        "start": 6429.660217285156,
        "end": 6434.380218505859,
        "text": " between the user and an assistant. So that takes a lot of special tokens. You might also try to"
    },
    {
        "id": 1250,
        "start": 6434.380218505859,
        "end": 6439.3402099609375,
        "text": " throw in more special tokens, for example, for using the browser or any other tool. And so it's"
    },
    {
        "id": 1251,
        "start": 6439.3402099609375,
        "end": 6444.380218505859,
        "text": " very tempting to add a lot of tokens for all kinds of special functionality. So if you want to be"
    },
    {
        "id": 1252,
        "start": 6444.380218505859,
        "end": 6449.260223388672,
        "text": " adding a token, that's totally possible, right? All we have to do is we have to resize this embedding."
    },
    {
        "id": 1253,
        "start": 6449.260223388672,
        "end": 6454.140228271484,
        "text": " So we have to add rows. We would initialize these parameters from scratch, which would be small"
    },
    {
        "id": 1254,
        "start": 6454.140228271484,
        "end": 6459.740203857422,
        "text": " random numbers. And then we have to extend the weight inside this linear. So we have to start"
    },
    {
        "id": 1255,
        "start": 6459.740203857422,
        "end": 6464.780212402344,
        "text": " making dot products with the associated parameters as well to basically calculate the probabilities"
    },
    {
        "id": 1256,
        "start": 6464.780212402344,
        "end": 6471.5802001953125,
        "text": " for these new tokens. So both of these are just resizing operation. It's a very mild model surgery"
    },
    {
        "id": 1257,
        "start": 6471.5802001953125,
        "end": 6475.1002197265625,
        "text": " and can be done fairly easily. And it's quite common that basically you would freeze the base"
    },
    {
        "id": 1258,
        "start": 6475.1002197265625,
        "end": 6479.180206298828,
        "text": " model, you introduce these new parameters, and then you only train these new parameters to"
    },
    {
        "id": 1259,
        "start": 6479.180206298828,
        "end": 6484.22021484375,
        "text": " introduce new tokens into the architecture. And so you can freeze arbitrary parts of it,"
    },
    {
        "id": 1260,
        "start": 6484.22021484375,
        "end": 6488.140228271484,
        "text": " or you can train arbitrary parts of it, and that's totally up to you. But basically minor"
    },
    {
        "id": 1261,
        "start": 6488.140228271484,
        "end": 6492.460205078125,
        "text": " surgery required if you'd like to introduce new tokens. And finally, I'd like to mention that"
    },
    {
        "id": 1262,
        "start": 6492.460205078125,
        "end": 6497.1002197265625,
        "text": " actually there's an entire design space of applications in terms of introducing new tokens"
    },
    {
        "id": 1263,
        "start": 6497.1002197265625,
        "end": 6501.900207519531,
        "text": " into a vocabulary that go way beyond just adding special tokens and special new functionality."
    },
    {
        "id": 1264,
        "start": 6501.900207519531,
        "end": 6505.5802001953125,
        "text": " So just to give you a sense of the design space, but this could be an entire video just by itself,"
    },
    {
        "id": 1265,
        "start": 6506.300201416016,
        "end": 6512.300201416016,
        "text": " this is a paper on learning to compress prompts with what they called GIST tokens. And the rough"
    },
    {
        "id": 1266,
        "start": 6512.300201416016,
        "end": 6516.940216064453,
        "text": " idea is, suppose that you're using language models in a setting that requires very long prompts."
    },
    {
        "id": 1267,
        "start": 6516.940216064453,
        "end": 6520.700225830078,
        "text": " Well, these long prompts just slow everything down because you have to encode them, and then you have"
    },
    {
        "id": 1268,
        "start": 6520.700225830078,
        "end": 6525.820220947266,
        "text": " to use them, and then you're tending over them, and it's just heavy to have very large prompts."
    },
    {
        "id": 1269,
        "start": 6526.540222167969,
        "end": 6534.300201416016,
        "text": " So instead, what they do here in this paper is they introduce new tokens. And imagine basically"
    },
    {
        "id": 1270,
        "start": 6534.300201416016,
        "end": 6539.260223388672,
        "text": " having a few new tokens, you put them in a sequence, and then you train the model by"
    },
    {
        "id": 1271,
        "start": 6539.260223388672,
        "end": 6544.380218505859,
        "text": " distillation. So you are keeping the entire model frozen, and you're only training the representations"
    },
    {
        "id": 1272,
        "start": 6544.380218505859,
        "end": 6549.3402099609375,
        "text": " of the new tokens, their embeddings, and you're optimizing over the new tokens such that the"
    },
    {
        "id": 1273,
        "start": 6549.3402099609375,
        "end": 6556.780212402344,
        "text": " behavior of the language model is identical to the model that has a very long prompt that works"
    },
    {
        "id": 1274,
        "start": 6556.780212402344,
        "end": 6561.020202636719,
        "text": " for you. And so it's a compression technique of compressing that very long prompt into those few"
    },
    {
        "id": 1275,
        "start": 6561.020202636719,
        "end": 6565.740203857422,
        "text": " new GIST tokens. And so you can train this, and then at test time you can discard your old prompt"
    },
    {
        "id": 1276,
        "start": 6565.740203857422,
        "end": 6571.260223388672,
        "text": " and just swap in those tokens, and they sort of like stand in for that very long prompt and have"
    },
    {
        "id": 1277,
        "start": 6571.260223388672,
        "end": 6577.260223388672,
        "text": " an almost identical performance. And so this is one technique in a class of parameter-efficient"
    },
    {
        "id": 1278,
        "start": 6577.260223388672,
        "end": 6581.820220947266,
        "text": " fine-tuning techniques where most of the model is basically fixed, and there's no training of"
    },
    {
        "id": 1279,
        "start": 6581.820220947266,
        "end": 6586.460205078125,
        "text": " the model weights, there's no training of LoRa or anything like that, of new parameters. The"
    },
    {
        "id": 1280,
        "start": 6586.460205078125,
        "end": 6591.5802001953125,
        "text": " parameters that you're training are now just the token embeddings. So that's just one example, but"
    },
    {
        "id": 1281,
        "start": 6591.5802001953125,
        "end": 6595.260223388672,
        "text": " this could again be like an entire video, but just to give you a sense that there's a whole design"
    },
    {
        "id": 1282,
        "start": 6595.260223388672,
        "end": 6599.3402099609375,
        "text": " space here that is potentially worth exploring in the future. The next thing I want to briefly"
    },
    {
        "id": 1283,
        "start": 6599.3402099609375,
        "end": 6604.380218505859,
        "text": " address is that I think recently there's a lot of momentum in how you actually could construct"
    },
    {
        "id": 1284,
        "start": 6604.380218505859,
        "end": 6608.540222167969,
        "text": " transformers that can simultaneously process not just text as the input modality, but a lot"
    },
    {
        "id": 1285,
        "start": 6608.540222167969,
        "end": 6614.140228271484,
        "text": " of other modalities, so be it images, videos, audio, etc. And how do you feed in all these"
    },
    {
        "id": 1286,
        "start": 6614.140228271484,
        "end": 6619.260223388672,
        "text": " modalities and potentially predict these modalities from a transformer? Do you have to change the"
    },
    {
        "id": 1287,
        "start": 6619.260223388672,
        "end": 6622.8602294921875,
        "text": " architecture in some fundamental way? And I think what a lot of people are starting to converge"
    },
    {
        "id": 1288,
        "start": 6622.8602294921875,
        "end": 6626.8602294921875,
        "text": " towards is that you're not changing the architecture, you stick with the transformer, you just kind of"
    },
    {
        "id": 1289,
        "start": 6626.8602294921875,
        "end": 6631.820220947266,
        "text": " tokenize your input domains, and then call it a day and pretend it's just text tokens and just"
    },
    {
        "id": 1290,
        "start": 6631.820220947266,
        "end": 6636.8602294921875,
        "text": " do everything else in an identical manner. So here, for example, there was an early paper that"
    },
    {
        "id": 1291,
        "start": 6636.8602294921875,
        "end": 6641.820220947266,
        "text": " has a nice graphic for how you can take an image and you can truncate it into integers."
    },
    {
        "id": 1292,
        "start": 6643.180206298828,
        "end": 6649.1002197265625,
        "text": " So these would basically become the tokens of images, as an example, and"
    },
    {
        "id": 1293,
        "start": 6649.1002197265625,
        "end": 6654.700225830078,
        "text": " these tokens can be hard tokens where you force them to be integers. They can also be soft tokens"
    },
    {
        "id": 1294,
        "start": 6655.3402099609375,
        "end": 6661.980224609375,
        "text": " where you sort of don't require these to be discrete, but you do force these representations"
    },
    {
        "id": 1295,
        "start": 6661.980224609375,
        "end": 6668.060211181641,
        "text": " to go through bottlenecks like in autoencoders. Also in this paper that came out from OpenAI, Sora,"
    },
    {
        "id": 1296,
        "start": 6668.060211181641,
        "end": 6673.820220947266,
        "text": " which I think really blew the mind of many people and inspired a lot of people in terms of what's"
    },
    {
        "id": 1297,
        "start": 6673.820220947266,
        "end": 6679.900207519531,
        "text": " possible, they have a graphic here and they talk briefly about how LLMs have text tokens, Sora has"
    },
    {
        "id": 1298,
        "start": 6679.900207519531,
        "end": 6685.900207519531,
        "text": " visual patches. So again, they came up with a way to truncate videos into basically tokens with"
    },
    {
        "id": 1299,
        "start": 6685.900207519531,
        "end": 6690.140228271484,
        "text": " their own vocabularies, and then you can either process discrete tokens, say, with autoregressive"
    },
    {
        "id": 1300,
        "start": 6690.140228271484,
        "end": 6697.820220947266,
        "text": " models or even soft tokens with diffusion models. And all of that is sort of being actively worked"
    },
    {
        "id": 1301,
        "start": 6697.820220947266,
        "end": 6701.020202636719,
        "text": " on, designed on, and it's beyond the scope of this video, but just something I wanted to mention"
    },
    {
        "id": 1302,
        "start": 6701.020202636719,
        "end": 6705.980224609375,
        "text": " briefly. Okay, now that we have come quite deep into the tokenization algorithm and we understand"
    },
    {
        "id": 1303,
        "start": 6705.980224609375,
        "end": 6710.540222167969,
        "text": " a lot more about how it works, let's loop back around to the beginning of this video and go"
    },
    {
        "id": 1304,
        "start": 6710.540222167969,
        "end": 6715.740234375,
        "text": " through some of these bullet points and really see why they happen. So first of all, why can't"
    },
    {
        "id": 1305,
        "start": 6715.740234375,
        "end": 6722.6202392578125,
        "text": " my LLM spell words very well or do other spell-related tasks? So fundamentally this is because,"
    },
    {
        "id": 1306,
        "start": 6722.6202392578125,
        "end": 6728.140197753906,
        "text": " as we saw, these characters are chunked up into tokens, and some of these tokens are actually"
    },
    {
        "id": 1307,
        "start": 6728.140197753906,
        "end": 6734.380187988281,
        "text": " fairly long. So as an example, I went to the GPT-4 vocabulary and I looked at one of the longer"
    },
    {
        "id": 1308,
        "start": 6734.380187988281,
        "end": 6740.060241699219,
        "text": " tokens. So .defaultstyle turns out to be a single individual token. So that's a lot of characters"
    },
    {
        "id": 1309,
        "start": 6740.060241699219,
        "end": 6744.780212402344,
        "text": " for a single token. So my suspicion is that there's just too much crammed into this single"
    },
    {
        "id": 1310,
        "start": 6744.780212402344,
        "end": 6751.500244140625,
        "text": " token, and my suspicion was that the model should not be very good at tasks related to spelling of"
    },
    {
        "id": 1311,
        "start": 6751.500244140625,
        "end": 6759.740234375,
        "text": " this single token. So I asked how many letters L are there in the word .defaultstyle, and of course"
    },
    {
        "id": 1312,
        "start": 6760.300231933594,
        "end": 6766.380187988281,
        "text": " my prompt is intentionally done that way, and you see how .defaultstyle will be a single token. So"
    },
    {
        "id": 1313,
        "start": 6766.380187988281,
        "end": 6770.780212402344,
        "text": " this is what the model sees. So my suspicion is that it wouldn't be very good at this, and indeed"
    },
    {
        "id": 1314,
        "start": 6770.780212402344,
        "end": 6774.940185546875,
        "text": " it is not. It doesn't actually know how many L's are in there. It thinks there are three, and"
    },
    {
        "id": 1315,
        "start": 6774.940185546875,
        "end": 6781.020202636719,
        "text": " actually there are four, if I'm not getting this wrong myself. So that didn't go extremely well."
    },
    {
        "id": 1316,
        "start": 6781.020202636719,
        "end": 6788.780212402344,
        "text": " Let's look at another kind of character-level task. So for example, here I asked GPT-4 to reverse"
    },
    {
        "id": 1317,
        "start": 6788.780212402344,
        "end": 6794.380187988281,
        "text": " the string .defaultstyle, and it tried to use a code interpreter, and I stopped it, and I said"
    },
    {
        "id": 1318,
        "start": 6794.380187988281,
        "end": 6801.740234375,
        "text": " just do it, just try it, and it gave me jumble. So it doesn't actually really know how to reverse"
    },
    {
        "id": 1319,
        "start": 6801.740234375,
        "end": 6807.740234375,
        "text": " this string going from right to left, so it gave a wrong result. So again, like working with this"
    },
    {
        "id": 1320,
        "start": 6807.740234375,
        "end": 6812.8602294921875,
        "text": " working hypothesis that maybe this is due to the tokenization, I tried a different approach. I said"
    },
    {
        "id": 1321,
        "start": 6812.8602294921875,
        "end": 6818.060241699219,
        "text": " okay, let's reverse the exact same string, but take the following approach. Step one, just print"
    },
    {
        "id": 1322,
        "start": 6818.060241699219,
        "end": 6823.180236816406,
        "text": " out every single character separated by spaces, and then as a step two, reverse that list, and it"
    },
    {
        "id": 1323,
        "start": 6823.180236816406,
        "end": 6828.460205078125,
        "text": " again tried to use a tool, but when I stopped it, it first produced all the characters, and that was"
    },
    {
        "id": 1324,
        "start": 6828.460205078125,
        "end": 6833.260192871094,
        "text": " actually correct, and then it reversed them, and that was correct once it had this. So somehow it"
    },
    {
        "id": 1325,
        "start": 6833.260192871094,
        "end": 6838.7001953125,
        "text": " can't reverse it directly, but when you go just first, you know, listing it out in order, it can"
    },
    {
        "id": 1326,
        "start": 6838.7001953125,
        "end": 6844.300231933594,
        "text": " do that somehow, and then it can, once it's broken up this way, this becomes all these individual"
    },
    {
        "id": 1327,
        "start": 6844.300231933594,
        "end": 6849.8201904296875,
        "text": " characters, and so now this is much easier for it to see these individual tokens and reverse them"
    },
    {
        "id": 1328,
        "start": 6849.8201904296875,
        "end": 6857.900207519531,
        "text": " and print them out. So that is kind of interesting. So let's continue now. Why are LLMs worse at"
    },
    {
        "id": 1329,
        "start": 6858.540222167969,
        "end": 6864.540222167969,
        "text": " non-English languages? And I briefly covered this already, but basically it's not only that"
    },
    {
        "id": 1330,
        "start": 6864.540222167969,
        "end": 6869.740234375,
        "text": " the language model sees less non-English data during training of the model parameters, but also"
    },
    {
        "id": 1331,
        "start": 6869.740234375,
        "end": 6877.020202636719,
        "text": " the tokenizer is not sufficiently trained on non-English data, and so here, for example,"
    },
    {
        "id": 1332,
        "start": 6877.020202636719,
        "end": 6883.900207519531,
        "text": " hello, how are you is five tokens, and its translation is 15 tokens, so this is a three times blow up,"
    },
    {
        "id": 1333,
        "start": 6884.7001953125,
        "end": 6890.140197753906,
        "text": " and so for example, annyeonghaseyo is just hello, basically in Korean, and that ends up being three"
    },
    {
        "id": 1334,
        "start": 6890.140197753906,
        "end": 6893.8201904296875,
        "text": " tokens. I'm actually kind of surprised by that, because that is a very common phrase, and there's"
    },
    {
        "id": 1335,
        "start": 6893.8201904296875,
        "end": 6898.540222167969,
        "text": " just a typical greeting of like hello, and that ends up being three tokens, whereas our hello is a"
    },
    {
        "id": 1336,
        "start": 6898.540222167969,
        "end": 6902.6202392578125,
        "text": " single token, and so basically everything is a lot more bloated and diffuse, and this is, I think,"
    },
    {
        "id": 1337,
        "start": 6902.6202392578125,
        "end": 6909.900207519531,
        "text": " partly the reason that the model works worse on other languages. Coming back, why is LLM bad at"
    },
    {
        "id": 1338,
        "start": 6909.900207519531,
        "end": 6918.300231933594,
        "text": " simple arithmetic? That has to do with the tokenization of numbers, and so you'll notice"
    },
    {
        "id": 1339,
        "start": 6918.300231933594,
        "end": 6923.900207519531,
        "text": " that, for example, addition is very sort of like, there's an algorithm that is like character level"
    },
    {
        "id": 1340,
        "start": 6923.900207519531,
        "end": 6928.940185546875,
        "text": " for doing addition, so for example, here we would first add the ones, and then the tens, and then the"
    },
    {
        "id": 1341,
        "start": 6928.940185546875,
        "end": 6935.5802001953125,
        "text": " hundreds. You have to refer to specific parts of these digits, but these numbers are represented"
    },
    {
        "id": 1342,
        "start": 6935.5802001953125,
        "end": 6939.660217285156,
        "text": " completely arbitrarily based on whatever happened to merge or not merge during the tokenization"
    },
    {
        "id": 1343,
        "start": 6939.660217285156,
        "end": 6944.380187988281,
        "text": " process. There's an entire blog post about this that I think is quite good. Integer tokenization"
    },
    {
        "id": 1344,
        "start": 6944.380187988281,
        "end": 6950.22021484375,
        "text": " is insane, and this person basically systematically explores the tokenization of numbers in, I believe,"
    },
    {
        "id": 1345,
        "start": 6950.300231933594,
        "end": 6957.020202636719,
        "text": " this is GPT-2, and so they noticed that, for example, for four-digit numbers, you can take"
    },
    {
        "id": 1346,
        "start": 6957.020202636719,
        "end": 6963.660217285156,
        "text": " a look at whether it is a single token, or whether it is two tokens that is a 1-3, or a 2-2, or a 3-1"
    },
    {
        "id": 1347,
        "start": 6963.660217285156,
        "end": 6968.140197753906,
        "text": " combination, and so all the different numbers are all the different combinations, and you can imagine"
    },
    {
        "id": 1348,
        "start": 6968.140197753906,
        "end": 6974.380187988281,
        "text": " this is all completely arbitrarily so, and the model unfortunately sometimes sees a token"
    },
    {
        "id": 1349,
        "start": 6974.380187988281,
        "end": 6979.5802001953125,
        "text": " for all four digits, sometimes for three, sometimes for two, sometimes for one, and it's in an"
    },
    {
        "id": 1350,
        "start": 6979.5802001953125,
        "end": 6985.740234375,
        "text": " arbitrary manner, and so this is definitely a headwind, if you will, for the language model,"
    },
    {
        "id": 1351,
        "start": 6985.740234375,
        "end": 6989.740234375,
        "text": " and it's kind of incredible that it can kind of do it and deal with it, but it's also kind of"
    },
    {
        "id": 1352,
        "start": 6989.740234375,
        "end": 6994.8602294921875,
        "text": " not ideal, and so that's why, for example, we saw that Meta, when they trained the LLAMA2 algorithm"
    },
    {
        "id": 1353,
        "start": 6994.8602294921875,
        "end": 7001.260192871094,
        "text": " and they used sentence piece, they make sure to split up all the digits, as an example, for"
    },
    {
        "id": 1354,
        "start": 7002.22021484375,
        "end": 7006.460205078125,
        "text": " LLAMA2, and this is partly to improve simple arithmetic kind of performance."
    },
    {
        "id": 1355,
        "start": 7006.780212402344,
        "end": 7012.7001953125,
        "text": " And finally, why is GPT-2 not as good in Python? Again, this is partly a modeling issue"
    },
    {
        "id": 1356,
        "start": 7012.7001953125,
        "end": 7016.540222167969,
        "text": " in the architecture, and the data set, and the strength of the model, but it's also"
    },
    {
        "id": 1357,
        "start": 7016.540222167969,
        "end": 7022.780212402344,
        "text": " partly tokenization, because as we saw here with the simple Python example, the encoding efficiency"
    },
    {
        "id": 1358,
        "start": 7022.780212402344,
        "end": 7027.3402099609375,
        "text": " of the tokenizer for handling spaces in Python is terrible, and every single space is an individual"
    },
    {
        "id": 1359,
        "start": 7027.3402099609375,
        "end": 7032.540222167969,
        "text": " token, and this dramatically reduces the context length that the model can attend across, so that's"
    },
    {
        "id": 1360,
        "start": 7033.500244140625,
        "end": 7039.420227050781,
        "text": " almost like a tokenization bug for GPT-2, and that was later fixed with GPT-4. Okay,"
    },
    {
        "id": 1361,
        "start": 7039.420227050781,
        "end": 7044.140197753906,
        "text": " so here's another fun one. My LLM abruptly halts when it sees the string end of text."
    },
    {
        "id": 1362,
        "start": 7044.780212402344,
        "end": 7051.1002197265625,
        "text": " So here's a very strange behavior. Print a string end of text, is what I told GPT-4,"
    },
    {
        "id": 1363,
        "start": 7051.1002197265625,
        "end": 7056.140197753906,
        "text": " and it says, could you please specify the string? And I'm telling it, give me end of text,"
    },
    {
        "id": 1364,
        "start": 7056.140197753906,
        "end": 7061.020202636719,
        "text": " and it seems like there's an issue. It's not seeing end of text, and then I give it end of"
    },
    {
        "id": 1365,
        "start": 7061.020202636719,
        "end": 7065.980224609375,
        "text": " text as the string, and then here's the string, and then it just doesn't print it. So obviously,"
    },
    {
        "id": 1366,
        "start": 7065.980224609375,
        "end": 7069.180236816406,
        "text": " something is breaking here with respect to the handling of the special token,"
    },
    {
        "id": 1367,
        "start": 7069.180236816406,
        "end": 7073.8201904296875,
        "text": " and I don't actually know what OpenAI is doing under the hood here, and whether they are"
    },
    {
        "id": 1368,
        "start": 7073.8201904296875,
        "end": 7083.420227050781,
        "text": " potentially parsing this as an actual token, instead of this just being end of text as like"
    },
    {
        "id": 1369,
        "start": 7083.420227050781,
        "end": 7088.940185546875,
        "text": " individual sort of pieces of it, without the special token handling logic, and so it might"
    },
    {
        "id": 1370,
        "start": 7088.940185546875,
        "end": 7094.460205078125,
        "text": " be that someone, when they're calling .encode, they are passing in the allowed special, and they"
    },
    {
        "id": 1371,
        "start": 7094.460205078125,
        "end": 7099.980224609375,
        "text": " are allowing end of text as a special character in the user prompt, but the user prompt, of course,"
    },
    {
        "id": 1372,
        "start": 7099.980224609375,
        "end": 7106.540222167969,
        "text": " is a sort of attacker-controlled text, so you would hope that they don't really parse or use special"
    },
    {
        "id": 1373,
        "start": 7106.540222167969,
        "end": 7112.380187988281,
        "text": " tokens from that kind of input, but it appears that there's something definitely going wrong here,"
    },
    {
        "id": 1374,
        "start": 7112.380187988281,
        "end": 7117.8201904296875,
        "text": " and so your knowledge of these special tokens ends up being an attack surface, potentially,"
    },
    {
        "id": 1375,
        "start": 7117.8201904296875,
        "end": 7124.140197753906,
        "text": " and so if you'd like to confuse LLMs, then just try to give them some special tokens and see if"
    },
    {
        "id": 1376,
        "start": 7124.140197753906,
        "end": 7130.060241699219,
        "text": " you're breaking something by chance. Okay, so this next one is a really fun one, the trailing"
    },
    {
        "id": 1377,
        "start": 7130.060241699219,
        "end": 7137.980224609375,
        "text": " whitespace issue. So if you come to Playground, and we come here to GPT 3.5 Turbo Instruct,"
    },
    {
        "id": 1378,
        "start": 7137.980224609375,
        "end": 7143.020202636719,
        "text": " so this is not a chat model, this is a completion model, so think of it more like it's a lot more"
    },
    {
        "id": 1379,
        "start": 7143.020202636719,
        "end": 7148.8602294921875,
        "text": " closer to a base model. It does completion, it will continue the token sequence. So here's a"
    },
    {
        "id": 1380,
        "start": 7148.8602294921875,
        "end": 7154.22021484375,
        "text": " tagline for Ice Cream Shop, and we want to continue the sequence, and so we can submit and get a bunch"
    },
    {
        "id": 1381,
        "start": 7154.22021484375,
        "end": 7162.6202392578125,
        "text": " of tokens. Okay, no problem, but now suppose I do this, but instead of pressing submit here, I do"
    },
    {
        "id": 1382,
        "start": 7162.6202392578125,
        "end": 7168.140197753906,
        "text": " here's a tagline for Ice Cream Shop space, so I have a space here before I click submit."
    },
    {
        "id": 1383,
        "start": 7168.7001953125,
        "end": 7172.780212402344,
        "text": " We get a warning, your text ends in the trailing space, which causes worse performance due to how"
    },
    {
        "id": 1384,
        "start": 7172.780212402344,
        "end": 7179.3402099609375,
        "text": " API splits text into tokens. So what's happening here, it still gave us a sort of completion here,"
    },
    {
        "id": 1385,
        "start": 7179.3402099609375,
        "end": 7183.980224609375,
        "text": " but let's take a look at what's happening. So here's a tagline for an Ice Cream Shop,"
    },
    {
        "id": 1386,
        "start": 7185.1002197265625,
        "end": 7190.7001953125,
        "text": " and then what does this look like in the actual training data? Suppose you found the completion"
    },
    {
        "id": 1387,
        "start": 7190.7001953125,
        "end": 7194.940185546875,
        "text": " in the training document somewhere on the internet, and the LLM trained on this data,"
    },
    {
        "id": 1388,
        "start": 7194.940185546875,
        "end": 7199.5802001953125,
        "text": " so maybe it's something like, oh yeah, maybe that's the tagline, that's a terrible tagline,"
    },
    {
        "id": 1389,
        "start": 7199.5802001953125,
        "end": 7206.7001953125,
        "text": " but notice here that when I create O, you see that because there's the space characters always"
    },
    {
        "id": 1390,
        "start": 7206.7001953125,
        "end": 7213.500244140625,
        "text": " a prefix to these tokens in GPT, so it's not an O token, it's a space O token. The space"
    },
    {
        "id": 1391,
        "start": 7213.500244140625,
        "end": 7221.18017578125,
        "text": " is part of the O, and together they are token 8840, that's space O. So what's happening here"
    },
    {
        "id": 1392,
        "start": 7222.1402587890625,
        "end": 7229.1002197265625,
        "text": " is that when I just have it like this, and I let it complete the next token, it can sample the"
    },
    {
        "id": 1393,
        "start": 7229.1002197265625,
        "end": 7234.8602294921875,
        "text": " space O token, but instead if I have this and I add my space, then what I'm doing here when I"
    },
    {
        "id": 1394,
        "start": 7234.8602294921875,
        "end": 7241.420166015625,
        "text": " encode this string, is I have basically, here's the tagline for an Ice Cream Shop, and this space"
    },
    {
        "id": 1395,
        "start": 7241.420166015625,
        "end": 7248.8602294921875,
        "text": " at the very end becomes a token 220. And so we've added token 220, and this token otherwise would be"
    },
    {
        "id": 1396,
        "start": 7248.8602294921875,
        "end": 7254.1402587890625,
        "text": " part of the tagline, because if there actually is a tagline here, so space O is the token,"
    },
    {
        "id": 1397,
        "start": 7254.940185546875,
        "end": 7260.5401611328125,
        "text": " and so this is suddenly out of distribution for the model, because this space is part of the next"
    },
    {
        "id": 1398,
        "start": 7260.5401611328125,
        "end": 7267.740234375,
        "text": " token, but we're putting it here like this, and the model has seen very very little data of actual"
    },
    {
        "id": 1399,
        "start": 7267.740234375,
        "end": 7272.7001953125,
        "text": " space by itself, and we're asking it to complete the sequence, like add in more tokens, but the"
    },
    {
        "id": 1400,
        "start": 7272.7001953125,
        "end": 7278.5401611328125,
        "text": " problem is that we've sort of begun the first token, and now it's been split up, and now we're"
    },
    {
        "id": 1401,
        "start": 7278.5401611328125,
        "end": 7283.66015625,
        "text": " out of distribution, and now arbitrary bad things happen, and it's just a very rare example for it"
    },
    {
        "id": 1402,
        "start": 7283.66015625,
        "end": 7289.020263671875,
        "text": " to see something like that, and that's why we did the warning. So the fundamental issue here is of"
    },
    {
        "id": 1403,
        "start": 7289.020263671875,
        "end": 7295.420166015625,
        "text": " course that the LLM is on top of these tokens, and these tokens are text chunks, they're not"
    },
    {
        "id": 1404,
        "start": 7295.420166015625,
        "end": 7300.22021484375,
        "text": " characters in a way you and I would think of them, they are, these are the atoms of what the LLM is"
    },
    {
        "id": 1405,
        "start": 7300.22021484375,
        "end": 7305.18017578125,
        "text": " seeing, and there's a bunch of weird stuff that comes out of it. Let's go back to our default"
    },
    {
        "id": 1406,
        "start": 7305.18017578125,
        "end": 7311.740234375,
        "text": " cell style. I bet you that the model has never in its training set seen default cell star"
    },
    {
        "id": 1407,
        "start": 7312.5401611328125,
        "end": 7319.020263671875,
        "text": " without le in there. It's always seen this as a single group, because this is some kind of a"
    },
    {
        "id": 1408,
        "start": 7319.020263671875,
        "end": 7324.6202392578125,
        "text": " function in, I don't actually know what this is part of, it's some kind of API, but I bet you"
    },
    {
        "id": 1409,
        "start": 7324.6202392578125,
        "end": 7330.8602294921875,
        "text": " that it's never seen this combination of tokens in its training data, because, or I think it"
    },
    {
        "id": 1410,
        "start": 7330.8602294921875,
        "end": 7336.0601806640625,
        "text": " would be extremely rare. So I took this and I copied pasted it here, and I had, I tried to"
    },
    {
        "id": 1411,
        "start": 7336.0601806640625,
        "end": 7340.8602294921875,
        "text": " complete from it, and that it immediately gave me a big error, and it said the model predicted"
    },
    {
        "id": 1412,
        "start": 7340.8602294921875,
        "end": 7344.3001708984375,
        "text": " completion that begins with a stop sequence resulting in no output. Consider adjusting"
    },
    {
        "id": 1413,
        "start": 7344.3001708984375,
        "end": 7349.500244140625,
        "text": " your prompt or stop sequences. So what happened here when I clicked submit is that immediately"
    },
    {
        "id": 1414,
        "start": 7349.500244140625,
        "end": 7355.1002197265625,
        "text": " the model emitted and sort of like end of text token, I think, or something like that, it basically"
    },
    {
        "id": 1415,
        "start": 7355.1002197265625,
        "end": 7359.5802001953125,
        "text": " predicted the stop sequence immediately, so it had no completion, and so this is why I'm getting a"
    },
    {
        "id": 1416,
        "start": 7359.5802001953125,
        "end": 7365.020263671875,
        "text": " warning again, because we're off the data distribution and the model is just predicting"
    },
    {
        "id": 1417,
        "start": 7365.9002685546875,
        "end": 7370.3802490234375,
        "text": " just totally arbitrary things. It's just really confused basically. This is giving it brain"
    },
    {
        "id": 1418,
        "start": 7370.3802490234375,
        "end": 7374.6202392578125,
        "text": " damage. It's never seen this before. It's shocked, and it's predicting end of text or something."
    },
    {
        "id": 1419,
        "start": 7375.18017578125,
        "end": 7380.1402587890625,
        "text": " I tried it again here, and in this case it completed it, but then for some reason this"
    },
    {
        "id": 1420,
        "start": 7380.1402587890625,
        "end": 7386.8602294921875,
        "text": " request may violate our usage policies. This was flagged. Basically something just like goes wrong,"
    },
    {
        "id": 1421,
        "start": 7386.8602294921875,
        "end": 7390.460205078125,
        "text": " and there's something like jank. You can just feel the jank because the model is like extremely"
    },
    {
        "id": 1422,
        "start": 7390.460205078125,
        "end": 7394.1402587890625,
        "text": " unhappy with just this, and it doesn't know how to complete it because it's never occurred in a"
    },
    {
        "id": 1423,
        "start": 7394.1402587890625,
        "end": 7400.3001708984375,
        "text": " training set. In a training set it always appears like this and becomes a single token. So these"
    },
    {
        "id": 1424,
        "start": 7400.3001708984375,
        "end": 7405.420166015625,
        "text": " kinds of issues where tokens are either you sort of like complete the first character of the next"
    },
    {
        "id": 1425,
        "start": 7405.420166015625,
        "end": 7410.1402587890625,
        "text": " token, or you are sort of, you have long tokens that you then have just some of the characters"
    },
    {
        "id": 1426,
        "start": 7410.1402587890625,
        "end": 7415.8201904296875,
        "text": " off. All of these are kind of like issues with partial tokens is how I would describe it."
    },
    {
        "id": 1427,
        "start": 7416.5401611328125,
        "end": 7422.3802490234375,
        "text": " And if you actually dig into the Tuk token repository, go to the Rust code and search for"
    },
    {
        "id": 1428,
        "start": 7422.3802490234375,
        "end": 7430.3001708984375,
        "text": " unstable, and you'll see in code unstable native, unstable tokens, and a lot of like special case"
    },
    {
        "id": 1429,
        "start": 7430.3001708984375,
        "end": 7435.8201904296875,
        "text": " handling. None of this stuff about unstable tokens is documented anywhere, but there's a ton of code"
    },
    {
        "id": 1430,
        "start": 7435.8201904296875,
        "end": 7442.0601806640625,
        "text": " dealing with unstable tokens, and unstable tokens is exactly kind of like what I'm describing here."
    },
    {
        "id": 1431,
        "start": 7442.0601806640625,
        "end": 7446.7001953125,
        "text": " What you would like out of a completion API is something a lot more fancy. Like if we're putting"
    },
    {
        "id": 1432,
        "start": 7446.7001953125,
        "end": 7451.18017578125,
        "text": " in default cell star, if we're asking for the next token sequence, we're not actually trying"
    },
    {
        "id": 1433,
        "start": 7451.18017578125,
        "end": 7456.3802490234375,
        "text": " to append the next token exactly after this list. We're actually trying to append, we're trying to"
    },
    {
        "id": 1434,
        "start": 7456.3802490234375,
        "end": 7464.3802490234375,
        "text": " consider lots of tokens that if we were, I guess like we're trying to search over characters that"
    },
    {
        "id": 1435,
        "start": 7465.3402099609375,
        "end": 7470.940185546875,
        "text": " if we re-tokenized would be of high probability, if that makes sense, so that we can actually add"
    },
    {
        "id": 1436,
        "start": 7470.940185546875,
        "end": 7476.3802490234375,
        "text": " a single individual character instead of just like adding the next full token that comes after"
    },
    {
        "id": 1437,
        "start": 7476.3802490234375,
        "end": 7481.8201904296875,
        "text": " this partial token list. So this is very tricky to describe, and I invite you to maybe like look"
    },
    {
        "id": 1438,
        "start": 7481.8201904296875,
        "end": 7486.1402587890625,
        "text": " through this. It ends up being extremely gnarly and hairy kind of topic, and it comes from"
    },
    {
        "id": 1439,
        "start": 7486.1402587890625,
        "end": 7491.1002197265625,
        "text": " tokenization fundamentally, so maybe I can even spend an entire video talking about unstable"
    },
    {
        "id": 1440,
        "start": 7491.18017578125,
        "end": 7495.5802001953125,
        "text": " tokens sometime in the future. Okay, and I'm really saving the best for last. My favorite"
    },
    {
        "id": 1441,
        "start": 7495.5802001953125,
        "end": 7503.66015625,
        "text": " one by far is this solid gold Magikarp. This comes from this blog post, solid gold Magikarp,"
    },
    {
        "id": 1442,
        "start": 7504.460205078125,
        "end": 7511.740234375,
        "text": " and this is internet famous now for those of us in LLMs, and basically I would advise you to"
    },
    {
        "id": 1443,
        "start": 7511.740234375,
        "end": 7517.8201904296875,
        "text": " read this blog post in full, but basically what this person was doing is this person went to the"
    },
    {
        "id": 1444,
        "start": 7518.3001708984375,
        "end": 7524.6202392578125,
        "text": " token embedding stable and clustered the tokens based on their embedding representation,"
    },
    {
        "id": 1445,
        "start": 7525.500244140625,
        "end": 7530.1402587890625,
        "text": " and this person noticed that there's a cluster of tokens that look really strange."
    },
    {
        "id": 1446,
        "start": 7530.1402587890625,
        "end": 7535.420166015625,
        "text": " So there's a cluster here, at rot, eStream fame, solid gold Magikarp, signet message,"
    },
    {
        "id": 1447,
        "start": 7535.420166015625,
        "end": 7542.460205078125,
        "text": " like really weird tokens in basically in this embedding cluster, and so where are these tokens,"
    },
    {
        "id": 1448,
        "start": 7542.460205078125,
        "end": 7545.66015625,
        "text": " and where did they even come from? Like what is solid gold Magikarp? It makes no sense,"
    },
    {
        "id": 1449,
        "start": 7546.3001708984375,
        "end": 7552.5401611328125,
        "text": " and then they found a bunch of these tokens, and then they noticed that actually the plot"
    },
    {
        "id": 1450,
        "start": 7552.5401611328125,
        "end": 7558.460205078125,
        "text": " thickens here, because if you ask the model about these tokens, like you ask it some very benign"
    },
    {
        "id": 1451,
        "start": 7558.460205078125,
        "end": 7563.3402099609375,
        "text": " question, like please can you repeat back to me the string sold gold Magikarp, then you get a"
    },
    {
        "id": 1452,
        "start": 7563.3402099609375,
        "end": 7569.66015625,
        "text": " variety of basically totally broken LLM behavior. So either you get evasion, so I'm sorry I can't"
    },
    {
        "id": 1453,
        "start": 7569.66015625,
        "end": 7575.980224609375,
        "text": " hear you, or you get a bunch of hallucinations as a response. You can even get back like insults,"
    },
    {
        "id": 1454,
        "start": 7575.980224609375,
        "end": 7581.66015625,
        "text": " so you ask it about streamer bot, and it tells the model actually just calls you names,"
    },
    {
        "id": 1455,
        "start": 7582.940185546875,
        "end": 7587.740234375,
        "text": " or it kind of comes up with like weird humor. Like you're actually breaking the model by asking"
    },
    {
        "id": 1456,
        "start": 7587.740234375,
        "end": 7592.7802734375,
        "text": " about these very simple strings like at rot and solid gold Magikarp. So like what the hell is"
    },
    {
        "id": 1457,
        "start": 7592.7802734375,
        "end": 7597.8201904296875,
        "text": " happening? And there's a variety of here documented behaviors. There's a bunch of tokens, not just"
    },
    {
        "id": 1458,
        "start": 7597.8201904296875,
        "end": 7602.5401611328125,
        "text": " sold gold Magikarp, that have that kind of behavior. And so basically there's a bunch of like"
    },
    {
        "id": 1459,
        "start": 7602.5401611328125,
        "end": 7607.26025390625,
        "text": " trigger words, and if you ask the model about these trigger words, or you just include them in your"
    },
    {
        "id": 1460,
        "start": 7607.26025390625,
        "end": 7613.26025390625,
        "text": " prompt, the model goes haywire and has all kinds of really strange behaviors, including sort of ones"
    },
    {
        "id": 1461,
        "start": 7613.26025390625,
        "end": 7618.460205078125,
        "text": " that violate typical safety guidelines and the alignment of the model, like it's swearing back at"
    },
    {
        "id": 1462,
        "start": 7618.460205078125,
        "end": 7624.7001953125,
        "text": " you. So what is happening here, and how can this possibly be true? Well this again comes down to"
    },
    {
        "id": 1463,
        "start": 7624.7001953125,
        "end": 7629.8201904296875,
        "text": " tokenization. So what's happening here is that sold gold Magikarp, if you actually dig into it,"
    },
    {
        "id": 1464,
        "start": 7629.8201904296875,
        "end": 7636.5401611328125,
        "text": " is a reddit user. So there's a u slash sold gold Magikarp, and probably what happened here,"
    },
    {
        "id": 1465,
        "start": 7636.5401611328125,
        "end": 7641.5802001953125,
        "text": " even though I don't know that this has been like really definitively explored, but what is thought"
    },
    {
        "id": 1466,
        "start": 7641.5802001953125,
        "end": 7647.8201904296875,
        "text": " to have happened is that the tokenization dataset was very different from the training dataset for"
    },
    {
        "id": 1467,
        "start": 7647.8201904296875,
        "end": 7653.18017578125,
        "text": " the actual language model. So in the tokenization dataset there was a ton of reddit data potentially,"
    },
    {
        "id": 1468,
        "start": 7653.18017578125,
        "end": 7659.26025390625,
        "text": " where the user sold gold Magikarp was mentioned in the text. Because sold gold Magikarp was a very"
    },
    {
        "id": 1469,
        "start": 7659.26025390625,
        "end": 7665.020263671875,
        "text": " common sort of person who would post a lot, this would be a string that occurs many times in a"
    },
    {
        "id": 1470,
        "start": 7665.020263671875,
        "end": 7670.460205078125,
        "text": " tokenization dataset. Because it occurs many times in the tokenization dataset, these tokens would"
    },
    {
        "id": 1471,
        "start": 7670.460205078125,
        "end": 7675.980224609375,
        "text": " end up getting merged to a single individual token for that single reddit user sold gold Magikarp."
    },
    {
        "id": 1472,
        "start": 7675.980224609375,
        "end": 7681.420166015625,
        "text": " So they would have a dedicated token in a vocabulary of, was it 50,000 tokens in GPT-2,"
    },
    {
        "id": 1473,
        "start": 7681.420166015625,
        "end": 7687.18017578125,
        "text": " that is devoted to that reddit user. And then what happens is the tokenization dataset has"
    },
    {
        "id": 1474,
        "start": 7687.18017578125,
        "end": 7691.740234375,
        "text": " those strings, but then later when you train the model, the language model itself,"
    },
    {
        "id": 1475,
        "start": 7693.18017578125,
        "end": 7698.7001953125,
        "text": " this data from reddit was not present. And so therefore in the entire training set for the"
    },
    {
        "id": 1476,
        "start": 7698.7001953125,
        "end": 7704.940185546875,
        "text": " language model, sold gold Magikarp never occurs. That token never appears in the training set for"
    },
    {
        "id": 1477,
        "start": 7704.940185546875,
        "end": 7710.7802734375,
        "text": " the actual language model later. So this token never gets activated, it's initialized at random"
    },
    {
        "id": 1478,
        "start": 7710.7802734375,
        "end": 7715.020263671875,
        "text": " in the beginning of optimization. Then you have forward-backward passes and updates to the model,"
    },
    {
        "id": 1479,
        "start": 7715.020263671875,
        "end": 7719.1002197265625,
        "text": " and this token is just never updated in the embedding table. That row vector never gets"
    },
    {
        "id": 1480,
        "start": 7719.1002197265625,
        "end": 7723.5802001953125,
        "text": " sampled, it never gets used, so it never gets trained, and it's completely untrained. It's"
    },
    {
        "id": 1481,
        "start": 7723.5802001953125,
        "end": 7728.940185546875,
        "text": " kind of like unallocated memory in a typical binary program written in C or something like that."
    },
    {
        "id": 1482,
        "start": 7728.940185546875,
        "end": 7734.1402587890625,
        "text": " So it's unallocated memory. And then at test time, if you evoke this token, then you're basically"
    },
    {
        "id": 1483,
        "start": 7734.1402587890625,
        "end": 7737.9002685546875,
        "text": " plucking out a row of the embedding table that is completely untrained, and that feeds into a"
    },
    {
        "id": 1484,
        "start": 7737.9002685546875,
        "end": 7741.980224609375,
        "text": " transformer and creates undefined behavior. And that's what we're seeing here, this completely"
    },
    {
        "id": 1485,
        "start": 7741.980224609375,
        "end": 7747.3402099609375,
        "text": " undefined, never-before-seen-in-a-training behavior. And so any of these kind of like weird tokens"
    },
    {
        "id": 1486,
        "start": 7747.3402099609375,
        "end": 7755.020263671875,
        "text": " would evoke this behavior, because fundamentally the model is out of sample, out of distribution."
    },
    {
        "id": 1487,
        "start": 7755.8201904296875,
        "end": 7759.26025390625,
        "text": " Okay, and the very last thing I wanted to just briefly mention and point out, although I think"
    },
    {
        "id": 1488,
        "start": 7759.26025390625,
        "end": 7763.020263671875,
        "text": " a lot of people are quite aware of this, is that different kinds of formats and different"
    },
    {
        "id": 1489,
        "start": 7763.020263671875,
        "end": 7768.5401611328125,
        "text": " representations in different languages and so on might be more or less efficient with GPT tokenizers,"
    },
    {
        "id": 1490,
        "start": 7769.26025390625,
        "end": 7773.66015625,
        "text": " or any tokenizers for any other LLM for that matter. So for example, JSON is actually really"
    },
    {
        "id": 1491,
        "start": 7773.66015625,
        "end": 7780.5401611328125,
        "text": " dense in tokens, and YAML is a lot more efficient in tokens. So for example, these are"
    },
    {
        "id": 1492,
        "start": 7780.5401611328125,
        "end": 7788.6202392578125,
        "text": " the same in JSON and in YAML. The JSON is 116 and the YAML is 99, so quite a bit of an improvement."
    },
    {
        "id": 1493,
        "start": 7789.1002197265625,
        "end": 7795.420166015625,
        "text": " And so in the token economy, where you are paying per token in many ways, and you are paying in the"
    },
    {
        "id": 1494,
        "start": 7795.420166015625,
        "end": 7800.5401611328125,
        "text": " context length, and you're paying in dollar amount for the cost of processing all this kind of"
    },
    {
        "id": 1495,
        "start": 7800.5401611328125,
        "end": 7806.22021484375,
        "text": " structured data when you have to, so I prefer to use YAMLs over JSONs. And in general, kind of like the"
    },
    {
        "id": 1496,
        "start": 7806.22021484375,
        "end": 7811.500244140625,
        "text": " tokenization density is something that you have to sort of care about and worry about at all times,"
    },
    {
        "id": 1497,
        "start": 7811.500244140625,
        "end": 7816.22021484375,
        "text": " and try to find efficient encoding schemes, and spend a lot of time in TIC tokenizer and measure"
    },
    {
        "id": 1498,
        "start": 7816.3001708984375,
        "end": 7820.7802734375,
        "text": " the different token efficiencies of different formats and settings and so on. Okay, so that"
    },
    {
        "id": 1499,
        "start": 7820.7802734375,
        "end": 7827.1002197265625,
        "text": " concludes my fairly long video on tokenization. I know it's try, I know it's annoying, I know it's"
    },
    {
        "id": 1500,
        "start": 7827.1002197265625,
        "end": 7832.460205078125,
        "text": " irritating, I personally really dislike the stage. But what I do have to say at this point is don't"
    },
    {
        "id": 1501,
        "start": 7832.460205078125,
        "end": 7838.7001953125,
        "text": " brush it off. There's a lot of foot guns, sharp edges here, security issues, AI safety issues,"
    },
    {
        "id": 1502,
        "start": 7838.7001953125,
        "end": 7844.940185546875,
        "text": " as we saw plugging in unallocated memory into language models. So it's worth understanding"
    },
    {
        "id": 1503,
        "start": 7844.940185546875,
        "end": 7851.1002197265625,
        "text": " this stage. That said, I will say that eternal glory goes to anyone who can get rid of it."
    },
    {
        "id": 1504,
        "start": 7851.1002197265625,
        "end": 7856.6202392578125,
        "text": " I showed you one possible paper that tried to do that, and I think, I hope a lot more can follow"
    },
    {
        "id": 1505,
        "start": 7856.6202392578125,
        "end": 7861.5802001953125,
        "text": " over time. And my final recommendations for the application right now are if you can reuse the"
    },
    {
        "id": 1506,
        "start": 7861.5802001953125,
        "end": 7865.980224609375,
        "text": " GPT-4 tokens and vocabulary in your application, then that's something you should consider and"
    },
    {
        "id": 1507,
        "start": 7865.980224609375,
        "end": 7872.7001953125,
        "text": " just use TIC token because it is very efficient and nice library for inference for BP. I also"
    },
    {
        "id": 1508,
        "start": 7872.7001953125,
        "end": 7878.8602294921875,
        "text": " really like the byte level BP that TIC token and OpenAI uses. If you for some reason want to train"
    },
    {
        "id": 1509,
        "start": 7878.8602294921875,
        "end": 7887.5802001953125,
        "text": " your own vocabulary from scratch, then I would use the BP with sentence piece. Oops. As I mentioned,"
    },
    {
        "id": 1510,
        "start": 7887.5802001953125,
        "end": 7893.9002685546875,
        "text": " I'm not a huge fan of sentence piece. I don't like its byte fallback. And I don't like that"
    },
    {
        "id": 1511,
        "start": 7893.9002685546875,
        "end": 7898.7001953125,
        "text": " it's doing BP on Unicode code points. I think it's it also has like a million settings. And I think"
    },
    {
        "id": 1512,
        "start": 7898.7001953125,
        "end": 7902.3001708984375,
        "text": " there's a lot of foot guns here. And I think it's really easy to miscalibrate them. And you end up"
    },
    {
        "id": 1513,
        "start": 7902.3001708984375,
        "end": 7906.5401611328125,
        "text": " cropping your sentences or something like that because of some type of parameter that you don't"
    },
    {
        "id": 1514,
        "start": 7906.5401611328125,
        "end": 7912.3802490234375,
        "text": " fully understand. So be very careful with the settings. Try to copy paste exactly maybe what"
    },
    {
        "id": 1515,
        "start": 7912.3802490234375,
        "end": 7917.26025390625,
        "text": " Meta did or basically spend a lot of time looking at all the hyperparameters and go through the code"
    },
    {
        "id": 1516,
        "start": 7917.26025390625,
        "end": 7922.5401611328125,
        "text": " of sentence piece and make sure that you have this correct. But even if you have all the settings"
    },
    {
        "id": 1517,
        "start": 7922.5401611328125,
        "end": 7927.980224609375,
        "text": " correct, I still think that the algorithm is kind of inferior to what's happening here. And maybe"
    },
    {
        "id": 1518,
        "start": 7928.0601806640625,
        "end": 7931.740234375,
        "text": " the best if you really need to train your vocabulary, maybe the best thing is to just"
    },
    {
        "id": 1519,
        "start": 7931.740234375,
        "end": 7937.66015625,
        "text": " wait for minBPE to become as efficient as possible. And that's something that maybe I"
    },
    {
        "id": 1520,
        "start": 7937.66015625,
        "end": 7943.26025390625,
        "text": " hope to work on. And at some point, maybe we can be training basically, really what we want is we"
    },
    {
        "id": 1521,
        "start": 7943.26025390625,
        "end": 7948.7802734375,
        "text": " want TIC token, but training code. And that is the ideal thing that currently does not exist."
    },
    {
        "id": 1522,
        "start": 7949.420166015625,
        "end": 7953.740234375,
        "text": " And minBPE is an implementation of it, but currently it's in Python."
    },
    {
        "id": 1523,
        "start": 7954.6202392578125,
        "end": 7959.420166015625,
        "text": " So that's currently what I have to say for tokenization. There might be an advanced video"
    },
    {
        "id": 1524,
        "start": 7959.420166015625,
        "end": 7963.020263671875,
        "text": " that has even drier and even more detailed in the future. But for now,"
    },
    {
        "id": 1525,
        "start": 7963.020263671875,
        "end": 7967.9002685546875,
        "text": " I think we're going to leave things off here and I hope that was helpful. Bye."
    },
    {
        "id": 1526,
        "start": 7973.8201904296875,
        "end": 7981.8201904296875,
        "text": " And they increased this context size from GPT-1 of 512 to 1024 in GPT-4... 2."
    },
    {
        "id": 1527,
        "start": 7981.8201904296875,
        "end": 7988.8602294921875,
        "text": " The next... Okay. Next, I would like us to briefly walk through the"
    },
    {
        "id": 1528,
        "start": 7988.8602294921875,
        "end": 7991.5802001953125,
        "text": " code for OpenAI on the GPT-2 encoder.py."
    },
    {
        "id": 1529,
        "start": 7996.5401611328125,
        "end": 8000.5401611328125,
        "text": " I'm sorry, I'm going to sneeze. And then what's happening here is..."
    },
    {
        "id": 1530,
        "start": 8002.6202392578125,
        "end": 8008.0601806640625,
        "text": " This is a spurious layer that I will explain in a bit. What's happening here is..."
    }
]