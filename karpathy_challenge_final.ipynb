{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceb3ab89-7990-4469-8aad-43c24472b1f9",
   "metadata": {},
   "source": [
    "<h1>Video Companion Guide Challenge</h1>\n",
    "<h2>Submission by: Adam Łucek</h2>\n",
    "<h4>Automating the creation and media population of markdown companion guides with ✨AI✨</h4>\n",
    "<div>\n",
    "<img src=\"tweet.png\" width=\"500\"/>\n",
    "</div>\n",
    "<hr></hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a5282e-f372-42cd-ae4a-c79e17e1377f",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "07739e56-7509-4f02-9844-48df5d97b990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube as pyt\n",
    "from moviepy.editor import *\n",
    "import os\n",
    "from moviepy.editor import AudioFileClip\n",
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "import cv2\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "os.environ[\"IMAGEIO_FFMPEG_EXE\"] = \"/opt/homebrew/bin/ffmpeg\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dce99a-e995-4e3d-9290-2b3d07f7ec56",
   "metadata": {},
   "source": [
    "### Time Keeping Function\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b34131ab-8ea8-477e-b851-62ed6694eadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_timer():\n",
    "    global start_time\n",
    "    start_time = time.time()\n",
    "\n",
    "def show_current_runtime():\n",
    "        return round(time.time() - start_time, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d76fb8f-84fd-4c8b-aefc-42f0b0353b0e",
   "metadata": {},
   "source": [
    "### Downloading the Audio & Video\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8da796c3-f9bb-463c-855d-b3ad5dec9a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_video(url, path):\n",
    "    yt = pyt(url)\n",
    "    stream = yt.streams.get_highest_resolution()\n",
    "    stream.download(output_path=f\"{path}/original_files/video\", filename=\"video_file.mp4\")\n",
    "    audio_stream = yt.streams.filter(only_audio=True).first()\n",
    "    audio_stream.download(output_path=f\"{path}/original_files/audio\", filename=\"audio_file.mp4\")\n",
    "\n",
    "def get_title(url):\n",
    "    yt = pyt(url)\n",
    "    return yt.title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23a065e-bf3b-4265-b717-a1b8699dad31",
   "metadata": {},
   "source": [
    "### Checking Audio File Size, and Chunking it if Large\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e7f5597-aa5d-4f34-885f-106d62c13451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio(file_path, chunk_size_mb=12, output_folder=\"split_chunks\"):\n",
    "    global split_audio_return\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File {file_path} not found.\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "    print(f\"File size: {file_size_mb:.2f} MB\")\n",
    "\n",
    "    if file_size_mb <= chunk_size_mb:\n",
    "        print(\"File size is within the limit. No need to split.\")\n",
    "        return\n",
    "    else:\n",
    "        split_audio_return = True\n",
    "\n",
    "    clip = AudioFileClip(file_path)\n",
    "    total_duration = clip.duration\n",
    "    chunk_duration = (chunk_size_mb / file_size_mb) * total_duration\n",
    "\n",
    "    # Split the audio\n",
    "    start = 0\n",
    "    part = 1\n",
    "    while start < total_duration:\n",
    "        end = min(start + chunk_duration, total_duration)\n",
    "        chunk = clip.subclip(start, end)\n",
    "        chunk_filename = os.path.join(output_folder, f\"{os.path.splitext(os.path.basename(file_path))[0]}_part{part}.mp4\")\n",
    "        chunk.write_audiofile(chunk_filename, bitrate=\"64k\", codec=\"aac\")\n",
    "\n",
    "        print(f\"Created chunk: {chunk_filename}\")\n",
    "\n",
    "        start = end\n",
    "        part += 1\n",
    "\n",
    "    clip.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f3142e-6afa-4ac0-9247-f720862ab98e",
   "metadata": {},
   "source": [
    "### Transcribing with Whisper-1 & Writing to JSON File(s)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eab4f585-ac13-478d-8d80-b96458e816c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_json(split_audio_return, input_folder, output_folder=\"transcript_json\"):\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"The folder {input_folder} does not exist.\")\n",
    "        return\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    client = OpenAI()\n",
    "\n",
    "    if not split_audio_return:\n",
    "        # If audio is not split, use the path to the original file\n",
    "        original_audio_path = f\"{input_folder}/original_files/audio\"        \n",
    "        if os.path.exists(original_audio_path):\n",
    "            for filename in os.listdir(original_audio_path):\n",
    "                if filename.endswith(\".mp4\"):\n",
    "                    process_audio_file(client, original_audio_path, filename, output_folder)\n",
    "        else:\n",
    "            print(f\"The original audio folder {original_audio_path} does not exist.\")\n",
    "    else:\n",
    "        input_folder_chunks = f'{path}/split_chunks'\n",
    "        # If audio is split, iterate over the split audio files\n",
    "        for filename in os.listdir(f'{path}/split_chunks'):\n",
    "            if filename.endswith(\".mp4\"):\n",
    "                process_audio_file(client, input_folder_chunks, filename, output_folder)\n",
    "\n",
    "def process_audio_file(client, folder_path, filename, output_folder):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    with open(file_path, \"rb\") as audio_file:\n",
    "        transcript = client.audio.transcriptions.create(\n",
    "            file=audio_file,\n",
    "            model=\"whisper-1\",\n",
    "            response_format=\"verbose_json\",\n",
    "            timestamp_granularities=[\"segment\"]\n",
    "        )\n",
    "\n",
    "        json_filename = f\"{os.path.splitext(filename)[0]}_transcript.json\"\n",
    "        output_path = os.path.join(output_folder, json_filename)\n",
    "\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(transcript.segments, f, indent=4)\n",
    "\n",
    "        print(f\"Transcript for {filename} saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054935e3-818d-474f-8102-0460f81eeef5",
   "metadata": {},
   "source": [
    "### Cleaning & Concatenating the Transcription JSON files\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2152362-7151-47ee-aa19-34c004d52805",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_part_number(filename):\n",
    "    match = re.search(r'part(\\d+)', filename)\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "def process_file(filepath, max_id, last_end_time):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        time_adjustment = last_end_time - float(data[0]['start']) if last_end_time else 0\n",
    "        new_data = []\n",
    "        for entry in data:\n",
    "            new_entry = {\n",
    "                'id': max_id + 1,\n",
    "                'start': float(entry['start']) + time_adjustment,\n",
    "                'end': float(entry['end']) + time_adjustment,\n",
    "                'text': entry['text']\n",
    "            }\n",
    "            new_data.append(new_entry)\n",
    "            max_id += 1\n",
    "        return new_data, max_id, new_data[-1]['end'] if new_data else last_end_time\n",
    "\n",
    "def process_transcripts(directory_path, split_audio_return):\n",
    "    files = os.listdir(directory_path)\n",
    "    combined_data = []\n",
    "    max_id = -1\n",
    "    last_end_time = 0.0\n",
    "\n",
    "    if split_audio_return:\n",
    "        sorted_files = sorted(\n",
    "            [file for file in files if file.startswith('audio') and file.endswith('.json')],\n",
    "            key=extract_part_number\n",
    "        )\n",
    "    else:\n",
    "        sorted_files = [file for file in files if file.endswith('.json') and not 'part' in file]\n",
    "\n",
    "    for filename in sorted_files:\n",
    "        full_path = os.path.join(directory_path, filename)\n",
    "        processed_data, max_id, last_end_time = process_file(full_path, max_id, last_end_time)\n",
    "        combined_data.extend(processed_data)\n",
    "\n",
    "    # Output the combined data to a new JSON file\n",
    "    output_path = os.path.join(directory_path, 'combined_data.json')\n",
    "    with open(output_path, 'w') as outfile:\n",
    "        json.dump(combined_data, outfile, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b56ef6-96ba-4c14-acec-2b7a84c8b2b3",
   "metadata": {},
   "source": [
    "### Also Grabbing the Full Transcript\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ff9b387-63a1-4c52-b609-734b25e220be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_transcript(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    transcript = \"\"\n",
    "    for entry in data:\n",
    "        transcript += entry['text']\n",
    "    return transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123e38fa-15de-40de-938c-e41bfc264c24",
   "metadata": {},
   "source": [
    "### Function to Combine Short Chunks of Transcription\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2cfbee6b-a42d-478d-abfe-6dd7de05becd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def merge_short_documents(documents, min_length=2000):\n",
    "    i = 0\n",
    "    while i < len(documents) - 1:\n",
    "        current_doc = documents[i]\n",
    "        if len(current_doc.page_content) < min_length:\n",
    "            documents[i + 1].page_content = current_doc.page_content + documents[i + 1].page_content\n",
    "            del documents[i]\n",
    "        else:\n",
    "            i += 1\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a378a120-6934-4d21-9122-38830f00bbcc",
   "metadata": {},
   "source": [
    "### Main Markdown File Prompting with GPT-4-Turbo & LangChain Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ba169ab4-c63e-411e-8358-42dff42e6733",
   "metadata": {},
   "outputs": [],
   "source": [
    "guide_prompt_template = \"\"\"\n",
    "\n",
    "Below is a script from a video that I am making into a companion guide blog post first. \\\n",
    "You are a helpful assistant made to assist in the creation I'm doing. \\\n",
    "This is a continuation of a guide so include chapters, key summaries, and incorporate visual aids and direct links to relevant parts of the video, \\\n",
    "however do not include any conclusion or overarching title. \\\n",
    "For visual aids, specific frames from the video will be identified where images can be inserted to enhance understanding. \\\n",
    "For direct links, portions of the text should be hyperlinked to their corresponding times in the video. \\\n",
    "To indicate that a sentence should be hyperlinked, insert the raw text of the transcript next to the word with the indicator <HYPERLINK: \"corresponding transcript text\">. \\\n",
    "To indicate a picture regarding the text, insert the indicator <PICTURE: \"corresponding transcript text\">. \\\n",
    "It is crucial to use the raw text from the transcript that will be used, as the additional tools that will be inserting the hyperlinks and pictures need this to know where in the video to look.\n",
    "\n",
    "In this blog post, in addition to the paragraphs: \\\n",
    "\n",
    "Create titles or headings that encapsulate main points and ideas \\\n",
    "\n",
    "Format your response in markdown, ensuring distinction and clean styling between titles and paragraphs. \\\n",
    "Be sure to include the image placeholders, and hyperlinks with enough distinguishable text WITHOUT ANY QUOTATIONS, as the placeholders will be fed into a semantic search algorithm. \\\n",
    "This structured approach will be applied to the entire transcript. \\\n",
    "The example below only shows one style, but use multiple styles including different headings, bullet points, and other markdown elements when needed. \\\n",
    "\n",
    "Here are shortened example of the input and shortened expected output:\n",
    "\n",
    "example input:\n",
    "\n",
    "Hi everyone. So in this video I'd like us to cover the process of tokenization in large language models. Now you see here that I have a sad face and that's because tokenization is my least favorite part of working with large language models but unfortunately it is necessary to understand in some detail because it is fairly hairy, gnarly and there's a lot of hidden foot gums to be aware of and a lot of oddness with large language models typically traces back to tokenization. So what is tokenization? Now in my previous video Let's Build GPT from Scratch we actually already did tokenization but we did a very naive simple version of tokenization. So when you go to the Google Colab for that video you see here that we loaded our training set and our training set was this Shakespeare dataset. Now in the beginning the Shakespeare dataset is just a large string in Python it's just text and so the question is how do we plug text into large language models and in this case here we created a vocabulary of 65 possible characters that we saw occur in this string. These were the possible characters and we saw that there are 65 of them and then we created a lookup table for converting from every possible character a little string piece into a token an integer. So here for example we tokenized the string hi there and we received this sequence of tokens and here we took the first 1000 characters of our dataset and we encoded it into tokens and because this is character level we received 1000 tokens in a sequence so token 18, 47, etc. Now later we saw that the way we plug these tokens into the language model is by using an embedding table and so basically if we have 65 possible tokens then this embedding table is going to have 65 rows and roughly speaking we're taking the integer associated with every single token we're using that as a lookup into this table and we're plucking out the corresponding row and this row is trainable parameters that we're going to train using backpropagation and this is the vector that then feeds into the transformer and that's how the transformer sort of perceives every single token. So here we had a very naive tokenization process that was a character level tokenizer\n",
    "\n",
    "example output:\n",
    "\n",
    "Introduction to Tokenization\n",
    "----------------------------\n",
    "\n",
    "Welcome to our comprehensive guide on tokenization in large language models (LLMs). Tokenization is a critical yet complex aspect of working with LLMs, essential for understanding how these models process text data. Despite its challenges, tokenization is foundational, as it converts strings of text into sequences of tokens, small units of text that LLMs can manage more effectively.\n",
    "\n",
    "<PICTURE: Now you see here that I have a sad face and that's because tokenization is my least favorite part of working with large language models but unfortunately it is necessary to understand in some detail because it is fairly hairy, gnarly and there's a lot of hidden foot gums>\n",
    "\n",
    "Understanding the Basics of Tokenization\n",
    "----------------------------------------\n",
    "\n",
    "Tokenization involves creating a vocabulary from all unique characters or words in a dataset and converting each into a corresponding integer token. This process was briefly introduced in our \"Let's Build GPT from Scratch\" video, where we tokenized a Shakespeare dataset at a character level, creating a vocabulary of 65 possible characters.\n",
    "\n",
    "<HYPERLINK: So what is tokenization? Now in my previous video Let's Build GPT from Scratch we actually already did tokenization but we did a very naive simple version of tokenization. So when you go to the Google Colab for that video you see here that we loaded>\n",
    "\n",
    "The Role of Embedding Tables in Tokenization\n",
    "--------------------------------------------\n",
    "\n",
    "After tokenization, the next step involves using an embedding table, where each token's integer is used as a lookup to extract a row of trainable parameters. These parameters, once trained, feed into the transformer model, allowing it to perceive each token effectively.\n",
    "\n",
    "<PICTURE: using backpropagation and this is the vector that then feeds into the transformer and that's how the transformer sort of perceives every single token. So here we had a very naive tokenization process that was a character level tokenizer>\n",
    "\n",
    "end examples.\n",
    "\n",
    "Here is the transcript:\n",
    "\n",
    "{transcript}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "llm = ChatOpenAI(temperature=0.0, model=\"gpt-4-turbo-preview\")\n",
    "guide_prompt = ChatPromptTemplate.from_template(guide_prompt_template)\n",
    "\n",
    "guide_chain = (\n",
    "    {\"transcript\": RunnablePassthrough()} \n",
    "    | guide_prompt\n",
    "    | llm\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "def generate_markdown(merged_docs, path, guide_chain):\n",
    "    markdown_outputs = []\n",
    "    for doc in merged_docs:\n",
    "        output = guide_chain.invoke(doc.page_content)\n",
    "        markdown_outputs.append(output)\n",
    "    combined_output = '\\n\\n'.join(markdown_outputs)\n",
    "    with open(f'{path}/transcript_json/llm_outline.txt', 'w') as file:\n",
    "        file.write(combined_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297e6ef2-663a-46e8-9c05-759eb7a53a78",
   "metadata": {},
   "source": [
    "### Dealing with the Placeholders, Grabbing Pictures & Formatting Hyperlinks\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "59c00fc4-b700-4869-b46f-f8bdfb64a3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_frame(video, second):\n",
    "    frames_dir = 'frames'\n",
    "    if not os.path.exists(frames_dir):\n",
    "        os.makedirs(frames_dir)\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return None\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_number = round(int(second * fps))\n",
    "    total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    if frame_number >= total_frames:\n",
    "        print(f\"Error: Frame number {frame_number} exceeds total frames in video.\")\n",
    "        cap.release()\n",
    "        return None\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Error: Could not read frame.\")\n",
    "        cap.release()\n",
    "        return None\n",
    "\n",
    "    frame_path = os.path.join(frames_dir, f'frame_{second}.jpg')\n",
    "    cv2.imwrite(frame_path, frame)\n",
    "    cap.release()\n",
    "\n",
    "    return frame_path\n",
    "\n",
    "def retrieve_time(segment):\n",
    "    docs = retriever.get_relevant_documents(segment)\n",
    "    docs_dict = json.loads(docs[0].page_content)\n",
    "    start_time = docs_dict[\"start\"]\n",
    "    end_time = docs_dict[\"end\"]\n",
    "    time = (start_time + end_time) / 2\n",
    "    final_time = round(time)\n",
    "    return final_time\n",
    "\n",
    "def create_hyperlink(segment, url):\n",
    "    time = retrieve_time(segment)\n",
    "    time_link = f\"{url}&t={time}s\"\n",
    "    return time_link\n",
    "\n",
    "def format_seconds_to_hms(seconds):\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    seconds = seconds % 60\n",
    "    return f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n",
    "\n",
    "def process_placeholder(placeholder):\n",
    "    if placeholder.startswith(\"<PICTURE:\"):\n",
    "        description = placeholder[9:-1]\n",
    "        time = retrieve_time(description)\n",
    "        image_path = grab_frame(video_path, time)\n",
    "        # Embed the image using markdown with a specified width\n",
    "        return f'<img src=\"{image_path}\" alt=\"{description}\" width=\"450\"/>'\n",
    "    elif placeholder.startswith(\"<HYPERLINK:\"):\n",
    "        text = placeholder[11:-1]\n",
    "        time = retrieve_time(text)\n",
    "        formatted_time = format_seconds_to_hms(time)\n",
    "        hyperlink = create_hyperlink(text, url)\n",
    "        return f'[Jump to this part of the video: {formatted_time}]({hyperlink})'\n",
    "    else:\n",
    "        return placeholder\n",
    "\n",
    "def replace_placeholders(content):\n",
    "    placeholders = re.findall(r\"<[^>]+>\", content)\n",
    "    for placeholder in placeholders:\n",
    "        replacement = process_placeholder(placeholder)\n",
    "        content = content.replace(placeholder, replacement, 1)\n",
    "    return content\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def convert_txt(path, title, db):\n",
    "    txt_file_path = f'{path}/transcript_json/llm_outline.txt'\n",
    "    output_file_path = f'{path}/companion_guide.txt'\n",
    "    global video_path\n",
    "    video_path = f'{path}/original_files/video/video_file.mp4'\n",
    "    global retriever\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": 1})\n",
    "    \n",
    "    content = read_file(txt_file_path)\n",
    "    updated_content = replace_placeholders(content)\n",
    "    \n",
    "    with open(output_file_path, 'w') as file:\n",
    "        file.write(updated_content)\n",
    "    \n",
    "    print(f\"Updated markdown content has been written to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb42def-5b26-471c-80d9-89ba10d77928",
   "metadata": {},
   "source": [
    "### Main Script\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2081558-f942-422e-9f68-af398a6dd149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Video & Audio, Runtime: 0.0\n",
      "Video & Audio Downloaded, Runtime: 11.26\n",
      "Checking File Size & Splitting if Necessary, Runtime: 11.26\n",
      "File size: 46.61 MB\n",
      "MoviePy - Writing audio in split_chunks/audio_file_part1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Created chunk: split_chunks/audio_file_part1.mp4\n",
      "MoviePy - Writing audio in split_chunks/audio_file_part2.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Created chunk: split_chunks/audio_file_part2.mp4\n",
      "MoviePy - Writing audio in split_chunks/audio_file_part3.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Created chunk: split_chunks/audio_file_part3.mp4\n",
      "MoviePy - Writing audio in split_chunks/audio_file_part4.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Created chunk: split_chunks/audio_file_part4.mp4\n",
      "Audio Checked & Split, Runtime: 110.94\n",
      "Processing Audio File with Whisper-1, Runtime: 110.94\n",
      "Transcript for audio_file_part4.mp4 saved to transcript_json/audio_file_part4_transcript.json\n",
      "Transcript for audio_file_part3.mp4 saved to transcript_json/audio_file_part3_transcript.json\n",
      "Transcript for audio_file_part2.mp4 saved to transcript_json/audio_file_part2_transcript.json\n",
      "Transcript for audio_file_part1.mp4 saved to transcript_json/audio_file_part1_transcript.json\n",
      "Audio Processed with Whisper-1, Runtime: 474.22\n",
      "Cleaning Data, Runtime: 474.22\n",
      "Data Cleaned, Runtime: 474.24\n",
      "Pulling Full Transcript, Runtime: 474.24\n",
      "Transcript Pulled, Runtime: 474.24\n",
      "Chunking & Splitting Transcript, Runtime: 474.24\n",
      "Transcript Chunked, Runtime: 477.68\n",
      "Embedding Transcript, Runtime: 477.68\n",
      "Transcript Embedded, Runtime: 482.44\n",
      "Generating Markdown Outline with GPT-4-T, Runtime: 482.44\n",
      "Markdown File Generated, Runtime: 1278.96\n",
      "Replacing Placeholders With Pictures & Links, Runtime: 1278.96\n",
      "Updated markdown content has been written to /Users/adamlucek/Documents/Jupyter/karpathy_guide_challenge/companion_guide.txt\n",
      "Report Finished, Runtime: 1316.19\n"
     ]
    }
   ],
   "source": [
    "# main script\n",
    "start_timer()\n",
    "# URL & Path of Interest\n",
    "url = 'https://www.youtube.com/watch?v=zduSFxRajkE'\n",
    "path = '/Users/adamlucek/Documents/Jupyter/karpathy_guide_challenge'\n",
    "\n",
    "print(f\"Downloading Video & Audio, Runtime: {show_current_runtime()}\")\n",
    "# download video, audio, and details\n",
    "download_video(url, path)\n",
    "title = get_title(url)\n",
    "print(f\"Video & Audio Downloaded, Runtime: {show_current_runtime()}\")\n",
    "\n",
    "print(f\"Checking File Size & Splitting if Necessary, Runtime: {show_current_runtime()}\")\n",
    "# Check filesize, split into multiple files if needed\n",
    "split_audio_return = False\n",
    "split_audio(f\"{path}/original_files/audio/audio_file.mp4\")\n",
    "print(f\"Audio Checked & Split, Runtime: {show_current_runtime()}\")\n",
    "\n",
    "print(f\"Processing Audio File with Whisper-1, Runtime: {show_current_runtime()}\")\n",
    "# Process audio files with Whisper and create JSON files of output\n",
    "create_json(split_audio_return, path)\n",
    "print(f\"Audio Processed with Whisper-1, Runtime: {show_current_runtime()}\")\n",
    "\n",
    "print(f\"Cleaning Data, Runtime: {show_current_runtime()}\")\n",
    "# Combine if needed, clean extra data\n",
    "process_transcripts(f\"{path}/transcript_json\", split_audio_return)\n",
    "print(f\"Data Cleaned, Runtime: {show_current_runtime()}\")\n",
    "\n",
    "print(f\"Pulling Full Transcript, Runtime: {show_current_runtime()}\")\n",
    "# Pull the full transcript\n",
    "video_transcript = full_transcript(f'{path}/transcript_json/combined_data.json')\n",
    "print(f\"Transcript Pulled, Runtime: {show_current_runtime()}\")\n",
    "\n",
    "print(f\"Chunking & Splitting Transcript, Runtime: {show_current_runtime()}\")\n",
    "# Embed and chunk transcript\n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings())\n",
    "split_docs = text_splitter.create_documents([video_transcript])\n",
    "merged_docs = merge_short_documents(split_docs)\n",
    "print(f\"Transcript Chunked, Runtime: {show_current_runtime()}\")\n",
    "\n",
    "print(f\"Embedding Transcript, Runtime: {show_current_runtime()}\")\n",
    "# Embed documents\n",
    "json_loader = JSONLoader(f\"{path}/transcript_json/combined_data.json\", jq_schema=\".[]\", text_content=False)\n",
    "json_texts = json_loader.load()\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(json_texts, embeddings)\n",
    "print(f\"Transcript Embedded, Runtime: {show_current_runtime()}\")\n",
    "\n",
    "print(f\"Generating Markdown Outline with GPT-4-T, Runtime: {show_current_runtime()}\")\n",
    "# Generate markdown of file with GPT-4-T\n",
    "generate_markdown(merged_docs, path, guide_chain)\n",
    "print(f\"Markdown File Generated, Runtime: {show_current_runtime()}\")\n",
    "\n",
    "print(f\"Replacing Placeholders With Pictures & Links, Runtime: {show_current_runtime()}\")\n",
    "# Replace placeholders with hyperlinks and pictures\n",
    "convert_txt(path, title, db)\n",
    "print(f\"Report Finished, Runtime: {show_current_runtime()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b049fe4-bf58-42bd-9d31-489843148bd3",
   "metadata": {},
   "source": [
    "## **Notes**\n",
    "#### **Cost:**  ~$1.91 for this pass\n",
    "\n",
    "* \\$1.10 from GPT\\-4\\-Turbo\n",
    "  * 56,546 Context Tokens \\& 17,710 Generated Tokens\n",
    "* \\$0.80 from Whisper\\-1\n",
    "  * 8,016 seconds transcribed\n",
    "* \\$0.02 from text\\-embedding\\-002\\-v2\n",
    "  * 172,561 Context Tokens\n",
    "  \n",
    "#### **Time from URL to Full Report:** 21 minutes, 56 Seconds\n",
    "\n",
    "### **Limitations: (aka my todo list)** \n",
    "* vector search can sometimes be innacurate, point towards wrong part of the video out of order with this methods.\n",
    "* Inconsistency of number of pictures and hyperlinks across the board, as it's done automatically\n",
    "* Unnable to input entire script at once (believe me I tried), so all the downsides that come with sequential processing\n",
    "* Could be prettier \n",
    "* My own lack of programming knowledge\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
