{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceb3ab89-7990-4469-8aad-43c24472b1f9",
   "metadata": {},
   "source": [
    "<h1>Video Companion Guide Challenge</h1>\n",
    "<h2>Submission by: Adam Łucek</h2>\n",
    "<h4>Automating the creation and media population of markdown companion guides with ✨AI✨</h4>\n",
    "<div>\n",
    "<img src=\"tweet.png\" width=\"500\"/>\n",
    "</div>\n",
    "<hr></hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a5282e-f372-42cd-ae4a-c79e17e1377f",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "07739e56-7509-4f02-9844-48df5d97b990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube as pyt\n",
    "from moviepy.editor import *\n",
    "import os\n",
    "from moviepy.editor import AudioFileClip\n",
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "import cv2\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "os.environ[\"IMAGEIO_FFMPEG_EXE\"] = \"/opt/homebrew/bin/ffmpeg\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dce99a-e995-4e3d-9290-2b3d07f7ec56",
   "metadata": {},
   "source": [
    "### Time Keeping Function\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b34131ab-8ea8-477e-b851-62ed6694eadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_timer():\n",
    "    global start_time\n",
    "    start_time = time.time()\n",
    "\n",
    "def show_current_runtime():\n",
    "        return round(time.time() - start_time, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d76fb8f-84fd-4c8b-aefc-42f0b0353b0e",
   "metadata": {},
   "source": [
    "### Downloading the Audio & Video\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8da796c3-f9bb-463c-855d-b3ad5dec9a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_video(url, path):\n",
    "    yt = pyt(url)\n",
    "    stream = yt.streams.get_highest_resolution()\n",
    "    stream.download(output_path=f\"{path}/original_files/video\", filename=\"video_file.mp4\")\n",
    "    audio_stream = yt.streams.filter(only_audio=True).first()\n",
    "    audio_stream.download(output_path=f\"{path}/original_files/audio\", filename=\"audio_file.mp4\")\n",
    "\n",
    "def get_title(url):\n",
    "    yt = pyt(url)\n",
    "    return yt.title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23a065e-bf3b-4265-b717-a1b8699dad31",
   "metadata": {},
   "source": [
    "### Checking Audio File Size, and Chunking it if Large\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e7f5597-aa5d-4f34-885f-106d62c13451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio(file_path, chunk_size_mb=12, output_folder=\"split_chunks\"):\n",
    "    global split_audio_return\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File {file_path} not found.\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "    print(f\"File size: {file_size_mb:.2f} MB\")\n",
    "\n",
    "    if file_size_mb <= chunk_size_mb:\n",
    "        print(\"File size is within the limit. No need to split.\")\n",
    "        return\n",
    "    else:\n",
    "        split_audio_return = True\n",
    "\n",
    "    clip = AudioFileClip(file_path)\n",
    "    total_duration = clip.duration\n",
    "    chunk_duration = (chunk_size_mb / file_size_mb) * total_duration\n",
    "\n",
    "    # Split the audio\n",
    "    start = 0\n",
    "    part = 1\n",
    "    while start < total_duration:\n",
    "        end = min(start + chunk_duration, total_duration)\n",
    "        chunk = clip.subclip(start, end)\n",
    "        chunk_filename = os.path.join(output_folder, f\"{os.path.splitext(os.path.basename(file_path))[0]}_part{part}.mp4\")\n",
    "        chunk.write_audiofile(chunk_filename, bitrate=\"64k\", codec=\"aac\")\n",
    "\n",
    "        print(f\"Created chunk: {chunk_filename}\")\n",
    "\n",
    "        start = end\n",
    "        part += 1\n",
    "\n",
    "    clip.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f3142e-6afa-4ac0-9247-f720862ab98e",
   "metadata": {},
   "source": [
    "### Transcribing with Whisper-1 & Writing to JSON File(s)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eab4f585-ac13-478d-8d80-b96458e816c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_json(split_audio_return, input_folder, output_folder=\"transcript_json\"):\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"The folder {input_folder} does not exist.\")\n",
    "        return\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    client = OpenAI()\n",
    "\n",
    "    if not split_audio_return:\n",
    "        # If audio is not split, use the path to the original file\n",
    "        original_audio_path = f\"{input_folder}/original_files/audio\"        \n",
    "        if os.path.exists(original_audio_path):\n",
    "            for filename in os.listdir(original_audio_path):\n",
    "                if filename.endswith(\".mp4\"):\n",
    "                    process_audio_file(client, original_audio_path, filename, output_folder)\n",
    "        else:\n",
    "            print(f\"The original audio folder {original_audio_path} does not exist.\")\n",
    "    else:\n",
    "        input_folder_chunks = f'{path}/split_chunks'\n",
    "        # If audio is split, iterate over the split audio files\n",
    "        for filename in os.listdir(f'{path}/split_chunks'):\n",
    "            if filename.endswith(\".mp4\"):\n",
    "                process_audio_file(client, input_folder_chunks, filename, output_folder)\n",
    "\n",
    "def process_audio_file(client, folder_path, filename, output_folder):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    with open(file_path, \"rb\") as audio_file:\n",
    "        transcript = client.audio.transcriptions.create(\n",
    "            file=audio_file,\n",
    "            model=\"whisper-1\",\n",
    "            response_format=\"verbose_json\",\n",
    "            timestamp_granularities=[\"segment\"]\n",
    "        )\n",
    "\n",
    "        json_filename = f\"{os.path.splitext(filename)[0]}_transcript.json\"\n",
    "        output_path = os.path.join(output_folder, json_filename)\n",
    "\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(transcript.segments, f, indent=4)\n",
    "\n",
    "        print(f\"Transcript for {filename} saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054935e3-818d-474f-8102-0460f81eeef5",
   "metadata": {},
   "source": [
    "### Cleaning & Concatenating the Transcription JSON files\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2152362-7151-47ee-aa19-34c004d52805",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_part_number(filename):\n",
    "    match = re.search(r'part(\\d+)', filename)\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "def process_file(filepath, max_id, last_end_time):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        time_adjustment = last_end_time - float(data[0]['start']) if last_end_time else 0\n",
    "        new_data = []\n",
    "        for entry in data:\n",
    "            new_entry = {\n",
    "                'id': max_id + 1,\n",
    "                'start': float(entry['start']) + time_adjustment,\n",
    "                'end': float(entry['end']) + time_adjustment,\n",
    "                'text': entry['text']\n",
    "            }\n",
    "            new_data.append(new_entry)\n",
    "            max_id += 1\n",
    "        return new_data, max_id, new_data[-1]['end'] if new_data else last_end_time\n",
    "\n",
    "def process_transcripts(directory_path, split_audio_return):\n",
    "    files = os.listdir(directory_path)\n",
    "    combined_data = []\n",
    "    max_id = -1\n",
    "    last_end_time = 0.0\n",
    "\n",
    "    if split_audio_return:\n",
    "        sorted_files = sorted(\n",
    "            [file for file in files if file.startswith('audio') and file.endswith('.json')],\n",
    "            key=extract_part_number\n",
    "        )\n",
    "    else:\n",
    "        sorted_files = [file for file in files if file.endswith('.json') and not 'part' in file]\n",
    "\n",
    "    for filename in sorted_files:\n",
    "        full_path = os.path.join(directory_path, filename)\n",
    "        processed_data, max_id, last_end_time = process_file(full_path, max_id, last_end_time)\n",
    "        combined_data.extend(processed_data)\n",
    "\n",
    "    # Output the combined data to a new JSON file\n",
    "    output_path = os.path.join(directory_path, 'combined_data.json')\n",
    "    with open(output_path, 'w') as outfile:\n",
    "        json.dump(combined_data, outfile, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b56ef6-96ba-4c14-acec-2b7a84c8b2b3",
   "metadata": {},
   "source": [
    "### Also Grabbing the Full Transcript\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ff9b387-63a1-4c52-b609-734b25e220be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_transcript(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    transcript = \"\"\n",
    "    for entry in data:\n",
    "        transcript += entry['text']\n",
    "    return transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123e38fa-15de-40de-938c-e41bfc264c24",
   "metadata": {},
   "source": [
    "### Function to Combine Short Chunks of Transcription\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2cfbee6b-a42d-478d-abfe-6dd7de05becd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def merge_short_documents(documents, min_length=2000):\n",
    "    i = 0\n",
    "    while i < len(documents) - 1:\n",
    "        current_doc = documents[i]\n",
    "        if len(current_doc.page_content) < min_length:\n",
    "            documents[i + 1].page_content = current_doc.page_content + documents[i + 1].page_content\n",
    "            del documents[i]\n",
    "        else:\n",
    "            i += 1\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a378a120-6934-4d21-9122-38830f00bbcc",
   "metadata": {},
   "source": [
    "### Main Markdown File Prompting with GPT-4-Turbo & LangChain Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ba169ab4-c63e-411e-8358-42dff42e6733",
   "metadata": {},
   "outputs": [],
   "source": [
    "guide_prompt_template = \"\"\"\n",
    "\n",
    "Below is a script from a video that I am making into a companion guide blog post first. \\\n",
    "You are a helpful assistant made to assist in the creation I'm doing. \\\n",
    "This is a continuation of a guide so include chapters, key summaries, and incorporate visual aids and direct links to relevant parts of the video, \\\n",
    "however do not include any conclusion or overarching title. \\\n",
    "For visual aids, specific frames from the video will be identified where images can be inserted to enhance understanding. \\\n",
    "For direct links, portions of the text should be hyperlinked to their corresponding times in the video. \\\n",
    "To indicate that a sentence should be hyperlinked, insert the raw text of the transcript next to the word with the indicator <HYPERLINK: \"corresponding transcript text\">. \\\n",
    "To indicate a picture regarding the text, insert the indicator <PICTURE: \"corresponding transcript text\">. \\\n",
    "It is crucial to use the raw text from the transcript that will be used, as the additional tools that will be inserting the hyperlinks and pictures need this to know where in the video to look.\n",
    "\n",
    "In this blog post, in addition to the paragraphs: \\\n",
    "\n",
    "Create titles or headings that encapsulate main points and ideas \\\n",
    "\n",
    "Format your response in markdown, ensuring distinction and clean styling between titles and paragraphs. \\\n",
    "Be sure to include the image placeholders, and hyperlinks with enough distinguishable text WITHOUT ANY QUOTATIONS, as the placeholders will be fed into a semantic search algorithm. \\\n",
    "This structured approach will be applied to the entire transcript. \\\n",
    "The example below only shows one style, but use multiple styles including different headings, bullet points, and other markdown elements when needed. \\\n",
    "\n",
    "Here are shortened example of the input and shortened expected output:\n",
    "\n",
    "example input:\n",
    "\n",
    "Hi everyone. So in this video I'd like us to cover the process of tokenization in large language models. Now you see here that I have a sad face and that's because tokenization is my least favorite part of working with large language models but unfortunately it is necessary to understand in some detail because it is fairly hairy, gnarly and there's a lot of hidden foot gums to be aware of and a lot of oddness with large language models typically traces back to tokenization. So what is tokenization? Now in my previous video Let's Build GPT from Scratch we actually already did tokenization but we did a very naive simple version of tokenization. So when you go to the Google Colab for that video you see here that we loaded our training set and our training set was this Shakespeare dataset. Now in the beginning the Shakespeare dataset is just a large string in Python it's just text and so the question is how do we plug text into large language models and in this case here we created a vocabulary of 65 possible characters that we saw occur in this string. These were the possible characters and we saw that there are 65 of them and then we created a lookup table for converting from every possible character a little string piece into a token an integer. So here for example we tokenized the string hi there and we received this sequence of tokens and here we took the first 1000 characters of our dataset and we encoded it into tokens and because this is character level we received 1000 tokens in a sequence so token 18, 47, etc. Now later we saw that the way we plug these tokens into the language model is by using an embedding table and so basically if we have 65 possible tokens then this embedding table is going to have 65 rows and roughly speaking we're taking the integer associated with every single token we're using that as a lookup into this table and we're plucking out the corresponding row and this row is trainable parameters that we're going to train using backpropagation and this is the vector that then feeds into the transformer and that's how the transformer sort of perceives every single token. So here we had a very naive tokenization process that was a character level tokenizer\n",
    "\n",
    "example output:\n",
    "\n",
    "Introduction to Tokenization\n",
    "----------------------------\n",
    "\n",
    "Welcome to our comprehensive guide on tokenization in large language models (LLMs). Tokenization is a critical yet complex aspect of working with LLMs, essential for understanding how these models process text data. Despite its challenges, tokenization is foundational, as it converts strings of text into sequences of tokens, small units of text that LLMs can manage more effectively.\n",
    "\n",
    "<PICTURE: Now you see here that I have a sad face and that's because tokenization is my least favorite part of working with large language models but unfortunately it is necessary to understand in some detail because it is fairly hairy, gnarly and there's a lot of hidden foot gums>\n",
    "\n",
    "Understanding the Basics of Tokenization\n",
    "----------------------------------------\n",
    "\n",
    "Tokenization involves creating a vocabulary from all unique characters or words in a dataset and converting each into a corresponding integer token. This process was briefly introduced in our \"Let's Build GPT from Scratch\" video, where we tokenized a Shakespeare dataset at a character level, creating a vocabulary of 65 possible characters.\n",
    "\n",
    "<HYPERLINK: So what is tokenization? Now in my previous video Let's Build GPT from Scratch we actually already did tokenization but we did a very naive simple version of tokenization. So when you go to the Google Colab for that video you see here that we loaded>\n",
    "\n",
    "The Role of Embedding Tables in Tokenization\n",
    "--------------------------------------------\n",
    "\n",
    "After tokenization, the next step involves using an embedding table, where each token's integer is used as a lookup to extract a row of trainable parameters. These parameters, once trained, feed into the transformer model, allowing it to perceive each token effectively.\n",
    "\n",
    "<PICTURE: using backpropagation and this is the vector that then feeds into the transformer and that's how the transformer sort of perceives every single token. So here we had a very naive tokenization process that was a character level tokenizer>\n",
    "\n",
    "end examples.\n",
    "\n",
    "Here is the transcript:\n",
    "\n",
    "{transcript}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "llm = ChatOpenAI(temperature=0.0, model=\"gpt-4-turbo-preview\")\n",
    "guide_prompt = ChatPromptTemplate.from_template(guide_prompt_template)\n",
    "\n",
    "guide_chain = (\n",
    "    {\"transcript\": RunnablePassthrough()} \n",
    "    | guide_prompt\n",
    "    | llm\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "def generate_markdown(merged_docs, path, guide_chain):\n",
    "    markdown_outputs = []\n",
    "    for doc in merged_docs:\n",
    "        output = guide_chain.invoke(doc.page_content)\n",
    "        markdown_outputs.append(output)\n",
    "    combined_output = '\\n\\n'.join(markdown_outputs)\n",
    "    with open(f'{path}/transcript_json/llm_outline.txt', 'w') as file:\n",
    "        file.write(combined_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297e6ef2-663a-46e8-9c05-759eb7a53a78",
   "metadata": {},
   "source": [
    "### Dealing with the Placeholders, Grabbing Pictures & Formatting Hyperlinks\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "59c00fc4-b700-4869-b46f-f8bdfb64a3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_frame(video, second):\n",
    "    frames_dir = 'frames'\n",
    "    if not os.path.exists(frames_dir):\n",
    "        os.makedirs(frames_dir)\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return None\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_number = round(int(second * fps))\n",
    "    total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    if frame_number >= total_frames:\n",
    "        print(f\"Error: Frame number {frame_number} exceeds total frames in video.\")\n",
    "        cap.release()\n",
    "        return None\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Error: Could not read frame.\")\n",
    "        cap.release()\n",
    "        return None\n",
    "\n",
    "    frame_path = os.path.join(frames_dir, f'frame_{second}.jpg')\n",
    "    cv2.imwrite(frame_path, frame)\n",
    "    cap.release()\n",
    "\n",
    "    return frame_path\n",
    "\n",
    "def retrieve_time(segment):\n",
    "    docs = retriever.get_relevant_documents(segment)\n",
    "    docs_dict = json.loads(docs[0].page_content)\n",
    "    start_time = docs_dict[\"start\"]\n",
    "    end_time = docs_dict[\"end\"]\n",
    "    time = (start_time + end_time) / 2\n",
    "    final_time = round(time)\n",
    "    return final_time\n",
    "\n",
    "def create_hyperlink(segment, url):\n",
    "    time = retrieve_time(segment)\n",
    "    time_link = f\"{url}&t={time}s\"\n",
    "    return time_link\n",
    "\n",
    "def format_seconds_to_hms(seconds):\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    seconds = seconds % 60\n",
    "    return f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n",
    "\n",
    "def process_placeholder(placeholder):\n",
    "    if placeholder.startswith(\"<PICTURE:\"):\n",
    "        description = placeholder[9:-1]\n",
    "        time = retrieve_time(description)\n",
    "        image_path = grab_frame(video_path, time)\n",
    "        # Embed the image using markdown with a specified width\n",
    "        return f'<img src=\"{image_path}\" alt=\"{description}\" width=\"450\"/>'\n",
    "    elif placeholder.startswith(\"<HYPERLINK:\"):\n",
    "        text = placeholder[11:-1]\n",
    "        time = retrieve_time(text)\n",
    "        formatted_time = format_seconds_to_hms(time)\n",
    "        hyperlink = create_hyperlink(text, url)\n",
    "        return f'[Jump to this part of the video: {formatted_time}]({hyperlink})'\n",
    "    else:\n",
    "        return placeholder\n",
    "\n",
    "def replace_placeholders(content):\n",
    "    placeholders = re.findall(r\"<[^>]+>\", content)\n",
    "    for placeholder in placeholders:\n",
    "        replacement = process_placeholder(placeholder)\n",
    "        content = content.replace(placeholder, replacement, 1)\n",
    "    return content\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def convert_txt(path, title, db):\n",
    "    txt_file_path = f'{path}/transcript_json/llm_outline.txt'\n",
    "    output_file_path = f'{path}/companion_guide.txt'\n",
    "    global video_path\n",
    "    video_path = f'{path}/original_files/video/video_file.mp4'\n",
    "    global retriever\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": 1})\n",
    "    \n",
    "    content = read_file(txt_file_path)\n",
    "    updated_content = replace_placeholders(content)\n",
    "    \n",
    "    with open(output_file_path, 'w') as file:\n",
    "        file.write(updated_content)\n",
    "    \n",
    "    print(f\"Updated markdown content has been written to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb42def-5b26-471c-80d9-89ba10d77928",
   "metadata": {},
   "source": [
    "### Main Script\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2081558-f942-422e-9f68-af398a6dd149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Video & Audio, Runtime: 0.0\n",
      "Video & Audio Downloaded, Runtime: 11.26\n",
      "Checking File Size & Splitting if Necessary, Runtime: 11.26\n",
      "File size: 46.61 MB\n",
      "MoviePy - Writing audio in split_chunks/audio_file_part1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Created chunk: split_chunks/audio_file_part1.mp4\n",
      "MoviePy - Writing audio in split_chunks/audio_file_part2.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Created chunk: split_chunks/audio_file_part2.mp4\n",
      "MoviePy - Writing audio in split_chunks/audio_file_part3.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Created chunk: split_chunks/audio_file_part3.mp4\n",
      "MoviePy - Writing audio in split_chunks/audio_file_part4.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Created chunk: split_chunks/audio_file_part4.mp4\n",
      "Audio Checked & Split, Runtime: 110.94\n",
      "Processing Audio File with Whisper-1, Runtime: 110.94\n",
      "Transcript for audio_file_part4.mp4 saved to transcript_json/audio_file_part4_transcript.json\n",
      "Transcript for audio_file_part3.mp4 saved to transcript_json/audio_file_part3_transcript.json\n",
      "Transcript for audio_file_part2.mp4 saved to transcript_json/audio_file_part2_transcript.json\n",
      "Transcript for audio_file_part1.mp4 saved to transcript_json/audio_file_part1_transcript.json\n",
      "Audio Processed with Whisper-1, Runtime: 474.22\n",
      "Cleaning Data, Runtime: 474.22\n",
      "Data Cleaned, Runtime: 474.24\n",
      "Pulling Full Transcript, Runtime: 474.24\n",
      "Transcript Pulled, Runtime: 474.24\n",
      "Chunking & Splitting Transcript, Runtime: 474.24\n",
      "Transcript Chunked, Runtime: 477.68\n",
      "Embedding Transcript, Runtime: 477.68\n",
      "Transcript Embedded, Runtime: 482.44\n",
      "Generating Markdown Outline with GPT-4-T, Runtime: 482.44\n",
      "Markdown File Generated, Runtime: 1278.96\n",
      "Replacing Placeholders With Pictures & Links, Runtime: 1278.96\n",
      "Updated markdown content has been written to /Users/adamlucek/Documents/Jupyter/karpathy_guide_challenge/companion_guide.txt\n",
      "Report Finished, Runtime: 1316.19\n"
     ]
    }
   ],
   "source": [
    "# main script\n",
    "start_timer()\n",
    "# URL & Path of Interest\n",
    "url = 'https://www.youtube.com/watch?v=zduSFxRajkE'\n",
    "path = '/Users/adamlucek/Documents/Jupyter/karpathy_guide_challenge'\n",
    "\n",
    "print(f\"Downloading Video & Audio, Runtime: {show_current_runtime()}\")\n",
    "# download video, audio, and details\n",
    "download_video(url, path)\n",
    "title = get_title(url)\n",
    "print(f\"Video & Audio Downloaded, Runtime: {show_current_runtime()}\")\n",
    "\n",
    "print(f\"Checking File Size & Splitting if Necessary, Runtime: {show_current_runtime()}\")\n",
    "# Check filesize, split into multiple files if needed\n",
    "split_audio_return = False\n",
    "split_audio(f\"{path}/original_files/audio/audio_file.mp4\")\n",
    "print(f\"Audio Checked & Split, Runtime: {show_current_runtime()}\")\n",
    "\n",
    "print(f\"Processing Audio File with Whisper-1, Runtime: {show_current_runtime()}\")\n",
    "# Process audio files with Whisper and create JSON files of output\n",
    "create_json(split_audio_return, path)\n",
    "print(f\"Audio Processed with Whisper-1, Runtime: {show_current_runtime()}\")\n",
    "\n",
    "print(f\"Cleaning Data, Runtime: {show_current_runtime()}\")\n",
    "# Combine if needed, clean extra data\n",
    "process_transcripts(f\"{path}/transcript_json\", split_audio_return)\n",
    "print(f\"Data Cleaned, Runtime: {show_current_runtime()}\")\n",
    "\n",
    "print(f\"Pulling Full Transcript, Runtime: {show_current_runtime()}\")\n",
    "# Pull the full transcript\n",
    "video_transcript = full_transcript(f'{path}/transcript_json/combined_data.json')\n",
    "print(f\"Transcript Pulled, Runtime: {show_current_runtime()}\")\n",
    "\n",
    "print(f\"Chunking & Splitting Transcript, Runtime: {show_current_runtime()}\")\n",
    "# Embed and chunk transcript\n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings())\n",
    "split_docs = text_splitter.create_documents([video_transcript])\n",
    "merged_docs = merge_short_documents(split_docs)\n",
    "print(f\"Transcript Chunked, Runtime: {show_current_runtime()}\")\n",
    "\n",
    "print(f\"Embedding Transcript, Runtime: {show_current_runtime()}\")\n",
    "# Embed documents\n",
    "json_loader = JSONLoader(f\"{path}/transcript_json/combined_data.json\", jq_schema=\".[]\", text_content=False)\n",
    "json_texts = json_loader.load()\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(json_texts, embeddings)\n",
    "print(f\"Transcript Embedded, Runtime: {show_current_runtime()}\")\n",
    "\n",
    "print(f\"Generating Markdown Outline with GPT-4-T, Runtime: {show_current_runtime()}\")\n",
    "# Generate markdown of file with GPT-4-T\n",
    "generate_markdown(merged_docs, path, guide_chain)\n",
    "print(f\"Markdown File Generated, Runtime: {show_current_runtime()}\")\n",
    "\n",
    "print(f\"Replacing Placeholders With Pictures & Links, Runtime: {show_current_runtime()}\")\n",
    "# Replace placeholders with hyperlinks and pictures\n",
    "convert_txt(path, title, db)\n",
    "print(f\"Report Finished, Runtime: {show_current_runtime()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b049fe4-bf58-42bd-9d31-489843148bd3",
   "metadata": {},
   "source": [
    "## **Notes**\n",
    "#### **Cost:**  ~$1.91 for this pass\n",
    "\n",
    "* \\$1.10 from GPT\\-4\\-Turbo\n",
    "  * 56,546 Context Tokens \\& 17,710 Generated Tokens\n",
    "* \\$0.80 from Whisper\\-1\n",
    "  * 8,016 seconds transcribed\n",
    "* \\$0.02 from text\\-embedding\\-002\\-v2\n",
    "  * 172,561 Context Tokens\n",
    "  \n",
    "#### **Time from URL to Full Report:** 21 minutes, 56 Seconds\n",
    "\n",
    "### **Limitations: (aka my todo list)** \n",
    "* vector search can sometimes be innacurate, point towards wrong part of the video out of order with this methods.\n",
    "* Inconsistency of number of pictures and hyperlinks across the board, as it's done automatically\n",
    "* Unnable to input entire script at once (believe me I tried), so all the downsides that come with sequential processing\n",
    "* Could be prettier \n",
    "* My own lack of programming knowledge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0b5e1b-d934-4d80-8f3d-6607f2752d46",
   "metadata": {},
   "source": [
    "# Program Output\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3d1daf-8d8a-47bb-86d6-3de13c1dae31",
   "metadata": {},
   "source": [
    "Introduction to Tokenization in Large Language Models\n",
    "------------------------------------------------------\n",
    "\n",
    "Welcome to our exploration of tokenization in large language models (LLMs). Tokenization is a crucial, albeit complex, process that plays a foundational role in how LLMs understand and process text data. Despite its challenges, mastering tokenization is essential for anyone working with LLMs.\n",
    "\n",
    "<img src=\"frames/frame_7.jpg\" alt=\" Now you see here that I have a sad face and that's because tokenization is my least favorite part\" width=\"450\"/>\n",
    "\n",
    "### What is Tokenization?\n",
    "\n",
    "Tokenization is the process of converting strings of text into sequences of tokens, which are smaller, manageable units of text for LLMs. This process involves creating a vocabulary of all unique characters or words in a dataset and assigning each a unique integer token.\n",
    "\n",
    "[Jump to this part of the video: 00:00:27](https://www.youtube.com/watch?v=zduSFxRajkE&t=27s)\n",
    "\n",
    "### The Role of Embedding Tables\n",
    "\n",
    "After tokenization, embedding tables come into play. These tables use the integer associated with each token as a lookup to extract a row of trainable parameters. These parameters, once trained, are what the transformer model uses to \"understand\" each token.\n",
    "\n",
    "<img src=\"frames/frame_139.jpg\" alt=\" So here we had a very naive tokenization process that was a character level tokenizer\" width=\"450\"/>\n",
    "\n",
    "### Beyond Character-Level Tokenization\n",
    "\n",
    "While the initial example was a simple character-level tokenizer, state-of-the-art language models use more sophisticated methods for constructing token vocabularies. One such method is byte pair encoding (BPE), which we will delve into, demonstrating its application and significance in tokenization for LLMs.\n",
    "\n",
    "### Byte Pair Encoding (BPE)\n",
    "\n",
    "BPE is a method used to construct character chunks for token vocabularies, moving beyond simple character-level tokenization. This algorithm plays a crucial role in how modern LLMs, like GPT-2, handle tokenization, allowing for a more efficient and nuanced understanding of text.\n",
    "\n",
    "[Jump to this part of the video: 00:02:55](https://www.youtube.com/watch?v=zduSFxRajkE&t=175s)\n",
    "\n",
    "### Tokenization's Impact on Language Model Performance\n",
    "\n",
    "Tokenization is not just a technical detail; it significantly impacts the performance of LLMs. Issues such as difficulty with spelling tasks, simple string processing, handling non-English languages, and even simple arithmetic can often be traced back to tokenization strategies.\n",
    "\n",
    "### Exploring Tokenization with Live Examples\n",
    "\n",
    "To better understand tokenization's effects, we explore live examples using a web app that allows for real-time tokenization in your browser. This demonstration highlights how tokenization can vary significantly based on the context, language, and even formatting of the text.\n",
    "\n",
    "<img src=\"frames/frame_365.jpg\" alt=\" So I have it loaded here and what I like about this web app is that tokenization is running a sort of live in your browser in JavaScript\" width=\"450\"/>\n",
    "\n",
    "### Tokenization and Non-English Languages\n",
    "\n",
    "The tokenization process can disproportionately affect non-English languages, often resulting in a higher number of tokens for the same content when compared to English. This discrepancy can lead to inefficiencies and limitations in how LLMs process and understand non-English text.\n",
    "\n",
    "### The Challenge of Tokenizing Code\n",
    "\n",
    "Tokenization also presents unique challenges when dealing with code, such as Python. The way spaces and indentation are handled can significantly impact the model's ability to understand and generate code, highlighting the importance of considering tokenization in the context of programming languages.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Understanding tokenization is essential for working with large language models. By exploring its complexities, impacts, and the methods used to optimize it, such as byte pair encoding, we gain a deeper insight into the inner workings of LLMs and how they process text data.\n",
    "\n",
    "Optimizing Token Space in Language Models\n",
    "-----------------------------------------\n",
    "\n",
    "In the quest for efficiency within large language models (LLMs), the management of token space emerges as a critical consideration. The transition from GPT-2 to GPT-4 tokenizers showcases a significant leap towards optimizing this space, reducing the token count for identical strings by approximately half.\n",
    "\n",
    "### The Impact of Tokenizer Choice on Token Count\n",
    "\n",
    "When comparing tokenizers, a striking difference in efficiency is observed. The GPT-2 tokenizer, for instance, generates a token count of 300 for a specific string. Switching to the GPT-4 tokenizer, labeled as CL 100k base, the token count dramatically drops to 185 for the same string. This reduction is attributed to the GPT-4 tokenizer's larger vocabulary size, approximately doubling that of its predecessor, GPT-2.\n",
    "\n",
    "[Jump to this part of the video: 01:13:18](https://www.youtube.com/watch?v=zduSFxRajkE&t=4398s)\n",
    "\n",
    "### Achieving a Denser Input for the Transformer\n",
    "\n",
    "The densification of input to the transformer is a direct consequence of this reduction in token count. With fewer tokens representing the same amount of text, the transformer can now process a broader context within the same computational constraints. This efficiency is crucial for enhancing the model's ability to predict subsequent tokens based on a more extensive textual context.\n",
    "\n",
    "### Balancing the Token Vocabulary\n",
    "\n",
    "However, the expansion of the token vocabulary is not without its challenges. As the number of tokens increases, so does the size of the embedding table and the complexity of the output prediction mechanism, particularly the softmax layer. This necessitates a careful balance, finding a \"sweet spot\" in the vocabulary size that ensures both density and efficiency in processing.\n",
    "\n",
    "### Enhanced Handling of White Space in Python Code\n",
    "\n",
    "A noteworthy improvement in the GPT-4 tokenizer is its handling of white spaces in Python code. By grouping multiple spaces into a single token, the tokenizer significantly enhances the efficiency of representing Python code. This design choice by OpenAI not only densifies the input but also improves the model's performance in code prediction tasks.\n",
    "\n",
    "<img src=\"frames/frame_849.jpg\" alt=\" You see that here these four spaces are represented as one single token for the three spaces here and then the token spaces and here seven spaces were all grouped into a single token\" width=\"450\"/>\n",
    "\n",
    "This strategic grouping of white spaces into fewer tokens exemplifies the thoughtful design considerations that contribute to the overall improvement in LLMs from GPT-2 to GPT-4, beyond mere architectural and optimization tweaks.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The evolution of tokenizers from GPT-2 to GPT-4 highlights a significant stride towards optimizing token space in LLMs. By efficiently managing token count and improving the handling of specific data types like Python code, these advancements promise a more effective and nuanced approach to text processing and prediction within LLMs.\n",
    "\n",
    "## Tokenization and Encoding in Language Models\n",
    "### Understanding the Need for Tokenization\n",
    "\n",
    "Tokenization is the process of converting strings of text into integers within a fixed vocabulary, which are then used to make lookups into a table of vectors. These vectors are fed into the transformer as input. This process becomes complex when we consider supporting multiple languages and special characters, such as emojis.\n",
    "\n",
    "<img src=\"frames/frame_901.jpg\" alt=\" So remember what we want to do. We want to take strings and feed them into language models\" width=\"450\"/>\n",
    "\n",
    "### The Challenge of Supporting Multiple Languages and Characters\n",
    "\n",
    "The complexity of tokenization arises from the need to support a wide range of characters found across different languages and on the internet. Unicode code points serve as a universal standard, defining roughly 150,000 characters across 161 scripts. However, directly using Unicode code points for tokenization is not practical due to the vast vocabulary size and the evolving nature of the Unicode standard.\n",
    "\n",
    "[Jump to this part of the video: 00:15:53](https://www.youtube.com/watch?v=zduSFxRajkE&t=953s)\n",
    "\n",
    "### Exploring Unicode and Encodings\n",
    "\n",
    "Unicode code points are a way to represent characters from various languages and scripts. Python's `ORD` function can be used to find the code point of a character. However, the direct use of Unicode code points is inefficient due to the large vocabulary size. Instead, encodings like UTF-8, UTF-16, and UTF-32 are used to translate Unicode text into binary data or byte streams, with UTF-8 being the most common due to its variable length encoding and compatibility with ASCII.\n",
    "\n",
    "<img src=\"frames/frame_961.jpg\" alt=\" So Unicode code points are defined by the Unicode consortium as part of the Unicode standard\" width=\"450\"/>\n",
    "\n",
    "### The Role of UTF-8 Encoding\n",
    "\n",
    "UTF-8 encoding translates Unicode code points into a byte stream, which can range from one to four bytes per code point. This encoding is preferred for its efficiency and compatibility with ASCII. However, using UTF-8 naively would result in a small vocabulary size and inefficiently long sequences for language models.\n",
    "\n",
    "[Jump to this part of the video: 00:18:42](https://www.youtube.com/watch?v=zduSFxRajkE&t=1122s)\n",
    "\n",
    "### Byte Pair Encoding (BPE) Algorithm\n",
    "\n",
    "To address the limitations of using raw UTF-8 byte streams, the Byte Pair Encoding (BPE) algorithm is employed. BPE compresses byte sequences by iteratively finding and replacing the most frequent pairs of bytes with a new token, thereby reducing the sequence length while expanding the vocabulary in a controlled manner. This method allows for efficient encoding of text into tokens that can be used by language models.\n",
    "\n",
    "<img src=\"frames/frame_1431.jpg\" alt=\" So as I mentioned the byte pair encoding algorithm is not all that complicated\" width=\"450\"/>\n",
    "\n",
    "### Implementing Byte Pair Encoding\n",
    "\n",
    "The implementation of BPE involves encoding text into UTF-8, converting bytes to integers, and then applying the BPE algorithm to iteratively compress the byte sequences. This process involves identifying the most common pairs of bytes, replacing them with a new token, and repeating the process to gradually reduce the sequence length and adjust the vocabulary size.\n",
    "\n",
    "[Jump to this part of the video: 00:31:07](https://www.youtube.com/watch?v=zduSFxRajkE&t=1867s)\n",
    "\n",
    "### Practical Application of BPE\n",
    "\n",
    "A practical example of applying BPE is demonstrated by encoding a paragraph into UTF-8, identifying the most common byte pairs, and iteratively merging them to compress the sequence. This process is repeated until a desired vocabulary size or sequence length is achieved, showcasing the effectiveness of BPE in preparing text for language models.\n",
    "\n",
    "<img src=\"frames/frame_1625.jpg\" alt=\" So here's what i did i went to this blog post that i enjoyed and i took the first paragraph\" width=\"450\"/>\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The process of tokenization, particularly through the use of the Byte Pair Encoding algorithm, is essential for efficiently preparing text for language models. By understanding and implementing BPE, developers can effectively manage the challenges of supporting multiple languages and characters, ensuring that language models can process text data effectively.\n",
    "\n",
    "[Jump to this part of the video: 00:34:21](https://www.youtube.com/watch?v=zduSFxRajkE&t=2061s)\n",
    "\n",
    "Optimizing Vocabulary Size for Efficiency\n",
    "-----------------------------------------\n",
    "\n",
    "Finding the ideal vocabulary size is crucial for the efficiency of tokenization in large language models. A larger vocabulary can reduce the sequence length but requires careful tuning to find the optimal balance. GPT-4, for example, utilizes a vocabulary of approximately 100,000 tokens, demonstrating the scale at which modern models operate.\n",
    "\n",
    "<img src=\"frames/frame_2073.jpg\" alt=\" The more steps we take, the larger will be our vocabulary, and the shorter will be our sequence\" width=\"450\"/>\n",
    "\n",
    "Advanced Tokenization Techniques\n",
    "--------------------------------\n",
    "\n",
    "### Iterative Byte-Pair Encoding\n",
    "\n",
    "To refine our tokenizer, we employ an iterative process known as Byte-Pair Encoding (BPE), which allows for more representative statistics and sensible results by analyzing longer texts. This method involves encoding text into bytes, then iteratively merging the most common byte pairs to reduce the overall sequence length while expanding the vocabulary.\n",
    "\n",
    "[Jump to this part of the video: 00:35:09](https://www.youtube.com/watch?v=zduSFxRajkE&t=2109s)\n",
    "\n",
    "### Setting the Final Vocabulary Size\n",
    "\n",
    "Determining the final vocabulary size is a critical step in the tokenization process, acting as a hyperparameter that influences the tokenizer's performance. For our example, we aim for a vocabulary size of 276 tokens, achieved through 20 specific merges, starting from an initial set of 256 byte tokens.\n",
    "\n",
    "<img src=\"frames/frame_2170.jpg\" alt=\" So let's say for us, we're going to use 276 because that way we're going to be doing exactly 20 merges\" width=\"450\"/>\n",
    "\n",
    "### The Process of Merging Tokens\n",
    "\n",
    "The merging process involves identifying the most common pairs of tokens and combining them into a new token, effectively reducing the total number of tokens while maintaining the ability to represent the original text. This process is repeated iteratively, with each new token becoming eligible for further merging, thereby creating a more efficient encoding scheme.\n",
    "\n",
    "[Jump to this part of the video: 00:37:46](https://www.youtube.com/watch?v=zduSFxRajkE&t=2266s)\n",
    "\n",
    "Evaluating Compression Efficiency\n",
    "---------------------------------\n",
    "\n",
    "After completing the specified number of merges, it's essential to assess the tokenizer's efficiency by examining the compression ratio achieved. This ratio indicates how effectively the tokenizer can reduce the size of the original text, with a higher ratio signifying greater efficiency. In our example, a compression ratio of approximately 1.27 was achieved after 20 merges.\n",
    "\n",
    "<img src=\"frames/frame_2324.jpg\" alt=\" One thing we can take a look at as well is we can take a look at the compression ratio that we've achieved\" width=\"450\"/>\n",
    "\n",
    "Training the Tokenizer: A Separate Preprocessing Stage\n",
    "------------------------------------------------------\n",
    "\n",
    "### The Role of the Tokenizer\n",
    "\n",
    "The tokenizer serves as a distinct preprocessing stage, separate from the training of the large language model itself. It is responsible for converting raw text into a sequence of tokens and vice versa, facilitating the model's ability to process and understand the text.\n",
    "\n",
    "[Jump to this part of the video: 00:10:06](https://www.youtube.com/watch?v=zduSFxRajkE&t=606s)\n",
    "\n",
    "### Importance of a Dedicated Training Set\n",
    "\n",
    "For optimal performance, the tokenizer may require a different training set than the large language model, encompassing a variety of languages and data types. This diversity ensures that the tokenizer can effectively handle different kinds of text, resulting in more efficient tokenization and, consequently, better model performance.\n",
    "\n",
    "<img src=\"frames/frame_2503.jpg\" alt=\" So for example, when you're training the tokenizer, as I mentioned, we don't just care about the performance of English text\" width=\"450\"/>\n",
    "\n",
    "Encoding and Decoding with the Trained Tokenizer\n",
    "-------------------------------------------------\n",
    "\n",
    "### The Process of Decoding\n",
    "\n",
    "Decoding is the process of converting a sequence of tokens back into the original text. This step is crucial for understanding the output of the language model and for further processing or analysis of the generated text.\n",
    "\n",
    "[Jump to this part of the video: 00:42:45](https://www.youtube.com/watch?v=zduSFxRajkE&t=2565s)\n",
    "\n",
    "## Crafting a Custom Vocabulary for Tokenization\n",
    "### Understanding the Vocabulary Creation Process\n",
    "\n",
    "Creating a custom vocabulary is a pivotal step in the tokenization process for large language models. This involves mapping token IDs to their corresponding byte objects, starting with the raw bytes for tokens from 0 to 255 and then incorporating all merges in a sorted manner. This method ensures a comprehensive bytes representation of tokens, crucial for effective tokenization.\n",
    "\n",
    "<img src=\"frames/frame_2932.jpg\" alt=\" So there are many different ways to do it.Here's one way\" width=\"450\"/>\n",
    "\n",
    "### The Importance of Python Version in Tokenization\n",
    "\n",
    "It's essential to note the significance of using modern Python versions, specifically Python 3.7 and above, for tokenization tasks. These versions guarantee the order of items in a dictionary, which is critical for the correct iteration and insertion of elements into the merges dictionary during the vocabulary creation process.\n",
    "\n",
    "[Jump to this part of the video: 00:44:16](https://www.youtube.com/watch?v=zduSFxRajkE&t=2656s)\n",
    "\n",
    "## Handling Token Encoding and Decoding\n",
    "### Converting Token IDs to Text\n",
    "\n",
    "The process of converting token IDs back into readable text involves looking up their byte representations in the vocabulary, concatenating these bytes, and then decoding them back into Python strings using UTF-8. This step is crucial for transforming the raw bytes into a format that can be easily understood and utilized.\n",
    "\n",
    "<img src=\"frames/frame_2686.jpg\" alt=\" And then here, this is one way in Python to concatenate all these bytes together to create our tokens\" width=\"450\"/>\n",
    "\n",
    "### Addressing UTF-8 Decoding Errors\n",
    "\n",
    "A common challenge in tokenization is handling invalid UTF-8 byte sequences, which can result in decoding errors. To overcome this, it's recommended to use the `errors=replace` option in the `bytes.decode` function, which substitutes invalid byte sequences with a special replacement character. This approach ensures that the decoding process is robust and can handle any anomalies in the token IDs.\n",
    "\n",
    "[Jump to this part of the video: 00:47:11](https://www.youtube.com/watch?v=zduSFxRajkE&t=2831s)\n",
    "\n",
    "## Implementing Token Encoding\n",
    "### Encoding Strings into Token IDs\n",
    "\n",
    "The final step in the tokenization process is encoding strings back into token IDs. This involves creating a function that takes a string as input and outputs a list of integers representing the token IDs. This function is essential for converting text data into a format that can be processed by large language models.\n",
    "\n",
    "<img src=\"frames/frame_2907.jpg\" alt=\" So we are going to implement this error right here, where we are going to be given a string, and we want to encode it into tokens\" width=\"450\"/>\n",
    "\n",
    "Advanced Tokenization Techniques\n",
    "--------------------------------\n",
    "\n",
    "In this section, we delve into a more sophisticated approach to tokenization, moving beyond the basics to explore how encoding and merging strategies can enhance the process for large language models.\n",
    "\n",
    "### Encoding Text into UTF-8 Bytes\n",
    "\n",
    "The initial step in this advanced tokenization process involves encoding the text into UTF-8, converting it into a series of raw bytes. This method provides a foundation by transforming the text into a more manageable form for further processing.\n",
    "\n",
    "<img src=\"frames/frame_2940.jpg\" alt=\" So this is one of the ways that I came up with. So the first thing we're going to do is we are going to take our text, encode it into UTF-8 to get the raw bytes.\" width=\"450\"/>\n",
    "\n",
    "### Utilizing a Merges Dictionary\n",
    "\n",
    "Following the encoding, the process utilizes a merges dictionary to identify and combine specific byte pairs. This dictionary is crucial for determining which bytes can be merged based on their occurrence and relevance within the text.\n",
    "\n",
    "[Jump to this part of the video: 00:49:23](https://www.youtube.com/watch?v=zduSFxRajkE&t=2963s)\n",
    "\n",
    "### Sequential Merging Based on Precedence\n",
    "\n",
    "An essential aspect of this tokenization method is the sequential merging of bytes, adhering to a predefined order. This ensures that merges are performed logically, respecting the hierarchical structure of the merges dictionary.\n",
    "\n",
    "[Jump to this part of the video: 00:49:33](https://www.youtube.com/watch?v=zduSFxRajkE&t=2973s)\n",
    "\n",
    "### Identifying Merge Candidates\n",
    "\n",
    "To efficiently merge byte pairs, the process involves identifying potential candidates for merging. This is achieved by analyzing the sequence of tokens and utilizing statistical methods to pinpoint pairs that are eligible for merging.\n",
    "\n",
    "[Jump to this part of the video: 00:50:44](https://www.youtube.com/watch?v=zduSFxRajkE&t=3044s)\n",
    "\n",
    "### Implementing a Min Function for Optimal Merging\n",
    "\n",
    "A sophisticated technique employed in this process is the use of a `min` function to determine the most suitable pair for merging. This approach ensures that the merging sequence is optimized, focusing on pairs that align with the merges dictionary's order.\n",
    "\n",
    "[Jump to this part of the video: 00:51:16](https://www.youtube.com/watch?v=zduSFxRajkE&t=3076s)\n",
    "\n",
    "### Handling Non-Mergable Pairs\n",
    "\n",
    "An important consideration in this tokenization method is the handling of byte pairs that cannot be merged. By assigning an infinite value to non-mergable pairs, the process effectively excludes them from the merging candidates, ensuring a smooth and logical merging sequence.\n",
    "\n",
    "[Jump to this part of the video: 00:52:46](https://www.youtube.com/watch?v=zduSFxRajkE&t=3166s)\n",
    "\n",
    "### Potential Challenges and Solutions\n",
    "\n",
    "While this advanced tokenization technique offers significant benefits, it's crucial to be aware of potential challenges. One such challenge is the possibility of the merging function failing under certain conditions. Understanding and addressing these challenges is key to implementing a robust tokenization process.\n",
    "\n",
    "[Jump to this part of the video: 00:53:05](https://www.youtube.com/watch?v=zduSFxRajkE&t=3185s)\n",
    "\n",
    "Understanding the Merging Process in Tokenization\n",
    "-------------------------------------------------\n",
    "\n",
    "In the journey of tokenization, a critical step involves identifying and merging pairs of tokens based on specific criteria. This process is essential for reducing the complexity of the data and making it more manageable for language models.\n",
    "\n",
    "### The Challenge of Finding Mergeable Pairs\n",
    "\n",
    "The algorithm seeks pairs that can be merged, but it encounters a significant hurdle when no pairs meet the merging criteria. This situation is indicated by all pairs evaluating to `float infs` for the merging criterion, leading to a scenario where the algorithm cannot proceed with any merges.\n",
    "\n",
    "<img src=\"frames/frame_3193.jpg\" alt=\" If there's nothing to merge, then there's nothing in merges that is satisfied anymore\" width=\"450\"/>\n",
    "\n",
    "### Breaking Out When No Merges Are Possible\n",
    "\n",
    "A crucial part of the process is identifying when no further merges can be made. This is detected when a pair, not in the `merges` dictionary, becomes the first element in `stats` by default, signaling that merging cannot continue. At this point, the algorithm will break out of the loop, concluding that no additional pairs can be merged.\n",
    "\n",
    "[Jump to this part of the video: 00:53:49](https://www.youtube.com/watch?v=zduSFxRajkE&t=3229s)\n",
    "\n",
    "### Implementing the Merge\n",
    "\n",
    "When a mergeable pair is found, the algorithm proceeds to merge it. This involves looking up the pair in the `mergers` dictionary to find its index and then replacing every occurrence of the pair in the tokens list with this index. This step is repeated until no more pairs can be merged, effectively simplifying the token list.\n",
    "\n",
    "<img src=\"frames/frame_3261.jpg\" alt=\" So we're going to look into the mergers dictionary for that pair to look up the index\" width=\"450\"/>\n",
    "\n",
    "### Handling Special Cases\n",
    "\n",
    "The implementation also needs to account for special cases, such as when the token list contains a single character or is empty. In these scenarios, the `stats` list would be empty, causing issues with the merging process. To address this, the algorithm includes a condition to return early if the length of the tokens is less than two, thereby avoiding errors related to empty or singular token lists.\n",
    "\n",
    "[Jump to this part of the video: 00:55:23](https://www.youtube.com/watch?v=zduSFxRajkE&t=3323s)\n",
    "\n",
    "By understanding these steps and challenges, we gain insight into the complexities of the tokenization process and the intricate details involved in preparing data for language models.\n",
    "\n",
    "Exploring Tokenization and Its Complexities\n",
    "-------------------------------------------\n",
    "\n",
    "In this section, we delve into the intricacies of tokenization, a fundamental process in the operation of large language models (LLMs). Tokenization is not just about converting text into tokens; it's about understanding the nuances that come with encoding and decoding text, ensuring the integrity of the data throughout the process.\n",
    "\n",
    "### The Basics of Encoding and Decoding\n",
    "\n",
    "Tokenization involves two primary processes: encoding and decoding. Encoding transforms a string of text into a sequence of tokens, while decoding attempts to revert tokens back to the original text. However, it's crucial to note that decoding doesn't always guarantee a perfect return to the original string due to the limitations in token sequences being valid UTF-8 byte streams.\n",
    "\n",
    "<img src=\"frames/frame_3346.jpg\" alt=\" Okay, and then second, I have a few test cases here for us as well\" width=\"450\"/>\n",
    "\n",
    "### Byte Pair Encoding: A Closer Look\n",
    "\n",
    "Byte Pair Encoding (BPE) is a tokenization strategy that builds a dictionary of merges from a training set, creating a binary forest atop raw bytes. This method allows for efficient encoding and decoding between raw text and token sequences, laying the groundwork for more advanced tokenization techniques used in state-of-the-art LLMs.\n",
    "\n",
    "[Jump to this part of the video: 00:56:57](https://www.youtube.com/watch?v=zduSFxRajkE&t=3417s)\n",
    "\n",
    "### Advancing to State-of-the-Art Tokenizers\n",
    "\n",
    "As we transition from basic tokenization methods to those employed by leading-edge LLMs, the complexity of the process increases significantly. We begin this exploration by examining the GPT series, specifically focusing on the tokenizer used in GPT-2.\n",
    "\n",
    "#### GPT-2 and Byte-Pair Encoding\n",
    "\n",
    "GPT-2 utilizes the byte-pair encoding algorithm, but with a twist. It applies a more sophisticated approach to avoid unnecessary merges, such as combining frequently occurring words with punctuation. This method aims to separate semantics from punctuation, enhancing the model's understanding of text.\n",
    "\n",
    "<img src=\"frames/frame_3459.jpg\" alt=\" So let's kick things off by looking at the GPT series\" width=\"450\"/>\n",
    "\n",
    "#### Enforcing Merging Rules\n",
    "\n",
    "To refine the tokenization process, GPT-2 introduces manual rules to prevent certain types of characters from merging. This top-down approach ensures that the tokenizer maintains a clear distinction between different elements of the text, optimizing the model's performance.\n",
    "\n",
    "[Jump to this part of the video: 00:59:11](https://www.youtube.com/watch?v=zduSFxRajkE&t=3551s)\n",
    "\n",
    "### GPT-2 Tokenizer Implementation\n",
    "\n",
    "A deep dive into GPT-2's tokenizer reveals the intricacies of its implementation. Despite being named `encoder.py`, this script encompasses both encoding and decoding functionalities, highlighting the tokenizer's critical role in the model's architecture.\n",
    "\n",
    "[Jump to this part of the video: 00:59:29](https://www.youtube.com/watch?v=zduSFxRajkE&t=3569s)\n",
    "\n",
    "This exploration of tokenization, from its basic principles to its application in cutting-edge LLMs like GPT-2, underscores the importance of this process in the development and operation of language models. By understanding and refining tokenization, researchers and developers can enhance the efficiency and effectiveness of LLMs.\n",
    "\n",
    "Exploring Advanced Tokenization Techniques\n",
    "------------------------------------------\n",
    "\n",
    "Tokenization is a fundamental step in preparing text for processing by large language models (LLMs). It involves breaking down text into manageable pieces, known as tokens, which the model can interpret. This section delves into the complexities of tokenization, highlighting the use of regular expressions (regex) to enforce specific tokenization rules.\n",
    "\n",
    "### Understanding Regex in Tokenization\n",
    "\n",
    "Regex patterns play a crucial role in defining how text should be tokenized. They allow for the specification of rules that determine which parts of the text should not be merged, ensuring more precise tokenization.\n",
    "\n",
    "<img src=\"frames/frame_3595.jpg\" alt=\" \"They create a regex pattern here that looks very complicated, and we're going to go through it in a bit.\"\" width=\"450\"/>\n",
    "\n",
    "#### The Power of the `regex` Package\n",
    "\n",
    "It's important to distinguish between the standard Python `re` module and the `regex` package. The latter is an extension that offers more capabilities and is essential for complex tokenization tasks.\n",
    "\n",
    "[Jump to this part of the video: 01:00:18](https://www.youtube.com/watch?v=zduSFxRajkE&t=3618s)\n",
    "\n",
    "### Breaking Down the Regex Pattern\n",
    "\n",
    "The regex pattern used in advanced tokenization can seem daunting at first glance. However, understanding its components reveals how it effectively separates text into tokens.\n",
    "\n",
    "<img src=\"frames/frame_3631.jpg\" alt=\" \"So let's take a look at this pattern and what it's doing and why this is actually doing the separation that they are looking for.\"\" width=\"450\"/>\n",
    "\n",
    "#### How the Pattern Works\n",
    "\n",
    "The pattern is designed to match specific sequences in the text, using a combination of raw strings and logical ORs (`|`). This approach allows it to identify and separate different elements within the text, such as words from spaces or punctuation.\n",
    "\n",
    "[Jump to this part of the video: 01:00:46](https://www.youtube.com/watch?v=zduSFxRajkE&t=3646s)\n",
    "\n",
    "### Practical Application: Tokenizing a String\n",
    "\n",
    "To see the regex pattern in action, we apply it to an example string. This demonstrates how the pattern matches different parts of the text, effectively breaking it down into tokens.\n",
    "\n",
    "<img src=\"frames/frame_3646.jpg\" alt=\" \"So what exactly is this doing? Well, re.findall will take this pattern and try to match it against this string.\"\" width=\"450\"/>\n",
    "\n",
    "#### The Outcome of Tokenization\n",
    "\n",
    "By applying the regex pattern to a sample string, we can observe how it successfully identifies and separates words. This process is crucial for feeding accurately tokenized text into an LLM like GPT-2.\n",
    "\n",
    "[Jump to this part of the video: 01:02:54](https://www.youtube.com/watch?v=zduSFxRajkE&t=3774s)\n",
    "\n",
    "Advanced Tokenization Techniques\n",
    "--------------------------------\n",
    "\n",
    "Tokenization is not just about converting text into tokens; it's about intelligently splitting text to ensure that the model processes it in the most efficient and accurate way possible. This section delves into the sophisticated methods used to split text before it undergoes tokenization, highlighting the importance of this preliminary step.\n",
    "\n",
    "<img src=\"frames/frame_3781.jpg\" alt=\" Now, what is this doing and why is this important?\" width=\"450\"/>\n",
    "\n",
    "### Splitting Text for Better Tokenization\n",
    "\n",
    "The process begins by dividing the text into a list of smaller text elements. Each element is then tokenized independently, and the resulting sequences of tokens are concatenated. This method ensures that certain combinations of characters, such as a letter followed by a space, are never merged, maintaining the integrity of the tokenization process.\n",
    "\n",
    "[Jump to this part of the video: 00:59:49](https://www.youtube.com/watch?v=zduSFxRajkE&t=3589s)\n",
    "\n",
    "### Utilizing Regex for Text Segmentation\n",
    "\n",
    "Regular expressions (regex) play a crucial role in segmenting text into manageable pieces. By applying specific regex patterns, the tokenizer can separate letters, numbers, and punctuation, preventing unwanted merges during the tokenization process. This technique is essential for maintaining the granularity of the tokenized text.\n",
    "\n",
    "<img src=\"frames/frame_3874.jpg\" alt=\" So basically using this regex pattern to chunk up the text is just one way of enforcing that some merges are not to happen\" width=\"450\"/>\n",
    "\n",
    "#### Handling Numeric Characters\n",
    "\n",
    "The tokenizer uses the `\\p{n}` pattern to identify numeric characters across various scripts, ensuring that numbers are treated as distinct entities. This separation is vital for correctly processing text that includes both letters and numbers.\n",
    "\n",
    "[Jump to this part of the video: 01:04:57](https://www.youtube.com/watch?v=zduSFxRajkE&t=3897s)\n",
    "\n",
    "#### Dealing with Apostrophes\n",
    "\n",
    "Apostrophes present a unique challenge in tokenization. The tokenizer includes specific patterns to identify common apostrophes, separating them from adjacent letters. However, issues arise with Unicode apostrophes, which are not always correctly segmented, demonstrating the complexities of tokenization.\n",
    "\n",
    "<img src=\"frames/frame_3926.jpg\" alt=\" Let's see how these apostrophes work\" width=\"450\"/>\n",
    "\n",
    "This advanced approach to tokenization, involving text splitting and the use of regex, underscores the intricate processes required to prepare text for large language models. By understanding these techniques, developers can better manage the challenges of tokenization, ensuring that their models process text as accurately as possible.\n",
    "\n",
    "Exploring the Intricacies of Tokenization in GPT Models\n",
    "=======================================================\n",
    "\n",
    "Tokenization Challenges and Solutions\n",
    "-------------------------------------\n",
    "\n",
    "Tokenization in GPT models involves complex patterns and rules to effectively separate text into tokens. This process is crucial for the model's understanding and processing of text data. One notable challenge is handling apostrophes in different cases, which can lead to inconsistent tokenization.\n",
    "\n",
    "<img src=\"frames/frame_3989.jpg\" alt=\" And so it's basically hard-coded for this specific kind of apostrophe, and otherwise they become completely separate tokens.\" width=\"450\"/>\n",
    "\n",
    "GPT-2 Documentation Insights\n",
    "----------------------------\n",
    "\n",
    "The GPT-2 documentation highlights the importance of considering case sensitivity in tokenization patterns. The absence of `re.ignorecase` results in different tokenization outcomes for uppercase and lowercase versions of words with apostrophes, demonstrating the nuanced nature of tokenization rules.\n",
    "\n",
    "[Jump to this part of the video: 01:06:42](https://www.youtube.com/watch?v=zduSFxRajkE&t=4002s)\n",
    "\n",
    "Language-Specific Tokenization Issues\n",
    "-------------------------------------\n",
    "\n",
    "Tokenization rules can also be language-specific, leading to inconsistencies when dealing with languages that use apostrophes differently. This highlights the challenge of creating a universally effective tokenizer across different languages.\n",
    "\n",
    "<img src=\"frames/frame_4002.jpg\" alt=\" In addition to this, you can go to the GPT-2 docs, and here when they define the pattern, they say, should have added re.ignore case, so BP mergers can happen for capitalized versions of contractions.\" width=\"450\"/>\n",
    "\n",
    "Regex Patterns in Tokenization\n",
    "------------------------------\n",
    "\n",
    "The tokenizer uses regex patterns to match and separate punctuation, numbers, and letters. This method ensures that punctuation is correctly identified and separated, showcasing the tokenizer's ability to handle various text elements.\n",
    "\n",
    "<img src=\"frames/frame_4069.jpg\" alt=\" And what this is saying is, again, optional space followed by something that is not a letter, number, or a space, and one or more of that.\" width=\"450\"/>\n",
    "\n",
    "Handling Whitespace in Tokenization\n",
    "-----------------------------------\n",
    "\n",
    "GPT-2's tokenizer employs a unique approach to whitespace, using negative lookahead assertions in regex to manage spaces efficiently. This technique ensures that common tokens with spaces are consistently tokenized, maintaining the model's performance.\n",
    "\n",
    "<img src=\"frames/frame_4107.jpg\" alt=\" And finally, this is also a little bit confusing. So this is matching whitespace, but this is using a negative lookahead assertion in regex.\" width=\"450\"/>\n",
    "\n",
    "Real-World Tokenization Example\n",
    "-------------------------------\n",
    "\n",
    "A practical example of tokenization in action is demonstrated with a piece of Python code. The tokenizer's ability to split text into distinct elements, without merging spaces, showcases its effectiveness in handling complex text structures.\n",
    "\n",
    "<img src=\"frames/frame_4194.jpg\" alt=\" I wanted to show one more real world example here. So if we have this string, which is a piece of Python code, and then we try to split it up, then this is the kind of output we get.\" width=\"450\"/>\n",
    "\n",
    "Understanding OpenAI's Tokenization Approach\n",
    "--------------------------------------------\n",
    "\n",
    "OpenAI's methodology for training the GPT-2 tokenizer remains partially undisclosed, with only the inference code released. This leaves some aspects of the tokenizer's training process a mystery, highlighting the complexity of developing effective tokenization strategies.\n",
    "\n",
    "<img src=\"frames/frame_4267.jpg\" alt=\" Now, the training code for the GPT-2 tokenizer was never released, so all we have is the code that I've already shown you, but this code here that they've released is only the inference code for the tokens.\" width=\"450\"/>\n",
    "\n",
    "Introducing the TickToken Library\n",
    "---------------------------------\n",
    "\n",
    "The TickToken library is OpenAI's official tool for tokenization inference. It simplifies the process of tokenizing text for GPT models, including the latest GPT-4, which introduces changes in whitespace handling and regex patterns for improved tokenization.\n",
    "\n",
    "<img src=\"frames/frame_4301.jpg\" alt=\" Next, I wanted to introduce you to the TickToken library from OpenAI, which is the official library for tokenization from OpenAI.\" width=\"450\"/>\n",
    "\n",
    "GPT-4 Tokenizer Enhancements\n",
    "----------------------------\n",
    "\n",
    "GPT-4 introduces significant changes in its tokenizer, including a revised regex pattern and the handling of special tokens. These modifications aim to enhance the model's ability to process and understand text more effectively.\n",
    "\n",
    "<img src=\"frames/frame_4398.jpg\" alt=\" And then if you scroll down to CL100K, this is the GPT-4 tokenizer, you see that the pattern has changed.\" width=\"450\"/>\n",
    "\n",
    "## Enhancements in GPT-4 Tokenization\n",
    "### Case Sensitivity and Apostrophe Handling\n",
    "GPT-4 introduces several changes to improve tokenization, including case-insensitive matching for contractions such as 's, 'd, 'm, etc., ensuring both lowercase and uppercase versions are recognized equally.\n",
    "\n",
    "<img src=\"frames/frame_4436.jpg\" alt=\" And so the comment that we saw earlier on, oh, we should have used re.uppercase, basically we're now going to be matching these, apostrophe s, apostrophe d, apostrophe m, etc.\" width=\"450\"/>\n",
    "\n",
    "### Number Tokenization Limitations\n",
    "Another notable adjustment is the limitation on number tokenization, where sequences of more than three digits are not merged, aiming to prevent excessively long number sequences in tokens.\n",
    "\n",
    "<img src=\"frames/frame_4458.jpg\" alt=\" And then one more thing here is you will notice that when they match the numbers, they only match one to three numbers.\" width=\"450\"/>\n",
    "\n",
    "### Vocabulary Size Expansion\n",
    "The vocabulary size has significantly increased from approximately 50k to around 100k, enhancing the model's ability to understand and generate a wider array of text.\n",
    "\n",
    "## Deep Dive into GPT-2's Encoder.py\n",
    "### Understanding OpenAI's Tokenizer Files\n",
    "The `encoder.json` and `vocab.bpe` files are crucial for the tokenizer's functionality, with the former acting as the vocabulary and the latter detailing the merges.\n",
    "\n",
    "[Jump to this part of the video: 01:15:13](https://www.youtube.com/watch?v=zduSFxRajkE&t=4513s)\n",
    "\n",
    "### The Byte Encoder and Decoder\n",
    "OpenAI's implementation includes a byte encoder and decoder, which, despite being an additional layer to the tokenizer, are not extensively covered due to their straightforward nature.\n",
    "\n",
    "<img src=\"frames/frame_4613.jpg\" alt=\" And this is actually unfortunately just kind of a spurious implementation detail.\" width=\"450\"/>\n",
    "\n",
    "### The Core of BPE Tokenization\n",
    "The BPE (Byte Pair Encoding) function is the heart of the tokenizer, identifying and merging bigrams in a loop until no further merges are possible, mirroring the process we've previously discussed.\n",
    "\n",
    "<img src=\"frames/frame_4659.jpg\" alt=\" And you should recognize this loop here, which is very similar to our own while loop,\" width=\"450\"/>\n",
    "\n",
    "## Special Tokens in Tokenization\n",
    "### The Role of Special Tokens\n",
    "Special tokens, such as the end-of-text token, are used to delimit documents or sections within the data, aiding the model in understanding when one segment ends and another begins.\n",
    "\n",
    "<img src=\"frames/frame_4797.jpg\" alt=\" So when we're creating the training data, we have all these documents, and we tokenize them and get a stream of tokens.\" width=\"450\"/>\n",
    "\n",
    "### Implementing Special Tokens\n",
    "The implementation of special tokens varies, with some being handled outside the typical BPE algorithm, allowing for the addition of arbitrary tokens to the tokenizer's vocabulary.\n",
    "\n",
    "[Jump to this part of the video: 01:21:39](https://www.youtube.com/watch?v=zduSFxRajkE&t=4899s)\n",
    "\n",
    "### Extending Tokenizers with Special Tokens\n",
    "The ability to extend tokenizers with new special tokens is highlighted, showcasing the flexibility in customizing tokenization for specific needs or applications.\n",
    "\n",
    "<img src=\"frames/frame_5015.jpg\" alt=\" Now we can also go back to this file, which we looked at previously.\" width=\"450\"/>\n",
    "\n",
    "### GPT-4's Additional Special Tokens\n",
    "GPT-4 introduces new special tokens, including FIM (fill in the middle) tokens, to facilitate more complex tokenization strategies, reflecting advancements in tokenizer design.\n",
    "\n",
    "[Jump to this part of the video: 01:24:04](https://www.youtube.com/watch?v=zduSFxRajkE&t=5044s)\n",
    "\n",
    "Adding Special Tokens to Your Model\n",
    "-----------------------------------\n",
    "\n",
    "Incorporating special tokens into a language model requires careful adjustments, often referred to as \"model surgery.\" This process is essential when fine-tuning a model for specific tasks, such as adapting a base model into a chat model like ChatGPT. Adding a special token involves extending the embedding matrix and the final classifier layer to accommodate the new token.\n",
    "\n",
    "- **Embedding Matrix Extension**: A new row is added to the embedding matrix for each special token, typically initialized with small random numbers.\n",
    "- **Classifier Layer Adjustment**: The projection into the classifier at the end of the transformer model must also be extended to include the new token.\n",
    "\n",
    "This operation is common in model fine-tuning and is crucial for those looking to customize their models for specific applications.\n",
    "\n",
    "<img src=\"frames/frame_5065.jpg\" alt=\" And then there's one additional SERP token here. So that's that encoding as well.\" width=\"450\"/>\n",
    "\n",
    "Building Your Own GPT-4 Tokenizer\n",
    "---------------------------------\n",
    "\n",
    "To build a GPT-4 tokenizer, you can follow a structured approach, breaking down the task into manageable steps. The MinBPE repository and accompanying exercise progression provide a roadmap for developing a tokenizer capable of encoding and decoding strings to and from tokens. This process is vital for anyone looking to understand or implement their own version of a tokenizer for large language models.\n",
    "\n",
    "- **MinBPE Repository**: Offers code and tests that can be referenced when building your tokenizer.\n",
    "- **Exercise Progression**: A guide divided into four steps, designed to help you build up to a fully functional GPT-4 tokenizer.\n",
    "\n",
    "Following these resources closely and experimenting with the provided code can lead to a deeper understanding of tokenization processes and their implementation.\n",
    "\n",
    "[Jump to this part of the video: 01:25:43](https://www.youtube.com/watch?v=zduSFxRajkE&t=5143s)\n",
    "\n",
    "Visualizing Token Merges and Vocabulary Training\n",
    "------------------------------------------------\n",
    "\n",
    "Training your tokenizer involves understanding how tokens are merged and visualizing the vocabulary that results from this process. The MinBPE repository showcases the initial merges and the order in which they occur, providing insights into the tokenizer's training process. For example, the first merge performed by GPT-4 was combining two spaces into a single token. This visualization helps in comprehending how token vocabularies evolve during training.\n",
    "\n",
    "- **GPT-4 Merges Visualization**: Demonstrates the sequence of token merges during the training of GPT-4, offering a glimpse into the model's learning process.\n",
    "- **Token Vocabulary Training**: By training your tokenizer, you can develop a custom token vocabulary, as shown in the MinBPE code examples.\n",
    "\n",
    "This aspect of tokenizer development is crucial for those aiming to tailor their models to specific datasets or applications.\n",
    "\n",
    "<img src=\"frames/frame_5225.jpg\" alt=\" So here's some of the code inside MinBPE, shows the token vocabularies that you might obtain.\" width=\"450\"/>\n",
    "\n",
    "Exploring Advanced Tokenization Techniques\n",
    "------------------------------------------\n",
    "\n",
    "In the realm of large language models (LLMs), understanding the nuances of tokenization techniques is crucial for optimizing model performance. This section delves into the comparison between different tokenization strategies employed by state-of-the-art models like GPT-4 and the tools used for tokenization, such as TICToken and SentencePiece.\n",
    "\n",
    "### Comparing Tokenization in GPT-4 and Custom Models\n",
    "\n",
    "Tokenization plays a pivotal role in how LLMs interpret and process text. A fascinating observation is the way GPT-4 and custom models handle tokenization differently, primarily due to the variance in their training datasets.\n",
    "\n",
    "<img src=\"frames/frame_5282.jpg\" alt=\" And so as an example, here GPT-4 merged IN to become IN\" width=\"450\"/>\n",
    "\n",
    "For instance, GPT-4's tokenizer might merge certain tokens differently compared to a tokenizer trained on a distinct dataset, like Wikipedia pages. This difference underscores the impact of the training set's nature on the tokenizer's behavior.\n",
    "\n",
    "[Jump to this part of the video: 01:28:02](https://www.youtube.com/watch?v=zduSFxRajkE&t=5282s)\n",
    "\n",
    "### Introduction to SentencePiece\n",
    "\n",
    "Moving beyond the traditional tokenization approach, we encounter SentencePiece, a versatile library favored for its efficiency in both training and inference phases of LLMs.\n",
    "\n",
    "- **Efficiency Across Phases**: SentencePiece stands out by supporting a wide array of algorithms, including the byte-pair encoding (BPE) algorithm, making it a go-to choice for many language models.\n",
    "\n",
    "[Jump to this part of the video: 01:28:57](https://www.youtube.com/watch?v=zduSFxRajkE&t=5337s)\n",
    "\n",
    "- **Unique Approach to Tokenization**: Unlike TICToken, which operates on bytes post UTF-8 encoding, SentencePiece directly manipulates the code points in the string. This fundamental difference in approach allows SentencePiece to offer a unique perspective on tokenization.\n",
    "\n",
    "<img src=\"frames/frame_5378.jpg\" alt=\" So in the case of TICToken, we first take our code points in the string, we encode them using UTF-8 to bytes, and then we're merging bytes\" width=\"450\"/>\n",
    "\n",
    "### The Subtleties of SentencePiece's Methodology\n",
    "\n",
    "SentencePiece's methodology is nuanced, focusing on the code points from the training set and merging them. This process is distinct from the byte-level merging seen in other tokenization methods.\n",
    "\n",
    "- **Handling of Rare Code Points**: A notable feature of SentencePiece is its handling of rare code points. Depending on the character coverage hyperparameter, rare code points might be mapped to a special unknown token or, with the byte fallback option, encoded into UTF-8 bytes and then tokenized.\n",
    "\n",
    "[Jump to this part of the video: 01:30:05](https://www.youtube.com/watch?v=zduSFxRajkE&t=5405s)\n",
    "\n",
    "This nuanced approach to tokenization, focusing on code points and their rarity, highlights the adaptability and depth of SentencePiece in managing the complexities of language model training.\n",
    "\n",
    "### Personal Insights on Tokenization Methods\n",
    "\n",
    "The comparison between TICToken and SentencePiece reveals a significant, albeit subtle, difference in their tokenization philosophies. While TICToken's method is perceived as cleaner due to its straightforward byte merging, SentencePiece offers a more intricate strategy by directly dealing with code points and providing fallback mechanisms for rare instances.\n",
    "\n",
    "<img src=\"frames/frame_5446.jpg\" alt=\" Personally, I find the TICToken way significantly cleaner, but it's kind of like a subtle but pretty major difference between the way they approach tokenization\" width=\"450\"/>\n",
    "\n",
    "Understanding these advanced tokenization techniques is essential for anyone looking to delve deeper into the workings of LLMs and optimize their models for better performance and efficiency.\n",
    "\n",
    "Exploring SentencePiece for Tokenization\n",
    "-----------------------------------------\n",
    "\n",
    "In our journey to understand tokenization in large language models, we delve into the use of SentencePiece, a robust tool designed to handle the complexities of tokenization with a wide array of configurations. SentencePiece stands out due to its flexibility and ability to manage diverse linguistic data, making it a valuable asset in the tokenization process.\n",
    "\n",
    "<img src=\"frames/frame_5461.jpg\" alt=\" This is how we can import SentencePiece\" width=\"450\"/>\n",
    "\n",
    "Creating a Toy Dataset for SentencePiece\n",
    "----------------------------------------\n",
    "\n",
    "To demonstrate SentencePiece's capabilities, we create a simple toy dataset contained within a \"toy.txt\" file. This practical example helps illustrate how SentencePiece operates, emphasizing its preference for file-based input, which is crucial for its functionality.\n",
    "\n",
    "[Jump to this part of the video: 01:31:04](https://www.youtube.com/watch?v=zduSFxRajkE&t=5464s)\n",
    "\n",
    "Understanding SentencePiece's Configuration Complexity\n",
    "-------------------------------------------------------\n",
    "\n",
    "SentencePiece's vast range of options and configurations can be overwhelming, attributed to its long history and aim to accommodate a broad spectrum of use cases. While many options may seem irrelevant for specific tasks, understanding these configurations is key to leveraging SentencePiece effectively.\n",
    "\n",
    "<img src=\"frames/frame_5482.jpg\" alt=\" And the reason this is so is because SentencePiece has been around, I think, for a while\" width=\"450\"/>\n",
    "\n",
    "Configuring SentencePiece for LLM Tokenization\n",
    "-----------------------------------------------\n",
    "\n",
    "Our goal is to configure SentencePiece in a manner that aligns with the tokenization approach used by Llama2, as trained by Meta. By examining and replicating the relevant options from the tokenizer.model file released by Meta, we aim to achieve a setup that mirrors their methodology, focusing on the BP (byte-pair) encoding algorithm and a vocabulary size of 400.\n",
    "\n",
    "[Jump to this part of the video: 01:32:44](https://www.youtube.com/watch?v=zduSFxRajkE&t=5564s)\n",
    "\n",
    "The Role of Normalization in Tokenization\n",
    "-----------------------------------------\n",
    "\n",
    "Normalization plays a significant role in traditional natural language processing tasks, such as machine translation and text classification. However, in the context of language models, the preference often leans towards minimal interference with the raw data, aiming to preserve its original form as much as possible.\n",
    "\n",
    "<img src=\"frames/frame_5589.jpg\" alt=\" Normalization used to be very prevalent, I would say, before LLMs in natural language processing\" width=\"450\"/>\n",
    "\n",
    "SentencePiece and the Concept of Sentences\n",
    "------------------------------------------\n",
    "\n",
    "SentencePiece introduces the concept of sentences as individual training examples, a notion that may not align perfectly with the requirements of large language models. This distinction highlights the challenges in defining what constitutes a sentence, especially when considering the nuances present in different languages and datasets.\n",
    "\n",
    "[Jump to this part of the video: 01:33:45](https://www.youtube.com/watch?v=zduSFxRajkE&t=5625s)\n",
    "\n",
    "Exploring Advanced Tokenization Techniques\n",
    "------------------------------------------\n",
    "\n",
    "Tokenization is not just about splitting text into manageable pieces; it involves intricate rules and considerations, especially when dealing with large language models. This section delves into the complexities of tokenization, highlighting the challenges of handling rare word characters, digits, whitespace, and the implementation of merge rules.\n",
    "\n",
    "<img src=\"frames/frame_5683.jpg\" alt=\" It has a lot of treatment around rare word characters, and when I say word, I mean code points\" width=\"450\"/>\n",
    "\n",
    "Understanding Special Tokens and Vocabulary Creation\n",
    "----------------------------------------------------\n",
    "\n",
    "Special tokens play a crucial role in tokenization, serving as markers for the beginning and end of sentences, padding, and undefined tokens (UNCTOKEN). These tokens are essential for training language models effectively. The process of creating a vocabulary file and a model file during training is demonstrated, showcasing how SentenceBees handles vocabulary with a focus on special tokens, byte tokens, merge tokens, and individual code point tokens.\n",
    "\n",
    "[Jump to this part of the video: 01:35:25](https://www.youtube.com/watch?v=zduSFxRajkE&t=5725s)\n",
    "\n",
    "Byte Tokens and Merge Tokens: A Closer Look\n",
    "--------------------------------------------\n",
    "\n",
    "Byte tokens are a fallback mechanism in tokenization, ensuring that every possible byte (character) is represented in the vocabulary. This section explains the significance of byte tokens and how they are incorporated into the vocabulary, followed by an exploration of merge tokens, which represent parent nodes in the tokenization process.\n",
    "\n",
    "<img src=\"frames/frame_5781.jpg\" alt=\" So here we saw that BYTE fallback in Llama was turned on, so it's true\" width=\"450\"/>\n",
    "\n",
    "Decoding and Understanding the Vocabulary Structure\n",
    "---------------------------------------------------\n",
    "\n",
    "The structure of the vocabulary in SentenceBees is meticulously organized, starting with special tokens, followed by byte tokens, merge tokens, and finally, individual code point tokens. This organization reflects the prioritization of tokens, from the most essential to the rarest. The section also discusses how rare code points are handled, emphasizing the model's approach to maintaining a manageable and efficient vocabulary.\n",
    "\n",
    "[Jump to this part of the video: 01:37:29](https://www.youtube.com/watch?v=zduSFxRajkE&t=5849s)\n",
    "\n",
    "Encoding and Decoding Tokens: Practical Applications\n",
    "----------------------------------------------------\n",
    "\n",
    "Once a vocabulary is established, encoding text into token IDs and decoding them back into text are crucial capabilities of tokenization systems. This part of the guide provides insights into how SentenceBees performs encoding and decoding, illustrating the practical applications of these processes in working with large language models.\n",
    "\n",
    "<img src=\"frames/frame_5865.jpg\" alt=\" Once we have a vocabulary, we can encode into IDs, and we can sort of get a list\" width=\"450\"/>\n",
    "\n",
    "Exploring Token IDs and Unknown Tokens\n",
    "--------------------------------------\n",
    "\n",
    "In our journey through tokenization, we encounter a fascinating aspect when dealing with characters outside the training set, such as Korean characters in this example. These characters are not recognized by the model, leading to the use of unknown tokens (UNK) for representation. However, with byte fallback enabled, the model cleverly resorts to UTF-8 encoding to represent these unfamiliar bytes.\n",
    "\n",
    "<img src=\"frames/frame_5880.jpg\" alt=\" So let's take a look at what happened here. Hello, space, annyeonghaseyo. So these are the token IDs we got back.\" width=\"450\"/>\n",
    "\n",
    "Understanding Byte Fallback Mechanism\n",
    "-------------------------------------\n",
    "\n",
    "Byte fallback is a crucial feature that allows the model to handle characters it hasn't encountered during training. By encoding these characters in UTF-8, the model can still process and represent them, albeit in a different form. This mechanism ensures that even unknown characters can be fed into the model, maintaining the flow of information.\n",
    "\n",
    "[Jump to this part of the video: 01:38:33](https://www.youtube.com/watch?v=zduSFxRajkE&t=5913s)\n",
    "\n",
    "The Impact of Disabling Byte Fallback\n",
    "-------------------------------------\n",
    "\n",
    "Disabling byte fallback leads to a significant change in how the model processes text. Without byte fallback, unknown characters are simply marked as UNK, significantly reducing the model's ability to represent and understand the input. This example highlights the importance of byte fallback in maintaining a rich and nuanced representation of text data.\n",
    "\n",
    "<img src=\"frames/frame_5955.jpg\" alt=\" So the first thing that happened is all the byte tokens disappeared, right?\" width=\"450\"/>\n",
    "\n",
    "Decoding Tokens and Visual Representation\n",
    "-----------------------------------------\n",
    "\n",
    "An interesting observation is made regarding the visual representation of spaces in the decoded tokens. The model appears to convert whitespace into bold underscore characters for visualization purposes. This peculiar choice might be aimed at making the spaces more noticeable during analysis.\n",
    "\n",
    "[Jump to this part of the video: 01:40:15](https://www.youtube.com/watch?v=zduSFxRajkE&t=6015s)\n",
    "\n",
    "Exploring the Quirks of Tokenization with SentencePiece\n",
    "--------------------------------------------------------\n",
    "\n",
    "Tokenization is a pivotal step in preparing text for processing by large language models (LLMs). It involves converting strings of text into manageable units, known as tokens, which the model can interpret. This section delves into the intricacies of tokenization using SentencePiece, highlighting its unique features and potential pitfalls.\n",
    "\n",
    "### The Mystery of the Leading Space in Tokenization\n",
    "\n",
    "One peculiar aspect of SentencePiece's tokenization process is the addition of an extra space at the beginning of tokens. This is controlled by the `Add dummy prefix` option, which is set to true by default.\n",
    "\n",
    "<img src=\"frames/frame_6035.jpg\" alt=\" Why do we have an extra space in the front of hello? Where is this coming from?\" width=\"450\"/>\n",
    "\n",
    "The rationale behind this is to standardize the treatment of words at the beginning of sentences and within sentences. By adding a dummy space, SentencePiece aims to make tokens like \"world\" and \" world\" identical in the eyes of the LLM, facilitating its learning process.\n",
    "\n",
    "[Jump to this part of the video: 01:40:55](https://www.youtube.com/watch?v=zduSFxRajkE&t=6055s)\n",
    "\n",
    "### SentencePiece's Approach to Preprocessing\n",
    "\n",
    "SentencePiece employs several preprocessing options to enhance the tokenization process. One notable feature is its use of a dummy prefix to unify the representation of words, regardless of their position in a sentence. This preprocessing step is crucial for models like Lama 2, which also utilize this option for consistent tokenization.\n",
    "\n",
    "<img src=\"frames/frame_6128.jpg\" alt=\" And that's, I think, everything that I want to say from my preview of sentence piece and how it is different\" width=\"450\"/>\n",
    "\n",
    "### The Intricacies of SentencePiece Configuration\n",
    "\n",
    "The configuration of SentencePiece is intricate, with various settings that influence its behavior. For those looking to replicate the tokenization process of the Meta Lama 2 model, understanding these settings is essential. The raw protocol buffer representation of the tokenizer provides a blueprint for configuring SentencePiece accordingly.\n",
    "\n",
    "[Jump to this part of the video: 01:42:20](https://www.youtube.com/watch?v=zduSFxRajkE&t=6140s)\n",
    "\n",
    "### Summary and Reflections on SentencePiece\n",
    "\n",
    "In summary, SentencePiece is a widely used tool in the industry for its efficiency in both training and inference phases of tokenization. However, it comes with its share of complexities and \"foot guns,\" such as the handling of sentence lengths and the necessity of an UNCTOKEN. Additionally, its documentation leaves much to be desired, posing challenges for users seeking to fully leverage its capabilities.\n",
    "\n",
    "<img src=\"frames/frame_6156.jpg\" alt=\" And yeah, I think that's it for this section\" width=\"450\"/>\n",
    "\n",
    "This exploration of SentencePiece's tokenization process sheds light on the nuanced considerations necessary for effective text processing in LLMs. Despite its quirks, SentencePiece remains a valuable tool for developers and researchers working with language models.\n",
    "\n",
    "Exploring the Complexities of Tokenization in Large Language Models\n",
    "====================================================================\n",
    "\n",
    "### Understanding Tokenization and Its Challenges\n",
    "\n",
    "Tokenization is a fundamental process in the operation of large language models (LLMs), involving the conversion of text data into manageable units known as tokens. This process, while essential, presents various complexities and considerations that significantly impact the performance and functionality of LLMs.\n",
    "\n",
    "### The Importance of a Well-Designed Tokenizer\n",
    "\n",
    "A well-designed tokenizer is crucial for the effective training and operation of LLMs. The choice of tokenizer and the decisions made during its design can have profound implications on the model's ability to understand and generate text.\n",
    "\n",
    "<img src=\"frames/frame_6193.jpg\" alt=\" so it took me a lot of time working with this myself and just visualizing things and trying to really understand what is happening here because the documentation unfortunately is, in my opinion, not super amazing\" width=\"450\"/>\n",
    "\n",
    "### Vocab Size Considerations in Model Architecture\n",
    "\n",
    "When designing the vocab size for an LLM, several factors must be considered to balance computational efficiency with the model's ability to accurately represent and process text data.\n",
    "\n",
    "- **Impact on Embedding Tables and Linear Layers**: As vocab size increases, both the token embedding table and the LM head layer's linear layer expand, requiring more computation and potentially leading to under-trained parameters due to the rarity of token occurrences in training data.\n",
    "\n",
    "[Jump to this part of the video: 01:43:40](https://www.youtube.com/watch?v=zduSFxRajkE&t=6220s)\n",
    "\n",
    "- **Sequence Length and Information Density**: Larger vocab sizes can lead to shorter sequences by compressing more information into each token. While this can enhance the model's ability to attend to larger contexts, it may also result in the model having insufficient capacity to process densely packed information within tokens effectively.\n",
    "\n",
    "### Extending Vocab Size in Pre-Trained Models\n",
    "\n",
    "Extending the vocab size of a pre-trained model involves adding new tokens to the tokenizer and adjusting the model's architecture to accommodate these additions. This process, often referred to as model surgery, requires careful consideration to ensure the new tokens are effectively integrated into the model without compromising its existing capabilities.\n",
    "\n",
    "[Jump to this part of the video: 01:48:15](https://www.youtube.com/watch?v=zduSFxRajkE&t=6495s)\n",
    "\n",
    "### Exploring New Frontiers: Beyond Text Tokenization\n",
    "\n",
    "The versatility of transformers is not limited to processing text; they can be adapted to handle various input modalities, including images, videos, and audio. This adaptability is achieved by tokenizing these different types of data into formats that the transformer can process, opening up new avenues for multimodal applications.\n",
    "\n",
    "[Jump to this part of the video: 01:50:06](https://www.youtube.com/watch?v=zduSFxRajkE&t=6606s)\n",
    "\n",
    "### Addressing Tokenization-Related Challenges in LLMs\n",
    "\n",
    "The process of tokenization, while essential, introduces specific challenges that can affect an LLM's performance, particularly in tasks related to spelling and handling of complex tokens.\n",
    "\n",
    "- **Case Study: Spelling Challenges with Complex Tokens**: An examination of the token \".defaultstyle\" from the GPT-4 vocabulary illustrates how the aggregation of multiple characters into a single token can hinder the model's ability to perform spelling-related tasks accurately.\n",
    "\n",
    "[Jump to this part of the video: 01:51:59](https://www.youtube.com/watch?v=zduSFxRajkE&t=6719s)\n",
    "\n",
    "This exploration of tokenization and its implications for LLMs underscores the importance of thoughtful tokenizer design and the ongoing exploration of techniques to enhance the versatility and effectiveness of these models in processing diverse data types.\n",
    "\n",
    "Exploring Character-Level Tasks with GPT-4\n",
    "------------------------------------------\n",
    "\n",
    "In our journey to understand the intricacies of large language models (LLMs), we encounter various tasks that highlight the importance and challenges of tokenization. A fascinating example involves asking GPT-4 to reverse a string, `.defaultstyle`. Initially, GPT-4 attempts to use a code interpreter, but upon simplification of the task—breaking it down into smaller, more manageable steps—the model successfully reverses the string.\n",
    "\n",
    "<img src=\"frames/frame_6792.jpg\" alt=\" So for example, here I asked GPT-4 to reverse the string .defaultstyle, and it tried to use a code interpreter\" width=\"450\"/>\n",
    "\n",
    "This experiment underscores the hypothesis that tokenization plays a crucial role in how LLMs process and understand tasks, especially when dealing with character-level data.\n",
    "\n",
    "[Jump to this part of the video: 01:53:05](https://www.youtube.com/watch?v=zduSFxRajkE&t=6785s)\n",
    "\n",
    "Challenges in Non-English Language Processing\n",
    "---------------------------------------------\n",
    "\n",
    "LLMs often exhibit reduced performance in non-English languages, a phenomenon partly attributed to the models encountering less non-English data during training. However, a significant factor is the tokenizer's insufficient training on non-English data, leading to inefficiencies in token representation.\n",
    "\n",
    "For instance, the English greeting \"hello, how are you\" translates into significantly more tokens in another language, illustrating the tokenizer's inefficiency and the resultant data bloat in non-English languages.\n",
    "\n",
    "<img src=\"frames/frame_6867.jpg\" alt=\" And I briefly covered this already, but basically it's not only that the language model sees less non-English data during training\" width=\"450\"/>\n",
    "\n",
    "This inefficiency not only affects the model's performance but also its ability to process and understand non-English languages effectively.\n",
    "\n",
    "Tokenization and Arithmetic in LLMs\n",
    "-----------------------------------\n",
    "\n",
    "One of the intriguing aspects of LLMs is their handling of arithmetic operations, which is directly influenced by the tokenization of numbers. The process is far from straightforward, with numbers being tokenized in an arbitrary manner, affecting the model's ability to perform simple arithmetic accurately.\n",
    "\n",
    "<img src=\"frames/frame_6914.jpg\" alt=\" That has to do with the tokenization of numbers\" width=\"450\"/>\n",
    "\n",
    "A detailed exploration of integer tokenization reveals the randomness in how numbers are broken down into tokens, posing a significant challenge for LLMs in arithmetic tasks.\n",
    "\n",
    "[Jump to this part of the video: 01:55:42](https://www.youtube.com/watch?v=zduSFxRajkE&t=6942s)\n",
    "\n",
    "Improving Python Code Understanding in GPT Models\n",
    "--------------------------------------------------\n",
    "\n",
    "The tokenization process also impacts the model's efficiency in understanding and processing Python code. GPT-2, for instance, struggled with encoding spaces in Python code, treating each space as an individual token and severely limiting the model's context length.\n",
    "\n",
    "<img src=\"frames/frame_7025.jpg\" alt=\" the encoding efficiency of the tokenizer for handling spaces in Python is terrible\" width=\"450\"/>\n",
    "\n",
    "This issue, identified as a tokenization bug, was addressed in GPT-4, highlighting the continuous efforts to enhance LLMs' understanding and processing capabilities across different languages and data types.\n",
    "\n",
    "[Jump to this part of the video: 01:14:53](https://www.youtube.com/watch?v=zduSFxRajkE&t=4493s)\n",
    "\n",
    "Exploring Special Tokens and Trailing Whitespace in LLMs\n",
    "========================================================\n",
    "\n",
    "### Handling Special Tokens: A Case Study with \"end of text\"\n",
    "------------------------------------------------------------\n",
    "\n",
    "Large Language Models (LLMs) like GPT-4 exhibit peculiar behaviors when encountering special tokens within user inputs. An intriguing example is the model's response to the string \"end of text.\" Despite explicit instructions to print this string, GPT-4 hesitates, questioning the specificity of the request. This incident highlights a potential parsing issue, where \"end of text\" might be recognized as a special token rather than a sequence of individual characters. Such anomalies suggest that the handling of special tokens by LLMs could serve as an attack surface, offering a method to confuse or manipulate model outputs.\n",
    "\n",
    "<img src=\"frames/frame_7042.jpg\" alt=\" My LLM abruptly halts when it sees the string end of text\" width=\"450\"/>\n",
    "\n",
    "[Jump to this part of the video: 01:57:28](https://www.youtube.com/watch?v=zduSFxRajkE&t=7048s)\n",
    "\n",
    "### The Trailing Whitespace Issue: Impact on Tokenization and Model Performance\n",
    "-------------------------------------------------------------------------------\n",
    "\n",
    "The behavior of LLMs is also significantly influenced by the presence of trailing whitespace in inputs. This is exemplified by an experiment using GPT 3.5 Turbo Instruct, where adding a space after a tagline prompt resulted in a warning about potential performance degradation. The warning sheds light on how LLMs tokenize inputs, revealing that trailing spaces are tokenized separately, potentially disrupting the expected token sequence. This scenario underscores the importance of understanding tokenization nuances, as even seemingly minor details like trailing whitespace can lead to out-of-distribution issues, affecting model predictions and performance.\n",
    "\n",
    "<img src=\"frames/frame_7134.jpg\" alt=\" So if you come to Playground, and we come here to GPT 3.5 Turbo Instruct\" width=\"450\"/>\n",
    "\n",
    "[Jump to this part of the video: 01:59:31](https://www.youtube.com/watch?v=zduSFxRajkE&t=7171s)\n",
    "\n",
    "### Tokenization and Its Quirks: Beyond Characters and Words\n",
    "------------------------------------------------------------\n",
    "\n",
    "The foundational concept of tokenization in LLMs extends beyond simple character or word recognition. Tokens represent chunks of text that the model perceives as its basic units of understanding. However, the tokenization process can introduce peculiarities, such as the unexpected handling of sequences like \"default cell style\" or the misinterpretation of special characters and whitespace. These examples highlight the complexity of tokenization and its critical role in shaping LLM behavior, emphasizing the need for careful consideration of input formatting to ensure optimal model performance.\n",
    "\n",
    "<img src=\"frames/frame_7308.jpg\" alt=\" Let's go back to our default cell style\" width=\"450\"/>\n",
    "\n",
    "[Jump to this part of the video: 02:01:48](https://www.youtube.com/watch?v=zduSFxRajkE&t=7308s)\n",
    "\n",
    "Exploring Unstable Tokens in Language Models\n",
    "--------------------------------------------\n",
    "\n",
    "Language models can sometimes exhibit unexpected and erratic behaviors when encountering certain inputs. This phenomenon is often due to what are termed \"unstable tokens,\" which can lead to a range of issues from the model predicting end-of-text prematurely to flagging inputs as policy violations.\n",
    "\n",
    "<img src=\"frames/frame_7373.jpg\" alt=\" This is giving it brain damage. It's never seen this before. It's shocked, and it's predicting end of text or something.\" width=\"450\"/>\n",
    "\n",
    "### The Challenge of Partial Tokens\n",
    "\n",
    "Partial tokens present a significant challenge in language processing. These occur when the model encounters fragments of tokens or incomplete character sequences that it fails to recognize based on its training data. This unfamiliarity can cause the model to react unpredictably, a situation colloquially described as the model feeling \"jank.\"\n",
    "\n",
    "[Jump to this part of the video: 02:03:09](https://www.youtube.com/watch?v=zduSFxRajkE&t=7389s)\n",
    "\n",
    "### Unstable Tokens: A Deep Dive\n",
    "\n",
    "The concept of unstable tokens is not widely documented, yet it plays a crucial role in understanding model behavior. By examining the codebase of tokenization tools, such as the Tuk token repository, one can find extensive handling for these unstable tokens, indicating their significance in model performance and output reliability.\n",
    "\n",
    "[Jump to this part of the video: 02:03:39](https://www.youtube.com/watch?v=zduSFxRajkE&t=7419s)\n",
    "\n",
    "### Advanced Tokenization Techniques\n",
    "\n",
    "Addressing the issue of unstable tokens requires sophisticated tokenization strategies. Instead of merely appending the next full token after a partial token sequence, a more nuanced approach involves considering a range of potential characters that, upon re-tokenization, would have a high probability of being the correct continuation. This method, however, is complex and highlights the intricate challenges inherent in tokenization.\n",
    "\n",
    "[Jump to this part of the video: 02:04:39](https://www.youtube.com/watch?v=zduSFxRajkE&t=7479s)\n",
    "\n",
    "The Curious Case of Solid Gold Magikarp\n",
    "----------------------------------------\n",
    "\n",
    "Among the most intriguing examples of unstable tokens is the \"solid gold Magikarp,\" a term that has gained notoriety within the language model community. This example illustrates how certain tokens, when clustered based on their embedding representations, can exhibit bizarre and unexpected behaviors.\n",
    "\n",
    "### Unraveling the Mystery of Odd Token Clusters\n",
    "\n",
    "A detailed analysis of token embeddings revealed clusters of seemingly unrelated and peculiar tokens, including the infamous solid gold Magikarp. This discovery raises questions about the origins of these tokens and their impact on model behavior.\n",
    "\n",
    "<img src=\"frames/frame_7500.jpg\" alt=\" My favorite one by far is this solid gold Magikarp.\" width=\"450\"/>\n",
    "\n",
    "### Trigger Words and Model Behavior\n",
    "\n",
    "The presence of specific trigger words, such as solid gold Magikarp, can cause language models to exhibit a wide array of strange responses. These range from evasion and hallucinations to outright insults, highlighting the unpredictable nature of model reactions to certain inputs.\n",
    "\n",
    "[Jump to this part of the video: 02:06:13](https://www.youtube.com/watch?v=zduSFxRajkE&t=7573s)\n",
    "\n",
    "### Investigating the Impact of Unstable Tokens\n",
    "\n",
    "The phenomenon of unstable tokens and their associated trigger words underscores the complexity of language model behavior. It reveals the need for further research and development to mitigate these issues, ensuring models operate within expected safety guidelines and alignment principles.\n",
    "\n",
    "[Jump to this part of the video: 02:07:02](https://www.youtube.com/watch?v=zduSFxRajkE&t=7622s)\n",
    "\n",
    "Exploring the Quirks of Tokenization in Language Models\n",
    "-------------------------------------------------------\n",
    "\n",
    "Tokenization plays a pivotal role in the functionality of large language models (LLMs), often leading to unexpected behaviors due to the nuances in its implementation. This section delves into a specific instance highlighting the importance of understanding tokenization's impact on LLMs.\n",
    "\n",
    "### The Case of the \"sold gold Magikarp\"\n",
    "<img src=\"frames/frame_347.jpg\" alt=\" Well this again comes down to tokenization\" width=\"450\"/>\n",
    "\n",
    "A curious case emerged involving a Reddit user known as \"sold gold Magikarp.\" This example illustrates how discrepancies between the tokenization dataset and the training dataset can lead to unforeseen model behavior. The tokenization process identified \"sold gold Magikarp\" as a frequently occurring string, assigning it a unique token within the model's vocabulary. However, this token was absent from the training data used to refine the model, resulting in an \"untrained\" token that, when invoked, could lead to unpredictable outcomes.\n",
    "\n",
    "[Jump to this part of the video: 02:07:13](https://www.youtube.com/watch?v=zduSFxRajkE&t=7633s)\n",
    "\n",
    "### Untrained Tokens and Undefined Behavior\n",
    "The phenomenon of untrained tokens, akin to unallocated memory in programming, underscores the challenges in managing LLMs' vocabularies. When such tokens are activated, they extract an untrained row from the embedding table, introducing undefined behavior into the model's output. This scenario emphasizes the critical nature of aligning tokenization and training datasets to ensure model reliability.\n",
    "\n",
    "<img src=\"frames/frame_7736.jpg\" alt=\" And then at test time, if you evoke this token, then you're basically plucking out a row of the embedding table that is completely untrained\" width=\"450\"/>\n",
    "\n",
    "### Tokenization Efficiency Across Formats\n",
    "The efficiency of tokenization can vary significantly across different data formats, impacting the model's performance and cost-effectiveness. A comparison between JSON and YAML demonstrates this variability, with YAML proving to be more token-efficient. This insight is valuable for optimizing the processing of structured data, encouraging the selection of formats that minimize token usage and, by extension, computational resources.\n",
    "\n",
    "[Jump to this part of the video: 02:09:37](https://www.youtube.com/watch?v=zduSFxRajkE&t=7777s)\n",
    "\n",
    "### Concluding Thoughts on Tokenization\n",
    "Despite its complexities and potential pitfalls, tokenization remains a crucial stage in the development and operation of LLMs. The challenges it presents, from security issues to AI safety concerns, highlight the importance of thorough understanding and careful management of this process. The pursuit of more efficient and effective tokenization methods continues, with the potential to significantly enhance the performance and applicability of LLMs in various domains.\n",
    "\n",
    "<img src=\"frames/frame_7824.jpg\" alt=\" Okay, so that concludes my fairly long video on tokenization\" width=\"450\"/>\n",
    "\n",
    "### Recommendations for Tokenization Practices\n",
    "For those working with LLMs, leveraging existing tokens and vocabularies, such as those from GPT-4, can offer efficiency benefits. Tools like TIC token provide valuable resources for inference, while custom vocabulary training may benefit from sentence piece BP, depending on specific project requirements.\n",
    "\n",
    "[Jump to this part of the video: 02:11:04](https://www.youtube.com/watch?v=zduSFxRajkE&t=7864s)\n",
    "\n",
    "Challenges with SentencePiece Tokenization\n",
    "------------------------------------------\n",
    "\n",
    "In the realm of tokenization for large language models, SentencePiece is a tool that comes with its own set of challenges. Despite its widespread use, there are several aspects of SentencePiece that can make it less than ideal for certain applications.\n",
    "\n",
    "- **Byte Fallback and Unicode Code Points**: One of the primary concerns with SentencePiece is its reliance on byte fallback and the handling of Unicode code points during the Byte Pair Encoding (BPE) process. This approach can introduce complications, especially when dealing with a diverse set of characters and symbols.\n",
    "\n",
    "<img src=\"frames/frame_7891.jpg\" alt=\" As I mentioned, I'm not a huge fan of sentence piece. I don't like its byte fallback\" width=\"450\"/>\n",
    "\n",
    "- **Complexity and Configuration**: SentencePiece is known for its myriad of settings, which can be both a blessing and a curse. The flexibility allows for fine-tuning, but it also introduces the risk of misconfiguration, leading to potential issues such as inadvertently cropping sentences due to misunderstood parameters.\n",
    "\n",
    "<img src=\"frames/frame_7901.jpg\" alt=\" And I think there's a lot of foot guns here\" width=\"450\"/>\n",
    "\n",
    "- **Recommendations for Use**: For those who decide to use SentencePiece, it's advised to proceed with caution regarding its settings. One strategy is to replicate the configurations used by established projects, such as those by Meta, to minimize the risk of errors. Additionally, spending time to thoroughly understand the available hyperparameters and their implications is crucial for successful implementation.\n",
    "\n",
    "[Jump to this part of the video: 02:11:49](https://www.youtube.com/watch?v=zduSFxRajkE&t=7909s)\n",
    "\n",
    "Exploring Alternatives: The Promise of minBPE\n",
    "---------------------------------------------\n",
    "\n",
    "While SentencePiece has its place in the toolkit of tokenization methods, the search for more efficient and effective alternatives is ongoing. One promising direction is the development of minBPE, a method that aims to combine the benefits of token-based approaches with the efficiency of code training.\n",
    "\n",
    "- **The Ideal of TIC Token with Training Code**: The ultimate goal in tokenization is to achieve a method that combines the granularity of token-based approaches with the adaptability of code training. This combination would offer the best of both worlds, providing a robust solution for tokenization challenges.\n",
    "\n",
    "<img src=\"frames/frame_7946.jpg\" alt=\" And that is the ideal thing that currently does not exist\" width=\"450\"/>\n",
    "\n",
    "- **Current State of minBPE**: As of now, minBPE represents a step towards this ideal, though it is still in the development phase and primarily implemented in Python. The anticipation for minBPE to reach its full potential is high, as it promises to bring significant improvements to the process of tokenization.\n",
    "\n",
    "[Jump to this part of the video: 02:12:10](https://www.youtube.com/watch?v=zduSFxRajkE&t=7930s)\n",
    "\n",
    "Looking Ahead: Future Directions in Tokenization\n",
    "------------------------------------------------\n",
    "\n",
    "The journey of refining tokenization methods is far from over. With ongoing research and development, new approaches like minBPE are on the horizon, offering hope for more efficient and effective solutions. As the field progresses, staying informed and adaptable will be key for those working with large language models.\n",
    "\n",
    "- **Anticipation for Advanced Developments**: The possibility of more detailed and advanced discussions on tokenization in the future is open. As the technology evolves, so too will the strategies for optimizing the tokenization process.\n",
    "\n",
    "[Jump to this part of the video: 02:12:41](https://www.youtube.com/watch?v=zduSFxRajkE&t=7961s)\n",
    "\n",
    "- **Continued Evolution of Context Size in Models**: An interesting development in the field is the increase in context size from earlier models like GPT-1's 512 to GPT-2's 1024. This evolution highlights the ongoing efforts to enhance the capabilities of large language models through improvements in tokenization and model architecture.\n",
    "\n",
    "[Jump to this part of the video: 02:12:58](https://www.youtube.com/watch?v=zduSFxRajkE&t=7978s)\n",
    "\n",
    "In conclusion, while challenges with current tokenization methods like SentencePiece exist, the future holds promise for more refined and efficient approaches. Through careful consideration of settings and an eye towards emerging technologies like minBPE, the field continues to advance towards more effective solutions for processing and understanding language at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ec77e2-7ebb-465d-908f-0ab219c84f08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
